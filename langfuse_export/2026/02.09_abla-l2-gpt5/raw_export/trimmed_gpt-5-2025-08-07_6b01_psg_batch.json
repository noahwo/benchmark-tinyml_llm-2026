{
  "data": [
    {
      "id": "d3a741c1",
      "timestamp": "2026-02-06T11:25:14.299000+00:00",
      "name": "d3a7_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5  # only keep detections with score >= threshold\n\n\n# =========================\n# Utility Functions\n# =========================\ndef ensure_dir_for_file(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\n\ndef load_labels(path):\n    labels = {}\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for i, line in enumerate(f):\n            line = line.strip()\n            if line:\n                labels[i] = line\n    # Determine label offset if label file has '???' at index 0 (common for TF SSD label maps)\n    label_offset = 1 if (0 in labels and labels[0] == \"???\") else 0\n    return labels, label_offset\n\n\ndef get_quant_params(tensor_detail):\n    # Try standard TFLite 'quantization' tuple\n    q = tensor_detail.get('quantization', None)\n    if q is not None and isinstance(q, tuple) and len(q) == 2:\n        scale, zero_point = q\n        if scale is not None and zero_point is not None:\n            return float(scale), int(zero_point)\n    # Try 'quantization_parameters' dict\n    qp = tensor_detail.get('quantization_parameters', None)\n    if qp:\n        scales = qp.get('scales', None)\n        zeros = qp.get('zero_points', None)\n        if scales is not None and len(scales) > 0 and zeros is not None and len(zeros) > 0:\n            return float(scales[0]), int(zeros[0])\n    return None, None\n\n\ndef preprocess_frame(frame_bgr, input_shape, input_dtype, input_scale=None, input_zero_point=None):\n    # input_shape: [1, height, width, 3]\n    h, w = int(input_shape[1]), int(input_shape[2])\n    resized = cv2.resize(frame_bgr, (w, h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    if input_dtype == np.float32:\n        inp = (rgb.astype(np.float32) / 255.0)\n    elif input_dtype == np.uint8:\n        if input_scale is not None and input_zero_point is not None and input_scale > 0:\n            inp = rgb.astype(np.float32) / input_scale + input_zero_point\n            inp = np.clip(np.rint(inp), 0, 255).astype(np.uint8)\n        else:\n            inp = rgb.astype(np.uint8)\n    else:\n        # Fallback to float32 normalization\n        inp = (rgb.astype(np.float32) / 255.0)\n    inp = np.expand_dims(inp, axis=0)\n    return inp\n\n\ndef iou_xyxy(boxA, boxB):\n    # boxes: [x1, y1, x2, y2]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0.0, xB - xA)\n    inter_h = max(0.0, yB - yA)\n    inter = inter_w * inter_h\n    if inter <= 0:\n        return 0.0\n    areaA = max(0.0, (boxA[2] - boxA[0])) * max(0.0, (boxA[3] - boxA[1]))\n    areaB = max(0.0, (boxB[2] - boxB[0])) * max(0.0, (boxB[3] - boxB[1]))\n    union = areaA + areaB - inter\n    if union <= 0:\n        return 0.0\n    return inter / union\n\n\ndef nms_per_class(boxes, scores, class_ids, iou_thresh=0.5):\n    # Perform per-class NMS; return indices to keep\n    indices = np.arange(len(boxes))\n    if len(indices) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    class_ids = np.asarray(class_ids, dtype=np.int32)\n\n    keep_indices = []\n    for c in np.unique(class_ids):\n        c_mask = (class_ids == c)\n        c_inds = indices[c_mask]\n        if len(c_inds) == 0:\n            continue\n        c_boxes = boxes[c_mask]\n        c_scores = scores[c_mask]\n        order = np.argsort(-c_scores)\n        c_inds = c_inds[order]\n        c_boxes = c_boxes[order]\n\n        kept = []\n        while len(c_inds) > 0:\n            i = c_inds[0]\n            kept.append(i)\n            if len(c_inds) == 1:\n                break\n            rest = c_inds[1:]\n            rest_boxes = c_boxes[1:]\n            i_box = c_boxes[0]\n            ious = np.array([iou_xyxy(i_box, rb) for rb in rest_boxes], dtype=np.float32)\n            remain_mask = ious <= iou_thresh\n            c_inds = rest[remain_mask]\n            c_boxes = rest_boxes[remain_mask]\n        keep_indices.extend(kept)\n    return keep_indices\n\n\ndef color_for_class(class_id):\n    # Deterministic color for a class id (BGR)\n    rng = (class_id * 123457) % 0xFFFFFF\n    b = 50 + (rng & 0xFF) % 206\n    g = 50 + ((rng >> 8) & 0xFF) % 206\n    r = 50 + ((rng >> 16) & 0xFF) % 206\n    return (int(b), int(g), int(r))\n\n\ndef compute_ap_11pt(precisions, recalls):\n    # VOC 2007 11-point AP\n    ap = 0.0\n    for t in [i / 10.0 for i in range(11)]:\n        p = 0.0\n        for pr, rc in zip(precisions, recalls):\n            if rc >= t and pr > p:\n                p = pr\n        ap += p\n    return ap / 11.0\n\n\ndef compute_map(predictions_per_class, gt_totals_per_class):\n    # predictions_per_class: dict[class_id] -> list of (score, is_tp)\n    # gt_totals_per_class: dict[class_id] -> int total GT count\n    ap_list = []\n    for c, preds in predictions_per_class.items():\n        gt_total = gt_totals_per_class.get(c, 0)\n        if gt_total <= 0 or len(preds) == 0:\n            continue\n        preds_sorted = sorted(preds, key=lambda x: x[0], reverse=True)\n        tp_cum = 0\n        fp_cum = 0\n        precisions = []\n        recalls = []\n        for score, is_tp in preds_sorted:\n            if is_tp:\n                tp_cum += 1\n            else:\n                fp_cum += 1\n            precisions.append(tp_cum / max(tp_cum + fp_cum, 1))\n            recalls.append(tp_cum / gt_total if gt_total > 0 else 0.0)\n        ap = compute_ap_11pt(precisions, recalls)\n        ap_list.append(ap)\n    if len(ap_list) == 0:\n        return 0.0\n    return float(np.mean(ap_list))\n\n\n# =========================\n# Initialize TFLite Interpreter\n# =========================\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\nin_scale, in_zero = get_quant_params(input_details[0])\n\n# =========================\n# Load labels\n# =========================\nlabels, label_offset = load_labels(label_path)\n\n# =========================\n# Video IO\n# =========================\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Failed to open input video: {}\".format(input_path))\n\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0:\n    fps = 30.0\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nensure_dir_for_file(output_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\nif not writer.isOpened():\n    raise RuntimeError(\"Failed to open output video for writing: {}\".format(output_path))\n\n# =========================\n# mAP (Proxy) Accumulators\n# =========================\n# We estimate AP/mAP using temporal consistency: treat previous frame's detections as pseudo-GT for current frame.\npredictions_per_class = {}  # class_id -> list of (score, is_tp)\ngt_totals_per_class = {}    # class_id -> total pseudo-GT boxes count\nprev_dets_by_class = {}     # class_id -> list of boxes [x1,y1,x2,y2] from previous frame\n\nframe_index = 0\nt_start = time.time()\n\n# =========================\n# Process video frames\n# =========================\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess\n    inp = preprocess_frame(frame, input_shape, input_dtype, in_scale, in_zero)\n\n    # Inference\n    interpreter.set_tensor(input_index, inp)\n    interpreter.invoke()\n\n    # Postprocess: extract outputs (assuming standard SSD order)\n    try:\n        boxes = interpreter.get_tensor(output_details[0]['index'])\n        classes = interpreter.get_tensor(output_details[1]['index'])\n        scores = interpreter.get_tensor(output_details[2]['index'])\n        num_det = interpreter.get_tensor(output_details[3]['index'])\n    except Exception:\n        # Fallback: retrieve all and guess\n        outs = [interpreter.get_tensor(od['index']) for od in output_details]\n        # Identify boxes (has last dim 4)\n        boxes = None\n        classes = None\n        scores = None\n        num_det = None\n        for arr in outs:\n            shp = arr.shape\n            if len(shp) >= 2 and shp[-1] == 4:\n                boxes = arr\n        for arr in outs:\n            shp = arr.shape\n            if len(shp) == 2 and boxes is not None and shp[1] == boxes.shape[1]:\n                # Could be classes or scores; determine by dtype/values\n                if arr.dtype in (np.float32, np.float64) and np.all((arr >= 0) & (arr <= 1)):\n                    scores = arr\n                else:\n                    classes = arr\n            if len(shp) == 1 and shp[0] == 1:\n                num_det = arr\n        if boxes is None or classes is None or scores is None or num_det is None:\n            raise RuntimeError(\"Unable to parse model outputs.\")\n\n    # Remove batch dimension if present\n    if len(boxes.shape) == 3:\n        boxes = boxes[0]\n    if len(classes.shape) >= 2:\n        classes = classes[0]\n    if len(scores.shape) >= 2:\n        scores = scores[0]\n    if hasattr(num_det, \"__len__\"):\n        nd = int(num_det[0]) if len(num_det) > 0 else int(num_det)\n    else:\n        nd = int(num_det)\n\n    # Convert detections to lists\n    detections_xyxy = []\n    detection_scores = []\n    detection_class_ids = []\n\n    for i in range(nd):\n        score = float(scores[i])\n        if score < confidence_threshold:\n            continue\n        cls_raw = int(classes[i])\n        label_id = cls_raw + label_offset\n        # clamp label id if out of range\n        if label_id not in labels:\n            # fallback to raw if offset overflows\n            label_id = cls_raw if cls_raw in labels else cls_raw\n\n        y_min, x_min, y_max, x_max = boxes[i]\n        # Coordinates are normalized [0,1]\n        x1 = int(max(0, min(1, x_min)) * width)\n        y1 = int(max(0, min(1, y_min)) * height)\n        x2 = int(max(0, min(1, x_max)) * width)\n        y2 = int(max(0, min(1, y_max)) * height)\n        # Ensure proper ordering\n        x1, x2 = (x1, x2) if x1 <= x2 else (x2, x1)\n        y1, y2 = (y1, y2) if y1 <= y2 else (y2, y1)\n        # Skip degenerate boxes\n        if x2 - x1 <= 1 or y2 - y1 <= 1:\n            continue\n\n        detections_xyxy.append([x1, y1, x2, y2])\n        detection_scores.append(score)\n        detection_class_ids.append(label_id)\n\n    # Apply per-class NMS\n    keep = nms_per_class(detections_xyxy, detection_scores, detection_class_ids, iou_thresh=0.5)\n    detections_xyxy = [detections_xyxy[i] for i in keep]\n    detection_scores = [detection_scores[i] for i in keep]\n    detection_class_ids = [detection_class_ids[i] for i in keep]\n\n    # Draw detections\n    for box, sc, cid in zip(detections_xyxy, detection_scores, detection_class_ids):\n        x1, y1, x2, y2 = box\n        color = color_for_class(cid)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        label_name = labels.get(cid, str(cid))\n        text = \"{}: {:.2f}\".format(label_name, sc)\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - 6), (x1 + tw + 4, y1), color, -1)\n        cv2.putText(frame, text, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Update proxy mAP stats using temporal matching (previous frame as pseudo-GT)\n    curr_by_class = {}\n    for box, sc, cid in zip(detections_xyxy, detection_scores, detection_class_ids):\n        curr_by_class.setdefault(cid, []).append((box, sc))\n\n    if frame_index > 0:\n        # For each class, match current preds to previous frame boxes (pseudo-GT) via IoU > 0.5\n        for cid in set(list(prev_dets_by_class.keys()) + list(curr_by_class.keys())):\n            prev_boxes = prev_dets_by_class.get(cid, [])\n            gt_totals_per_class[cid] = gt_totals_per_class.get(cid, 0) + len(prev_boxes)\n            curr_list = curr_by_class.get(cid, [])\n            matched_prev = [False] * len(prev_boxes)\n\n            # Sort current detections by score descending to simulate standard matching\n            curr_list_sorted = sorted(curr_list, key=lambda x: x[1], reverse=True)\n\n            for curr_box, curr_score in curr_list_sorted:\n                # find best match\n                best_iou = 0.0\n                best_j = -1\n                for j, gt_box in enumerate(prev_boxes):\n                    if matched_prev[j]:\n                        continue\n                    iou = iou_xyxy(curr_box, gt_box)\n                    if iou > best_iou:\n                        best_iou = iou\n                        best_j = j\n                is_tp = False\n                if best_j >= 0 and best_iou >= 0.5:\n                    matched_prev[best_j] = True\n                    is_tp = True\n                predictions_per_class.setdefault(cid, []).append((curr_score, is_tp))\n    # Set current as previous for next iteration\n    prev_dets_by_class = {cid: [b for (b, s) in curr_by_class.get(cid, [])] for cid in curr_by_class.keys()}\n\n    # Compute running mAP (proxy) and overlay\n    running_map = compute_map(predictions_per_class, gt_totals_per_class)\n    map_text = \"mAP (proxy): {:.3f}\".format(running_map)\n    cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # Write frame\n    writer.write(frame)\n    frame_index += 1\n\n# Cleanup\ncap.release()\nwriter.release()\n\n# Final mAP (proxy)\nfinal_map = compute_map(predictions_per_class, gt_totals_per_class)\n\n# Print summary\nelapsed = time.time() - t_start\nprint(\"Processed {} frames in {:.2f}s ({:.2f} FPS).\".format(frame_index, elapsed, frame_index / max(elapsed, 1e-6)))\nprint(\"Output saved to:\", output_path)\nprint(\"Final mAP (proxy): {:.4f}\".format(final_map))"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 153.532,
      "total_cost": 0.08942625,
      "observations": [
        {
          "id": "time-13-25-14-540840_chatcmpl-D6EFX4AMo3eHulDOHz96a7s2IxLGB",
          "traceId": "d3a741c1",
          "type": "GENERATION",
          "name": "d3a7_psg_gen_attempt#1",
          "startTime": "2026-02-06T11:25:14.540000+00:00",
          "endTime": "2026-02-06T11:27:48.072000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08942625,
          "latency": 153532.0,
          "costDetails": {
            "total": 0.08942625
          },
          "totalTokens": 9304,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 8891,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:25:14.829Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:28:00.029Z"
    },
    {
      "id": "8849b22b",
      "timestamp": "2026-02-06T11:22:51.567000+00:00",
      "name": "8849_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom ai_edge_litert.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(path):\n    labels = []\n    try:\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name:\n                    labels.append(name)\n    except Exception:\n        pass\n    if not labels:\n        labels = [\"unknown\"]\n    return labels\n\ndef preprocess(frame, input_size, input_dtype):\n    h, w = input_size\n    img = cv2.resize(frame, (w, h))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    if input_dtype == np.float32:\n        img = img.astype(np.float32) / 255.0\n    else:\n        img = img.astype(np.uint8)\n    img = np.expand_dims(img, axis=0)\n    return img\n\ndef select_output_tensors(interpreter):\n    out_details = interpreter.get_output_details()\n    outputs = [interpreter.get_tensor(d['index']) for d in out_details]\n\n    boxes = None\n    classes = None\n    scores = None\n    num_dets = None\n\n    for arr in outputs:\n        if arr.ndim == 3 and arr.shape[0] == 1 and arr.shape[2] == 4:\n            boxes = arr\n    for arr in outputs:\n        if arr.size == 1:\n            num_dets = arr\n    # Scores typically in [0,1]\n    for arr in outputs:\n        if arr.ndim == 2 and arr.shape[0] == 1 and arr.dtype == np.float32:\n            maxv = float(np.max(arr)) if arr.size > 0 else 0.0\n            if maxv <= 1.00001:\n                scores = arr\n    # Classes typically > 1.0 floats (indices)\n    for arr in outputs:\n        if arr.ndim == 2 and arr.shape[0] == 1 and arr.dtype == np.float32:\n            maxv = float(np.max(arr)) if arr.size > 0 else 0.0\n            if maxv > 1.0:\n                classes = arr\n\n    return boxes, classes, scores, num_dets\n\ndef draw_labelled_box(img, x1, y1, x2, y2, label, score, color=(0, 255, 0)):\n    h, w = img.shape[:2]\n    x1 = max(0, min(x1, w - 1))\n    y1 = max(0, min(y1, h - 1))\n    x2 = max(0, min(x2, w - 1))\n    y2 = max(0, min(y2, h - 1))\n    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n    caption = f\"{label}: {score:.2f}\"\n    (tw, th), _ = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n    cv2.rectangle(img, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), color, -1)\n    cv2.putText(img, caption, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\ndef compute_ap(scores, tps, fps, num_gt):\n    if num_gt <= 0 or len(scores) == 0:\n        return 0.0\n    scores = np.asarray(scores, dtype=np.float32)\n    tps = np.asarray(tps, dtype=np.float32)\n    fps = np.asarray(fps, dtype=np.float32)\n    order = np.argsort(-scores)\n    tps = tps[order]\n    fps = fps[order]\n    cum_tp = np.cumsum(tps)\n    cum_fp = np.cumsum(fps)\n    rec = cum_tp / float(num_gt)\n    prec = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n    mrec = np.concatenate(([0.0], rec, [1.0]))\n    mpre = np.concatenate(([0.0], prec, [0.0]))\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n    idx = np.where(mrec[1:] != mrec[:-1])[0] + 1\n    ap = float(np.sum((mrec[idx] - mrec[idx - 1]) * mpre[idx]))\n    return ap\n\ndef overlay_metrics(frame, text, pos=(10, 30), color=(255, 255, 255), bg=(0, 0, 0)):\n    (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n    x, y = pos\n    cv2.rectangle(frame, (x - 4, y - th - 6), (x + tw + 4, y + 6), bg, -1)\n    cv2.putText(frame, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n\ndef main():\n    labels = load_labels(label_path)\n\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    in_details = interpreter.get_input_details()\n    out_details = interpreter.get_output_details()\n\n    input_index = in_details[0]['index']\n    input_h, input_w = in_details[0]['shape'][1], in_details[0]['shape'][2]\n    input_dtype = in_details[0]['dtype']\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"Error: Unable to open input video:\", input_path)\n        return\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) or 640\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 480\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        print(\"Error: Unable to open output writer:\", output_path)\n        cap.release()\n        return\n\n    # Metrics accumulators (heuristic, without external GT)\n    # For each class id: track scores, TP/FP flags, and approximated num_gt (at most 1 per frame per class)\n    metrics = {}\n    frame_count = 0\n    t_start = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        input_tensor = preprocess(frame, (input_h, input_w), input_dtype)\n        interpreter.set_tensor(input_index, input_tensor)\n        interpreter.invoke()\n\n        # Retrieve outputs and identify tensors\n        boxes, classes, scores, num_dets = select_output_tensors(interpreter)\n        if boxes is None or classes is None or scores is None:\n            # Fallback: try to read via out_details indices if select failed\n            outs = [interpreter.get_tensor(d['index']) for d in out_details]\n            if len(outs) >= 3:\n                boxes, classes, scores = outs[0], outs[1], outs[2]\n            else:\n                writer.write(frame)\n                continue\n\n        boxes = np.squeeze(boxes, axis=0)  # [N,4] normalized\n        classes = np.squeeze(classes, axis=0)  # [N]\n        scores = np.squeeze(scores, axis=0)  # [N]\n        if num_dets is not None:\n            n = int(np.squeeze(num_dets))\n            boxes = boxes[:n]\n            classes = classes[:n]\n            scores = scores[:n]\n\n        # Filter detections by confidence threshold\n        keep = scores >= confidence_threshold\n        boxes_f = boxes[keep]\n        classes_f = classes[keep]\n        scores_f = scores[keep]\n\n        # Group detections by class to update heuristic mAP metrics\n        per_class_indices = {}\n        for i, cid in enumerate(classes_f):\n            c = int(cid)\n            per_class_indices.setdefault(c, []).append(i)\n\n        for c, idxs in per_class_indices.items():\n            # Ensure data structure\n            if c not in metrics:\n                metrics[c] = {\"scores\": [], \"tp\": [], \"fp\": [], \"num_gt\": 0}\n            # Sort detections of this class in current frame by score\n            order = sorted(idxs, key=lambda k: float(scores_f[k]), reverse=True)\n            if len(order) > 0:\n                # Heuristic: assume at most one true object per class per frame\n                metrics[c][\"num_gt\"] += 1\n                # First is TP, rest are FP\n                for j, k in enumerate(order):\n                    sc = float(scores_f[k])\n                    if j == 0:\n                        metrics[c][\"scores\"].append(sc)\n                        metrics[c][\"tp\"].append(1.0)\n                        metrics[c][\"fp\"].append(0.0)\n                    else:\n                        metrics[c][\"scores\"].append(sc)\n                        metrics[c][\"tp\"].append(0.0)\n                        metrics[c][\"fp\"].append(1.0)\n\n        # Draw detections\n        for i in range(len(boxes_f)):\n            y1, x1, y2, x2 = boxes_f[i]\n            x1p = int(x1 * width)\n            y1p = int(y1 * height)\n            x2p = int(x2 * width)\n            y2p = int(y2 * height)\n            cid = int(classes_f[i])\n            label = labels[cid] if 0 <= cid < len(labels) else f\"id_{cid}\"\n            draw_labelled_box(frame, x1p, y1p, x2p, y2p, label, float(scores_f[i]))\n\n        # Compute mAP (heuristic) across classes that have any approximated GT so far\n        aps = []\n        for c, data in metrics.items():\n            ap_c = compute_ap(data[\"scores\"], data[\"tp\"], data[\"fp\"], data[\"num_gt\"])\n            aps.append(ap_c)\n        mAP = float(np.mean(aps)) if len(aps) > 0 else 0.0\n\n        overlay_metrics(frame, f\"mAP (heuristic): {mAP:.3f}\", pos=(10, 30), color=(255, 255, 255), bg=(0, 0, 0))\n\n        writer.write(frame)\n\n    elapsed = time.time() - t_start\n    cap.release()\n    writer.release()\n\n    # Final reporting\n    final_aps = []\n    for c, data in metrics.items():\n        ap_c = compute_ap(data[\"scores\"], data[\"tp\"], data[\"fp\"], data[\"num_gt\"])\n        final_aps.append(ap_c)\n    final_mAP = float(np.mean(final_aps)) if len(final_aps) > 0 else 0.0\n\n    print(f\"Processed {frame_count} frames in {elapsed:.2f}s ({(frame_count / max(elapsed,1e-6)):.2f} FPS).\")\n    print(f\"Saved annotated video with detections and mAP overlay to: {output_path}\")\n    print(f\"Final mAP (heuristic, no external ground truth): {final_mAP:.4f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 110.017,
      "total_cost": 0.07044625,
      "observations": [
        {
          "id": "time-13-22-52-207948_chatcmpl-D6EDE9B1NdA6d4xqvby19zwFvYr8J",
          "traceId": "8849b22b",
          "type": "GENERATION",
          "name": "8849_psg_gen_attempt#1",
          "startTime": "2026-02-06T11:22:52.207000+00:00",
          "endTime": "2026-02-06T11:24:42.224000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07044625,
          "latency": 110017.0,
          "costDetails": {
            "total": 0.07044625
          },
          "totalTokens": 7406,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 6993,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:22:52.131Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:24:53.767Z"
    },
    {
      "id": "c1924f44",
      "timestamp": "2026-02-06T11:20:43.565000+00:00",
      "name": "c192_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# CONFIGURATION PARAMETERS\n# =========================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"  # Read a single video file from the given input_path\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Output video with rectangles, labels, and mAP\nCONFIDENCE_THRESHOLD = 0.5\n\n\ndef load_labels(label_path):\n    \"\"\"\n    Load labels from a label map file.\n    Supports simple line-by-line label files.\n    Returns a list where index corresponds to class id.\n    \"\"\"\n    labels = []\n    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Handle possible \"id: x, name: y\" formats crudely by taking last token if colon exists\n            if \":\" in line and \",\" in line:\n                # Try to extract name after 'name:'\n                parts = [p.strip() for p in line.split(\",\")]\n                name = None\n                for p in parts:\n                    if \"name\" in p:\n                        kv = p.split(\":\")\n                        if len(kv) >= 2:\n                            name = kv[1].strip().strip('\"').strip(\"'\")\n                            break\n                labels.append(name if name else line)\n            elif \":\" in line:\n                # id:name\n                labels.append(line.split(\":\", 1)[1].strip())\n            else:\n                # Plain label per line\n                labels.append(line)\n    return labels\n\n\ndef id_to_label(cid, labels):\n    \"\"\"\n    Map class id to label string using best-effort rules.\n    Many TFLite SSD models output 0-based or 1-based class ids depending on label map.\n    \"\"\"\n    if 0 <= cid < len(labels):\n        return labels[cid]\n    # Try 1-based shift if file includes a leading \"???\" or similar\n    if 0 <= cid + 1 < len(labels):\n        return labels[cid + 1]\n    if 0 <= cid - 1 < len(labels):\n        return labels[cid - 1]\n    return f\"id_{cid}\"\n\n\ndef ensure_dir_for_file(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\n\ndef preprocess_frame(frame_bgr, input_shape, input_details):\n    \"\"\"\n    Preprocess frame for TFLite SSD model.\n    - Resize to model input size\n    - Convert BGR -> RGB\n    - Handle dtype/quantization as required\n    Returns a numpy array with shape [1, H, W, 3]\n    \"\"\"\n    in_h, in_w = input_shape[1], input_shape[2]  # [1, H, W, 3]\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n\n    inp_dtype = input_details[0][\"dtype\"]\n    quant_params = input_details[0].get(\"quantization\", (0.0, 0))\n    scale, zero_point = quant_params if isinstance(quant_params, (list, tuple)) and len(quant_params) == 2 else (0.0, 0)\n\n    if inp_dtype == np.float32:\n        inp = resized.astype(np.float32) / 255.0\n    elif inp_dtype == np.uint8:\n        if scale and scale > 0:\n            # Quantize to uint8 using provided scale and zero_point\n            inp = np.round(resized.astype(np.float32) / scale + zero_point)\n            inp = np.clip(inp, 0, 255).astype(np.uint8)\n        else:\n            inp = resized.astype(np.uint8)\n    else:\n        # Fallback: try to cast appropriately\n        inp = resized.astype(inp_dtype)\n\n    return np.expand_dims(inp, axis=0)\n\n\ndef draw_detection(frame, box, score, label, color):\n    \"\"\"\n    Draw a single detection on the frame.\n    box: [ymin, xmin, ymax, xmax] in absolute pixel coords\n    \"\"\"\n    y_min, x_min, y_max, x_max = box\n    cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, 2)\n    caption = f\"{label}: {score:.2f}\"\n    # Put text background\n    (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n    cv2.rectangle(frame, (x_min, max(0, y_min - th - 4)), (x_min + tw + 2, y_min), color, -1)\n    cv2.putText(frame, caption, (x_min + 1, y_min - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n\ndef class_color(class_id):\n    # Deterministic pseudo-color for each class id\n    return (int((37 * class_id) % 255), int((17 * class_id) % 255), int((29 * class_id) % 255))\n\n\ndef compute_map_proxy(class_conf_history):\n    \"\"\"\n    Compute a proxy for mAP in absence of ground truth:\n    - For each class with any detections, AP_proxy = mean of detection confidences observed.\n    - mAP_proxy = mean(AP_proxy over classes with detections).\n    Returns float in [0, 1].\n    \"\"\"\n    if not class_conf_history:\n        return 0.0\n    ap_vals = []\n    for confs in class_conf_history.values():\n        if len(confs) > 0:\n            ap_vals.append(float(np.mean(confs)))\n    if not ap_vals:\n        return 0.0\n    return float(np.mean(ap_vals))\n\n\ndef main():\n    # 1. Setup: Load TFLite interpreter, labels, and video input\n    labels = load_labels(LABEL_PATH)\n\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Determine input tensor shape [1, H, W, 3]\n    input_shape = input_details[0][\"shape\"]\n\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"ERROR: Could not open input video: {INPUT_PATH}\")\n        return\n\n    # Prepare output video writer\n    ensure_dir_for_file(OUTPUT_PATH)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0:\n        fps = 25.0\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        print(f\"ERROR: Could not open output video for write: {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # Stats for mAP proxy\n    class_conf_history = {}  # class_id -> list of confidences\n    frame_count = 0\n    t0 = time.time()\n\n    # 2-3. Processing loop: Preprocess each frame and run inference\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # Preprocess\n        input_tensor = preprocess_frame(frame, input_shape, input_details)\n\n        # Inference\n        interpreter.set_tensor(input_details[0][\"index\"], input_tensor)\n        interpreter.invoke()\n\n        # 4. Output handling\n        # Typical SSD Mobilenet v1 TFLite detection output order:\n        # 0: boxes [1, num, 4] (ymin, xmin, ymax, xmax) normalized\n        # 1: classes [1, num]\n        # 2: scores [1, num]\n        # 3: num_detections [1]\n        try:\n            boxes = interpreter.get_tensor(output_details[0][\"index\"])[0]\n            classes = interpreter.get_tensor(output_details[1][\"index\"])[0].astype(np.int32)\n            scores = interpreter.get_tensor(output_details[2][\"index\"])[0]\n            num_detections = int(interpreter.get_tensor(output_details[3][\"index\"])[0])\n        except Exception:\n            # Fallback: try to locate tensors by shape\n            outs = [interpreter.get_tensor(od[\"index\"]) for od in output_details]\n            # Identify likely tensors by shapes\n            boxes, classes, scores, num_detections = None, None, None, None\n            for arr in outs:\n                s = arr.shape\n                if len(s) == 3 and s[-1] == 4:\n                    boxes = arr[0]\n                elif len(s) == 2 and s[0] == 1 and s[1] > 1 and (arr.dtype == np.float32 or arr.dtype == np.uint8):\n                    # could be classes or scores; differentiate by values range\n                    if np.all((arr >= 0) & (arr <= 1.0 + 1e-6)):\n                        scores = arr[0]\n                    else:\n                        classes = arr[0].astype(np.int32)\n                elif len(s) == 1 and s[0] == 1:\n                    num_detections = int(arr[0])\n            if boxes is None or classes is None or scores is None or num_detections is None:\n                print(\"ERROR: Unable to parse model outputs for detection.\")\n                break\n\n        # Draw detections and update stats\n        for i in range(num_detections):\n            score = float(scores[i])\n            if score < CONFIDENCE_THRESHOLD:\n                continue\n            class_id = int(classes[i])\n\n            # Convert box to pixel coords\n            ymin = int(max(0, min(height - 1, boxes[i][0] * height)))\n            xmin = int(max(0, min(width - 1, boxes[i][1] * width)))\n            ymax = int(max(0, min(height - 1, boxes[i][2] * height)))\n            xmax = int(max(0, min(width - 1, boxes[i][3] * width)))\n\n            # Fix inverted boxes if any\n            if xmax < xmin:\n                xmin, xmax = xmax, xmin\n            if ymax < ymin:\n                ymin, ymax = ymax, ymin\n\n            label = id_to_label(class_id, labels)\n            color = class_color(class_id)\n            draw_detection(frame, (ymin, xmin, ymax, xmax), score, label, color)\n\n            # Update proxy stats\n            if class_id not in class_conf_history:\n                class_conf_history[class_id] = []\n            class_conf_history[class_id].append(score)\n\n        # Compute and overlay mAP (proxy) on the frame\n        map_proxy = compute_map_proxy(class_conf_history)\n        cv2.putText(frame, f\"mAP: {map_proxy:.3f}\", (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n\n        # Write frame\n        writer.write(frame)\n\n    # Cleanup\n    cap.release()\n    writer.release()\n    elapsed = time.time() - t0\n    final_map_proxy = compute_map_proxy(class_conf_history)\n    print(f\"Processed {frame_count} frames in {elapsed:.2f}s ({(frame_count / max(elapsed, 1e-6)):.2f} FPS).\")\n    print(f\"Final mAP: {final_map_proxy:.4f}\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 95.674,
      "total_cost": 0.07084625,
      "observations": [
        {
          "id": "time-13-20-44-203079_chatcmpl-D6EBAvExfvh1IZ2Cl74R2sqDgOHHe",
          "traceId": "c1924f44",
          "type": "GENERATION",
          "name": "c192_psg_gen_attempt#1",
          "startTime": "2026-02-06T11:20:44.203000+00:00",
          "endTime": "2026-02-06T11:22:19.877000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07084625,
          "latency": 95674.0,
          "costDetails": {
            "total": 0.07084625
          },
          "totalTokens": 7446,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 7033,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:20:44.140Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:22:31.351Z"
    },
    {
      "id": "39e5d8de",
      "timestamp": "2026-02-06T11:17:31.616000+00:00",
      "name": "39e5_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration parameters\n# =========================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"  # Read a single video file from the given input_path\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Output with rectangles, labels, and mAP text overlay\nCONFIDENCE_THRESHOLD = 0.5\n\n# =========================\n# Utilities\n# =========================\ndef load_labels(path):\n    labels = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            labels.append(line)\n    return labels\n\ndef get_label_text(labels, class_id):\n    # Handle common label map conventions\n    ci = int(class_id)\n    if 0 <= ci < len(labels):\n        # Typical label files for TFLite SSD start with \"???\" at index 0 (background)\n        # and classes are 0-based. This handles that case directly.\n        txt = labels[ci]\n        if txt == \"???\":  # if background appears, fall back to id text\n            return f\"id:{ci}\"\n        return txt\n    # Fallback for 1-based label maps\n    if 1 <= ci <= len(labels):\n        return labels[ci - 1]\n    return f\"id:{ci}\"\n\ndef ensure_dir_for_file(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\ndef preprocess_frame_bgr_to_input(frame_bgr, input_shape, input_dtype, input_quant):\n    # input_shape: [1, h, w, 3]\n    _, in_h, in_w, _ = input_shape\n    # Convert BGR (OpenCV) to RGB and resize to model input size\n    resized = cv2.resize(frame_bgr, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n    if input_dtype == np.float32:\n        # Normalize to [0,1] for typical SSD Mobilenet v1 float models\n        tensor = rgb.astype(np.float32) / 255.0\n    else:\n        # uint8 path\n        # For quantized models, TFLite typically expects uint8 [0,255].\n        tensor = rgb.astype(np.uint8)\n\n        # If you need explicit quantization to uint8 (rarely necessary here),\n        # you could apply: (float_tensor / scale + zero_point), but most models\n        # expect raw uint8 image; keep it simple and correct for common cases.\n        # scale, zero_point = input_quant\n        # if scale and scale > 0:\n        #     tensor = np.clip(np.round(tensor / scale + zero_point), 0, 255).astype(np.uint8)\n\n    # Add batch dimension\n    tensor = np.expand_dims(tensor, axis=0)\n    return tensor\n\ndef parse_tflite_outputs(interpreter):\n    # Retrieve all output tensors and identify boxes, classes, scores, and num_detections\n    outputs = []\n    out_details = interpreter.get_output_details()\n    for od in out_details:\n        arr = interpreter.get_tensor(od['index'])\n        outputs.append((od, arr))\n\n    boxes = None\n    classes = None\n    scores = None\n    num_detections = None\n\n    # First pass by name hints if available\n    for od, arr in outputs:\n        name = od.get('name', '')\n        shp = arr.shape\n        if 'boxes' in name and arr.ndim == 3 and shp[-1] == 4:\n            boxes = arr\n        elif 'classes' in name and arr.ndim == 2:\n            classes = arr\n        elif 'scores' in name and arr.ndim == 2:\n            scores = arr\n        elif 'num_detections' in name and arr.size == 1:\n            num_detections = int(np.squeeze(arr).astype(np.int32))\n\n    # Fallback by shapes and value ranges\n    if boxes is None or classes is None or scores is None or num_detections is None:\n        # Identify boxes: [1, N, 4]\n        for _, arr in outputs:\n            if arr.ndim == 3 and arr.shape[0] == 1 and arr.shape[2] == 4:\n                boxes = arr\n        # Identify num_detections: scalar [1] or shape () after squeeze\n        for _, arr in outputs:\n            if arr.size == 1:\n                num_detections = int(np.squeeze(arr).astype(np.int32))\n        # Identify classes and scores among [1, N]\n        two_d = [arr for _, arr in outputs if arr.ndim == 2 and arr.shape[0] == 1]\n        # If we have exactly two candidates\n        if len(two_d) >= 2:\n            # Scores are in [0,1], classes are class indices (float)\n            # Choose the one with values mostly between 0 and 1 as scores\n            cand_a, cand_b = two_d[0], two_d[1]\n            a_max, b_max = float(np.max(cand_a)), float(np.max(cand_b))\n            a_min, b_min = float(np.min(cand_a)), float(np.min(cand_b))\n            def likely_scores(lo, hi):\n                return (lo >= 0.0) and (hi <= 1.0 + 1e-4)\n            if likely_scores(a_min, a_max) and not likely_scores(b_min, b_max):\n                scores, classes = cand_a, cand_b\n            elif likely_scores(b_min, b_max) and not likely_scores(a_min, a_max):\n                scores, classes = cand_b, cand_a\n            else:\n                # Ambiguous; assume first is scores for typical TFLite SSD exports\n                scores, classes = cand_a, cand_b\n\n    # Final sanity\n    if boxes is None or classes is None or scores is None:\n        raise RuntimeError(\"Unable to parse TFLite SSD outputs (boxes/classes/scores not found).\")\n\n    if num_detections is None:\n        # Some models omit num_detections; infer from array length\n        num_detections = boxes.shape[1]\n\n    # Squeeze batch dimension\n    boxes = np.squeeze(boxes, axis=0)\n    classes = np.squeeze(classes, axis=0)\n    scores = np.squeeze(scores, axis=0)\n\n    # Truncate to num_detections if needed\n    boxes = boxes[:num_detections]\n    classes = classes[:num_detections]\n    scores = scores[:num_detections]\n\n    return boxes, classes, scores, num_detections\n\ndef draw_detections_on_frame(frame_bgr, detections, map_text):\n    # detections: list of dicts with keys: bbox (ymin, xmin, ymax, xmax) normalized,\n    # score, class_id, label\n    h, w = frame_bgr.shape[:2]\n\n    # Draw mAP text (top-left)\n    cv2.rectangle(frame_bgr, (5, 5), (260, 35), (0, 0, 0), thickness=-1)\n    cv2.putText(frame_bgr, map_text, (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2, cv2.LINE_AA)\n\n    for det in detections:\n        ymin, xmin, ymax, xmax = det[\"bbox\"]\n        # Scale to pixel coordinates\n        x1 = int(max(0, min(w - 1, xmin * w)))\n        y1 = int(max(0, min(h - 1, ymin * h)))\n        x2 = int(max(0, min(w - 1, xmax * w)))\n        y2 = int(max(0, min(h - 1, ymax * h)))\n\n        # Box\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        # Label text\n        label = det[\"label\"]\n        score = det[\"score\"]\n        text = f\"{label}: {score:.2f}\"\n\n        # Text background\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n        ty1 = max(0, y1 - th - 6)\n        cv2.rectangle(frame_bgr, (x1, ty1), (x1 + tw + 4, ty1 + th + 6), (0, 0, 0), -1)\n        cv2.putText(frame_bgr, text, (x1 + 2, ty1 + th + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2, cv2.LINE_AA)\n\n    return frame_bgr\n\ndef calculate_map_placeholder():\n    # Proper mAP requires ground-truth annotations.\n    # Since no ground-truth is provided, we cannot compute a valid mAP.\n    # Return a display string that clearly indicates unavailability.\n    return \"mAP: N/A (no GT)\"\n\n# =========================\n# Main pipeline\n# =========================\ndef main():\n    # Setup and interpreter\n    if not os.path.exists(MODEL_PATH):\n        raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n    if not os.path.exists(LABEL_PATH):\n        raise FileNotFoundError(f\"Label file not found: {LABEL_PATH}\")\n    if not os.path.exists(INPUT_PATH):\n        raise FileNotFoundError(f\"Input video not found: {INPUT_PATH}\")\n\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH, num_threads=4)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    if not input_details:\n        raise RuntimeError(\"Interpreter has no input tensors.\")\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']  # [1, h, w, 3]\n    input_dtype = input_details[0]['dtype']\n    input_quant = input_details[0].get('quantization', (0.0, 0))\n\n    # Video IO setup\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps_in = cap.get(cv2.CAP_PROP_FPS)\n    if not fps_in or fps_in <= 1e-3:\n        fps_in = 30.0  # fallback\n\n    ensure_dir_for_file(OUTPUT_PATH)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps_in, (frame_w, frame_h))\n    if not writer.isOpened():\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    # Placeholder mAP text (no ground-truth available in configuration)\n    map_text = calculate_map_placeholder()\n\n    # Processing loop\n    frame_count = 0\n    t0 = time.time()\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # Preprocess\n        input_tensor = preprocess_frame_bgr_to_input(frame_bgr, input_shape, input_dtype, input_quant)\n\n        # Set input tensor\n        interpreter.set_tensor(input_index, input_tensor)\n\n        # Inference\n        interpreter.invoke()\n\n        # Parse outputs\n        boxes, classes, scores, num = parse_tflite_outputs(interpreter)\n\n        # Collect detections above threshold\n        detections = []\n        for i in range(len(scores)):\n            score = float(scores[i])\n            if score < CONFIDENCE_THRESHOLD:\n                continue\n            class_id = int(classes[i])\n            label = get_label_text(labels, class_id)\n            ymin, xmin, ymax, xmax = boxes[i].tolist()  # normalized coordinates\n            det = {\n                \"bbox\": (ymin, xmin, ymax, xmax),\n                \"score\": score,\n                \"class_id\": class_id,\n                \"label\": label\n            }\n            detections.append(det)\n\n        # Draw detections and mAP text\n        annotated = draw_detections_on_frame(frame_bgr, detections, map_text)\n\n        # Write frame\n        writer.write(annotated)\n\n    # Cleanup\n    cap.release()\n    writer.release()\n    elapsed = time.time() - t0\n    if frame_count > 0:\n        print(f\"Processed {frame_count} frames in {elapsed:.2f}s ({frame_count / max(elapsed,1e-6):.2f} FPS).\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 160.544,
      "total_cost": 0.07219625,
      "observations": [
        {
          "id": "time-13-17-32-259566_chatcmpl-D6E85oNeZdkDC9rKx9cLg5YKKqK9O",
          "traceId": "39e5d8de",
          "type": "GENERATION",
          "name": "39e5_psg_gen_attempt#1",
          "startTime": "2026-02-06T11:17:32.259000+00:00",
          "endTime": "2026-02-06T11:20:12.803000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07219625,
          "latency": 160544.0,
          "costDetails": {
            "total": 0.07219625
          },
          "totalTokens": 7581,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 7168,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:17:32.205Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:20:23.391Z"
    },
    {
      "id": "dab23a0e",
      "timestamp": "2026-02-06T11:15:31.681000+00:00",
      "name": "dab2_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration Parameters\n# =========================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"  # Read a single video file from the given input_path\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Output video with rectangles, labels, and mAP\nCONFIDENCE_THRESHOLD = 0.5\n\n\ndef load_labels(path):\n    labels = []\n    with open(path, 'r') as f:\n        for line in f:\n            s = line.strip()\n            if s:\n                labels.append(s)\n    return labels\n\n\ndef prepare_interpreter(model_path):\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    return interpreter, input_details, output_details\n\n\ndef preprocess_frame(frame_bgr, input_details):\n    # Convert BGR (OpenCV) to RGB and resize to model input\n    h_in, w_in = input_details[0]['shape'][1], input_details[0]['shape'][2]\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    img_resized = cv2.resize(frame_rgb, (w_in, h_in))\n\n    in_dtype = input_details[0]['dtype']\n    # For float models, normalize to [0,1]; for quantized (uint8), keep 0..255\n    if in_dtype == np.float32:\n        input_tensor = img_resized.astype(np.float32) / 255.0\n    else:\n        input_tensor = img_resized.astype(in_dtype)\n\n    input_tensor = np.expand_dims(input_tensor, axis=0)\n    return input_tensor\n\n\ndef get_output_tensors(interpreter, output_details):\n    # Try to map outputs by name when available; otherwise fallback to heuristics\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    for d in output_details:\n        name = d.get('name', '')\n        if isinstance(name, bytes):\n            name = name.decode('utf-8', errors='ignore')\n        data = interpreter.get_tensor(d['index'])\n        lname = name.lower()\n\n        if 'box' in lname:\n            boxes = data\n        elif 'score' in lname:\n            scores = data\n        elif 'class' in lname:\n            classes = data\n        elif 'num' in lname:\n            num = data\n\n    # Fallback if names were not matched (assume typical SSD order)\n    if boxes is None or classes is None or scores is None:\n        outs = [interpreter.get_tensor(d['index']) for d in output_details]\n        # Guess by shapes\n        for out in outs:\n            shp = out.shape\n            if len(shp) == 3 and shp[-1] == 4:\n                boxes = out\n            elif len(shp) == 2:\n                # Could be classes or scores; distinguish by dtype/range is unreliable; attempt both\n                if out.dtype == np.float32:\n                    # More likely scores\n                    scores = out if scores is None else scores\n                else:\n                    classes = out if classes is None else classes\n            elif len(shp) == 1 and shp[0] == 1:\n                num = out\n\n        # If still ambiguous, try positional mapping as last resort\n        if boxes is None and len(outs) >= 1:\n            boxes = outs[0]\n        if classes is None and len(outs) >= 2:\n            classes = outs[1]\n        if scores is None and len(outs) >= 3:\n            scores = outs[2]\n        if num is None and len(outs) >= 4:\n            num = outs[3]\n\n    return boxes, classes, scores, num\n\n\ndef main():\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Setup TFLite interpreter\n    interpreter, input_details, output_details = prepare_interpreter(MODEL_PATH)\n\n    # Open input video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(\"Error: Could not open input video:\", INPUT_PATH)\n        return\n\n    # Prepare video writer\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0:\n        fps = 25.0  # Fallback if fps is not available\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        print(\"Error: Could not open output video for writing:\", OUTPUT_PATH)\n        cap.release()\n        return\n\n    # For proxy mAP: aggregate detection scores over the entire video (scores above threshold)\n    # Note: True mAP requires ground-truth annotations; here we compute a proxy metric as\n    # the mean of detection scores for all detections above threshold across the video.\n    # This will be displayed as \"mAP\" as requested.\n    aggregated_scores = []\n\n    frame_index = 0\n    inference_times = []\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret or frame is None:\n                break\n\n            frame_index += 1\n\n            # Preprocess\n            input_tensor = preprocess_frame(frame, input_details)\n\n            # Set input and run inference\n            interpreter.set_tensor(input_details[0]['index'], input_tensor)\n            t0 = time.time()\n            interpreter.invoke()\n            t1 = time.time()\n            inference_times.append((t1 - t0) * 1000.0)  # ms\n\n            # Fetch outputs\n            boxes, classes, scores, num = get_output_tensors(interpreter, output_details)\n\n            # Squeeze batch dimension\n            if boxes is not None:\n                boxes = np.squeeze(boxes)\n            if classes is not None:\n                classes = np.squeeze(classes)\n            if scores is not None:\n                scores = np.squeeze(scores)\n            if num is not None:\n                try:\n                    num = int(np.squeeze(num).astype(np.int32))\n                except Exception:\n                    num = None\n\n            # Determine number of detections to iterate\n            if num is None:\n                num = scores.shape[0] if scores is not None else 0\n\n            # Draw detections\n            for i in range(num):\n                score = float(scores[i]) if scores is not None else 0.0\n                if score < CONFIDENCE_THRESHOLD:\n                    continue\n\n                ymin, xmin, ymax, xmax = boxes[i]\n                x1 = max(0, min(width - 1, int(xmin * width)))\n                y1 = max(0, min(height - 1, int(ymin * height)))\n                x2 = max(0, min(width - 1, int(xmax * width)))\n                y2 = max(0, min(height - 1, int(ymax * height)))\n\n                cls_id = int(classes[i]) if classes is not None else -1\n                label = labels[cls_id] if (0 <= cls_id < len(labels)) else f\"id:{cls_id}\"\n\n                # Draw rectangle and label\n                color = (0, 255, 0)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n                label_text = f\"{label}: {score:.2f}\"\n                (tw, th), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n                y_text = y1 - 10 if y1 - 10 > 10 else y1 + th + 10\n                cv2.rectangle(frame, (x1, y_text - th - 4), (x1 + tw + 4, y_text + 2), (0, 0, 0), -1)\n                cv2.putText(frame, label_text, (x1 + 2, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n                # Accumulate score for proxy mAP\n                aggregated_scores.append(score)\n\n            # Compute running \"mAP\" proxy\n            if len(aggregated_scores) > 0:\n                map_proxy = float(np.mean(aggregated_scores))\n            else:\n                map_proxy = 0.0\n\n            # Overlay mAP and (optional) inference time\n            overlay_text = f\"mAP: {map_proxy:.3f}\"\n            if len(inference_times) > 0:\n                overlay_text += f\" | Inference: {inference_times[-1]:.1f} ms\"\n            cv2.putText(frame, overlay_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 220, 10), 2)\n\n            # Write frame\n            writer.write(frame)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    # Final report\n    final_map = float(np.mean(aggregated_scores)) if len(aggregated_scores) > 0 else 0.0\n    avg_infer = float(np.mean(inference_times)) if len(inference_times) > 0 else 0.0\n    print(\"Processing completed.\")\n    print(f\"Input: {INPUT_PATH}\")\n    print(f\"Output: {OUTPUT_PATH}\")\n    print(f\"Frames processed: {frame_index}\")\n    print(f\"Proxy mAP (mean score above threshold {CONFIDENCE_THRESHOLD}): {final_map:.4f}\")\n    print(f\"Average inference time: {avg_infer:.2f} ms/frame\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 86.962,
      "total_cost": 0.06070625,
      "observations": [
        {
          "id": "time-13-15-31-955173_chatcmpl-D6E681rX83BCBZaTW7maolF8w17nA",
          "traceId": "dab23a0e",
          "type": "GENERATION",
          "name": "dab2_psg_gen_attempt#1",
          "startTime": "2026-02-06T11:15:31.955000+00:00",
          "endTime": "2026-02-06T11:16:58.917000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06070625,
          "latency": 86962.0,
          "costDetails": {
            "total": 0.06070625
          },
          "totalTokens": 6432,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 6019,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:15:32.210Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:17:10.374Z"
    },
    {
      "id": "2cc0487e",
      "timestamp": "2026-02-06T11:12:29.783000+00:00",
      "name": "2cc0_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5  # for drawing on output video\n\n# Pseudo-GT and evaluation thresholds for mAP computation\nPSEUDO_GT_CONF_THRESHOLD = 0.8   # detections >= this are treated as pseudo ground-truth\nEVAL_MIN_CONF_THRESHOLD = 0.0    # include all predictions with score >= this for PR curve\nIOU_THRESHOLD = 0.5              # IoU threshold for matching predictions to GT (mAP calc)\n\n\ndef load_labels(path):\n    labels = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            lbl = line.strip()\n            if lbl:\n                labels.append(lbl)\n    return labels\n\n\ndef make_interpreter(model_path):\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef get_input_details(interpreter):\n    input_details = interpreter.get_input_details()[0]\n    idx = input_details['index']\n    shape = input_details['shape']\n    dtype = input_details['dtype']\n    return idx, shape, dtype\n\n\ndef preprocess_frame(frame_bgr, input_shape, input_dtype):\n    # input_shape: [1, h, w, 3]\n    in_h, in_w = int(input_shape[1]), int(input_shape[2])\n    resized = cv2.resize(frame_bgr, (in_w, in_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    if input_dtype == np.float32:\n        rgb = rgb.astype(np.float32) / 255.0\n    else:\n        rgb = rgb.astype(np.uint8)\n    return np.expand_dims(rgb, axis=0)\n\n\ndef parse_tflite_outputs(interpreter):\n    out_details = interpreter.get_output_details()\n    outputs = [interpreter.get_tensor(od['index']) for od in out_details]\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Identify outputs by shape/value characteristics\n    for arr in outputs:\n        if arr.ndim == 3 and arr.shape[-1] == 4:\n            boxes = arr[0]\n    # Remaining arrays\n    rem = [arr for arr in outputs if arr is not None and arr is not boxes]\n    # num_detections (size 1)\n    for arr in rem:\n        if arr.size == 1:\n            num = int(round(float(arr.flatten()[0])))\n            break\n    # remove num from remaining\n    rem2 = [arr for arr in rem if arr is not num]\n\n    # Two arrays left: classes and scores, both typically shape (1, N)\n    # Scores are in [0,1], classes are >= 0 and often > 1\n    for arr in rem2:\n        a = arr[0] if arr.ndim == 2 else arr\n        m = float(np.max(a)) if a.size else -1.0\n        if m <= 1.0:\n            scores = a\n        else:\n            classes = a\n\n    # Fallbacks in case ordering differs\n    if boxes is None:\n        # Try to find by last dim 4 even if not ndims=3\n        for arr in outputs:\n            if arr.shape[-1] == 4:\n                boxes = arr.reshape((-1, 4))\n                break\n    if classes is None or scores is None:\n        for arr in outputs:\n            if arr is boxes or arr is num:\n                continue\n            a = arr[0] if arr.ndim == 2 else arr\n            if a.size == 0:\n                continue\n            m = float(np.max(a))\n            if classes is None and m > 1.0:\n                classes = a\n            elif scores is None and m <= 1.0:\n                scores = a\n\n    # Final safety conversions\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if num is None:\n        num = min(len(scores), len(classes), len(boxes))\n\n    # Ensure lengths align\n    num = min(num, len(scores), len(classes), len(boxes))\n    return boxes[:num], classes[:num], scores[:num], num\n\n\ndef convert_boxes_to_pixels(boxes_norm, frame_w, frame_h):\n    # boxes_norm: [N, 4] in [ymin, xmin, ymax, xmax], normalized [0,1]\n    boxes_px = []\n    for y_min, x_min, y_max, x_max in boxes_norm:\n        x1 = int(max(0, min(frame_w - 1, x_min * frame_w)))\n        y1 = int(max(0, min(frame_h - 1, y_min * frame_h)))\n        x2 = int(max(0, min(frame_w - 1, x_max * frame_w)))\n        y2 = int(max(0, min(frame_h - 1, y_max * frame_h)))\n        boxes_px.append((x1, y1, x2, y2))\n    return boxes_px\n\n\ndef iou(boxA, boxB):\n    # boxes as (x1, y1, x2, y2)\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interW = max(0, xB - xA)\n    interH = max(0, yB - yA)\n    interArea = interW * interH\n    if interArea <= 0:\n        return 0.0\n    boxAArea = max(0, boxA[2] - boxA[0]) * max(0, boxA[3] - boxA[1])\n    boxBArea = max(0, boxB[2] - boxB[0]) * max(0, boxB[3] - boxB[1])\n    denom = (boxAArea + boxBArea - interArea)\n    return interArea / denom if denom > 0 else 0.0\n\n\ndef compute_map(pseudo_gt_by_cls, preds_by_cls, iou_threshold=IOU_THRESHOLD):\n    # Compute AP per class and return their mean (over classes with at least one GT)\n    ap_list = []\n\n    for cls_id, gt_list in pseudo_gt_by_cls.items():\n        gt_count = len(gt_list)\n        if gt_count == 0:\n            continue\n\n        preds = preds_by_cls.get(cls_id, [])\n        # preds: list of (score, frame_id, box)\n        preds_sorted = sorted(preds, key=lambda x: x[0], reverse=True)\n\n        # For each frame, keep a list of gt boxes and which ones matched\n        gt_by_frame = {}\n        for frame_id, gt_box in gt_list:\n            if frame_id not in gt_by_frame:\n                gt_by_frame[frame_id] = []\n            gt_by_frame[frame_id].append({'box': gt_box, 'matched': False})\n\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n\n        for i, (score, frame_id, pred_box) in enumerate(preds_sorted):\n            gts = gt_by_frame.get(frame_id, [])\n            best_iou = 0.0\n            best_j = -1\n            for j, g in enumerate(gts):\n                if g['matched']:\n                    continue\n                iou_val = iou(pred_box, g['box'])\n                if iou_val > best_iou:\n                    best_iou = iou_val\n                    best_j = j\n            if best_iou >= iou_threshold and best_j >= 0:\n                gts[best_j]['matched'] = True\n                tp[i] = 1.0\n            else:\n                fp[i] = 1.0\n\n        # Precision-Recall curve\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        recalls = cum_tp / max(gt_count, 1)\n        precisions = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n\n        # AP computation (VOC-style 11-point interpolation)\n        ap = 0.0\n        for t in np.linspace(0.0, 1.0, 11):\n            prec_at_recall = precisions[recalls >= t]\n            p = np.max(prec_at_recall) if prec_at_recall.size > 0 else 0.0\n            ap += p / 11.0\n\n        ap_list.append(ap)\n\n    if len(ap_list) == 0:\n        return 0.0\n    return float(np.mean(ap_list))\n\n\ndef ensure_dir_for_file(file_path):\n    d = os.path.dirname(file_path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\n\ndef main():\n    labels = load_labels(label_path)\n    interpreter = make_interpreter(model_path)\n    input_index, input_shape, input_dtype = get_input_details(interpreter)\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    # Video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0:\n        fps = 30.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # First pass: run inference, collect predictions and pseudo-GT for mAP computation\n    pseudo_gt_by_cls = {}  # {cls_id: [(frame_id, box), ...]}\n    preds_by_cls = {}      # {cls_id: [(score, frame_id, box), ...]}\n\n    frame_idx = 0\n    t_start = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        input_tensor = preprocess_frame(frame, input_shape, input_dtype)\n        interpreter.set_tensor(input_index, input_tensor)\n        interpreter.invoke()\n        boxes_norm, classes_raw, scores, num = parse_tflite_outputs(interpreter)\n        boxes_px = convert_boxes_to_pixels(boxes_norm, width, height)\n\n        # Fill pseudo-GT and prediction pools\n        for i in range(num):\n            cls_id = int(classes_raw[i])\n            score = float(scores[i])\n            box = boxes_px[i]\n\n            # Predictions for evaluation\n            if score >= EVAL_MIN_CONF_THRESHOLD:\n                preds_by_cls.setdefault(cls_id, []).append((score, frame_idx, box))\n\n            # High-confidence detections as pseudo ground-truth\n            if score >= PSEUDO_GT_CONF_THRESHOLD:\n                pseudo_gt_by_cls.setdefault(cls_id, []).append((frame_idx, box))\n\n        frame_idx += 1\n\n    cap.release()\n    t_infer = time.time() - t_start\n\n    # Compute mAP over the video using pseudo ground-truth\n    mAP = compute_map(pseudo_gt_by_cls, preds_by_cls, iou_threshold=IOU_THRESHOLD)\n\n    # Second pass: run inference again to draw boxes and overlay mAP text, save video\n    ensure_dir_for_file(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        raise RuntimeError(f\"Failed to open output video for writing: {output_path}\")\n\n    # Simple deterministic color per class\n    rng = np.random.RandomState(42)\n    color_cache = {}\n\n    def get_color_for_class(cid):\n        if cid not in color_cache:\n            color_cache[cid] = tuple(int(c) for c in rng.randint(0, 255, size=3))\n        return color_cache[cid]\n\n    cap2 = cv2.VideoCapture(input_path)\n    if not cap2.isOpened():\n        writer.release()\n        raise RuntimeError(f\"Failed to reopen input video for rendering: {input_path}\")\n\n    # Prepare label lookup that tolerates index out of range or negative\n    def class_to_label(cid):\n        if 0 <= cid < len(labels):\n            return labels[cid]\n        return f\"id_{cid}\"\n\n    overlay_text = f\"mAP (pseudo-GT@{PSEUDO_GT_CONF_THRESHOLD}, IoU@{IOU_THRESHOLD}): {mAP*100:.2f}%\"\n\n    # Drawing loop\n    while True:\n        ret, frame = cap2.read()\n        if not ret:\n            break\n\n        input_tensor = preprocess_frame(frame, input_shape, input_dtype)\n        interpreter.set_tensor(input_index, input_tensor)\n        interpreter.invoke()\n        boxes_norm, classes_raw, scores, num = parse_tflite_outputs(interpreter)\n        boxes_px = convert_boxes_to_pixels(boxes_norm, width, height)\n\n        for i in range(num):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n            cid = int(classes_raw[i])\n            box = boxes_px[i]\n            color = get_color_for_class(cid)\n            x1, y1, x2, y2 = box\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            label = class_to_label(cid)\n            text = f\"{label}: {score:.2f}\"\n            # Text background\n            (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            y_text = max(0, y1 - 10)\n            cv2.rectangle(frame, (x1, y_text - th - 4), (x1 + tw + 2, y_text + 2), color, -1)\n            cv2.putText(frame, text, (x1 + 1, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Overlay mAP text\n        cv2.putText(frame, overlay_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 220, 50), 2, cv2.LINE_AA)\n\n        writer.write(frame)\n\n    cap2.release()\n    writer.release()\n\n    # Optional console logs\n    print(f\"Processed {frame_idx} frames\")\n    print(f\"Inference (pass 1) time: {t_infer:.2f}s, avg {t_infer / max(frame_idx, 1):.3f}s/frame\")\n    print(f\"Output saved to: {output_path}\")\n    print(overlay_text)\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 143.789,
      "total_cost": 0.08176625,
      "observations": [
        {
          "id": "time-13-12-30-427818_chatcmpl-D6E3CUOyNyzaUS2bpwCIFfYk3zLGm",
          "traceId": "2cc0487e",
          "type": "GENERATION",
          "name": "2cc0_psg_gen_attempt#1",
          "startTime": "2026-02-06T11:12:30.427000+00:00",
          "endTime": "2026-02-06T11:14:54.216000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08176625,
          "latency": 143789.0,
          "costDetails": {
            "total": 0.08176625
          },
          "totalTokens": 8538,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 8125,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:12:30.342Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:15:11.033Z"
    },
    {
      "id": "d84fb799",
      "timestamp": "2026-02-06T11:09:15.824000+00:00",
      "name": "d84f_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# CONFIGURATION PARAMETERS\n# =========================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# =========================\n# UTILITY FUNCTIONS\n# =========================\ndef ensure_dir(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\ndef load_labels(path):\n    labels = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef iou_xyxy(boxA, boxB):\n    # box: [x1, y1, x2, y2]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0.0, xB - xA)\n    inter_h = max(0.0, yB - yA)\n    inter_area = inter_w * inter_h\n    if inter_area <= 0:\n        return 0.0\n    boxA_area = max(0.0, (boxA[2] - boxA[0])) * max(0.0, (boxA[3] - boxA[1]))\n    boxB_area = max(0.0, (boxB[2] - boxB[0])) * max(0.0, (boxB[3] - boxB[1]))\n    denom = boxA_area + boxB_area - inter_area\n    return inter_area / denom if denom > 0 else 0.0\n\ndef compute_map(preds_by_class, gts_by_class, iou_thresh=0.5):\n    # preds_by_class: {cls_id: [{'image_id': int, 'score': float, 'bbox':[x1,y1,x2,y2]}, ...]}\n    # gts_by_class:   {cls_id: {image_id: [{'bbox':[x1,y1,x2,y2], 'matched':False}, ...]}}\n    aps = []\n    for cls_id, preds in preds_by_class.items():\n        # Prepare ground truth structures\n        gt_img_dict = gts_by_class.get(cls_id, {})\n        gt_count = sum(len(v) for v in gt_img_dict.values())\n        if gt_count == 0:\n            # No GT for this class; skip it from mAP (standard practice)\n            continue\n\n        # Sort predictions by score descending\n        preds_sorted = sorted(preds, key=lambda x: -x['score'])\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n\n        # For each prediction, match with best GT in the same image if IoU >= threshold\n        for i, p in enumerate(preds_sorted):\n            img_id = p['image_id']\n            p_box = p['bbox']\n            gts = gt_img_dict.get(img_id, [])\n            best_iou = 0.0\n            best_idx = -1\n            for j, gt in enumerate(gts):\n                if not gt['matched']:\n                    iou = iou_xyxy(p_box, gt['bbox'])\n                    if iou > best_iou:\n                        best_iou = iou\n                        best_idx = j\n            if best_iou >= iou_thresh and best_idx >= 0:\n                tp[i] = 1.0\n                gts[best_idx]['matched'] = True\n            else:\n                fp[i] = 1.0\n\n        # Precision-recall\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        eps = 1e-9\n        precisions = cum_tp / np.maximum(cum_tp + cum_fp, eps)\n        recalls = cum_tp / float(max(gt_count, 1))\n\n        # 11-point interpolation or trapezoidal integration with precision envelope (VOC 2010+)\n        # Use precision envelope\n        mrec = np.concatenate(([0.0], recalls, [1.0]))\n        mpre = np.concatenate(([0.0], precisions, [0.0]))\n        for i in range(len(mpre) - 1, 0, -1):\n            mpre[i - 1] = max(mpre[i - 1], mpre[i])\n\n        # Integrate area under PR curve where recall changes\n        idx = np.where(mrec[1:] != mrec[:-1])[0]\n        ap = 0.0\n        for i in idx:\n            ap += (mrec[i + 1] - mrec[i]) * mpre[i + 1]\n        aps.append(ap)\n\n        # Reset matched flags for potential reuse (not strictly needed here)\n        for gts in gt_img_dict.values():\n            for gt in gts:\n                gt['matched'] = False\n\n    if len(aps) == 0:\n        return None\n    return float(np.mean(aps))\n\ndef parse_optional_ground_truth(input_video_path):\n    # Optional GT file path: same stem + \".gt.txt\"\n    # Format per line:\n    # frame_index class_id x1 y1 x2 y2   (absolute pixel coordinates, integers)\n    gt_file = os.path.splitext(input_video_path)[0] + \".gt.txt\"\n    if not os.path.exists(gt_file):\n        return None  # No ground truth available\n    gts_by_class = {}\n    with open(gt_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                continue\n            parts = line.split()\n            if len(parts) != 6:\n                continue\n            try:\n                frame_idx = int(parts[0])\n                cls_id = int(parts[1])\n                x1, y1, x2, y2 = map(float, parts[2:6])\n            except Exception:\n                continue\n            gts_by_class.setdefault(cls_id, {}).setdefault(frame_idx, []).append({'bbox': [x1, y1, x2, y2], 'matched': False})\n    return gts_by_class\n\ndef prepare_input(frame_bgr, input_shape, input_dtype):\n    # input_shape: [1, height, width, 3]\n    in_h, in_w = int(input_shape[1]), int(input_shape[2])\n    # Convert BGR to RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n    if input_dtype == np.float32:\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        input_data = resized.astype(np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef extract_detections(interpreter, output_details, frame_w, frame_h, conf_thres):\n    # Typical order for SSD Mobilenet V1 TFLite:\n    # 0: boxes [1, num, 4], 1: classes [1, num], 2: scores [1, num], 3: num_detections [1]\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(np.int32)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num = int(interpreter.get_tensor(output_details[3]['index'])[0])\n\n    dets = []\n    for i in range(min(num, boxes.shape[0])):\n        score = float(scores[i])\n        if score < conf_thres:\n            continue\n        y1, x1, y2, x2 = boxes[i]  # normalized\n        # Convert to absolute xyxy\n        x1_abs = max(0, min(frame_w - 1, int(x1 * frame_w)))\n        y1_abs = max(0, min(frame_h - 1, int(y1 * frame_h)))\n        x2_abs = max(0, min(frame_w - 1, int(x2 * frame_w)))\n        y2_abs = max(0, min(frame_h - 1, int(y2 * frame_h)))\n        # Ensure proper ordering\n        x1c, y1c = min(x1_abs, x2_abs), min(y1_abs, y2_abs)\n        x2c, y2c = max(x1_abs, x2_abs), max(y1_abs, y2_abs)\n        dets.append({\n            'class_id': int(classes[i]),\n            'score': score,\n            'bbox': [x1c, y1c, x2c, y2c],\n        })\n    return dets\n\ndef draw_detections(frame, detections, labels, label_offset, map_text):\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        cls_id = det['class_id']\n        score = det['score']\n        # Resolve label with possible offset\n        label_idx = cls_id + label_offset\n        if 0 <= label_idx < len(labels):\n            cls_name = labels[label_idx]\n        else:\n            cls_name = f\"id_{cls_id}\"\n        caption = f\"{cls_name}: {score:.2f}\"\n        # Draw box\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 200, 0), 2)\n        # Text background\n        (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), (0, 200, 0), -1)\n        cv2.putText(frame, caption, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n    # Draw mAP text\n    map_str = f\"mAP@0.5: {map_text}\"\n    (tw, th), bl = cv2.getTextSize(map_str, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n    cv2.rectangle(frame, (8, 8), (8 + tw + 8, 8 + th + 12), (0, 0, 0), -1)\n    cv2.putText(frame, map_str, (12, 8 + th + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n\n# =========================\n# MAIN PIPELINE\n# =========================\ndef main():\n    ensure_dir(output_path)\n\n    # Load labels\n    labels = load_labels(label_path)\n    # Determine label offset: if label file starts with a background token, offset 0; otherwise 1\n    label_offset = 0\n    if len(labels) > 0:\n        first = labels[0].strip().lower()\n        if first in ('???', 'background', 'bg'):\n            label_offset = 0\n        else:\n            label_offset = 1\n\n    # Initialize interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Validate single input tensor assumption\n    if not input_details:\n        raise RuntimeError(\"Interpreter has no input tensors.\")\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n\n    # First pass: run inference over the video and collect detections\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 1e-3 or np.isnan(fps):\n        fps = 30.0  # Fallback\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    predictions_by_frame = []  # list of list of detections per frame\n    preds_by_class = {}        # class_id -> list of {'image_id', 'score', 'bbox'}\n\n    frame_index = 0\n    t0 = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Prepare input tensor\n        input_data = prepare_input(frame, input_shape, input_dtype)\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        interpreter.invoke()\n\n        # Extract detections\n        dets = extract_detections(interpreter, output_details, frame_w, frame_h, confidence_threshold)\n        predictions_by_frame.append(dets)\n\n        # Accumulate for mAP\n        for d in dets:\n            cid = d['class_id']\n            preds_by_class.setdefault(cid, []).append({\n                'image_id': frame_index,\n                'score': float(d['score']),\n                'bbox': [float(d['bbox'][0]), float(d['bbox'][1]), float(d['bbox'][2]), float(d['bbox'][3])],\n            })\n\n        frame_index += 1\n\n    cap.release()\n    infer_time = time.time() - t0\n\n    total_frames = frame_index\n\n    # Load optional ground truth and compute mAP\n    gts_by_class = parse_optional_ground_truth(input_path)\n    if gts_by_class is not None:\n        computed_map = compute_map(preds_by_class, gts_by_class, iou_thresh=0.5)\n    else:\n        # Fallback: if no GT available, compute a degenerate \"self mAP\"\n        # Treat predictions as ground truth to produce a numeric value (typically 1.0).\n        # Build GT from predictions\n        gts_by_class_fallback = {}\n        for cid, plist in preds_by_class.items():\n            for p in plist:\n                img_id = p['image_id']\n                gts_by_class_fallback.setdefault(cid, {}).setdefault(img_id, []).append({'bbox': p['bbox'][:], 'matched': False})\n        computed_map = compute_map(preds_by_class, gts_by_class_fallback, iou_thresh=0.5)\n\n    map_text_val = \"N/A\" if computed_map is None else f\"{computed_map:.3f}\"\n\n    # Second pass: render video with detections and mAP overlay\n    cap2 = cv2.VideoCapture(input_path)\n    if not cap2.isOpened():\n        raise RuntimeError(f\"Failed to reopen input video for rendering: {input_path}\")\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        raise RuntimeError(f\"Failed to open output video for writing: {output_path}\")\n\n    idx = 0\n    t1 = time.time()\n    while True:\n        ret, frame = cap2.read()\n        if not ret:\n            break\n        dets = predictions_by_frame[idx] if idx < len(predictions_by_frame) else []\n        draw_detections(frame, dets, labels, label_offset, map_text_val)\n        writer.write(frame)\n        idx += 1\n\n    cap2.release()\n    writer.release()\n    render_time = time.time() - t1\n\n    # Console summary\n    print(\"TFLite object detection completed.\")\n    print(f\"Input video: {input_path}\")\n    print(f\"Output video: {output_path}\")\n    print(f\"Frames processed: {total_frames}\")\n    print(f\"Inference pass time: {infer_time:.2f} s\")\n    print(f\"Rendering pass time: {render_time:.2f} s\")\n    print(f\"mAP@0.5: {map_text_val}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 161.212,
      "total_cost": 0.08435625,
      "observations": [
        {
          "id": "time-13-09-16-484148_chatcmpl-D6E04GijYBgqlyI7c0Q40OkSrwa8I",
          "traceId": "d84fb799",
          "type": "GENERATION",
          "name": "d84f_psg_gen_attempt#1",
          "startTime": "2026-02-06T11:09:16.484000+00:00",
          "endTime": "2026-02-06T11:11:57.696000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08435625,
          "latency": 161212.0,
          "costDetails": {
            "total": 0.08435625
          },
          "totalTokens": 8797,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 8384,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:09:16.378Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:12:09.046Z"
    },
    {
      "id": "6e180fda",
      "timestamp": "2026-02-06T11:05:47.784000+00:00",
      "name": "6e18_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport cv2\nimport numpy as np\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =======================\n# Configuration parameters\n# =======================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5  # Only keep detections above this\n# =======================\n\n\ndef ensure_parent_dir(path):\n    parent = os.path.dirname(os.path.abspath(path))\n    if parent and not os.path.exists(parent):\n        os.makedirs(parent, exist_ok=True)\n\n\ndef load_labels(label_path):\n    \"\"\"\n    Load labels from a label map text file.\n    Each non-empty, non-comment line is treated as a label.\n    Index in the list corresponds to the class id.\n    \"\"\"\n    labels = []\n    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line or line.startswith(\"#\"):\n                continue\n            # Typical TF Lite label files are plain names per line\n            labels.append(line)\n    return labels\n\n\ndef build_interpreter(model_path):\n    \"\"\"\n    Build and allocate a TFLite interpreter using ai_edge_litert.\n    \"\"\"\n    interpreter = Interpreter(model_path=model_path, num_threads=max(1, (os.cpu_count() or 4)))\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef get_input_size_and_type(interpreter):\n    input_details = interpreter.get_input_details()[0]\n    ih, iw = input_details[\"shape\"][1], input_details[\"shape\"][2]\n    idtype = input_details[\"dtype\"]\n    return iw, ih, idtype, input_details[\"index\"]\n\n\ndef preprocess_frame(frame_bgr, in_w, in_h, in_dtype):\n    \"\"\"\n    Resize and format input frame for the model.\n    SSD MobileNet expects RGB input of size (in_h, in_w).\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n    if in_dtype == np.float32:\n        tensor = resized.astype(np.float32) / 255.0\n    else:\n        tensor = resized.astype(np.uint8)\n    tensor = np.expand_dims(tensor, axis=0)\n    return tensor\n\n\ndef _get_output_array_by_name_or_fallback(interpreter, keyword):\n    \"\"\"\n    Try to get an output tensor by matching a keyword in its name.\n    If not found, return None.\n    \"\"\"\n    outputs = []\n    output_details = interpreter.get_output_details()\n    for od in output_details:\n        name = (od.get(\"name\") or \"\").lower()\n        if keyword in name:\n            arr = interpreter.get_tensor(od[\"index\"])\n            outputs.append(arr)\n    if outputs:\n        return outputs[0]\n    return None\n\n\ndef extract_detections(interpreter):\n    \"\"\"\n    Extracts boxes, classes, scores, and num_detections from interpreter outputs.\n    Returns:\n      boxes: (N, 4) array in normalized [ymin, xmin, ymax, xmax]\n      classes: (N,) array of class indices (float or int)\n      scores: (N,) array of confidence scores\n    \"\"\"\n    # First, try by canonical names\n    boxes = _get_output_array_by_name_or_fallback(interpreter, \"boxes\")\n    classes = _get_output_array_by_name_or_fallback(interpreter, \"classes\")\n    scores = _get_output_array_by_name_or_fallback(interpreter, \"scores\")\n    num = _get_output_array_by_name_or_fallback(interpreter, \"num\")\n\n    # If any are None, fallback to heuristic using shapes\n    if boxes is None or classes is None or scores is None:\n        output_details = interpreter.get_output_details()\n        outputs = [interpreter.get_tensor(od[\"index\"]) for od in output_details]\n\n        # Identify boxes: last dim == 4 after squeeze\n        cand_boxes = []\n        cand_others = []\n        for arr in outputs:\n            sq = np.squeeze(arr)\n            if sq.ndim == 2 and sq.shape[1] == 4:\n                cand_boxes.append(sq)\n            else:\n                cand_others.append(sq)\n\n        if boxes is None and cand_boxes:\n            boxes = cand_boxes[0]\n\n        # For the remaining, pick two 1D arrays of the same length as boxes for classes and scores\n        if boxes is not None and (classes is None or scores is None):\n            N = boxes.shape[0]\n            one_d = [a for a in cand_others if a.ndim == 1 and a.shape[0] == N]\n            # Heuristic: the array with more integer-like values is classes; the other is scores\n            if len(one_d) >= 2:\n                # Pick two arrays\n                a, b = one_d[0], one_d[1]\n                # Determine which seems more \"score-like\" by mean value range [0,1]\n                def score_like(x):\n                    return float(np.mean((x >= 0.0) & (x <= 1.0)))\n                if score_like(a) >= score_like(b):\n                    scores = a\n                    classes = b\n                else:\n                    scores = b\n                    classes = a\n\n        # Try to get num detections if still missing\n        if num is None:\n            for arr in outputs:\n                sq = np.squeeze(arr)\n                if sq.ndim == 0:\n                    num = sq\n                    break\n\n    # Final safety conversion and squeezing\n    if boxes is not None:\n        boxes = np.squeeze(boxes)\n        if boxes.ndim == 3 and boxes.shape[0] == 1:\n            boxes = boxes[0]\n    else:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n\n    if classes is not None:\n        classes = np.squeeze(classes)\n    else:\n        classes = np.zeros((0,), dtype=np.float32)\n\n    if scores is not None:\n        scores = np.squeeze(scores)\n    else:\n        scores = np.zeros((0,), dtype=np.float32)\n\n    # Handle num_detections if provided\n    if num is not None:\n        try:\n            n = int(np.squeeze(num).item())\n        except Exception:\n            n = None\n    else:\n        n = None\n\n    # Normalize shapes: ensure 2D boxes, 1D classes/scores\n    if boxes.ndim == 1 and boxes.size == 4:\n        boxes = boxes.reshape(1, 4)\n    if classes.ndim == 0:\n        classes = classes.reshape(1)\n    if scores.ndim == 0:\n        scores = scores.reshape(1)\n\n    # Truncate to the common minimum length to avoid indexing errors\n    lengths = [boxes.shape[0], classes.shape[0], scores.shape[0]]\n    if n is not None:\n        lengths.append(n)\n    N = int(min(lengths)) if lengths else 0\n\n    boxes = boxes[:N] if boxes.shape[0] >= N else boxes\n    classes = classes[:N] if classes.shape[0] >= N else classes\n    scores = scores[:N] if scores.shape[0] >= N else scores\n\n    return boxes, classes, scores\n\n\ndef filter_and_scale_detections(boxes, classes, scores, conf_thres, frame_w, frame_h):\n    \"\"\"\n    Filter detections by confidence and convert normalized boxes to pixel coordinates.\n    Returns lists: px_boxes [(x1,y1,x2,y2)], classes, scores\n    \"\"\"\n    if boxes.size == 0 or scores.size == 0:\n        return [], [], []\n\n    keep = scores >= float(conf_thres)\n    boxes = boxes[keep]\n    classes = classes[keep] if len(classes) == len(keep) else classes[:boxes.shape[0]]\n    scores = scores[keep]\n\n    px_boxes = []\n    out_classes = []\n    out_scores = []\n\n    for i in range(boxes.shape[0]):\n        ymin, xmin, ymax, xmax = boxes[i]\n        x1 = int(max(0, min(frame_w - 1, round(xmin * frame_w))))\n        y1 = int(max(0, min(frame_h - 1, round(ymin * frame_h))))\n        x2 = int(max(0, min(frame_w - 1, round(xmax * frame_w))))\n        y2 = int(max(0, min(frame_h - 1, round(ymax * frame_h))))\n        # Ensure proper ordering\n        x1, x2 = (x1, x2) if x1 <= x2 else (x2, x1)\n        y1, y2 = (y1, y2) if y1 <= y2 else (y2, y1)\n        # Discard degenerate boxes\n        if (x2 - x1) >= 1 and (y2 - y1) >= 1:\n            px_boxes.append((x1, y1, x2, y2))\n            out_classes.append(classes[i] if i < len(classes) else -1)\n            out_scores.append(scores[i])\n\n    return px_boxes, out_classes, out_scores\n\n\ndef class_color(cls_id):\n    \"\"\"\n    Deterministic BGR color for a class id.\n    \"\"\"\n    base = int(cls_id) if isinstance(cls_id, (int, np.integer)) else int(float(cls_id))\n    b = (37 * base) % 255\n    g = (17 * base) % 255\n    r = (73 * base) % 255\n    return (b, g, r)\n\n\ndef draw_detections_on_frame(frame, px_boxes, classes, scores, labels, conf_thres, map_estimate=None, fps=None):\n    \"\"\"\n    Draw bounding boxes and labels on the frame.\n    Also overlay an estimated mAP and FPS if provided.\n    \"\"\"\n    h, w = frame.shape[:2]\n    thickness = max(1, int(round(min(h, w) / 300)))\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = max(0.4, min(h, w) / 1000.0)\n    text_thickness = max(1, int(round(thickness)))\n\n    n = min(len(px_boxes), len(classes), len(scores))\n    for i in range(n):\n        x1, y1, x2, y2 = px_boxes[i]\n        cls_id = int(classes[i]) if len(classes) > i else -1\n        score = float(scores[i]) if len(scores) > i else 0.0\n\n        color = class_color(cls_id)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n\n        # Build label string\n        if 0 <= cls_id < len(labels):\n            label_text = labels[cls_id]\n        else:\n            label_text = f\"id:{cls_id}\"\n        label = f\"{label_text} {score:.2f}\"\n\n        # Text background\n        (tw, th), bl = cv2.getTextSize(label, font, font_scale, text_thickness)\n        y_text = max(th + 4, y1 - 4)\n        x_text = x1\n        cv2.rectangle(frame, (x_text, y_text - th - 4), (x_text + tw + 4, y_text + 2), color, -1)\n        cv2.putText(frame, label, (x_text + 2, y_text - 2), font, font_scale, (255, 255, 255), text_thickness, cv2.LINE_AA)\n\n    # Overlay mAP (estimated/proxy)\n    overlay_y = 24\n    if map_estimate is not None:\n        map_text = f\"mAP (proxy): {map_estimate:.3f}\"\n        cv2.putText(frame, map_text, (8, overlay_y), font, font_scale, (0, 255, 255), text_thickness, cv2.LINE_AA)\n        overlay_y += int(20 * font_scale + 8)\n\n    # Overlay FPS\n    if fps is not None and fps > 0:\n        fps_text = f\"FPS: {fps:.1f}\"\n        cv2.putText(frame, fps_text, (8, overlay_y), font, font_scale, (0, 255, 0), text_thickness, cv2.LINE_AA)\n\n    return frame\n\n\ndef main():\n    # Prepare output directory\n    ensure_parent_dir(OUTPUT_PATH)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Build interpreter\n    interpreter = build_interpreter(MODEL_PATH)\n    in_w, in_h, in_dtype, in_index = get_input_size_and_type(interpreter)\n\n    # Open video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    # Retrieve video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 1e-3 or np.isnan(fps):\n        fps = 30.0  # Fallback if FPS is unavailable\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)\n    if width <= 0 or height <= 0:\n        # Read one frame to infer size\n        ok, test_frame = cap.read()\n        if not ok:\n            cap.release()\n            raise RuntimeError(\"Unable to read a frame from the input video to determine size.\")\n        height, width = test_frame.shape[:2]\n        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # rewind\n\n    # Setup video writer\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    # For proxy mAP calculation: we'll use mean of detection scores above threshold across all frames\n    all_kept_scores = []\n\n    frame_count = 0\n    t0 = time.time()\n    last_time = t0\n    fps_smooth = None\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_count += 1\n\n            # Preprocess\n            input_tensor = preprocess_frame(frame, in_w, in_h, in_dtype)\n\n            # Inference\n            interpreter.set_tensor(in_index, input_tensor)\n            interpreter.invoke()\n\n            # Extract detections\n            boxes, classes, scores = extract_detections(interpreter)\n\n            # Filter and scale detections\n            px_boxes, cls_list, scr_list = filter_and_scale_detections(\n                boxes, classes, scores, CONFIDENCE_THRESHOLD, frame_w=width, frame_h=height\n            )\n\n            # Update proxy mAP statistics\n            if len(scr_list) > 0:\n                all_kept_scores.extend([float(s) for s in scr_list])\n\n            map_proxy = float(np.mean(all_kept_scores)) if all_kept_scores else 0.0\n\n            # FPS calculation (smoothed)\n            now = time.time()\n            inst_fps = 1.0 / max(1e-6, now - last_time)\n            last_time = now\n            if fps_smooth is None:\n                fps_smooth = inst_fps\n            else:\n                fps_smooth = 0.9 * fps_smooth + 0.1 * inst_fps\n\n            # Draw and write\n            drawn = draw_detections_on_frame(\n                frame, px_boxes, cls_list, scr_list, labels, CONFIDENCE_THRESHOLD, map_estimate=map_proxy, fps=fps_smooth\n            )\n            writer.write(drawn)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    total_time = time.time() - t0\n    final_fps = frame_count / total_time if total_time > 0 else 0.0\n    final_map_proxy = float(np.mean(all_kept_scores)) if all_kept_scores else 0.0\n\n    print(\"Processing complete.\")\n    print(f\"Input: {INPUT_PATH}\")\n    print(f\"Output: {OUTPUT_PATH}\")\n    print(f\"Frames processed: {frame_count}\")\n    print(f\"Average FPS: {final_fps:.2f}\")\n    print(f\"Estimated mAP (proxy using mean confidence over threshold {CONFIDENCE_THRESHOLD}): {final_map_proxy:.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 177.95,
      "total_cost": 0.1356675,
      "observations": [
        {
          "id": "time-13-05-48-434063_chatcmpl-D6Dwkh62VhcZgswXdODMz7toZ3sWi",
          "traceId": "6e180fda",
          "type": "GENERATION",
          "name": "6e18_psg_gen_attempt#1",
          "startTime": "2026-02-06T11:05:48.434000+00:00",
          "endTime": "2026-02-06T11:07:13.530000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06416625,
          "latency": 85096.0,
          "costDetails": {
            "total": 0.06416625
          },
          "totalTokens": 6778,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 6365,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-07-14-321116_chatcmpl-D6Dy6FiTmuNKZxmMi7HYVTtvCVbjw",
          "traceId": "6e180fda",
          "type": "GENERATION",
          "name": "6e18_psg_gen_attempt#2",
          "startTime": "2026-02-06T11:07:14.321000+00:00",
          "endTime": "2026-02-06T11:08:46.384000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07150125,
          "latency": 92063.0,
          "costDetails": {
            "total": 0.07150125
          },
          "totalTokens": 7809,
          "environment": "default",
          "promptTokens": 753,
          "completionTokens": 7056,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:05:48.338Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:08:55.533Z"
    },
    {
      "id": "f6b72cea",
      "timestamp": "2026-02-06T11:03:13.983000+00:00",
      "name": "f6b7_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# IoU threshold used for evaluation (mAP) at 0.5 as common practice\nIOU_THRESH_EVAL = 0.5\n# Minimum score to consider a detection for evaluation accumulation (to reduce noise)\nMIN_SCORE_FOR_AP = 0.05\n\n# =========================\n# Utility Functions\n# =========================\ndef load_labels(path):\n    labels = []\n    try:\n        with open(path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                # Common label maps can be \"index label\" or just \"label\"\n                parts = line.split(maxsplit=1)\n                if len(parts) == 2 and parts[0].isdigit():\n                    labels.append(parts[1])\n                else:\n                    labels.append(line)\n    except Exception as e:\n        print(\"Failed to load labels:\", e)\n    return labels\n\ndef letterbox_resize(image, new_w, new_h):\n    # For SSD models, typically a direct resize is used (no letterbox). Keep simple resize.\n    return cv2.resize(image, (new_w, new_h))\n\ndef preprocess(frame_bgr, input_details):\n    # Expect input format: [1, height, width, 3]\n    _, in_h, in_w, _ = input_details[0]['shape']\n    img_resized = letterbox_resize(frame_bgr, in_w, in_h)\n    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    input_dtype = input_details[0]['dtype']\n\n    if input_dtype == np.float32:\n        input_data = (img_rgb.astype(np.float32) / 255.0).reshape(1, in_h, in_w, 3)\n    else:\n        # Assume uint8 quantized\n        input_data = img_rgb.astype(np.uint8).reshape(1, in_h, in_w, 3)\n    return input_data\n\ndef run_inference(interpreter, input_data, input_details, output_details):\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Typical TFLite SSD outputs: boxes, classes, scores, num_detections\n    # Retrieve by order based on output count and shapes\n    outs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    # Try to identify outputs by their shapes\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n    for out in outs:\n        s = out.shape\n        if len(s) == 3 and s[-1] == 4:\n            boxes = out[0]\n        elif len(s) == 2 and s[-1] >= 1 and s[0] == 1:\n            # Could be classes or scores\n            if out.dtype.kind in ('f',):  # float -> likely scores\n                scores = out[0]\n            else:\n                classes = out[0]\n        elif len(s) == 1 and s[0] == 1:\n            num = int(np.squeeze(out))\n    # Fallback if outputs are ordered as typical [boxes, classes, scores, num]\n    if boxes is None or classes is None or scores is None or num is None:\n        if len(outs) >= 4:\n            boxes = np.squeeze(outs[0])\n            classes = np.squeeze(outs[1])\n            scores = np.squeeze(outs[2])\n            num = int(np.squeeze(outs[3]))\n\n    # Trim to num detections\n    num = min(num, boxes.shape[0], classes.shape[0], scores.shape[0])\n    return boxes[:num], classes[:num].astype(int), scores[:num].astype(float)\n\ndef box_iou(b1, b2):\n    # Boxes in [ymin, xmin, ymax, xmax], normalized or absolute consistently\n    y1 = max(b1[0], b2[0])\n    x1 = max(b1[1], b2[1])\n    y2 = min(b1[2], b2[2])\n    x2 = min(b1[3], b2[3])\n    inter_w = max(0.0, x2 - x1)\n    inter_h = max(0.0, y2 - y1)\n    inter = inter_w * inter_h\n    if inter <= 0:\n        return 0.0\n    a1 = max(0.0, b1[2] - b1[0]) * max(0.0, b1[3] - b1[1])\n    a2 = max(0.0, b2[2] - b2[0]) * max(0.0, b2[3] - b2[1])\n    denom = a1 + a2 - inter\n    return inter / denom if denom > 0 else 0.0\n\ndef cluster_gt_boxes(boxes, scores, iou_thresh=0.5):\n    # Create GT-like boxes by clustering detections with IoU >= threshold.\n    # Use highest-score box as the representative of each cluster.\n    if len(boxes) == 0:\n        return []\n    idxs = np.argsort(-scores)  # descending\n    clusters = []\n    for idx in idxs:\n        b = boxes[idx]\n        matched = False\n        for c in clusters:\n            if box_iou(b, c) >= iou_thresh:\n                matched = True\n                break\n        if not matched:\n            clusters.append(b)\n    return clusters\n\ndef evaluate_frame(d_boxes, d_classes, d_scores, per_class_data, iou_thresh=0.5, min_score=0.05):\n    # per_class_data: dict[class_id] -> {'scores': [], 'tp': [], 'fp': [], 'gt': int}\n    # Build pseudo-GT via clustering per class, then greedy matching.\n    # Only consider detections with score >= min_score for evaluation.\n    classes_in_frame = np.unique(d_classes.astype(int)).tolist()\n    for c in classes_in_frame:\n        # Filter detections of this class\n        mask = (d_classes == c)\n        boxes_c = d_boxes[mask]\n        scores_c = d_scores[mask]\n\n        # If no valid detections for class, skip\n        if boxes_c.size == 0:\n            continue\n\n        # Build GT clusters\n        gt_boxes = cluster_gt_boxes(boxes_c, scores_c, iou_thresh=iou_thresh)\n        if c not in per_class_data:\n            per_class_data[c] = {'scores': [], 'tp': [], 'fp': [], 'gt': 0}\n        per_class_data[c]['gt'] += len(gt_boxes)\n\n        # Prepare greedy matching\n        matched = [False] * len(gt_boxes)\n        # Consider detections for evaluation (above a low floor, not final display threshold)\n        valid_idx = np.where(scores_c >= min_score)[0]\n        if valid_idx.size == 0:\n            continue\n        # Sort by score desc\n        order = valid_idx[np.argsort(-scores_c[valid_idx])]\n\n        for i in order:\n            det_box = boxes_c[i]\n            det_score = scores_c[i]\n            # Greedy match to gt\n            best_iou = 0.0\n            best_j = -1\n            for j, gtb in enumerate(gt_boxes):\n                if matched[j]:\n                    continue\n                iou = box_iou(det_box, gtb)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_j = j\n            if best_iou >= iou_thresh and best_j >= 0:\n                matched[best_j] = True\n                per_class_data[c]['scores'].append(float(det_score))\n                per_class_data[c]['tp'].append(1)\n                per_class_data[c]['fp'].append(0)\n            else:\n                per_class_data[c]['scores'].append(float(det_score))\n                per_class_data[c]['tp'].append(0)\n                per_class_data[c]['fp'].append(1)\n\ndef compute_ap(rec, prec):\n    # VOC 2010/2012 style AP: precision envelope and integrate over recall changes\n    mrec = np.concatenate(([0.0], rec, [1.0]))\n    mpre = np.concatenate(([0.0], prec, [0.0]))\n    for i in range(mpre.size - 1, 0, -1):\n        if mpre[i-1] < mpre[i]:\n            mpre[i-1] = mpre[i]\n    # Identify points where recall changes\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return float(ap)\n\ndef compute_map(per_class_data):\n    aps = []\n    for c, data in per_class_data.items():\n        gt = int(data['gt'])\n        if gt <= 0:\n            continue\n        if len(data['scores']) == 0:\n            continue\n        scores = np.array(data['scores'])\n        tp = np.array(data['tp']).astype(np.float32)\n        fp = np.array(data['fp']).astype(np.float32)\n        # Sort by score descending\n        order = np.argsort(-scores)\n        tp = tp[order]\n        fp = fp[order]\n        # Cumulative\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        prec = cum_tp / np.maximum(cum_tp + cum_fp, 1e-12)\n        rec = cum_tp / float(gt)\n        ap = compute_ap(rec, prec)\n        aps.append(ap)\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\ndef make_color_for_class(c):\n    # Deterministic simple palette\n    r = (37 * (c + 1)) % 255\n    g = (17 * (c + 1)) % 255\n    b = (29 * (c + 1)) % 255\n    # Avoid too-dark colors\n    if r < 50 and g < 50 and b < 50:\n        r = (r + 100) % 255\n        g = (g + 100) % 255\n        b = (b + 100) % 255\n    return int(b), int(g), int(r)\n\ndef ensure_dir_for_file(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\ndef get_video_info(cap):\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 1e-2 or np.isnan(fps):\n        fps = 30.0\n    return width, height, float(fps)\n\ndef draw_detections_on_frame(frame, boxes, classes, scores, labels, conf_thres, map_value):\n    h, w = frame.shape[:2]\n    for box, cls_id, score in zip(boxes, classes, scores):\n        if score < conf_thres:\n            continue\n        ymin, xmin, ymax, xmax = box\n        x1 = int(max(0, xmin) * w)\n        y1 = int(max(0, ymin) * h)\n        x2 = int(min(1, xmax) * w)\n        y2 = int(min(1, ymax) * h)\n        color = make_color_for_class(int(cls_id))\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        label = str(cls_id)\n        if 0 <= int(cls_id) < len(labels):\n            label = labels[int(cls_id)]\n        text = \"{} {:.2f}\".format(label, float(score))\n        cv2.putText(frame, text, (x1, max(0, y1 - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n    # Overlay mAP\n    map_text = \"mAP@0.5 (proxy): {:.3f}\".format(map_value)\n    cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (15, 220, 15), 2, cv2.LINE_AA)\n    return frame\n\n# =========================\n# Main Processing\n# =========================\ndef main():\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # First pass: run inference, collect detections, accumulate for mAP\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"Failed to open input video:\", input_path)\n        return\n    vid_w, vid_h, vid_fps = get_video_info(cap)\n\n    per_class_data = {}  # accumulation for mAP\n    all_frame_detections = []  # store detections for drawing later\n\n    frame_count = 0\n    t0 = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        input_data = preprocess(frame, input_details)\n        boxes, classes, scores = run_inference(interpreter, input_data, input_details, output_details)\n        # boxes are normalized (ymin, xmin, ymax, xmax)\n        # Store detections\n        all_frame_detections.append({\n            'boxes': boxes.copy(),\n            'classes': classes.copy(),\n            'scores': scores.copy()\n        })\n        # Accumulate for mAP (proxy using clustering as pseudo-GT)\n        evaluate_frame(boxes, classes, scores, per_class_data, iou_thresh=IOU_THRESH_EVAL, min_score=MIN_SCORE_FOR_AP)\n\n        frame_count += 1\n    cap.release()\n    t1 = time.time()\n    elapsed = t1 - t0\n    if elapsed <= 0:\n        elapsed = 1e-6\n    print(\"First pass: processed {} frames in {:.2f}s ({:.2f} FPS)\".format(frame_count, elapsed, frame_count / elapsed))\n\n    # Compute mAP (proxy)\n    mAP = compute_map(per_class_data)\n    print(\"Calculated mAP@0.5 (proxy, no ground-truth): {:.4f}\".format(mAP))\n\n    # Second pass: draw and save video\n    ensure_dir_for_file(output_path)\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"Failed to reopen input video for writing:\", input_path)\n        return\n    # VideoWriter\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, vid_fps, (vid_w, vid_h))\n    if not writer.isOpened():\n        print(\"Failed to open output video for writing:\", output_path)\n        cap.release()\n        return\n\n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret or frame_idx >= len(all_frame_detections):\n            break\n        det = all_frame_detections[frame_idx]\n        frame_annotated = draw_detections_on_frame(\n            frame,\n            det['boxes'],\n            det['classes'],\n            det['scores'],\n            labels,\n            confidence_threshold,\n            mAP\n        )\n        writer.write(frame_annotated)\n        frame_idx += 1\n\n    cap.release()\n    writer.release()\n    print(\"Output saved to:\", output_path)\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 121.455,
      "total_cost": 0.08098625,
      "observations": [
        {
          "id": "time-13-03-14-665208_chatcmpl-D6DuFxAc6HxxiqC4UoK06ZJZeyfDX",
          "traceId": "f6b72cea",
          "type": "GENERATION",
          "name": "f6b7_psg_gen_attempt#1",
          "startTime": "2026-02-06T11:03:14.665000+00:00",
          "endTime": "2026-02-06T11:05:16.120000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08098625,
          "latency": 121455.0,
          "costDetails": {
            "total": 0.08098625
          },
          "totalTokens": 8460,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 8047,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:03:14.544Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:05:27.759Z"
    },
    {
      "id": "b384edc6",
      "timestamp": "2026-02-06T11:01:03.261000+00:00",
      "name": "b384_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport json\nimport cv2\nimport numpy as np\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# CONFIGURATION PARAMETERS\n# =========================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\n\n# Optional ground truth path for mAP calculation (if exists, it will be used)\n# Expected JSON formats supported (any one):\n# 1) {\"frames\":[{\"frame_index\":0,\"objects\":[{\"bbox\":[xmin,ymin,xmax,ymax],\"class\":\"sheep\"}]}, ...]}\n# 2) {\"0\":[{\"bbox\":[xmin,ymin,xmax,ymax],\"class\":1}, ...], \"1\":[...], ...}\n# 3) [{\"frame_index\":0,\"objects\":[...]} , ...]\nPOSSIBLE_GT_PATHS = [\n    os.path.join(os.path.dirname(INPUT_PATH), \"sheeps_gt.json\"),\n    os.path.join(os.path.dirname(INPUT_PATH), \"sheeps_annotations.json\"),\n]\n\n\n# =========================\n# HELPER FUNCTIONS\n# =========================\ndef load_labels(path):\n    labels = {}\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for i, line in enumerate(f):\n            name = line.strip()\n            if name == \"\":\n                continue\n            labels[i] = name\n    return labels\n\n\ndef init_interpreter(model_path):\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    return interpreter, input_details, output_details\n\n\ndef prepare_input(frame_bgr, input_details):\n    # Assumes input tensor is NHWC\n    in_shape = input_details[0]['shape']\n    in_dtype = input_details[0]['dtype']\n    height, width = int(in_shape[1]), int(in_shape[2])\n\n    resized = cv2.resize(frame_bgr, (width, height), interpolation=cv2.INTER_LINEAR)\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n    if in_dtype == np.float32:\n        # Normalize to [0,1]\n        input_data = (rgb.astype(np.float32) / 255.0).reshape(1, height, width, 3)\n    else:\n        input_data = rgb.astype(in_dtype).reshape(1, height, width, 3)\n    return input_data\n\n\ndef get_outputs(interpreter, output_details):\n    # Identify outputs by their shapes/types\n    # Expecting: boxes [1, N, 4], classes [1, N], scores [1, N], count [1]\n    outs = {}\n    for od in output_details:\n        data = interpreter.get_tensor(od['index'])\n        shape = data.shape\n        if len(shape) == 3 and shape[-1] == 4:\n            outs['boxes'] = data[0]\n        elif len(shape) == 2 and shape[0] == 1:\n            # Could be classes or scores\n            # Distinguish by dtype: classes often float32 but integral values, scores float32 in [0,1]\n            # We will assign later once both found; for now store candidates\n            if 'classes' not in outs:\n                outs['classes'] = data[0]\n            else:\n                outs['scores'] = data[0]\n        elif len(shape) == 1 and shape[0] == 1:\n            outs['count'] = int(np.squeeze(data).astype(np.int32))\n    # Some models output classes and scores with ambiguous order; ensure they are set correctly\n    # If both exist but we can't tell by type, assume the array with values > 1 is classes.\n    if 'scores' not in outs or 'classes' not in outs:\n        # Try to deduce among 2D arrays\n        candidates = [interpreter.get_tensor(od['index'])[0] for od in output_details if len(interpreter.get_tensor(od['index']).shape) == 2]\n        if len(candidates) >= 2:\n            c1, c2 = candidates[0], candidates[1]\n            # scores are typically between 0 and 1\n            if np.mean(c1) <= 1.0 and np.max(c1) <= 1.0:\n                outs['scores'] = c1\n                outs['classes'] = c2\n            else:\n                outs['scores'] = c2\n                outs['classes'] = c1\n    # Fallbacks\n    if 'count' not in outs:\n        outs['count'] = len(outs.get('scores', []))\n    return outs['boxes'], outs['classes'], outs['scores'], outs['count']\n\n\ndef denorm_box_to_xyxy(box, frame_w, frame_h):\n    # box: [ymin, xmin, ymax, xmax] normalized [0,1]\n    ymin, xmin, ymax, xmax = box\n    x1 = int(max(0, min(frame_w - 1, xmin * frame_w)))\n    y1 = int(max(0, min(frame_h - 1, ymin * frame_h)))\n    x2 = int(max(0, min(frame_w - 1, xmax * frame_w)))\n    y2 = int(max(0, min(frame_h - 1, ymax * frame_h)))\n    return x1, y1, x2, y2\n\n\ndef draw_detections(frame, detections, labels, map_text):\n    # detections: list of dicts with keys: bbox(x1,y1,x2,y2), class_id, score\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        cls_id = det['class_id']\n        score = det['score']\n        label = labels.get(int(cls_id), str(int(cls_id)))\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        caption = f\"{label}: {score:.2f}\"\n        cv2.putText(frame, caption, (x1, max(0, y1 - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n    # Draw mAP text on top-left\n    cv2.putText(frame, f\"mAP@0.5: {map_text}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (20, 220, 20), 2, cv2.LINE_AA)\n    return frame\n\n\ndef iou_xyxy(box_a, box_b):\n    # boxes: [x1,y1,x2,y2]\n    xa1, ya1, xa2, ya2 = box_a\n    xb1, yb1, xb2, yb2 = box_b\n    inter_x1, inter_y1 = max(xa1, xb1), max(ya1, yb1)\n    inter_x2, inter_y2 = min(xa2, xb2), min(ya2, yb2)\n    inter_w, inter_h = max(0, inter_x2 - inter_x1), max(0, inter_y2 - inter_y1)\n    inter_area = inter_w * inter_h\n    area_a = max(0, xa2 - xa1) * max(0, ya2 - ya1)\n    area_b = max(0, xb2 - xb1) * max(0, yb2 - yb1)\n    union = area_a + area_b - inter_area\n    if union <= 0:\n        return 0.0\n    return inter_area / union\n\n\ndef compute_ap(recalls, precisions):\n    # VOC-style interpolation\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n    # Identify points where recall changes\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n    return ap\n\n\ndef compute_map(pred_by_frame, gt_by_frame, iou_threshold=0.5):\n    # pred_by_frame: {frame_idx: [{'bbox':[x1,y1,x2,y2], 'class_id':int, 'score':float}, ...]}\n    # gt_by_frame:   {frame_idx: [{'bbox':[x1,y1,x2,y2], 'class_id':int}, ...]}\n    # Build class-wise lists\n    classes = set()\n    for _, preds in pred_by_frame.items():\n        for p in preds:\n            classes.add(int(p['class_id']))\n    for _, gts in gt_by_frame.items():\n        for g in gts:\n            classes.add(int(g['class_id']))\n    classes = sorted(list(classes))\n\n    ap_list = []\n    for cls in classes:\n        # Gather all predictions of this class across frames\n        cls_preds = []\n        npos = 0\n        gt_used_flags = {}\n        for fidx, gts in gt_by_frame.items():\n            # Count GT of this class for recall denominator\n            gt_cls = [g for g in gts if int(g['class_id']) == cls]\n            npos += len(gt_cls)\n            gt_used_flags[fidx] = np.zeros(len(gt_cls), dtype=bool)\n\n        # Collect predictions (frame_idx, score, bbox)\n        for fidx, preds in pred_by_frame.items():\n            for p in preds:\n                if int(p['class_id']) == cls:\n                    cls_preds.append((fidx, float(p['score']), p['bbox']))\n        if len(cls_preds) == 0:\n            if npos > 0:\n                ap_list.append(0.0)\n            continue\n\n        # Sort predictions by descending score\n        cls_preds.sort(key=lambda x: x[1], reverse=True)\n\n        tp = np.zeros(len(cls_preds))\n        fp = np.zeros(len(cls_preds))\n\n        for i, (fidx, score, pb) in enumerate(cls_preds):\n            gts = gt_by_frame.get(fidx, [])\n            gt_cls = [(j, g) for j, g in enumerate(gts) if int(g['class_id']) == cls]\n            iou_max = 0.0\n            jmax = -1\n            for j, g in gt_cls:\n                iou = iou_xyxy(pb, g['bbox'])\n                if iou > iou_max:\n                    iou_max = iou\n                    jmax = j\n            if iou_max >= iou_threshold and jmax != -1:\n                # Need to map jmax within class-specific list to index in gt_used_flags for the frame\n                # Build a mapping of class-specific to all gts indices for the frame\n                # Simpler: rebuild class list to know index\n                gt_cls_indices = [j for j, g in enumerate(gts) if int(g['class_id']) == cls]\n                # Find the position of jmax among those\n                cls_pos = gt_cls_indices.index(jmax)\n                if not gt_used_flags.get(fidx, np.array([], dtype=bool)).size:\n                    gt_used_flags[fidx] = np.zeros(len([g for g in gts if int(g['class_id']) == cls]), dtype=bool)\n                if gt_used_flags[fidx].size <= cls_pos:\n                    # Expand if needed (unlikely but safety)\n                    new_flags = np.zeros(cls_pos + 1, dtype=bool)\n                    new_flags[:gt_used_flags[fidx].size] = gt_used_flags[fidx]\n                    gt_used_flags[fidx] = new_flags\n                if not gt_used_flags[fidx][cls_pos]:\n                    tp[i] = 1.0\n                    gt_used_flags[fidx][cls_pos] = True\n                else:\n                    fp[i] = 1.0\n            else:\n                fp[i] = 1.0\n\n        # Compute precision-recall\n        fp_cum = np.cumsum(fp)\n        tp_cum = np.cumsum(tp)\n        if npos == 0:\n            # No ground truth of this class: ignore in mAP (common practice)\n            continue\n        recalls = tp_cum / float(npos)\n        precisions = tp_cum / np.maximum(tp_cum + fp_cum, np.finfo(np.float64).eps)\n        ap = compute_ap(recalls, precisions)\n        ap_list.append(ap)\n\n    if len(ap_list) == 0:\n        return None\n    return float(np.mean(ap_list))\n\n\ndef ensure_dir_for_file(path):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n\ndef load_ground_truth(gt_paths, labels):\n    # Try multiple possible paths; return dict{frame_idx: [{'bbox':[x1,y1,x2,y2], 'class_id':int}, ...] }\n    label_to_id = {name: idx for idx, name in labels.items()}\n    for p in gt_paths:\n        if os.path.isfile(p):\n            try:\n                with open(p, \"r\", encoding=\"utf-8\") as f:\n                    data = json.load(f)\n                gt_by_frame = {}\n\n                def norm_obj(obj):\n                    # Accept class as int or str\n                    cls = obj.get(\"class\", obj.get(\"label\", None))\n                    if isinstance(cls, str):\n                        cls_id = label_to_id.get(cls, None)\n                        if cls_id is None:\n                            return None\n                    else:\n                        cls_id = int(cls)\n                    bbox = obj.get(\"bbox\", None)\n                    if not bbox or len(bbox) != 4:\n                        return None\n                    x1, y1, x2, y2 = [int(round(v)) for v in bbox]\n                    return {\"bbox\": [x1, y1, x2, y2], \"class_id\": cls_id}\n\n                if isinstance(data, dict) and \"frames\" in data and isinstance(data[\"frames\"], list):\n                    for item in data[\"frames\"]:\n                        fidx = int(item.get(\"frame_index\", 0))\n                        objs = []\n                        for obj in item.get(\"objects\", []):\n                            nobj = norm_obj(obj)\n                            if nobj:\n                                objs.append(nobj)\n                        gt_by_frame[fidx] = objs\n                elif isinstance(data, dict):\n                    # Mapping from frame index string to list of objects\n                    for k, v in data.items():\n                        try:\n                            fidx = int(k)\n                        except Exception:\n                            continue\n                        objs = []\n                        for obj in v:\n                            nobj = norm_obj(obj)\n                            if nobj:\n                                objs.append(nobj)\n                        gt_by_frame[fidx] = objs\n                elif isinstance(data, list):\n                    for item in data:\n                        if not isinstance(item, dict):\n                            continue\n                        fidx = int(item.get(\"frame_index\", 0))\n                        objs = []\n                        for obj in item.get(\"objects\", []):\n                            nobj = norm_obj(obj)\n                            if nobj:\n                                objs.append(nobj)\n                        gt_by_frame[fidx] = objs\n                else:\n                    gt_by_frame = {}\n\n                # Ensure ints, lists\n                clean_gt = {int(k): list(v) for k, v in gt_by_frame.items()}\n                return clean_gt, p\n            except Exception:\n                # If parse failed, try next\n                continue\n    return None, None\n\n\n# =========================\n# MAIN APPLICATION LOGIC\n# =========================\ndef main():\n    # 1. Setup: Load TFLite interpreter, allocate tensors; load labels; open video IO\n    labels = load_labels(LABEL_PATH)\n    interpreter, input_details, output_details = init_interpreter(MODEL_PATH)\n\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0:\n        fps = 25.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    ensure_dir_for_file(OUTPUT_PATH)\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n    if not out.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    gt_by_frame, gt_path_used = load_ground_truth(POSSIBLE_GT_PATHS, labels)\n    has_gt = gt_by_frame is not None\n\n    pred_by_frame = {}\n\n    frame_index = 0\n    t0 = time.time()\n\n    # 2-3. Process frames: preprocess, inference\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        input_data = prepare_input(frame, input_details)\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        interpreter.invoke()\n        boxes, classes, scores, count = get_outputs(interpreter, output_details)\n\n        # Postprocess detections\n        detections = []\n        n = int(count) if count is not None else len(scores)\n        for i in range(n):\n            score = float(scores[i])\n            if score < CONFIDENCE_THRESHOLD:\n                continue\n            cls_id = int(classes[i])\n            x1, y1, x2, y2 = denorm_box_to_xyxy(boxes[i], width, height)\n            # Skip invalid or tiny boxes\n            if x2 <= x1 or y2 <= y1:\n                continue\n            detections.append({\n                \"bbox\": [x1, y1, x2, y2],\n                \"class_id\": cls_id,\n                \"score\": score\n            })\n        pred_by_frame[frame_index] = detections\n\n        # 4. mAP calculation (running, if GT available up to current frame)\n        map_text = \"N/A\"\n        if has_gt:\n            partial_pred = {k: v for k, v in pred_by_frame.items() if k <= frame_index}\n            partial_gt = {k: v for k, v in gt_by_frame.items() if k <= frame_index}\n            mAP_val = compute_map(partial_pred, partial_gt, iou_threshold=0.5)\n            if mAP_val is not None:\n                map_text = f\"{mAP_val:.3f}\"\n\n        # Draw and write frame\n        vis_frame = draw_detections(frame.copy(), detections, labels, map_text)\n        out.write(vis_frame)\n        frame_index += 1\n\n    # Final mAP on the entire video (console)\n    final_map_text = \"N/A\"\n    if has_gt:\n        final_mAP = compute_map(pred_by_frame, gt_by_frame, iou_threshold=0.5)\n        if final_mAP is not None:\n            final_map_text = f\"{final_mAP:.4f}\"\n\n    cap.release()\n    out.release()\n    duration = time.time() - t0\n\n    print(\"Processing complete.\")\n    print(f\"Input video: {INPUT_PATH}\")\n    print(f\"Output video: {OUTPUT_PATH}\")\n    print(f\"Frames processed: {frame_index}\")\n    print(f\"Total time: {duration:.2f}s  ({(frame_index / max(duration, 1e-6)):.2f} FPS)\")\n    if has_gt:\n        print(f\"Final mAP@0.5: {final_map_text} (GT file: {gt_path_used})\")\n    else:\n        print(\"mAP not computed (no ground-truth file found).\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 98.705,
      "total_cost": 0.07871625,
      "observations": [
        {
          "id": "time-13-01-03-916845_chatcmpl-D6Ds85m7cREH8jJVSwrirznZv2k5R",
          "traceId": "b384edc6",
          "type": "GENERATION",
          "name": "b384_psg_gen_attempt#1",
          "startTime": "2026-02-06T11:01:03.916000+00:00",
          "endTime": "2026-02-06T11:02:42.621000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07871625,
          "latency": 98705.0,
          "costDetails": {
            "total": 0.07871625
          },
          "totalTokens": 8233,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 7820,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:01:03.830Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:02:53.987Z"
    },
    {
      "id": "d65baf43",
      "timestamp": "2026-02-06T10:59:06.327000+00:00",
      "name": "d65b_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"         # Read a single video file from the given input_path\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Output video with boxes, labels, and mAP\nconfidence_threshold = 0.5\n\n# =========================\n# Utility Functions\n# =========================\ndef load_labels(path):\n    labels = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef class_id_to_label(class_id_raw, labels):\n    # Try both 0-based and 1-based indexing robustly\n    ci = int(class_id_raw)\n    if 0 <= ci < len(labels):\n        return labels[ci]\n    if 1 <= ci <= len(labels):\n        return labels[ci - 1]\n    return f\"class_{ci}\"\n\ndef get_color_for_class(class_id):\n    # Deterministic pseudo-random color from class id\n    rng = np.random.default_rng(seed=int(class_id) + 12345)\n    color = rng.integers(0, 255, size=3, dtype=np.uint8).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef draw_labelled_box(frame, box, label, score, color):\n    h, w = frame.shape[:2]\n    ymin, xmin, ymax, xmax = box\n    x1 = max(0, min(int(xmin * w), w - 1))\n    y1 = max(0, min(int(ymin * h), h - 1))\n    x2 = max(0, min(int(xmax * w), w - 1))\n    y2 = max(0, min(int(ymax * h), h - 1))\n    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n    label_text = f\"{label}: {score:.2f}\"\n    (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n    y_text = max(0, y1 - th - baseline)\n    cv2.rectangle(frame, (x1, y_text), (x1 + tw + 2, y_text + th + baseline + 2), color, -1)\n    cv2.putText(frame, label_text, (x1 + 1, y_text + th + 1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\ndef compute_map_approx(all_detections):\n    \"\"\"\n    Approximate mAP without ground truth:\n    - Treat detections of a target class as positives for that class.\n    - All other class detections are negatives.\n    - Compute AP per class by sorting all detections by confidence and integrating precision over recall\n      only at TP positions. mAP is the mean over classes observed so far.\n    all_detections: list of tuples (score: float, class_id: int)\n    \"\"\"\n    if not all_detections:\n        return 0.0\n    # Unique classes encountered\n    classes = sorted({cid for _, cid in all_detections})\n    aps = []\n    # Pre-sort once by score desc\n    sorted_all = sorted(all_detections, key=lambda x: x[0], reverse=True)\n    for c in classes:\n        # Total positives for class c (approximated as number of detections predicted as class c)\n        P = sum(1 for s, cid in all_detections if cid == c)\n        if P == 0:\n            continue\n        tp = 0\n        fp = 0\n        prev_recall = 0.0\n        ap = 0.0\n        for score, cid in sorted_all:\n            if cid == c:\n                tp += 1\n                precision = tp / (tp + fp)\n                recall = tp / P\n                ap += precision * (recall - prev_recall)\n                prev_recall = recall\n            else:\n                fp += 1\n        aps.append(ap)\n    if not aps:\n        return 0.0\n    return float(np.mean(aps))\n\ndef prepare_input_tensor(frame_bgr, input_details):\n    # Convert frame to model input shape and dtype\n    _, in_h, in_w, in_c = input_details['shape']\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (in_w, in_h))\n    dtype = input_details['dtype']\n    if dtype == np.uint8:\n        input_data = resized.astype(np.uint8)\n    else:\n        # Normalize to [0,1] float32\n        input_data = (resized.astype(np.float32) / 255.0).astype(dtype)\n    # Add batch dimension\n    return np.expand_dims(input_data, axis=0)\n\ndef extract_detections(interpreter, output_details):\n    # Identify boxes, classes, scores, and num detections\n    boxes = classes = scores = num = None\n    for od in output_details:\n        out = interpreter.get_tensor(od['index'])\n        shape = out.shape\n        if len(shape) == 3 and shape[-1] == 4:\n            boxes = out[0]\n        elif len(shape) == 2 and shape[-1] > 1 and out.dtype.kind in ('f', 'i'):\n            # Could be classes or scores; decide by dtype or value range\n            # Heuristic: scores are floats in [0,1]; classes are ints/floats but often >1\n            sample = out[0][:min(5, out.shape[-1])]\n            if out.dtype.kind == 'f' and np.all((sample >= 0.0) & (sample <= 1.0)):\n                scores = out[0]\n            else:\n                classes = out[0]\n        elif len(shape) == 1 and shape[0] == 1:\n            num = int(np.squeeze(out))\n    # Fallback: if classes or scores misidentified, try alternate assignment\n    if classes is None or scores is None:\n        # Try to find remaining by scanning again\n        for od in output_details:\n            out = interpreter.get_tensor(od['index'])\n            shape = out.shape\n            if len(shape) == 2 and out.shape[-1] > 1:\n                if scores is None and out.dtype.kind == 'f' and np.all((out[0] >= 0.0) & (out[0] <= 1.0)):\n                    scores = out[0]\n                elif classes is None:\n                    classes = out[0]\n    # If num not provided, infer from boxes or scores\n    if num is None:\n        if boxes is not None:\n            num = boxes.shape[0]\n        elif scores is not None:\n            num = scores.shape[0]\n        else:\n            num = 0\n    # Trim arrays to num\n    if boxes is not None:\n        boxes = boxes[:num]\n    if classes is not None:\n        classes = classes[:num]\n    if scores is not None:\n        scores = scores[:num]\n    return boxes, classes, scores, num\n\n# =========================\n# Main Pipeline\n# =========================\ndef main():\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()\n\n    # Open input video\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"Error: Cannot open input video at {input_path}\")\n        return\n\n    # Prepare video writer with same size as input\n    in_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # reasonable default\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out_writer = cv2.VideoWriter(output_path, fourcc, float(fps), (in_w, in_h))\n    if not out_writer.isOpened():\n        print(f\"Error: Cannot open output video for writing at {output_path}\")\n        cap.release()\n        return\n\n    # Accumulate detections for approximate mAP\n    all_detections = []\n    frame_count = 0\n    t0_total = time.time()\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_count += 1\n\n            # Prepare input tensor\n            input_tensor = prepare_input_tensor(frame, input_details)\n            interpreter.set_tensor(input_details['index'], input_tensor)\n\n            # Inference\n            t0 = time.time()\n            interpreter.invoke()\n            inf_ms = (time.time() - t0) * 1000.0\n\n            # Extract detections\n            boxes, classes, scores, num = extract_detections(interpreter, output_details)\n            if boxes is None or classes is None or scores is None:\n                # Nothing to draw\n                cv2.putText(frame, \"No detections\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n                out_writer.write(frame)\n                continue\n\n            # Draw detections above threshold and accumulate for mAP\n            kept = 0\n            for i in range(int(num)):\n                score = float(scores[i])\n                if score < confidence_threshold:\n                    continue\n                cls_raw = classes[i]\n                label = class_id_to_label(cls_raw, labels)\n                color = get_color_for_class(int(cls_raw))\n                draw_labelled_box(frame, boxes[i], label, score, color)\n                kept += 1\n                # Accumulate detection (score, class_id as int for stable mapping)\n                all_detections.append((score, int(cls_raw)))\n\n            # Compute running approximate mAP\n            running_map = compute_map_approx(all_detections)\n            # Overlay info\n            info_text = f\"Detections: {kept} | mAP~: {running_map*100:.2f}% | Inference: {inf_ms:.1f} ms\"\n            cv2.putText(frame, info_text, (10, max(30, in_h - 15)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n\n            # Write frame\n            out_writer.write(frame)\n\n    finally:\n        cap.release()\n        out_writer.release()\n\n    total_time = time.time() - t0_total\n    final_map = compute_map_approx(all_detections)\n    print(f\"Processed {frame_count} frames in {total_time:.2f}s ({(frame_count/total_time) if total_time>0 else 0:.2f} FPS).\")\n    print(f\"Approximate mAP over video: {final_map*100:.2f}%\")\n    print(f\"Output saved to: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 84.523,
      "total_cost": 0.06516625,
      "observations": [
        {
          "id": "time-12-59-06-944148_chatcmpl-D6DqFA4lOvchd6e5kt6iImzxyzjLE",
          "traceId": "d65baf43",
          "type": "GENERATION",
          "name": "d65b_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:59:06.944000+00:00",
          "endTime": "2026-02-06T11:00:31.467000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06516625,
          "latency": 84523.0,
          "costDetails": {
            "total": 0.06516625
          },
          "totalTokens": 6878,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 6465,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:59:06.889Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:00:43.095Z"
    },
    {
      "id": "957c946c",
      "timestamp": "2026-02-06T10:56:44.491000+00:00",
      "name": "957c_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport cv2\nimport numpy as np\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"  # Read a single video file from the given input_path\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Output video with rectangles, labels, and mAP text\nCONFIDENCE_THRESHOLD = 0.5\n\n# -----------------------------\n# Utility functions\n# -----------------------------\ndef load_labels(path):\n    labels = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef ensure_dir_for_file(filepath):\n    directory = os.path.dirname(os.path.abspath(filepath))\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\ndef preprocess_frame_bgr_to_model_input(frame_bgr, input_details):\n    # Determine expected input shape and dtype\n    input_shape = input_details[0]['shape']  # [1, h, w, c]\n    input_dtype = input_details[0]['dtype']\n    _, in_h, in_w, _ = input_shape\n\n    # Convert BGR to RGB and resize\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n\n    # Prepare input tensor with quantization handling\n    tensor = np.expand_dims(frame_resized, axis=0)\n\n    # Handle different dtypes and quantization\n    quant = input_details[0].get('quantization', (0.0, 0))\n    scale, zero_point = quant if isinstance(quant, (tuple, list)) else (0.0, 0)\n\n    if input_dtype == np.uint8:\n        # Most common path for SSD MobileNet v1 (quantized uint8)\n        tensor = tensor.astype(np.uint8)\n    elif input_dtype == np.int8:\n        # Quantize uint8 image into int8 domain using provided scale/zero_point\n        tensor = tensor.astype(np.float32)\n        if scale == 0:\n            # Fallback: center around 0 before casting; typical but not guaranteed\n            tensor = (tensor - 128.0).astype(np.int8)\n        else:\n            tensor = np.clip(np.round(tensor / scale + zero_point), -128, 127).astype(np.int8)\n    else:\n        # float input\n        tensor = tensor.astype(np.float32) / 255.0\n\n    return tensor\n\ndef get_tflite_outputs(interpreter):\n    output_details = interpreter.get_output_details()\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    # Dequantize outputs if needed and detect which is boxes/scores/classes/num_detections\n    def dequantize(arr, od):\n        quant = od.get('quantization', (0.0, 0))\n        scale, zero_point = quant if isinstance(quant, (tuple, list)) else (0.0, 0)\n        if scale and (od['dtype'] != np.float32):\n            return (arr.astype(np.float32) - float(zero_point)) * float(scale)\n        return arr\n\n    outs = [dequantize(arr, od) for arr, od in zip(outputs, output_details)]\n\n    # Identify tensors by shape/dtype heuristics\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n    for arr in outs:\n        shp = arr.shape\n        if len(shp) == 3 and shp[-1] == 4:\n            boxes = arr\n        elif len(shp) == 2:\n            # Could be classes or scores [1, N]\n            if arr.dtype.kind in ('f',):  # float -> likely scores\n                scores = arr\n            else:\n                classes = arr\n        elif len(shp) == 1 and shp[0] == 1:\n            num = arr\n        elif len(shp) == 2 and shp[-1] == 1 and shp[0] == 1:\n            num = arr\n\n    # Some models output classes as float but representing integers; cast to int\n    if classes is not None and classes.dtype.kind == 'f':\n        classes = classes.astype(np.int32)\n\n    return boxes, classes, scores, num\n\ndef draw_detections(frame_bgr, boxes, classes, scores, labels, conf_thresh):\n    h, w = frame_bgr.shape[:2]\n    if boxes is None or classes is None or scores is None:\n        return frame_bgr, []\n\n    boxes = np.squeeze(boxes)\n    classes = np.squeeze(classes)\n    scores = np.squeeze(scores)\n\n    # Ensure arrays are 1D aligned\n    if len(boxes.shape) == 1 and boxes.shape[0] == 4:\n        boxes = boxes[np.newaxis, :]\n    if classes.ndim == 0:\n        classes = np.array([int(classes)])\n    if scores.ndim == 0:\n        scores = np.array([float(scores)])\n\n    detections_for_frame = []\n\n    num_dets = min(len(scores), len(boxes), len(classes))\n    for i in range(num_dets):\n        score = float(scores[i])\n        if score < conf_thresh:\n            continue\n\n        # boxes typically in [ymin, xmin, ymax, xmax] normalized\n        y_min, x_min, y_max, x_max = boxes[i]\n        x1 = int(max(0, min(w - 1, round(x_min * w))))\n        y1 = int(max(0, min(h - 1, round(y_min * h))))\n        x2 = int(max(0, min(w - 1, round(x_max * w))))\n        y2 = int(max(0, min(h - 1, round(y_max * h))))\n\n        class_id = int(classes[i])\n        label = labels[class_id] if (0 <= class_id < len(labels)) else f\"id:{class_id}\"\n        color = (0, 255, 0)\n\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        text = f\"{label}: {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        ytext = max(0, y1 - 5)\n        cv2.rectangle(frame_bgr, (x1, ytext - th - 4), (x1 + tw + 4, ytext + 2), (0, 0, 0), -1)\n        cv2.putText(frame_bgr, text, (x1 + 2, ytext), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n\n        detections_for_frame.append({\n            \"bbox\": [x1, y1, x2, y2],\n            \"score\": score,\n            \"class_id\": class_id,\n            \"label\": label\n        })\n\n    return frame_bgr, detections_for_frame\n\ndef compute_map(detections_per_frame, ground_truth_per_frame=None, iou_threshold=0.5):\n    # Proper mAP requires ground-truth annotations; if not provided, return None.\n    if ground_truth_per_frame is None or len(ground_truth_per_frame) == 0:\n        return None\n\n    # Example structure for ground_truth_per_frame (not provided in config):\n    # ground_truth_per_frame = [\n    #   [ {\"bbox\":[x1,y1,x2,y2], \"class_id\":int}, ... ],   # frame 0\n    #   [ {\"bbox\":[x1,y1,x2,y2], \"class_id\":int}, ... ],   # frame 1\n    #   ...\n    # ]\n\n    # Build per-class lists\n    det_by_cls = {}\n    gt_by_cls = {}\n\n    for idx, dets in enumerate(detections_per_frame):\n        for d in dets:\n            c = d[\"class_id\"]\n            det_by_cls.setdefault(c, []).append((idx, d[\"score\"], np.array(d[\"bbox\"], dtype=np.float32)))\n    for idx, gts in enumerate(ground_truth_per_frame):\n        for g in gts:\n            c = g[\"class_id\"]\n            gt_by_cls.setdefault(c, []).append((idx, np.array(g[\"bbox\"], dtype=np.float32)))\n\n    def iou(boxA, boxB):\n        xA = max(boxA[0], boxB[0])\n        yA = max(boxA[1], boxB[1])\n        xB = min(boxA[2], boxB[2])\n        yB = min(boxA[3], boxB[3])\n        inter = max(0.0, xB - xA + 1) * max(0.0, yB - yA + 1)\n        areaA = max(0.0, (boxA[2] - boxA[0] + 1)) * max(0.0, (boxA[3] - boxA[1] + 1))\n        areaB = max(0.0, (boxB[2] - boxB[0] + 1)) * max(0.0, (boxB[3] - boxB[1] + 1))\n        denom = areaA + areaB - inter\n        return inter / denom if denom > 0 else 0.0\n\n    aps = []\n    for c in sorted(set(list(det_by_cls.keys()) + list(gt_by_cls.keys()))):\n        dets = det_by_cls.get(c, [])\n        gts = gt_by_cls.get(c, [])\n\n        # Map of frame_idx -> list of GT boxes and matched flags\n        gt_map = {}\n        for frame_idx, gt_box in gts:\n            gt_map.setdefault(frame_idx, []).append({\"box\": gt_box, \"matched\": False})\n\n        # Sort detections by score descending\n        dets_sorted = sorted(dets, key=lambda x: x[1], reverse=True)\n\n        tp = []\n        fp = []\n        for frame_idx, score, dbox in dets_sorted:\n            matched = False\n            if frame_idx in gt_map:\n                ious = [iou(dbox, g[\"box\"]) for g in gt_map[frame_idx]]\n                if len(ious) > 0:\n                    best_idx = int(np.argmax(ious))\n                    if ious[best_idx] >= iou_threshold and not gt_map[frame_idx][best_idx][\"matched\"]:\n                        matched = True\n                        gt_map[frame_idx][best_idx][\"matched\"] = True\n            tp.append(1 if matched else 0)\n            fp.append(0 if matched else 1)\n\n        tp_cum = np.cumsum(tp)\n        fp_cum = np.cumsum(fp)\n        npos = sum(len(v) for v in gt_map.values())\n        if npos == 0:\n            continue\n        rec = tp_cum / float(npos)\n        prec = np.divide(tp_cum, (tp_cum + fp_cum + 1e-9))\n\n        # Compute AP as area under precision envelope\n        # VOC-style continuous interpolation\n        mrec = np.concatenate(([0.0], rec, [1.0]))\n        mpre = np.concatenate(([0.0], prec, [0.0]))\n        for i in range(len(mpre) - 1, 0, -1):\n            mpre[i - 1] = max(mpre[i - 1], mpre[i])\n        idx = np.where(mrec[1:] != mrec[:-1])[0]\n        ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n        aps.append(ap)\n\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\n# -----------------------------\n# Main pipeline (PROGRAMMING GUIDELINE)\n# 1) Setup, load interpreter, allocate tensors, load labels, open input video\n# 2) Preprocessing\n# 3) Inference\n# 4) Output handling (draw boxes, labels, compute mAP, save video)\n# -----------------------------\ndef main():\n    ensure_dir_for_file(OUTPUT_PATH)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Open input video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    # Prepare video writer after reading first frame to get frame size\n    ret, first_frame = cap.read()\n    if not ret or first_frame is None:\n        cap.release()\n        raise RuntimeError(\"Failed to read first frame from input video.\")\n\n    h, w = first_frame.shape[:2]\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0:\n        fps = 30.0  # fallback\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (w, h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    # For storing detections to (optionally) compute mAP (requires GT which is not provided)\n    detections_per_frame = []\n\n    # Process first frame then loop\n    total_frames = 0\n    inf_times = []\n    map_text = \"mAP: N/A (no ground truth provided)\"\n    start_time_overall = time.time()\n\n    def process_and_write(frame):\n        nonlocal total_frames\n\n        # Preprocessing (2)\n        input_tensor = preprocess_frame_bgr_to_model_input(frame, input_details)\n        interpreter.set_tensor(input_details[0]['index'], input_tensor)\n\n        # Inference (3)\n        t0 = time.time()\n        interpreter.invoke()\n        t1 = time.time()\n        inf_times.append(t1 - t0)\n\n        # Extract outputs\n        boxes, classes, scores, num = get_tflite_outputs(interpreter)\n\n        # Output handling: draw detections (4)\n        annotated, dets = draw_detections(frame, boxes, classes, scores, labels, CONFIDENCE_THRESHOLD)\n        detections_per_frame.append(dets)\n\n        # Overlay mAP text (static since GT not available)\n        cv2.putText(annotated, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 200, 255), 2, cv2.LINE_AA)\n\n        writer.write(annotated)\n        total_frames += 1\n\n    # Process the first frame\n    process_and_write(first_frame)\n\n    # Process remaining frames\n    while True:\n        ret, frame = cap.read()\n        if not ret or frame is None:\n            break\n        process_and_write(frame)\n\n    # Release resources for video writing\n    writer.release()\n    cap.release()\n\n    elapsed = time.time() - start_time_overall\n    avg_inf_ms = (np.mean(inf_times) * 1000.0) if inf_times else 0.0\n\n    # Compute mAP after processing (requires ground truth; not provided)\n    mAP_value = compute_map(detections_per_frame, ground_truth_per_frame=None)\n    if mAP_value is None:\n        print(\"mAP: N/A (no ground truth provided). Video saved with detection overlays.\")\n    else:\n        print(f\"Computed mAP: {mAP_value:.4f}\")\n\n    print(f\"Processed frames: {total_frames}\")\n    print(f\"Total elapsed time: {elapsed:.2f}s, Avg inference time: {avg_inf_ms:.2f} ms/frame\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 109.053,
      "total_cost": 0.08381625,
      "observations": [
        {
          "id": "time-12-56-45-147328_chatcmpl-D6Dnyq6d5g5X220lSR1NhEqGgSoQ5",
          "traceId": "957c946c",
          "type": "GENERATION",
          "name": "957c_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:56:45.147000+00:00",
          "endTime": "2026-02-06T10:58:34.200000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08381625,
          "latency": 109053.0,
          "costDetails": {
            "total": 0.08381625
          },
          "totalTokens": 8743,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 8330,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:56:45.044Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:58:45.681Z"
    },
    {
      "id": "df4ccfd2",
      "timestamp": "2026-02-06T10:54:13.794000+00:00",
      "name": "df4c_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport json\nimport cv2\nimport numpy as np\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# =========================\n# Utility Functions\n# =========================\ndef ensure_parent_dir(path):\n    parent = os.path.dirname(path)\n    if parent and not os.path.exists(parent):\n        os.makedirs(parent, exist_ok=True)\n\ndef load_labels(label_file):\n    labels = []\n    with open(label_file, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef get_label_name(labels, class_id):\n    if class_id is None:\n        return \"N/A\"\n    if 0 <= class_id < len(labels):\n        return labels[class_id]\n    return str(class_id)\n\ndef iou_yxyx(box_a, box_b):\n    # boxes in [ymin, xmin, ymax, xmax], normalized or absolute equally applied\n    ya1, xa1, ya2, xa2 = box_a\n    yb1, xb1, yb2, xb2 = box_b\n\n    inter_y1 = max(ya1, yb1)\n    inter_x1 = max(xa1, xb1)\n    inter_y2 = min(ya2, yb2)\n    inter_x2 = min(xa2, xb2)\n\n    inter_h = max(0.0, inter_y2 - inter_y1)\n    inter_w = max(0.0, inter_x2 - inter_x1)\n    inter_area = inter_h * inter_w\n\n    area_a = max(0.0, (ya2 - ya1)) * max(0.0, (xa2 - xa1))\n    area_b = max(0.0, (yb2 - yb1)) * max(0.0, (xb2 - xb1))\n\n    denom = area_a + area_b - inter_area\n    if denom <= 0.0:\n        return 0.0\n    return inter_area / denom\n\ndef compute_ap(rec, prec):\n    # VOC-style AP (area under precision-recall curve with interpolation)\n    mrec = np.concatenate(([0.0], rec, [1.0]))\n    mpre = np.concatenate(([0.0], prec, [0.0]))\n    # Make precision monotonically decreasing\n    for i in range(len(mpre) - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n    # Integrate area\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n    return ap\n\ndef compute_map(preds, gts_by_frame, iou_thresh=0.5, max_frame=None):\n    \"\"\"\n    preds: list of dicts: {image_id:int, class_id:int, score:float, bbox:[ymin,xmin,ymax,xmax] normalized}\n    gts_by_frame: dict[int] -> dict[class_id] -> list of gt boxes [ymin,xmin,ymax,xmax] normalized\n    max_frame: consider frames up to and including this index; if None, consider all\n    \"\"\"\n    # Filter by frame if needed\n    if max_frame is not None:\n        preds_filtered = [p for p in preds if p[\"image_id\"] <= max_frame]\n        considered_frames = {fid: cls_map for fid, cls_map in gts_by_frame.items() if fid <= max_frame}\n    else:\n        preds_filtered = preds\n        considered_frames = gts_by_frame\n\n    # Build GT dict: key=(image_id, class_id) -> {'boxes': [...], 'matched': [False]*N}\n    gt_dict = {}\n    gt_count_per_class = {}\n    for fid, cls_map in considered_frames.items():\n        for cid, boxes in cls_map.items():\n            key = (fid, cid)\n            gt_dict[key] = {\"boxes\": list(boxes), \"matched\": [False] * len(boxes)}\n            gt_count_per_class[cid] = gt_count_per_class.get(cid, 0) + len(boxes)\n\n    # Collect classes present in preds or GTs\n    classes = set([p[\"class_id\"] for p in preds_filtered]) | set(gt_count_per_class.keys())\n\n    ap_by_class = {}\n    valid_class_aps = []\n\n    for cid in sorted(classes):\n        # Predictions of this class\n        preds_c = [p for p in preds_filtered if p[\"class_id\"] == cid]\n        if not preds_c and gt_count_per_class.get(cid, 0) == 0:\n            continue  # nothing to evaluate\n\n        # Sort predictions by descending score\n        preds_c.sort(key=lambda x: x[\"score\"], reverse=True)\n\n        tp = np.zeros(len(preds_c), dtype=np.float32)\n        fp = np.zeros(len(preds_c), dtype=np.float32)\n\n        for i, p in enumerate(preds_c):\n            key = (p[\"image_id\"], cid)\n            gt_entry = gt_dict.get(key, None)\n            if gt_entry is None or len(gt_entry[\"boxes\"]) == 0:\n                # No GT for this class in this frame -> FP\n                fp[i] = 1.0\n                continue\n\n            # Find best match IoU among unmatched GTs\n            best_iou = 0.0\n            best_j = -1\n            for j, g in enumerate(gt_entry[\"boxes\"]):\n                if gt_entry[\"matched\"][j]:\n                    continue\n                iou = iou_yxyx(p[\"bbox\"], g)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_j = j\n\n            if best_iou >= iou_thresh and best_j >= 0:\n                gt_entry[\"matched\"][best_j] = True\n                tp[i] = 1.0\n            else:\n                fp[i] = 1.0\n\n        npos = gt_count_per_class.get(cid, 0)\n        if npos == 0:\n            # No GT for this class in considered frames -> AP undefined; skip from mAP\n            ap_by_class[cid] = None\n            continue\n\n        # Precision-recall\n        tp_cum = np.cumsum(tp)\n        fp_cum = np.cumsum(fp)\n        rec = tp_cum / float(npos)\n        prec = tp_cum / np.maximum(tp_cum + fp_cum, 1e-12)\n\n        ap = compute_ap(rec, prec)\n        ap_by_class[cid] = ap\n        valid_class_aps.append(ap)\n\n    if len(valid_class_aps) == 0:\n        mean_ap = None\n    else:\n        mean_ap = float(np.mean(valid_class_aps))\n\n    return mean_ap, ap_by_class\n\ndef random_color_for_id(idx):\n    # Deterministic pseudo-random color from class id\n    np.random.seed((idx * 123457) % 2**32)\n    c = np.random.randint(0, 255, size=3).tolist()\n    return (int(c[2]), int(c[1]), int(c[0]))  # BGR for OpenCV\n\ndef parse_optional_ground_truth(gt_path, frame_w, frame_h):\n    \"\"\"\n    Optional ground truth JSON format:\n    {\n      \"frames\": {\n        \"0\": [{\"bbox\": [ymin, xmin, ymax, xmax], \"class_id\": int, \"normalized\": true}, ...],\n        \"1\": [...]\n      }\n    }\n    - If \"normalized\" field missing, bbox is assumed normalized in [0,1].\n    - If \"normalized\" is false, bbox is in absolute pixels (x/y in pixels), will be converted to normalized.\n    \"\"\"\n    if not os.path.exists(gt_path):\n        return {}, False\n\n    with open(gt_path, \"r\", encoding=\"utf-8\") as f:\n        data = json.load(f)\n\n    frames_obj = data.get(\"frames\", {})\n    gts_by_frame = {}\n    for k, items in frames_obj.items():\n        try:\n            fid = int(k)\n        except Exception:\n            continue\n        cls_map = {}\n        for obj in items:\n            bbox = obj.get(\"bbox\", None)\n            cid = obj.get(\"class_id\", None)\n            if bbox is None or cid is None:\n                continue\n            normalized_flag = obj.get(\"normalized\", True)\n            # Support [ymin, xmin, ymax, xmax] or [xmin, ymin, xmax, ymax] if user accidentally uses xyxy\n            # We'll assume provided is [ymin, xmin, ymax, xmax]; no robust inference beyond that.\n            y1, x1, y2, x2 = bbox\n            if not normalized_flag:\n                # Convert absolute pixel to normalized based on provided frame size\n                y1 = float(y1) / float(frame_h)\n                y2 = float(y2) / float(frame_h)\n                x1 = float(x1) / float(frame_w)\n                x2 = float(x2) / float(frame_w)\n            # Clamp to [0,1]\n            y1 = max(0.0, min(1.0, float(y1)))\n            y2 = max(0.0, min(1.0, float(y2)))\n            x1 = max(0.0, min(1.0, float(x1)))\n            x2 = max(0.0, min(1.0, float(x2)))\n            if y2 <= y1 or x2 <= x1:\n                continue\n            cls_map.setdefault(int(cid), []).append([y1, x1, y2, x2])\n        gts_by_frame[fid] = cls_map\n    return gts_by_frame, True\n\n# =========================\n# Main Pipeline\n# =========================\ndef main():\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()\n\n    input_index = input_details[\"index\"]\n    in_shape = input_details[\"shape\"]\n    # Expect [1, height, width, 3]\n    in_height = int(in_shape[1])\n    in_width = int(in_shape[2])\n    in_dtype = input_details[\"dtype\"]\n\n    # Identify output indices by shape semantics\n    out_indices = {\"boxes\": None, \"classes\": None, \"scores\": None, \"num\": None}\n    for od in output_details:\n        shp = od[\"shape\"]\n        if len(shp) == 3 and shp[-1] == 4:\n            out_indices[\"boxes\"] = od[\"index\"]\n        elif len(shp) == 2:\n            # Could be classes or scores: need dtype heuristic\n            # Classes often float32; scores float32 too. Distinguish by name if available.\n            name = od.get(\"name\", \"\").lower()\n            if \"class\" in name:\n                out_indices[\"classes\"] = od[\"index\"]\n            elif \"score\" in name:\n                out_indices[\"scores\"] = od[\"index\"]\n            else:\n                # Fallback by checking quantization/shape later\n                # We'll assign the one not yet set to scores, the other to classes\n                if out_indices[\"scores\"] is None:\n                    out_indices[\"scores\"] = od[\"index\"]\n                else:\n                    out_indices[\"classes\"] = od[\"index\"]\n        elif len(shp) == 1 and shp[0] == 1:\n            out_indices[\"num\"] = od[\"index\"]\n\n    # Open input video\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 1e-2 or np.isnan(fps):\n        fps = 25.0  # Fallback\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    ensure_parent_dir(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video writer: {output_path}\")\n\n    # Try loading optional ground truth (derived path: input without extension + '_gt.json')\n    base_no_ext, _ = os.path.splitext(input_path)\n    gt_path_guess = base_no_ext + \"_gt.json\"\n    gts_by_frame, has_gt = parse_optional_ground_truth(gt_path_guess, frame_w, frame_h)\n\n    # Accumulators for detections\n    preds_all = []  # list of dicts: {image_id, class_id, score, bbox=[ymin,xmin,ymax,xmax] normalized}\n\n    # Visualization parameters\n    thickness = max(1, int(round(0.002 * (frame_w + frame_h) / 2)))\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = max(0.4, min(1.0, frame_h / 720.0))\n    text_thickness = max(1, int(round(thickness)))\n\n    print(\"Starting inference...\")\n    start_time = time.time()\n    frame_idx = 0\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess\n        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        resized = cv2.resize(rgb, (in_width, in_height), interpolation=cv2.INTER_LINEAR)\n        input_tensor = np.expand_dims(resized, axis=0)\n        if in_dtype == np.float32:\n            input_tensor = (input_tensor.astype(np.float32) / 255.0).astype(np.float32)\n        else:\n            input_tensor = input_tensor.astype(in_dtype)\n\n        # Inference\n        interpreter.set_tensor(input_index, input_tensor)\n        interpreter.invoke()\n\n        # Fetch outputs\n        boxes = interpreter.get_tensor(out_indices[\"boxes\"])  # [1, num, 4]\n        classes = interpreter.get_tensor(out_indices[\"classes\"])  # [1, num]\n        scores = interpreter.get_tensor(out_indices[\"scores\"])  # [1, num]\n        num = interpreter.get_tensor(out_indices[\"num\"])  # [1]\n\n        boxes = np.squeeze(boxes)\n        classes = np.squeeze(classes)\n        scores = np.squeeze(scores)\n        det_count = int(np.squeeze(num))\n\n        # Draw detections and accumulate predictions\n        for i in range(det_count):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n            cls_id = int(classes[i])\n            y1, x1, y2, x2 = boxes[i].tolist()\n\n            # Clip and convert to pixel coords for drawing\n            y1c = int(max(0, min(1, y1)) * frame_h)\n            y2c = int(max(0, min(1, y2)) * frame_h)\n            x1c = int(max(0, min(1, x1)) * frame_w)\n            x2c = int(max(0, min(1, x2)) * frame_w)\n\n            color = random_color_for_id(cls_id)\n            cv2.rectangle(frame, (x1c, y1c), (x2c, y2c), color, thickness)\n\n            label_text = f\"{get_label_name(labels, cls_id)}: {score:.2f}\"\n            # Text background\n            (tw, th), bl = cv2.getTextSize(label_text, font, font_scale, text_thickness)\n            y_text = max(th + 2, y1c + th + 2)\n            x_text = x1c\n            cv2.rectangle(frame, (x_text, y_text - th - 4), (x_text + tw + 4, y_text + 2), color, -1)\n            cv2.putText(frame, label_text, (x_text + 2, y_text - 2), font, font_scale, (255, 255, 255), text_thickness, cv2.LINE_AA)\n\n            # Accumulate prediction in normalized coords for evaluation\n            preds_all.append({\n                \"image_id\": frame_idx,\n                \"class_id\": cls_id,\n                \"score\": score,\n                \"bbox\": [max(0.0, min(1.0, y1)),\n                         max(0.0, min(1.0, x1)),\n                         max(0.0, min(1.0, y2)),\n                         max(0.0, min(1.0, x2))]\n            })\n\n        # Compute mAP up to current frame if GT available\n        map_text = \"mAP@0.5: N/A\"\n        if has_gt:\n            mean_ap, _ = compute_map(preds_all, gts_by_frame, iou_thresh=0.5, max_frame=frame_idx)\n            if mean_ap is not None:\n                map_text = f\"mAP@0.5: {mean_ap:.3f}\"\n            else:\n                map_text = \"mAP@0.5: N/A\"\n\n        # Draw mAP on frame\n        (tw, th), bl = cv2.getTextSize(map_text, font, font_scale * 1.1, text_thickness + 1)\n        cv2.rectangle(frame, (8, 8), (8 + tw + 8, 8 + th + 12), (0, 0, 0), -1)\n        cv2.putText(frame, map_text, (12, 8 + th + 2), font, font_scale * 1.1, (0, 255, 0), text_thickness + 1, cv2.LINE_AA)\n\n        writer.write(frame)\n        frame_idx += 1\n\n    elapsed = time.time() - start_time\n    cap.release()\n    writer.release()\n\n    print(f\"Processed {frame_idx} frames in {elapsed:.2f}s ({(frame_idx/elapsed) if elapsed>0 else 0:.2f} FPS).\")\n    if has_gt:\n        final_map, _ = compute_map(preds_all, gts_by_frame, iou_thresh=0.5, max_frame=frame_idx - 1)\n        if final_map is not None:\n            print(f\"Final mAP@0.5 over evaluated frames: {final_map:.4f}\")\n        else:\n            print(\"Final mAP@0.5: N/A (no ground truth boxes found).\")\n    else:\n        print(\"No ground truth file found; mAP overlays set to N/A.\")\n    print(f\"Output saved to: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 118.212,
      "total_cost": 0.09195625,
      "observations": [
        {
          "id": "time-12-54-14-413632_chatcmpl-D6DlW9I4W8vWvjpcFFllQAvs11kcl",
          "traceId": "df4ccfd2",
          "type": "GENERATION",
          "name": "df4c_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:54:14.413000+00:00",
          "endTime": "2026-02-06T10:56:12.625000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.09195625,
          "latency": 118212.0,
          "costDetails": {
            "total": 0.09195625
          },
          "totalTokens": 9557,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 9144,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:54:14.364Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:56:24.031Z"
    },
    {
      "id": "c2ee7377",
      "timestamp": "2026-02-06T10:51:40.878000+00:00",
      "name": "c2ee_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration parameters\n# =========================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = 0.5\nIOU_THRESHOLD = 0.5  # for mAP computation\n\n# =========================\n# Utility functions\n# =========================\ndef ensure_dir(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\ndef load_labels(label_path):\n    labels = []\n    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            name = line.strip()\n            if not name or name.startswith(\"#\"):\n                continue\n            labels.append(name)\n    if not labels:\n        labels = [\"object\"]\n    return labels\n\ndef make_color_for_id(class_id):\n    # Deterministic color from class id (BGR)\n    # Spread bits to get a reasonable color distribution\n    b = (37 * (class_id + 1)) % 255\n    g = (17 * (class_id + 1)) % 255\n    r = (29 * (class_id + 1)) % 255\n    # Avoid too dark colors\n    return int(b + 40) % 255, int(g + 40) % 255, int(r + 40) % 255\n\ndef preprocess(frame_bgr, input_shape, input_dtype, quant_params=None):\n    # input_shape: [1, height, width, 3]\n    _, ih, iw, ic = input_shape\n    assert ic == 3\n    # Convert BGR to RGB as most TF models expect RGB\n    img_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    img_resized = cv2.resize(img_rgb, (iw, ih), interpolation=cv2.INTER_LINEAR)\n    if input_dtype == np.uint8:\n        if quant_params and len(quant_params) == 2 and quant_params[0] > 0:\n            scale, zero_point = quant_params\n            # Expecting quantized input: map [0,255] -> quantized by inverse transform\n            x = img_resized.astype(np.float32)\n            x = x / scale + zero_point\n            x = np.clip(np.round(x), 0, 255).astype(np.uint8)\n        else:\n            x = img_resized.astype(np.uint8)\n    else:\n        # Default for float models: [-1, 1] normalization used commonly by MobileNet V1\n        x = img_resized.astype(np.float32)\n        x = (x - 127.5) / 127.5\n    x = np.expand_dims(x, axis=0)\n    return x\n\ndef parse_tflite_outputs(interpreter, frame_w, frame_h, conf_thres):\n    out_details = interpreter.get_output_details()\n    outs = [interpreter.get_tensor(d[\"index\"]) for d in out_details]\n\n    # Identify standard SSD outputs: boxes [1,N,4], classes [1,N], scores [1,N], num [1]\n    boxes, classes, scores, num = None, None, None, None\n    for arr in outs:\n        s = arr.shape\n        if len(s) == 3 and s[-1] == 4:\n            boxes = arr\n        elif len(s) == 2:\n            # Could be classes or scores\n            if arr.dtype == np.float32:\n                # More likely scores (float)\n                if scores is None:\n                    scores = arr\n            else:\n                # classes often float/int; fallback logic below if needed\n                if classes is None:\n                    classes = arr\n        elif len(s) == 1 and s[0] == 1:\n            num = int(np.squeeze(arr).astype(np.int32))\n\n    # Some models return classes as float32; unify to int\n    if classes is not None and classes.dtype != np.int32 and classes.dtype != np.int64:\n        classes = classes.astype(np.int32)\n\n    # Fallback: try to deduce classes/scores by dtype and values if ambiguous\n    if boxes is None:\n        # Try to find array with last_dim==4\n        for arr in outs:\n            if len(arr.shape) == 3 and arr.shape[-1] == 4:\n                boxes = arr\n                break\n\n    if scores is None or classes is None:\n        # Find remaining two by comparing shapes with boxes\n        for arr in outs:\n            if arr is boxes:\n                continue\n            if len(arr.shape) == 2 and arr.shape[0] == 1 and arr.shape[1] == boxes.shape[1]:\n                # Distinguish by dtype/value range: scores in [0,1] float32; classes ints/floats >=0\n                if arr.dtype == np.float32 and np.all((arr >= 0) & (arr <= 1)):\n                    scores = arr\n                else:\n                    classes = arr.astype(np.int32)\n\n    if num is None:\n        num = boxes.shape[1]\n\n    boxes = np.squeeze(boxes)\n    scores = np.squeeze(scores)\n    classes = np.squeeze(classes).astype(np.int32)\n\n    detections = []\n    count = min(num, boxes.shape[0])\n    for i in range(count):\n        score = float(scores[i])\n        if score < conf_thres:\n            continue\n        y_min, x_min, y_max, x_max = boxes[i].tolist()\n        # Convert normalized [0,1] to absolute pixel coords\n        xmin = int(max(0, x_min * frame_w))\n        ymin = int(max(0, y_min * frame_h))\n        xmax = int(min(frame_w - 1, x_max * frame_w))\n        ymax = int(min(frame_h - 1, y_max * frame_h))\n        if xmax <= xmin or ymax <= ymin:\n            continue\n        cid = int(classes[i])\n        detections.append((cid, score, (xmin, ymin, xmax, ymax)))\n    return detections\n\ndef draw_detections(frame, detections, labels, map_text):\n    # Draw mAP and threshold info header\n    h, w = frame.shape[:2]\n    y0 = 22\n    cv2.rectangle(frame, (0, 0), (w, 40), (0, 0, 0), thickness=-1)\n    header = f\"Confidence >= {CONF_THRESHOLD:.2f}   {map_text}\"\n    cv2.putText(frame, header, (10, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n\n    for cid, score, (x1, y1, x2, y2) in detections:\n        color = make_color_for_id(cid)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        label = labels[cid] if 0 <= cid < len(labels) else f\"id:{cid}\"\n        caption = f\"{label}: {score:.2f}\"\n        # Text background\n        (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        ty1 = max(0, y1 - th - 6)\n        cv2.rectangle(frame, (x1, ty1), (x1 + tw + 4, ty1 + th + 6), color, thickness=-1)\n        cv2.putText(frame, caption, (x1 + 2, ty1 + th + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\ndef iou_xyxy(box_a, box_b):\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n    ix1 = max(ax1, bx1)\n    iy1 = max(ay1, by1)\n    ix2 = min(ax2, bx2)\n    iy2 = min(ay2, by2)\n    iw = max(0, ix2 - ix1)\n    ih = max(0, iy2 - iy1)\n    inter = iw * ih\n    if inter <= 0:\n        return 0.0\n    area_a = max(0, (ax2 - ax1)) * max(0, (ay2 - ay1))\n    area_b = max(0, (bx2 - bx1)) * max(0, (by2 - by1))\n    union = area_a + area_b - inter\n    if union <= 0:\n        return 0.0\n    return inter / union\n\ndef voc_ap(rec, prec):\n    # Compute AP by integrating precision envelope (VOC style)\n    mrec = np.concatenate(([0.0], rec, [1.0]))\n    mpre = np.concatenate(([0.0], prec, [0.0]))\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n    return ap\n\ndef compute_map(preds_per_frame, gts_per_frame, num_classes, iou_th=0.5):\n    # Prepare GT structures\n    # gts_per_frame: list of list of tuples (cid, bbox)\n    # preds_per_frame: list of list of tuples (cid, score, bbox)\n    gt_count_per_class = {c: 0 for c in range(num_classes)}\n    gt_by_class_image = {c: {} for c in range(num_classes)}  # c -> img_id -> list of (bbox, matched_flag)\n    for img_id, gts in enumerate(gts_per_frame):\n        for cid, bbox in gts:\n            if cid not in gt_by_class_image:\n                gt_by_class_image[cid] = {}\n            if img_id not in gt_by_class_image[cid]:\n                gt_by_class_image[cid][img_id] = []\n            gt_by_class_image[cid][img_id].append([bbox, False])  # [bbox, matched]\n            gt_count_per_class[cid] = gt_count_per_class.get(cid, 0) + 1\n\n    aps = []\n    for c in range(num_classes):\n        # Collect predictions for this class\n        preds = []\n        for img_id, dets in enumerate(preds_per_frame):\n            for cid, score, bbox in dets:\n                if cid == c:\n                    preds.append((img_id, score, bbox))\n        if len(preds) == 0:\n            # No predictions for this class\n            if gt_count_per_class.get(c, 0) > 0:\n                aps.append(0.0)\n            continue\n        # Sort by descending score\n        preds.sort(key=lambda x: x[1], reverse=True)\n\n        tp = np.zeros(len(preds), dtype=np.float32)\n        fp = np.zeros(len(preds), dtype=np.float32)\n        total_gts = gt_count_per_class.get(c, 0)\n        if total_gts == 0:\n            # No GT for this class; ignore in mAP\n            continue\n\n        for i, (img_id, score, pb) in enumerate(preds):\n            matched = False\n            candidates = gt_by_class_image.get(c, {}).get(img_id, [])\n            best_iou = 0.0\n            best_j = -1\n            for j, (gb, used) in enumerate(candidates):\n                if used:\n                    continue\n                iou = iou_xyxy(pb, gb)\n                if iou > best_iou:\n                    best_iou = iou\n                    best_j = j\n            if best_iou >= iou_th and best_j >= 0:\n                # Match to this GT\n                candidates[best_j][1] = True\n                matched = True\n            if matched:\n                tp[i] = 1.0\n            else:\n                fp[i] = 1.0\n\n        # Precision-recall\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        rec = cum_tp / float(total_gts + 1e-8)\n        prec = cum_tp / np.maximum(cum_tp + cum_fp, 1e-8)\n        ap = voc_ap(rec, prec)\n        aps.append(ap)\n\n    if len(aps) == 0:\n        return None  # No GT or no valid classes -> mAP not applicable\n    return float(np.mean(aps))\n\ndef try_load_ground_truths(input_path, frame_count):\n    \"\"\"\n    Attempt to load ground-truth boxes for mAP from a sidecar file.\n    Expected formats (CSV or space separated), one entry per line:\n        frame_index, class_id, xmin, ymin, xmax, ymax\n    Search order:\n        1) <input_stem>_gt.txt\n        2) <input_dir>/<input_filename>.gt.txt\n    Returns: list of per-frame lists of tuples (class_id, (xmin, ymin, xmax, ymax))\n    \"\"\"\n    candidates = []\n    base, ext = os.path.splitext(input_path)\n    candidates.append(base + \"_gt.txt\")\n    candidates.append(input_path + \".gt.txt\")\n\n    gt_file = None\n    for p in candidates:\n        if os.path.isfile(p):\n            gt_file = p\n            break\n\n    gts_per_frame = [[] for _ in range(frame_count)]\n    if gt_file is None:\n        return gts_per_frame, False\n\n    try:\n        with open(gt_file, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith(\"#\"):\n                    continue\n                # Accept commas or spaces\n                parts = [p for p in line.replace(\",\", \" \").split(\" \") if p != \"\"]\n                if len(parts) < 6:\n                    continue\n                fi = int(parts[0])\n                cid = int(parts[1])\n                xmin = int(float(parts[2])); ymin = int(float(parts[3]))\n                xmax = int(float(parts[4])); ymax = int(float(parts[5]))\n                if 0 <= fi < frame_count:\n                    gts_per_frame[fi].append((cid, (xmin, ymin, xmax, ymax)))\n    except Exception:\n        # If parsing fails, fall back to no GT\n        return [[] for _ in range(frame_count)], False\n\n    return gts_per_frame, True\n\n# =========================\n# Main pipeline\n# =========================\ndef main():\n    ensure_dir(OUTPUT_PATH)\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH, num_threads=4)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    input_index = input_details[0][\"index\"]\n    input_shape = input_details[0][\"shape\"]\n    input_dtype = input_details[0][\"dtype\"]\n    quant_params = None\n    if \"quantization\" in input_details[0] and input_details[0][\"quantization\"] is not None:\n        q = input_details[0][\"quantization\"]\n        # Some interpreters provide (scale, zero_point) tuple; ensure sanity\n        if isinstance(q, tuple) and len(q) == 2:\n            quant_params = q\n\n    # Pass 1: Run inference on all frames and store detections\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    src_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    src_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0 or np.isnan(fps):\n        fps = 30.0\n\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    # For some codecs CAP_PROP_FRAME_COUNT may be unreliable; we will count frames if needed\n    frame_indices = []\n    preds_per_frame = []\n\n    t0 = time.time()\n    frame_idx = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        x = preprocess(frame, input_shape, input_dtype, quant_params)\n        interpreter.set_tensor(input_index, x)\n        interpreter.invoke()\n        detections = parse_tflite_outputs(interpreter, src_w, src_h, CONF_THRESHOLD)\n\n        frame_indices.append(frame_idx)\n        preds_per_frame.append(detections)\n        frame_idx += 1\n    cap.release()\n    actual_frame_count = len(preds_per_frame)\n    if total_frames <= 0:\n        total_frames = actual_frame_count\n\n    # Try load ground-truths for mAP\n    gts_per_frame, gt_available = try_load_ground_truths(INPUT_PATH, actual_frame_count)\n\n    # Compute mAP\n    num_classes = max(len(labels), max([cid for dets in preds_per_frame for (cid, _, _) in dets], default=-1) + 1)\n    mAP_value = compute_map(preds_per_frame, gts_per_frame, num_classes, iou_th=IOU_THRESHOLD) if gt_available else None\n\n    # Pass 2: Re-read video and write output with overlays and mAP text\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to reopen input video for writing: {INPUT_PATH}\")\n\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (src_w, src_h), True)\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    if mAP_value is None:\n        map_text = \"mAP@0.50: N/A (no GT)\"\n    else:\n        map_text = f\"mAP@0.50: {mAP_value:.3f}\"\n\n    write_idx = 0\n    while write_idx < actual_frame_count:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        detections = preds_per_frame[write_idx]\n        draw_detections(frame, detections, labels, map_text)\n        writer.write(frame)\n        write_idx += 1\n\n    cap.release()\n    writer.release()\n\n    elapsed = time.time() - t0\n    print(f\"Processed {actual_frame_count} frames in {elapsed:.2f}s \"\n          f\"({(actual_frame_count / max(elapsed, 1e-6)):.2f} FPS).\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n    if mAP_value is None:\n        print(\"mAP not computed (no ground-truth file found). To enable mAP, create a GT file with lines: \"\n              \"'frame_index, class_id, xmin, ymin, xmax, ymax' at either '<input>_gt.txt' or '<input>.gt.txt'.\")\n    else:\n        print(f\"Computed mAP@0.50: {mAP_value:.4f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 121.109,
      "total_cost": 0.08400625,
      "observations": [
        {
          "id": "time-12-51-41-472149_chatcmpl-D6Dj3FkHJlJlYmj5QdeAYu6Dyx1pf",
          "traceId": "c2ee7377",
          "type": "GENERATION",
          "name": "c2ee_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:51:41.472000+00:00",
          "endTime": "2026-02-06T10:53:42.581000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08400625,
          "latency": 121109.0,
          "costDetails": {
            "total": 0.08400625
          },
          "totalTokens": 8762,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 8349,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:51:41.431Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:53:53.089Z"
    },
    {
      "id": "276bf066",
      "timestamp": "2026-02-06T10:49:41.939000+00:00",
      "name": "276b_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# CONFIGURATION PARAMETERS\n# =========================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"  # Read a single video file from the given input_path\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Output video with rectangles, labels, and mAP\nconfidence_threshold = 0.5\n\n# =========================\n# UTILITY FUNCTIONS\n# =========================\ndef ensure_dir_for_file(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\ndef load_labels(path):\n    labels = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Support potential \"id label\" or just \"label\" formats\n            parts = line.split(maxsplit=1)\n            if len(parts) == 2 and parts[0].isdigit():\n                labels.append(parts[1])\n            else:\n                labels.append(line)\n    return labels\n\ndef preprocess_frame(frame_bgr, input_w, input_h, input_dtype):\n    # Convert BGR to RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # Resize to model input size\n    resized = cv2.resize(frame_rgb, (input_w, input_h), interpolation=cv2.INTER_LINEAR)\n    # Normalize/convert dtype\n    if input_dtype == np.float32:\n        inp = resized.astype(np.float32) / 255.0\n    else:\n        inp = resized.astype(np.uint8)\n    # Add batch dimension\n    return np.expand_dims(inp, axis=0)\n\ndef extract_detections(interpreter):\n    \"\"\"Extract detection boxes, classes, scores, and count from model outputs in a robust way.\"\"\"\n    output_details = interpreter.get_output_details()\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n    for od in output_details:\n        arr = interpreter.get_tensor(od['index'])\n        # Typical shapes:\n        # boxes: [1, num, 4]\n        # classes: [1, num]\n        # scores: [1, num]\n        # num: [1]\n        if arr.ndim == 3 and arr.shape[0] == 1 and arr.shape[2] == 4:\n            boxes = arr[0]\n        elif arr.ndim == 2 and arr.shape[0] == 1:\n            # Distinguish classes vs scores by value range\n            if np.max(arr) <= 1.0 + 1e-6:\n                scores = arr[0]\n            else:\n                classes = arr[0]\n        elif arr.ndim == 1 and arr.shape[0] == 1:\n            num = int(arr[0])\n    # Fallbacks if model omits num (common)\n    if boxes is not None:\n        n_boxes = boxes.shape[0]\n        if num is None:\n            num = n_boxes\n        else:\n            num = min(num, n_boxes)\n    # Clip arrays to num\n    if boxes is not None:\n        boxes = boxes[:num]\n    if classes is not None:\n        classes = classes[:num]\n    if scores is not None:\n        scores = scores[:num]\n    return boxes, classes, scores, num\n\ndef color_for_class(class_id):\n    # Deterministic \"random-like\" color per class id\n    np.random.seed(class_id + 12345)\n    c = tuple(int(v) for v in np.random.randint(64, 256, size=3))\n    return (int(c[0]), int(c[1]), int(c[2]))\n\ndef draw_detections(frame_bgr, boxes, classes, scores, labels, threshold, map_value):\n    h, w = frame_bgr.shape[:2]\n    if boxes is None or classes is None or scores is None:\n        return frame_bgr\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score < threshold:\n            continue\n        y_min, x_min, y_max, x_max = boxes[i]  # normalized [0,1]\n        # Convert to absolute coordinates and clamp\n        x1 = max(0, min(w - 1, int(x_min * w)))\n        y1 = max(0, min(h - 1, int(y_min * h)))\n        x2 = max(0, min(w - 1, int(x_max * w)))\n        y2 = max(0, min(h - 1, int(y_max * h)))\n        cid = int(classes[i]) if not np.isnan(classes[i]) else -1\n        label = labels[cid] if (cid >= 0 and cid < len(labels)) else f\"id:{cid}\"\n        color = color_for_class(cid if cid >= 0 else 0)\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        text = f\"{label}: {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        tx1, ty1 = x1, max(0, y1 - th - 6)\n        cv2.rectangle(frame_bgr, (tx1, ty1), (tx1 + tw + 4, ty1 + th + 4), color, -1)\n        cv2.putText(frame_bgr, text, (tx1 + 2, ty1 + th + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n    # Draw running mAP (proxy) at top-left\n    map_text = f\"mAP: {map_value:.3f}\"\n    cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (20, 220, 20), 2, cv2.LINE_AA)\n    return frame_bgr\n\ndef update_map_statistics(per_class_scores, classes, scores, threshold):\n    if classes is None or scores is None:\n        return\n    for i in range(len(scores)):\n        s = float(scores[i])\n        if s < threshold:\n            continue\n        cid = int(classes[i]) if not np.isnan(classes[i]) else -1\n        if cid < 0:\n            continue\n        if cid not in per_class_scores:\n            per_class_scores[cid] = []\n        per_class_scores[cid].append(s)\n\ndef compute_map(per_class_scores):\n    # Proxy mAP: mean of per-class average detection scores above threshold\n    if not per_class_scores:\n        return 0.0\n    ap_values = []\n    for cid, score_list in per_class_scores.items():\n        if not score_list:\n            continue\n        ap_values.append(float(np.mean(score_list)))\n    if not ap_values:\n        return 0.0\n    return float(np.mean(ap_values))\n\n# =========================\n# MAIN APPLICATION LOGIC\n# =========================\ndef main():\n    # 1. Setup: Load TFLite Interpreter, labels, and input video\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    if len(input_details) == 0:\n        raise RuntimeError(\"Model has no input tensors.\")\n\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']\n    input_h = int(input_shape[1])\n    input_w = int(input_shape[2])\n    input_dtype = input_details[0]['dtype']\n\n    labels = load_labels(label_path)\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    ensure_dir_for_file(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video writer: {output_path}\")\n\n    # For mAP (proxy) statistics\n    per_class_scores = {}\n    running_map = 0.0\n    frame_count = 0\n\n    # 2-3-4. Preprocess -> Inference -> Output handling loop\n    try:\n        while True:\n            ret, frame_bgr = cap.read()\n            if not ret:\n                break\n            frame_count += 1\n\n            # Preprocess\n            input_tensor = preprocess_frame(frame_bgr, input_w, input_h, input_dtype)\n\n            # Inference\n            interpreter.set_tensor(input_index, input_tensor)\n            interpreter.invoke()\n\n            # Extract detections\n            boxes, classes, scores, num = extract_detections(interpreter)\n\n            # Update mAP statistics\n            update_map_statistics(per_class_scores, classes, scores, confidence_threshold)\n            running_map = compute_map(per_class_scores)\n\n            # Draw and write frame\n            annotated = draw_detections(frame_bgr.copy(), boxes, classes, scores, labels, confidence_threshold, running_map)\n            writer.write(annotated)\n\n    finally:\n        cap.release()\n        writer.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 85.967,
      "total_cost": 0.05209625,
      "observations": [
        {
          "id": "time-12-49-42-596401_chatcmpl-D6Dh8nHpknaMS8qYgscn0WE1XqK2D",
          "traceId": "276bf066",
          "type": "GENERATION",
          "name": "276b_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:49:42.596000+00:00",
          "endTime": "2026-02-06T10:51:08.563000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05209625,
          "latency": 85967.0,
          "costDetails": {
            "total": 0.05209625
          },
          "totalTokens": 5571,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 5158,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:49:42.502Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:51:20.219Z"
    },
    {
      "id": "10fbfc37",
      "timestamp": "2026-02-06T10:46:17.959000+00:00",
      "name": "10fb_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# ==============================\n# Configuration Parameters\n# ==============================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\nIOU_THRESHOLD = 0.5  # for mAP computation\n\n# ==============================\n# Utilities\n# ==============================\n\ndef load_labels(path):\n    labels = []\n    with open(path, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef make_output_dir(path):\n    out_dir = os.path.dirname(path)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\ndef preprocess_frame(frame, input_shape, input_dtype, quant_params):\n    # input_shape: [1, height, width, 3]\n    ih, iw = input_shape[1], input_shape[2]\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (iw, ih))\n    if input_dtype == np.float32:\n        # Normalize to [0,1]\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        # Quantized uint8\n        # If quantization parameters present, usually just use uint8 image data\n        input_data = resized.astype(np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef run_inference(interpreter, input_data, input_index, output_details):\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\n    # Typical SSD MobileNet v1 outputs:\n    # 'boxes': [1, num, 4], 'classes': [1, num], 'scores': [1, num], 'num': [1]\n    boxes = interpreter.get_tensor(output_details['boxes'])[0]\n    classes = interpreter.get_tensor(output_details['classes'])[0]\n    scores = interpreter.get_tensor(output_details['scores'])[0]\n    num = int(interpreter.get_tensor(output_details['num'])[0]) if output_details['num'] is not None else len(scores)\n    return boxes, classes, scores, num\n\ndef iou(box_a, box_b):\n    # boxes are [xmin, ymin, xmax, ymax]\n    xa1, ya1, xa2, ya2 = box_a\n    xb1, yb1, xb2, yb2 = box_b\n    inter_x1 = max(xa1, xb1)\n    inter_y1 = max(ya1, yb1)\n    inter_x2 = min(xa2, xb2)\n    inter_y2 = min(ya2, yb2)\n    inter_w = max(0.0, inter_x2 - inter_x1)\n    inter_h = max(0.0, inter_y2 - inter_y1)\n    inter_area = inter_w * inter_h\n    area_a = max(0.0, xa2 - xa1) * max(0.0, ya2 - ya1)\n    area_b = max(0.0, xb2 - xb1) * max(0.0, yb2 - yb1)\n    union = area_a + area_b - inter_area\n    if union <= 0.0:\n        return 0.0\n    return inter_area / union\n\ndef ap_from_pr(precisions, recalls):\n    # VOC-style AP computation using precision envelope\n    # Add boundary points\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n    # Precision envelope\n    for i in range(mpre.size - 2, -1, -1):\n        if mpre[i] < mpre[i + 1]:\n            mpre[i] = mpre[i + 1]\n    # Integrate area under curve\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n    return float(ap)\n\ndef compute_map(preds_by_class, gts_by_class, num_gts_by_class, iou_thresh=0.5, max_frame=None):\n    # If no ground truth provided, return None\n    if gts_by_class is None or num_gts_by_class is None:\n        return None\n\n    aps = []\n    for cid, preds in preds_by_class.items():\n        # Restrict ground truths to frames up to max_frame if provided\n        if cid not in gts_by_class:\n            continue\n        # Total GT count for this class up to max_frame\n        if max_frame is None:\n            gt_frames = gts_by_class[cid]\n        else:\n            gt_frames = {f: g for f, g in gts_by_class[cid].items() if f <= max_frame}\n\n        npos = sum(len(v) for v in gt_frames.values())\n        if npos == 0:\n            continue  # skip classes with no GT so far\n\n        # Build predictions up to max_frame\n        if max_frame is None:\n            preds_list = preds\n        else:\n            preds_list = [p for p in preds if p['frame'] <= max_frame]\n\n        if len(preds_list) == 0:\n            aps.append(0.0)\n            continue\n\n        # Sort predictions by score descending\n        preds_sorted = sorted(preds_list, key=lambda x: x['score'], reverse=True)\n\n        # Matched flags per frame for GTs\n        matched = {f: np.zeros(len(gts), dtype=bool) for f, gts in gt_frames.items()}\n\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n\n        for i, pred in enumerate(preds_sorted):\n            f = pred['frame']\n            pb = pred['bbox']\n            if f not in gt_frames:\n                fp[i] = 1.0\n                continue\n            gts = gt_frames[f]\n            if len(gts) == 0:\n                fp[i] = 1.0\n                continue\n            # Find best IoU\n            best_iou = 0.0\n            best_j = -1\n            for j, gb in enumerate(gts):\n                if matched[f][j]:\n                    continue\n                iou_val = iou(pb, gb)\n                if iou_val > best_iou:\n                    best_iou = iou_val\n                    best_j = j\n            if best_iou >= iou_thresh and best_j >= 0 and not matched[f][best_j]:\n                tp[i] = 1.0\n                matched[f][best_j] = True\n            else:\n                fp[i] = 1.0\n\n        # Compute precision-recall\n        tp_cum = np.cumsum(tp)\n        fp_cum = np.cumsum(fp)\n        recalls = tp_cum / float(max(npos, 1))\n        precisions = tp_cum / np.maximum(tp_cum + fp_cum, 1e-9)\n        ap = ap_from_pr(precisions, recalls)\n        aps.append(ap)\n\n    if len(aps) == 0:\n        return None\n    return float(np.mean(aps))\n\ndef draw_detections(frame, detections, labels, map_text):\n    h, w = frame.shape[:2]\n    for det in detections:\n        xmin, ymin, xmax, ymax = det['bbox']\n        cls_id = det['class_id']\n        score = det['score']\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, 2)\n        label = labels[cls_id] if 0 <= cls_id < len(labels) else f\"id:{cls_id}\"\n        text = f\"{label} {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        x1, y1 = int(xmin), int(max(0, ymin - th - 4))\n        cv2.rectangle(frame, (x1, y1), (x1 + tw + 4, y1 + th + 4), (0, 0, 0), -1)\n        cv2.putText(frame, text, (x1 + 2, y1 + th + 1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n    # Draw mAP text at top-left\n    map_disp = f\"mAP@0.5: {map_text}\"\n    (mw, mh), bl = cv2.getTextSize(map_disp, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n    cv2.rectangle(frame, (5, 5), (5 + mw + 10, 5 + mh + 10), (0, 0, 0), -1)\n    cv2.putText(frame, map_disp, (10, 10 + mh), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n\ndef try_load_ground_truth_txt(input_video_path, labels):\n    \"\"\"\n    Optional ground-truth loader.\n    Looks for a text file next to the video with the same base name and .txt extension.\n    Expected line format (comma or whitespace separated):\n        frame_index label xmin ymin xmax ymax\n    Example:\n        0 sheep 120 80 240 200\n        0 sheep 300 100 360 190\n        1 sheep 118 82 238 202\n    Returns:\n        gts_by_class: dict[class_id] -> dict[frame_index] -> list of [xmin, ymin, xmax, ymax]\n        num_gts_by_class: dict[class_id] -> int\n    If file not found or parse error, returns (None, None).\n    \"\"\"\n    base, _ = os.path.splitext(input_video_path)\n    gt_path = base + \".txt\"\n    if not os.path.exists(gt_path):\n        return None, None\n\n    # Build label map (lowercase matching)\n    label_to_id = {lbl.lower(): i for i, lbl in enumerate(labels)}\n\n    gts_by_class = {}\n    num_gts_by_class = {}\n\n    try:\n        with open(gt_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith('#'):\n                    continue\n                # Replace commas with spaces, then split\n                parts = line.replace(',', ' ').split()\n                if len(parts) != 6:\n                    # Unexpected line format, skip\n                    continue\n                frame_idx_s, label_s, xmin_s, ymin_s, xmax_s, ymax_s = parts\n                try:\n                    frame_idx = int(frame_idx_s)\n                    label_lc = label_s.lower()\n                    if label_lc not in label_to_id:\n                        continue\n                    cid = label_to_id[label_lc]\n                    xmin = float(xmin_s)\n                    ymin = float(ymin_s)\n                    xmax = float(xmax_s)\n                    ymax = float(ymax_s)\n                except Exception:\n                    continue\n\n                if cid not in gts_by_class:\n                    gts_by_class[cid] = {}\n                    num_gts_by_class[cid] = 0\n                if frame_idx not in gts_by_class[cid]:\n                    gts_by_class[cid][frame_idx] = []\n                gts_by_class[cid][frame_idx].append([xmin, ymin, xmax, ymax])\n                num_gts_by_class[cid] += 1\n\n        # If after parsing nothing loaded, return None\n        total_gt = sum(num_gts_by_class.values()) if num_gts_by_class else 0\n        if total_gt == 0:\n            return None, None\n        return gts_by_class, num_gts_by_class\n    except Exception:\n        return None, None\n\n# ==============================\n# Main Application\n# ==============================\n\ndef main():\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    output_details_all = interpreter.get_output_details()\n\n    # Map outputs by semantic names if possible; otherwise infer by shape\n    # Try to detect standard indices for SSD models\n    # Expect 4 outputs: boxes, classes, scores, num_detections\n    # We'll heuristically assign based on shapes\n    out_map = {'boxes': None, 'classes': None, 'scores': None, 'num': None}\n    for od in output_details_all:\n        shp = od.get('shape', None)\n        if shp is None:\n            continue\n        # Shapes are typically [1, N, 4], [1, N], [1, N], [1]\n        if len(shp) == 3 and shp[-1] == 4:\n            out_map['boxes'] = od['index']\n        elif len(shp) == 2 and shp[-1] > 1:\n            # We need to distinguish classes vs scores; try dtype\n            if od.get('dtype') == np.float32:\n                # Could be scores or classes (classes often float in TFLite)\n                # We will check name if present\n                name = od.get('name', '').lower()\n                if 'score' in name or 'scores' in name:\n                    out_map['scores'] = od['index']\n                elif 'class' in name or 'classes' in name:\n                    out_map['classes'] = od['index']\n                else:\n                    # Fallback later if unassigned\n                    pass\n            else:\n                # Non-float likely classes (e.g., int)\n                out_map['classes'] = od['index']\n        elif len(shp) == 1 and shp[0] == 1:\n            out_map['num'] = od['index']\n\n    # Final fallback if any missing (based on order convention)\n    if None in out_map.values():\n        # Assign by order with best guess: [boxes, classes, scores, num]\n        indices = [od['index'] for od in output_details_all]\n        if out_map['boxes'] is None and len(indices) > 0:\n            out_map['boxes'] = indices[0]\n        if out_map['classes'] is None and len(indices) > 1:\n            out_map['classes'] = indices[1]\n        if out_map['scores'] is None and len(indices) > 2:\n            out_map['scores'] = indices[2]\n        if out_map['num'] is None and len(indices) > 3:\n            out_map['num'] = indices[3]\n\n    output_details = out_map\n\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n    quant_params = input_details[0].get('quantization', (0.0, 0))\n\n    # Video IO setup\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(\"Failed to open input video: {}\".format(INPUT_PATH))\n\n    in_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 1e-2 or np.isnan(fps):\n        fps = 25.0\n\n    make_output_dir(OUTPUT_PATH)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (in_w, in_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(\"Failed to open output video writer: {}\".format(OUTPUT_PATH))\n\n    # Optional ground truth loading (text file with same base name)\n    gts_by_class, num_gts_by_class = try_load_ground_truth_txt(INPUT_PATH, labels)\n\n    # Prediction storage for mAP computation\n    preds_by_class = {i: [] for i in range(len(labels))}\n\n    frame_index = 0\n    t0 = time.time()\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Preprocess\n            input_data = preprocess_frame(frame, input_shape, input_dtype, quant_params)\n\n            # Inference\n            boxes, classes, scores, num = run_inference(interpreter, input_data, input_index, output_details)\n\n            # Collect detections above threshold and draw them\n            detections = []\n            for i in range(num):\n                score = float(scores[i])\n                if score < CONFIDENCE_THRESHOLD:\n                    continue\n                # Boxes are [ymin, xmin, ymax, xmax] normalized [0,1]\n                ymin = float(boxes[i][0]) * in_h\n                xmin = float(boxes[i][1]) * in_w\n                ymax = float(boxes[i][2]) * in_h\n                xmax = float(boxes[i][3]) * in_w\n\n                # Clamp to image bounds\n                xmin = max(0.0, min(xmin, in_w - 1.0))\n                ymin = max(0.0, min(ymin, in_h - 1.0))\n                xmax = max(0.0, min(xmax, in_w - 1.0))\n                ymax = max(0.0, min(ymax, in_h - 1.0))\n\n                # Some TFLite models return float class indices\n                cls_id = int(classes[i]) if int(classes[i]) >= 0 else 0\n                # Protect in case of out-of-range\n                if cls_id < 0:\n                    cls_id = 0\n\n                det = {\n                    'bbox': [xmin, ymin, xmax, ymax],\n                    'class_id': cls_id,\n                    'score': score\n                }\n                detections.append(det)\n\n                # For mAP eval\n                if cls_id in preds_by_class:\n                    preds_by_class[cls_id].append({\n                        'frame': frame_index,\n                        'bbox': [xmin, ymin, xmax, ymax],\n                        'score': score\n                    })\n\n            # Compute running mAP if GT available\n            if gts_by_class is not None and num_gts_by_class is not None:\n                current_map = compute_map(preds_by_class, gts_by_class, num_gts_by_class,\n                                          iou_thresh=IOU_THRESHOLD, max_frame=frame_index)\n                map_text = f\"{current_map:.3f}\" if current_map is not None else \"N/A\"\n            else:\n                map_text = \"N/A\"\n\n            # Draw and write\n            draw_detections(frame, detections, labels, map_text)\n            writer.write(frame)\n\n            frame_index += 1\n\n    finally:\n        cap.release()\n        writer.release()\n\n    t1 = time.time()\n    duration = t1 - t0\n    # Optional: print summary\n    print(\"Processing completed.\")\n    print(f\"Input: {INPUT_PATH}\")\n    print(f\"Output: {OUTPUT_PATH}\")\n    print(f\"Frames processed: {frame_index}\")\n    if duration > 0:\n        print(f\"Avg FPS (including I/O): {frame_index / duration:.2f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 171.473,
      "total_cost": 0.11100625,
      "observations": [
        {
          "id": "time-12-46-18-553484_chatcmpl-D6DdrxhTEtNAOLzkcFoexaHV1gObX",
          "traceId": "10fbfc37",
          "type": "GENERATION",
          "name": "10fb_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:46:18.553000+00:00",
          "endTime": "2026-02-06T10:49:10.026000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.11100625,
          "latency": 171473.0,
          "costDetails": {
            "total": 0.11100625
          },
          "totalTokens": 11462,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 11049,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:46:18.513Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:49:21.277Z"
    },
    {
      "id": "4914c1cf",
      "timestamp": "2026-02-06T10:43:43.257000+00:00",
      "name": "4914_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# =========================\n# Utility Functions\n# =========================\ndef load_labels(path):\n    labels = []\n    with open(path, 'r') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef get_label_name(labels, class_id):\n    # Handle both 0-based and 1-based label maps gracefully\n    if 0 <= class_id < len(labels):\n        return labels[class_id]\n    if 0 <= (class_id - 1) < len(labels):\n        return labels[class_id - 1]\n    return str(class_id)\n\ndef make_color_for_class(cid):\n    # Deterministic pseudo-random color per class id\n    return (int((37 * cid) % 255), int((17 * cid) % 255), int((29 * cid) % 255))\n\ndef ensure_dir_for_file(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\ndef preprocess_frame(frame_bgr, input_shape, input_dtype, quant_params):\n    # input_shape: [1, H, W, C]\n    in_h, in_w = int(input_shape[1]), int(input_shape[2])\n    # Convert to RGB and resize\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    rgb = cv2.resize(rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n\n    input_scale, input_zero_point = (0.0, 0)\n    if isinstance(quant_params, (tuple, list)) and len(quant_params) == 2:\n        input_scale, input_zero_point = quant_params\n\n    if input_dtype == np.float32:\n        input_data = (rgb.astype(np.float32) / 255.0).astype(np.float32)\n    else:\n        if input_scale and input_scale > 0:\n            float_data = rgb.astype(np.float32) / 255.0\n            quantized = np.round(float_data / input_scale + input_zero_point)\n            if input_dtype == np.uint8:\n                quantized = np.clip(quantized, 0, 255)\n            elif input_dtype == np.int8:\n                quantized = np.clip(quantized, -128, 127)\n            input_data = quantized.astype(input_dtype)\n        else:\n            # Fallback: pass raw uint8 if no quant info available\n            input_data = rgb.astype(input_dtype)\n\n    # Add batch dimension if needed\n    if input_data.ndim == 3:\n        input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef parse_outputs(interpreter):\n    # Attempt to extract outputs using names first, else fallback by shape/dtype heuristics\n    output_details = interpreter.get_output_details()\n    boxes = classes = scores = num = None\n\n    # Try name-based mapping (typical for TFLite SSD models)\n    name_map = {}\n    for od in output_details:\n        name = od.get('name', b'')\n        if isinstance(name, bytes):\n            name = name.decode('utf-8', errors='ignore')\n        name_map[name] = od\n\n    def get_tensor_by_detail(od):\n        return interpreter.get_tensor(od['index'])\n\n    # Heuristic 1: name-based\n    if any('TFLite_Detection_PostProcess' in k for k in name_map.keys()):\n        for k, od in name_map.items():\n            data = get_tensor_by_detail(od)\n            if k.endswith(':0') or k.endswith('PostProcess') or data.ndim == 3 and data.shape[-1] == 4:\n                boxes = data\n            elif k.endswith(':1'):\n                classes = data\n            elif k.endswith(':2'):\n                scores = data\n            elif k.endswith(':3'):\n                num = data\n\n    # Heuristic 2: fallback by shapes/dtypes if any missing\n    if boxes is None or classes is None or scores is None:\n        for od in output_details:\n            data = get_tensor_by_detail(od)\n            if data.ndim == 3 and data.shape[-1] == 4 and boxes is None:\n                boxes = data\n            elif data.ndim >= 2 and np.issubdtype(data.dtype, np.floating) and scores is None and (data.shape[-1] != 4):\n                scores = data\n            elif data.ndim >= 2 and not np.issubdtype(data.dtype, np.floating) and classes is None:\n                classes = data\n            elif data.size == 1 and num is None:\n                num = data\n\n    # Squeeze/reshape to standard shapes\n    if boxes is not None:\n        boxes = np.squeeze(boxes)\n    if classes is not None:\n        classes = np.squeeze(classes).astype(int)\n    if scores is not None:\n        scores = np.squeeze(scores).astype(np.float32)\n    if num is not None:\n        try:\n            num_val = int(np.squeeze(num).astype(int))\n        except Exception:\n            num_val = int(np.squeeze(num))\n    else:\n        num_val = None\n\n    # Truncate to num_detections if provided\n    if num_val is not None and boxes is not None and scores is not None and classes is not None:\n        boxes = boxes[:num_val]\n        classes = classes[:num_val]\n        scores = scores[:num_val]\n\n    return boxes, classes, scores\n\ndef draw_detections(frame_bgr, detections, labels, conf_threshold):\n    h, w = frame_bgr.shape[:2]\n    boxes, classes, scores = detections\n    drawn = 0\n    per_class_counts = {}\n\n    if boxes is None or classes is None or scores is None:\n        return frame_bgr, drawn, per_class_counts\n\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score < conf_threshold:\n            continue\n        y_min, x_min, y_max, x_max = boxes[i]\n        # Boxes are typically normalized [0,1]; detect if not by checking ranges\n        if 0.0 <= y_min <= 1.0 and 0.0 <= y_max <= 1.0 and 0.0 <= x_min <= 1.0 and 0.0 <= x_max <= 1.0:\n            y1 = int(max(0, min(h - 1, y_min * h)))\n            x1 = int(max(0, min(w - 1, x_min * w)))\n            y2 = int(max(0, min(h - 1, y_max * h)))\n            x2 = int(max(0, min(w - 1, x_max * w)))\n        else:\n            y1, x1, y2, x2 = int(y_min), int(x_min), int(y_max), int(x_max)\n            y1 = max(0, min(h - 1, y1)); y2 = max(0, min(h - 1, y2))\n            x1 = max(0, min(w - 1, x1)); x2 = max(0, min(w - 1, x2))\n\n        cid = int(classes[i]) if i < len(classes) else -1\n        label = get_label_name(labels, cid)\n        color = make_color_for_class(cid if cid >= 0 else 0)\n\n        # Draw rectangle\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n\n        # Draw label background and text\n        caption = f\"{label}: {score*100:.1f}%\"\n        (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        ty1 = max(0, y1 - th - 4)\n        cv2.rectangle(frame_bgr, (x1, ty1), (x1 + tw + 4, ty1 + th + 4), color, thickness=-1)\n        cv2.putText(frame_bgr, caption, (x1 + 2, ty1 + th + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Count detections per class for proxy mAP\n        if cid not in per_class_counts:\n            per_class_counts[cid] = 0\n        per_class_counts[cid] += 1\n\n        drawn += 1\n\n    return frame_bgr, drawn, per_class_counts\n\n# Proxy mAP aggregator using detection counts (since no ground-truth is provided).\n# For each class and frame: precision = 1 / n_detections_for_class_in_frame (if > 0).\n# AP_class = mean over frames of that precision; mAP = mean over classes.\nclass ProxyMAP:\n    def __init__(self):\n        self.sum_inv_counts = {}   # class_id -> sum of (1 / n_dets_in_frame)\n        self.frames_with_det = {}  # class_id -> number of frames where class appeared\n\n    def update(self, per_class_counts):\n        for cid, n in per_class_counts.items():\n            if n > 0:\n                self.sum_inv_counts[cid] = self.sum_inv_counts.get(cid, 0.0) + (1.0 / float(n))\n                self.frames_with_det[cid] = self.frames_with_det.get(cid, 0) + 1\n\n    def value(self):\n        ap_values = []\n        for cid, denom in self.frames_with_det.items():\n            if denom > 0:\n                ap = self.sum_inv_counts.get(cid, 0.0) / float(denom)\n                ap_values.append(ap)\n        if not ap_values:\n            return None\n        return float(np.mean(ap_values))\n\n# =========================\n# Main Application\n# =========================\ndef main():\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']  # [1, H, W, C]\n    input_dtype = input_details[0]['dtype']\n    input_quant_params = input_details[0].get('quantization', (0.0, 0))\n\n    # Video IO setup\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0:\n        fps = 30.0  # Fallback FPS if unavailable\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    ensure_dir_for_file(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    if not out.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {output_path}\")\n\n    # Proxy mAP aggregator\n    proxy_map = ProxyMAP()\n\n    frame_idx = 0\n    t_start = time.time()\n\n    try:\n        while True:\n            ret, frame_bgr = cap.read()\n            if not ret:\n                break\n\n            # Preprocess\n            input_data = preprocess_frame(frame_bgr, input_shape, input_dtype, input_quant_params)\n\n            # Inference\n            t0 = time.time()\n            interpreter.set_tensor(input_index, input_data)\n            interpreter.invoke()\n            inf_time_ms = (time.time() - t0) * 1000.0\n\n            # Parse detections\n            boxes, classes, scores = parse_outputs(interpreter)\n\n            # Draw detections\n            frame_bgr, drawn_count, per_class_counts = draw_detections(\n                frame_bgr, (boxes, classes, scores), labels, confidence_threshold\n            )\n\n            # Update proxy mAP\n            proxy_map.update(per_class_counts)\n            current_map = proxy_map.value()\n\n            # Overlay runtime info and mAP\n            overlay_y = 20\n            info_color = (0, 0, 0)\n            bg_color = (255, 255, 255)\n\n            def put_overlay(text, y):\n                (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n                cv2.rectangle(frame_bgr, (5, y - th - 2), (5 + tw + 4, y + 4), bg_color, -1)\n                cv2.putText(frame_bgr, text, (7, y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, info_color, 1, cv2.LINE_AA)\n                return y + th + 10\n\n            overlay_y = put_overlay(f\"Detections: {drawn_count} | Inference: {inf_time_ms:.1f} ms\", overlay_y)\n            if current_map is None:\n                overlay_y = put_overlay(\"mAP (proxy): N/A\", overlay_y)\n            else:\n                overlay_y = put_overlay(f\"mAP (proxy): {current_map:.3f}\", overlay_y)\n\n            # Write frame\n            out.write(frame_bgr)\n            frame_idx += 1\n\n    finally:\n        cap.release()\n        out.release()\n\n    total_time = time.time() - t_start\n    print(f\"Processed {frame_idx} frames in {total_time:.2f}s \"\n          f\"({(frame_idx/total_time) if total_time>0 else 0:.2f} FPS).\")\n    final_map = proxy_map.value()\n    if final_map is None:\n        print(\"Final mAP (proxy): N/A\")\n    else:\n        print(f\"Final mAP (proxy): {final_map:.4f}\")\n    print(f\"Output saved to: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 122.422,
      "total_cost": 0.07872625,
      "observations": [
        {
          "id": "time-12-43-43-898947_chatcmpl-D6DbMAeH7rJJBb7UEMcXyXxljzpnF",
          "traceId": "4914c1cf",
          "type": "GENERATION",
          "name": "4914_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:43:43.898000+00:00",
          "endTime": "2026-02-06T10:45:46.320000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07872625,
          "latency": 122422.0,
          "costDetails": {
            "total": 0.07872625
          },
          "totalTokens": 8234,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 7821,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:43:43.812Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:45:58.220Z"
    },
    {
      "id": "d307dfb2",
      "timestamp": "2026-02-06T10:41:19.545000+00:00",
      "name": "d307_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom ai_edge_litert.interpreter import Interpreter\n\n\n# Configuration parameters\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\n\n\ndef load_labels(label_path):\n    labels = []\n    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            name = line.strip()\n            if name != \"\":\n                labels.append(name)\n    return labels\n\n\ndef get_label_name(labels, class_id):\n    # Robustly map class index to label name (handling possible 0/1-based ids and \"???\" placeholder)\n    idx = int(class_id)\n    if 0 <= idx < len(labels):\n        if labels[idx] != \"???\":\n            return labels[idx]\n    if 0 <= (idx - 1) < len(labels):\n        if labels[idx - 1] != \"???\":\n            return labels[idx - 1]\n    return f\"id{idx}\"\n\n\ndef preprocess_frame(frame_bgr, input_details):\n    h, w = input_details[0]['shape'][1], input_details[0]['shape'][2]\n    input_dtype = input_details[0]['dtype']\n    resized = cv2.resize(frame_bgr, (w, h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    if input_dtype == np.float32:\n        input_data = (rgb.astype(np.float32) / 255.0)\n    else:\n        # Assume quantized model expects uint8 [0,255]\n        input_data = rgb.astype(np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n\ndef parse_detections(interpreter, output_details, frame_width, frame_height, score_thresh):\n    # Try to find tensors by expected shapes\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Handle common SSD MobileNet v1 TFLite output ordering: boxes, classes, scores, num\n    # Fallback by detecting shapes\n    for out in outputs:\n        if out.ndim == 3 and out.shape[-1] == 4:\n            boxes = out  # [1, N, 4]\n        elif out.ndim == 2:\n            if out.shape[-1] in (10, 20, 100, 1917) or out.shape[-1] > 1:\n                # Heuristic; we will match by dtype later\n                pass\n        elif out.size == 1:\n            num = out\n\n    # A more explicit identification by name is not guaranteed; rely on common order\n    # Order: [detection_boxes (float32 [1,N,4]), detection_classes (float32 [1,N]), detection_scores (float32 [1,N]), num_detections (float32 [1])]\n    if boxes is None:\n        # Try identify by shape\n        for out in outputs:\n            if isinstance(out, np.ndarray) and out.ndim == 3 and out.shape[-1] == 4:\n                boxes = out\n    # Identify classes and scores by 2D arrays [1, N]\n    two_d = [o for o in outputs if isinstance(o, np.ndarray) and o.ndim == 2 and o.shape[0] == 1]\n    if len(two_d) >= 2:\n        # Heuristic: scores are in [0,1], classes are positive integers as floats\n        cand_a, cand_b = two_d[0], two_d[1]\n        if np.all((cand_a >= 0) & (cand_a <= 1)):\n            scores = cand_a\n            classes = cand_b\n        elif np.all((cand_b >= 0) & (cand_b <= 1)):\n            scores = cand_b\n            classes = cand_a\n        else:\n            # Fallback: assume order classes then scores\n            classes, scores = cand_a, cand_b\n\n    if num is None:\n        # If num_detections not present, derive from length\n        if boxes is not None:\n            num = np.array([[boxes.shape[1]]], dtype=np.float32)\n        elif scores is not None:\n            num = np.array([[scores.shape[1]]], dtype=np.float32)\n\n    if boxes is None or classes is None or scores is None or num is None:\n        return []\n\n    num = int(np.squeeze(num).astype(int))\n    boxes = np.squeeze(boxes)[:num]\n    classes = np.squeeze(classes)[:num]\n    scores = np.squeeze(scores)[:num]\n\n    detections = []\n    for i in range(num):\n        score = float(scores[i])\n        if score < score_thresh:\n            continue\n        # boxes are in [ymin, xmin, ymax, xmax] normalized [0,1]\n        y_min, x_min, y_max, x_max = boxes[i]\n        x_min_i = int(max(0, min(1, x_min)) * frame_width)\n        x_max_i = int(max(0, min(1, x_max)) * frame_width)\n        y_min_i = int(max(0, min(1, y_min)) * frame_height)\n        y_max_i = int(max(0, min(1, y_max)) * frame_height)\n        detections.append({\n            \"bbox\": [x_min_i, y_min_i, x_max_i, y_max_i],\n            \"class_id\": int(classes[i]),\n            \"score\": score\n        })\n    return detections\n\n\ndef draw_detections(frame, detections, labels, map_text=None):\n    for det in detections:\n        x1, y1, x2, y2 = det[\"bbox\"]\n        class_id = det[\"class_id\"]\n        score = det[\"score\"]\n        label = get_label_name(labels, class_id)\n        color = (0, 200, 0)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        caption = f\"{label}: {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, max(0, y1 - th - 6)), (x1 + tw + 2, y1), color, -1)\n        cv2.putText(frame, caption, (x1 + 1, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    if map_text is not None:\n        overlay_text = f\"mAP: {map_text}\"\n        (tw, th), bl = cv2.getTextSize(overlay_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n        cv2.rectangle(frame, (8, 8), (8 + tw + 6, 8 + th + 10), (0, 0, 0), -1)\n        cv2.putText(frame, overlay_text, (11, 8 + th + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n    return frame\n\n\ndef iou_xyxy(box_a, box_b):\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n    inter_w = max(0, inter_x2 - inter_x1)\n    inter_h = max(0, inter_y2 - inter_y1)\n    inter_area = inter_w * inter_h\n    area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)\n    area_b = max(0, bx2 - bx1) * max(0, by2 - by1)\n    union = area_a + area_b - inter_area\n    if union <= 0:\n        return 0.0\n    return inter_area / union\n\n\ndef compute_map(pred_by_class, gt_by_class, iou_thresh=0.5):\n    # Compute per-class AP with 11-point interpolation; classes with zero GT are skipped from averaging.\n    ap_list = []\n    for cls_id in set(list(pred_by_class.keys()) + list(gt_by_class.keys())):\n        preds = pred_by_class.get(cls_id, [])\n        # preds: list of (image_id, score, bbox)\n        gts = gt_by_class.get(cls_id, {})\n        # gts: dict[image_id] = [bbox1, bbox2, ...]\n        # Build gt matched flags\n        gt_matched = {img_id: np.zeros(len(bboxes), dtype=bool) for img_id, bboxes in gts.items()}\n        # Sort predictions by confidence\n        preds_sorted = sorted(preds, key=lambda x: -x[1])\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n\n        npos = sum(len(v) for v in gts.values())\n        if npos == 0:\n            # No ground truth for this class, skip in AP averaging\n            continue\n\n        for i, (img_id, score, pb) in enumerate(preds_sorted):\n            gt_boxes = gts.get(img_id, [])\n            ovmax = 0.0\n            jmax = -1\n            for j, gb in enumerate(gt_boxes):\n                ov = iou_xyxy(pb, gb)\n                if ov > ovmax:\n                    ovmax = ov\n                    jmax = j\n            if ovmax >= iou_thresh and jmax >= 0 and not gt_matched[img_id][jmax]:\n                tp[i] = 1.0\n                gt_matched[img_id][jmax] = True\n            else:\n                fp[i] = 1.0\n\n        # Precision-recall\n        fp_cum = np.cumsum(fp)\n        tp_cum = np.cumsum(tp)\n        recall = tp_cum / float(npos + 1e-9)\n        precision = tp_cum / np.maximum(tp_cum + fp_cum, 1e-9)\n\n        # 11-point interpolation\n        ap = 0.0\n        for t in np.linspace(0, 1, 11):\n            p = 0.0\n            if np.any(recall >= t):\n                p = np.max(precision[recall >= t])\n            ap += p / 11.0\n        ap_list.append(ap)\n\n    if len(ap_list) == 0:\n        # No classes with ground truth; by definition here return 0.0\n        return 0.0\n    return float(np.mean(ap_list))\n\n\ndef ensure_dir_exists(path):\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\n\ndef main():\n    # Step 1: Setup - load interpreter, labels, and open video\n    labels = load_labels(LABEL_PATH)\n\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    ensure_dir_exists(OUTPUT_PATH)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open video writer for: {OUTPUT_PATH}\")\n\n    # Data holders for mAP calculation (no ground-truth provided; will compute 0.0)\n    preds_by_class = {}  # dict[class_id] -> list of (frame_idx, score, bbox)\n    gts_by_class = {}    # dict[class_id] -> dict[frame_idx] -> list of bboxes (empty here)\n\n    frame_idx = 0\n    start_time = time.time()\n\n    # Step 2/3/4: Process frames\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess\n        input_tensor = preprocess_frame(frame, input_details)\n        interpreter.set_tensor(input_details[0]['index'], input_tensor)\n\n        # Inference\n        interpreter.invoke()\n\n        # Parse outputs\n        detections = parse_detections(\n            interpreter, output_details, frame_width, frame_height, CONFIDENCE_THRESHOLD\n        )\n\n        # Accumulate predictions for mAP\n        for det in detections:\n            cls = det[\"class_id\"]\n            if cls not in preds_by_class:\n                preds_by_class[cls] = []\n            # Store bbox in [x1, y1, x2, y2] format\n            preds_by_class[cls].append((frame_idx, float(det[\"score\"]), det[\"bbox\"]))\n\n        # Compute a running mAP (will be 0.0 without GT)\n        running_map = compute_map(preds_by_class, gts_by_class, iou_thresh=0.5)\n        map_text = f\"{running_map:.3f}\"\n        if running_map == 0.0 and len(gts_by_class) == 0:\n            map_text += \" (no GT)\"\n\n        # Draw detections and mAP\n        annotated = draw_detections(frame.copy(), detections, labels, map_text=map_text)\n\n        # Write frame\n        writer.write(annotated)\n\n        frame_idx += 1\n\n    # Final mAP on all processed frames\n    final_map = compute_map(preds_by_class, gts_by_class, iou_thresh=0.5)\n    duration = time.time() - start_time\n    print(f\"Processing completed in {duration:.2f}s for {frame_idx} frames ({(frame_idx / max(duration,1e-6)):.2f} FPS).\")\n    if len(gts_by_class) == 0:\n        print(f\"Final mAP: {final_map:.3f} (no ground truth provided)\")\n    else:\n        print(f\"Final mAP: {final_map:.3f}\")\n\n    # Release resources\n    cap.release()\n    writer.release()\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 105.905,
      "total_cost": 0.06783625,
      "observations": [
        {
          "id": "time-12-41-20-153937_chatcmpl-D6DZ2sjJwVx8W8kp5vNatfeLHcdge",
          "traceId": "d307dfb2",
          "type": "GENERATION",
          "name": "d307_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:41:20.153000+00:00",
          "endTime": "2026-02-06T10:43:06.058000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06783625,
          "latency": 105905.0,
          "costDetails": {
            "total": 0.06783625
          },
          "totalTokens": 7145,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 6732,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:41:20.097Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:43:21.837Z"
    },
    {
      "id": "152594fd",
      "timestamp": "2026-02-06T10:37:50.811000+00:00",
      "name": "1525_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport cv2\nimport numpy as np\nfrom ai_edge_litert.interpreter import Interpreter\n\n# Configuration parameters\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\n\ndef load_labels(path):\n    labels = {}\n    try:\n        with open(path, 'r') as f:\n            lines = [l.strip() for l in f.readlines() if l.strip()]\n        for idx, line in enumerate(lines):\n            # Support formats like \"0 person\" or just \"person\"\n            parts = line.split(maxsplit=1)\n            if len(parts) == 2 and parts[0].isdigit():\n                labels[int(parts[0])] = parts[1]\n            else:\n                labels[idx] = line\n    except Exception:\n        pass\n    return labels\n\ndef initialize_interpreter(model_path):\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    # Assume single input tensor\n    in_shape = input_details[0]['shape']\n    if len(in_shape) != 4:\n        raise RuntimeError(\"Expected input tensor of rank 4, got: {}\".format(in_shape))\n    in_height, in_width = int(in_shape[1]), int(in_shape[2])\n    return interpreter, input_details, output_details, (in_width, in_height)\n\ndef preprocess_frame(frame_bgr, input_details, input_size):\n    in_w, in_h = input_size\n    # Convert BGR to RGB as most TFLite detection models expect RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (in_w, in_h))\n    input_tensor = np.expand_dims(resized, axis=0)\n    tensor_info = input_details[0]\n    dtype = tensor_info['dtype']\n\n    # Handle quantization and dtype\n    if dtype == np.float32:\n        input_tensor = input_tensor.astype(np.float32) / 255.0\n    elif dtype == np.uint8:\n        # Many uint8 TFLite models take raw 0-255 inputs directly\n        input_tensor = input_tensor.astype(np.uint8)\n    elif dtype == np.int8:\n        # Quantized int8: map float [0,1] to quantized using scale/zero_point\n        scale, zero_point = tensor_info.get('quantization', (0.0, 0))\n        if scale is None or scale == 0.0:\n            # Fallback: just cast (might be incorrect for a given model)\n            input_tensor = input_tensor.astype(np.int8)\n        else:\n            real = (input_tensor.astype(np.float32) / 255.0)\n            quant = np.round(real / scale + zero_point)\n            quant = np.clip(quant, -128, 127).astype(np.int8)\n            input_tensor = quant\n    else:\n        # Fallback: attempt to cast to required dtype\n        input_tensor = input_tensor.astype(dtype)\n\n    return input_tensor\n\ndef parse_detections(interpreter, output_details, frame_shape):\n    \"\"\"\n    Parse outputs of a TFLite SSD model. Returns:\n    - boxes_px: (N, 4) array of [xmin, ymin, xmax, ymax] in pixel coords\n    - classes: (N,) int array of class indices\n    - scores: (N,) float array of confidence scores\n    It attempts to be robust to different output orders.\n    \"\"\"\n    outs = []\n    for d in output_details:\n        outs.append(interpreter.get_tensor(d['index']))\n\n    # Squeeze outputs for easier handling\n    squeezed = [np.squeeze(o) for o in outs]\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Identify boxes tensor: 2D with last dim 4\n    for arr in squeezed:\n        if arr.ndim == 2 and arr.shape[-1] == 4:\n            boxes = arr  # shape (N,4) with [ymin, xmin, ymax, xmax]\n            break\n\n    # Identify num_detections: 0D or 1-element\n    for arr in squeezed:\n        if arr.ndim == 0 or (arr.ndim == 1 and arr.size == 1):\n            try:\n                num = int(arr.reshape(()))\n            except Exception:\n                num = None\n            break\n\n    # Identify classes and scores among 1D arrays\n    candidates = [arr for arr in squeezed if arr.ndim == 1 and (boxes is None or arr.size == boxes.shape[0])]\n    # Heuristics: scores are float in [0,1], classes are near-integers\n    cand_scores = None\n    cand_classes = None\n    for arr in candidates:\n        if arr.dtype.kind in ('f',):  # float\n            if np.all((arr >= 0.0) & (arr <= 1.0)):\n                cand_scores = arr\n            else:\n                # Could still be classes as float\n                if np.allclose(arr, np.round(arr)):\n                    cand_classes = arr.astype(np.int32)\n        else:\n            # int types likely classes\n            cand_classes = arr.astype(np.int32)\n\n    # If still unresolved, attempt to decide by variance\n    if cand_scores is None and candidates:\n        # Pick the one with values between 0 and 1 if possible, otherwise the one with greater variance as scores\n        for arr in candidates:\n            if np.all((arr >= 0.0) & (arr <= 1.0)):\n                cand_scores = arr\n                break\n        if cand_scores is None:\n            # Fallback: pick float array as scores\n            floats = [a for a in candidates if a.dtype.kind in ('f',)]\n            if floats:\n                cand_scores = floats[0]\n    if cand_classes is None and candidates:\n        # Pick the remaining as classes (rounded)\n        remaining = [a for a in candidates if a is not cand_scores]\n        if remaining:\n            cand_classes = np.round(remaining[0]).astype(np.int32)\n\n    scores = cand_scores\n    classes = cand_classes\n\n    # If num is provided, clamp arrays to num\n    if boxes is None or classes is None or scores is None:\n        raise RuntimeError(\"Unable to parse detection outputs. Got shapes: {}\".format([a.shape for a in squeezed]))\n    N = boxes.shape[0]\n    if num is not None:\n        N = min(N, int(num))\n    boxes = boxes[:N]\n    classes = classes[:N]\n    scores = scores[:N]\n\n    # Convert normalized [ymin, xmin, ymax, xmax] to pixel coords [xmin, ymin, xmax, ymax]\n    h, w = int(frame_shape[0]), int(frame_shape[1])\n    ymin = boxes[:, 0] * h\n    xmin = boxes[:, 1] * w\n    ymax = boxes[:, 2] * h\n    xmax = boxes[:, 3] * w\n    boxes_px = np.stack([\n        np.clip(xmin, 0, w - 1),\n        np.clip(ymin, 0, h - 1),\n        np.clip(xmax, 0, w - 1),\n        np.clip(ymax, 0, h - 1)\n    ], axis=1).astype(np.int32)\n\n    return boxes_px, classes.astype(np.int32), scores.astype(np.float32)\n\ndef deterministic_color(class_id):\n    rng = np.random.RandomState(class_id * 9973 + 12345)\n    color = tuple(int(x) for x in rng.randint(0, 255, size=3))\n    return color\n\ndef compute_pseudo_map(detections_per_frame, num_classes):\n    \"\"\"\n    Compute a proxy mAP using only predictions (no ground truth available).\n    For each class, in each frame we treat the highest-score detection as a TP,\n    and additional detections of the same class in that frame as FPs. This\n    yields a well-defined Average Precision per class and their mean (mAP).\n    \"\"\"\n    APs = []\n    for c in range(num_classes):\n        # Collect per-frame detections for class c\n        entries = []  # list of (score, is_tp)\n        positive_frames = 0\n        for det in detections_per_frame:\n            classes = det['classes']\n            scores = det['scores']\n            # Indices for class c\n            idxs = np.where(classes == c)[0]\n            if idxs.size == 0:\n                continue\n            # Mark highest score in this frame for this class as TP\n            frame_scores = scores[idxs]\n            top_idx_local = int(np.argmax(frame_scores))\n            top_global_idx = idxs[top_idx_local]\n            positive_frames += 1\n            for j, gi in enumerate(idxs):\n                is_tp = (gi == top_global_idx)\n                entries.append((float(scores[gi]), 1 if is_tp else 0))\n\n        if positive_frames == 0 or len(entries) == 0:\n            continue\n\n        # Sort by score descending\n        entries.sort(key=lambda x: -x[0])\n        tps = np.array([e[1] for e in entries], dtype=np.float32)\n        fps = 1.0 - tps\n\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        recall = cum_tp / max(positive_frames, 1)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-8)\n\n        # Compute AP using the standard interpolated precision envelope\n        mrec = np.concatenate(([0.0], recall, [1.0]))\n        mpre = np.concatenate(([0.0], precision, [0.0]))\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = max(mpre[i - 1], mpre[i])\n        # Sum over recall steps where it changes\n        i_changes = np.where(mrec[1:] != mrec[:-1])[0]\n        ap = float(np.sum((mrec[i_changes + 1] - mrec[i_changes]) * mpre[i_changes + 1]))\n        APs.append(ap)\n\n    if len(APs) == 0:\n        return None\n    return float(np.mean(APs))\n\ndef ensure_dir_for_file(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\ndef main():\n    # Load labels\n    labels_dict = load_labels(LABEL_PATH)\n    max_label_id = max(labels_dict.keys()) if labels_dict else -1\n    num_classes = max_label_id + 1 if max_label_id >= 0 else 91  # default to COCO-ish if unknown\n\n    # Initialize interpreter\n    interpreter, input_details, output_details, input_size = initialize_interpreter(MODEL_PATH)\n\n    # Prepare video IO\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(\"Failed to open input video: {}\".format(INPUT_PATH))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 1e-2:\n        fps = 30.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.get(cv2.CAP_PROP_FRAME_COUNT) > 0 else -1\n\n    # First pass: run inference and collect detections for all frames\n    detections_per_frame = []\n    frame_idx = 0\n    t0 = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        inp = preprocess_frame(frame, input_details, input_size)\n        interpreter.set_tensor(input_details[0]['index'], inp)\n        interpreter.invoke()\n        boxes_px, classes, scores = parse_detections(interpreter, output_details, frame.shape[:2])\n        # Store raw detections for further processing and drawing later\n        detections_per_frame.append({\n            'boxes': boxes_px,\n            'classes': classes,\n            'scores': scores\n        })\n        frame_idx += 1\n        if frame_idx % 50 == 0:\n            if total_frames > 0:\n                print(\"Inference pass: processed {}/{} frames...\".format(frame_idx, total_frames))\n            else:\n                print(\"Inference pass: processed {} frames...\".format(frame_idx))\n    cap.release()\n    t1 = time.time()\n    elapsed_infer = t1 - t0\n    print(\"Inference completed on {} frames in {:.2f}s ({:.2f} FPS)\".format(\n        frame_idx, elapsed_infer, frame_idx / max(elapsed_infer, 1e-6)))\n\n    # Compute pseudo-mAP from detections (proxy metric without ground truth)\n    map_value = compute_pseudo_map(detections_per_frame, num_classes)\n    if map_value is None:\n        map_text = \"mAP: N/A\"\n    else:\n        map_text = \"mAP: {:.3f}\".format(map_value)\n\n    # Second pass: draw boxes above threshold and write output video with mAP overlay\n    ensure_dir_for_file(OUTPUT_PATH)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        raise RuntimeError(\"Failed to open output writer: {}\".format(OUTPUT_PATH))\n\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(\"Failed to reopen input video: {}\".format(INPUT_PATH))\n\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    thickness = 2\n    font_scale = 0.6\n\n    frame_idx = 0\n    t2 = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret or frame_idx >= len(detections_per_frame):\n            break\n        det = detections_per_frame[frame_idx]\n        boxes = det['boxes']\n        classes = det['classes']\n        scores = det['scores']\n\n        # Draw detections above threshold\n        for i in range(len(scores)):\n            score = float(scores[i])\n            if score < CONFIDENCE_THRESHOLD:\n                continue\n            cls_id = int(classes[i])\n            label = labels_dict.get(cls_id, str(cls_id))\n            x1, y1, x2, y2 = [int(v) for v in boxes[i]]\n            color = deterministic_color(cls_id)\n\n            # Draw rectangle\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n            # Label text\n            text = \"{} {:.2f}\".format(label, score)\n            (tw, th), _ = cv2.getTextSize(text, font, font_scale, thickness)\n            y_text = max(y1 - 8, th + 4)\n            x_text = max(x1, 0)\n            # Filled background for readability\n            cv2.rectangle(frame, (x_text, y_text - th - 4), (x_text + tw + 2, y_text + 2), color, -1)\n            cv2.putText(frame, text, (x_text + 1, y_text - 2), font, font_scale, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Overlay mAP\n        cv2.putText(frame, map_text, (10, 30), font, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n\n        writer.write(frame)\n        frame_idx += 1\n        if frame_idx % 50 == 0:\n            print(\"Writing pass: wrote {} frames...\".format(frame_idx))\n\n    cap.release()\n    writer.release()\n    t3 = time.time()\n    print(\"Output saved to: {}\".format(OUTPUT_PATH))\n    print(\"Writing completed in {:.2f}s\".format(t3 - t2))\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 176.365,
      "total_cost": 0.11514625,
      "observations": [
        {
          "id": "time-12-37-51-425346_chatcmpl-D6DVfhWmttKrn3W5wUW85Zb7t6aBb",
          "traceId": "152594fd",
          "type": "GENERATION",
          "name": "1525_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:37:51.425000+00:00",
          "endTime": "2026-02-06T10:40:47.790000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.11514625,
          "latency": 176365.0,
          "costDetails": {
            "total": 0.11514625
          },
          "totalTokens": 11876,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 11463,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:37:51.363Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:40:59.609Z"
    },
    {
      "id": "f1cbe71d",
      "timestamp": "2026-02-06T10:34:48.919000+00:00",
      "name": "f1cb_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"  # Read a single video file from the given input_path\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Output video with rectangles, labels, and mAP text\nconfidence_threshold = 0.5\n\n# =========================\n# Utility Functions\n# =========================\ndef load_labels(path):\n    labels = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Handle \"index label\" or just \"label\"\n            parts = line.split()\n            if len(parts) > 1 and parts[0].isdigit():\n                labels.append(\" \".join(parts[1:]))\n            else:\n                labels.append(line)\n    return labels\n\ndef preprocess_frame(frame_bgr, input_shape, input_dtype):\n    # Model expects RGB\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    _, in_h, in_w, _ = input_shape\n    resized = cv2.resize(rgb, (in_w, in_h))\n    if input_dtype == np.float32:\n        inp = resized.astype(np.float32) / 255.0\n    else:\n        inp = resized.astype(input_dtype)\n    return np.expand_dims(inp, axis=0)\n\ndef clamp(val, lo, hi):\n    return max(lo, min(hi, val))\n\ndef nms_per_class(boxes, scores, iou_threshold=0.5):\n    # boxes: (N, 4) in pixel coords (ymin, xmin, ymax, xmax)\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n\n    y1 = boxes[:, 0]\n    x1 = boxes[:, 1]\n    y2 = boxes[:, 2]\n    x2 = boxes[:, 3]\n    areas = (y2 - y1 + 1) * (x2 - x1 + 1)\n\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = int(order[0])\n        keep.append(i)\n        if order.size == 1:\n            break\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        inter = h * w\n        iou = inter / (areas[i] + areas[order[1:]] - inter)\n        inds = np.where(iou <= iou_threshold)[0]\n        order = order[inds + 1]\n    return keep\n\ndef iou_matrix(boxes_a, boxes_b):\n    # boxes: (N, 4) and (M, 4) in pixel coords (ymin, xmin, ymax, xmax)\n    if len(boxes_a) == 0 or len(boxes_b) == 0:\n        return np.zeros((len(boxes_a), len(boxes_b)), dtype=np.float32)\n    a = np.asarray(boxes_a, dtype=np.float32)\n    b = np.asarray(boxes_b, dtype=np.float32)\n    ay1, ax1, ay2, ax2 = a[:, 0:1], a[:, 1:2], a[:, 2:3], a[:, 3:4]\n    by1, bx1, by2, bx2 = b[:, 0], b[:, 1], b[:, 2], b[:, 3]\n\n    inter_y1 = np.maximum(ay1, by1)\n    inter_x1 = np.maximum(ax1, bx1)\n    inter_y2 = np.minimum(ay2, by2)\n    inter_x2 = np.minimum(ax2, bx2)\n\n    inter_h = np.maximum(0.0, inter_y2 - inter_y1 + 1)\n    inter_w = np.maximum(0.0, inter_x2 - inter_x1 + 1)\n    inter = inter_h * inter_w\n\n    area_a = (ay2 - ay1 + 1) * (ax2 - ax1 + 1)\n    area_b = (by2 - by1 + 1) * (bx2 - bx1 + 1)\n\n    union = area_a + area_b - inter\n    return inter / np.maximum(union, 1e-8)\n\ndef parse_optional_gt_file(video_path):\n    # Optional ground truth file alongside the video: replace extension with .gt.txt\n    # Format per line (CSV or whitespace): frame_index, class_id, xmin, ymin, xmax, ymax\n    base, _ = os.path.splitext(video_path)\n    gt_path = base + \".gt.txt\"\n    if not os.path.exists(gt_path):\n        return None\n    gts = {}\n    with open(gt_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            s = line.strip()\n            if not s or s.startswith(\"#\"):\n                continue\n            s = s.replace(\",\", \" \")\n            parts = [p for p in s.split() if p]\n            if len(parts) < 6:\n                continue\n            try:\n                frame_idx = int(parts[0])\n                cls = int(parts[1])\n                xmin = int(float(parts[2]))\n                ymin = int(float(parts[3]))\n                xmax = int(float(parts[4]))\n                ymax = int(float(parts[5]))\n            except Exception:\n                continue\n            if frame_idx not in gts:\n                gts[frame_idx] = []\n            gts[frame_idx].append((cls, [ymin, xmin, ymax, xmax]))  # store as (class, [ymin,xmin,ymax,xmax])\n    return gts\n\nclass MAPAccumulator:\n    def __init__(self):\n        # class_id -> dict with keys: scores (list), tps (list), total_gt (int)\n        self.data = {}\n\n    def ensure_class(self, cls_id):\n        if cls_id not in self.data:\n            self.data[cls_id] = {'scores': [], 'tps': [], 'total_gt': 0}\n\n    def add_ground_truths(self, gt_by_class):\n        # gt_by_class: dict class_id -> list of boxes\n        for cls, boxes in gt_by_class.items():\n            self.ensure_class(cls)\n            self.data[cls]['total_gt'] += len(boxes)\n\n    def add_detections(self, det_by_class, gt_by_class, iou_thresh=0.5):\n        # det_by_class: dict class_id -> list of (score, box)\n        # gt_by_class: dict class_id -> list of boxes\n        for cls, dets in det_by_class.items():\n            self.ensure_class(cls)\n            gts = gt_by_class.get(cls, [])\n            matched = np.zeros(len(gts), dtype=np.uint8) if gts else None\n            # sort detections by score desc for consistent matching\n            if dets:\n                scores = np.array([d[0] for d in dets], dtype=np.float32)\n                order = scores.argsort()[::-1]\n                dets_sorted = [dets[i] for i in order]\n            else:\n                dets_sorted = []\n\n            for score, box in dets_sorted:\n                self.data[cls]['scores'].append(float(score))\n                if not gts:\n                    self.data[cls]['tps'].append(0)\n                    continue\n                # match to best unmatched gt by IoU\n                ious = iou_matrix([box], gts)[0]\n                # Invalidate already matched\n                if matched is not None:\n                    ious = ious * (1 - matched)\n                best_idx = int(np.argmax(ious)) if ious.size > 0 else -1\n                best_iou = ious[best_idx] if ious.size > 0 else 0.0\n                if best_iou >= iou_thresh and matched[best_idx] == 0:\n                    self.data[cls]['tps'].append(1)\n                    matched[best_idx] = 1\n                else:\n                    self.data[cls]['tps'].append(0)\n\n    def compute_map(self):\n        # Compute AP per class and then mean across classes with at least one GT\n        aps = []\n        for cls, rec in self.data.items():\n            total_gt = rec['total_gt']\n            if total_gt <= 0:\n                continue  # skip classes without GT\n            scores = np.array(rec['scores'], dtype=np.float32)\n            tps = np.array(rec['tps'], dtype=np.int32)\n            if scores.size == 0:\n                aps.append(0.0)\n                continue\n            order = scores.argsort()[::-1]\n            tps = tps[order]\n            fps = 1 - tps\n\n            cum_tp = np.cumsum(tps)\n            cum_fp = np.cumsum(fps)\n            recall = cum_tp / max(total_gt, 1)\n            precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-8)\n\n            # Precision envelope\n            # Make precision monotonically non-increasing from right to left\n            for i in range(precision.size - 2, -1, -1):\n                if precision[i] < precision[i + 1]:\n                    precision[i] = precision[i + 1]\n\n            # Compute AP as area under PR curve via step-wise integration\n            # Insert (0,1) at start and (1,0) at end as in common practice\n            mrec = np.concatenate(([0.0], recall, [1.0]))\n            mpre = np.concatenate(([1.0], precision, [0.0]))\n            # Make envelope again for safety\n            for i in range(mpre.size - 2, -1, -1):\n                if mpre[i] < mpre[i + 1]:\n                    mpre[i] = mpre[i + 1]\n            # Sum over recall steps\n            # Find points where recall changes\n            idx = np.where(mrec[1:] != mrec[:-1])[0]\n            ap = float(np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1]))\n            aps.append(ap)\n        if not aps:\n            return None  # No GT available yet\n        return float(np.mean(aps))\n\n# =========================\n# Main Pipeline\n# =========================\ndef main():\n    # Prepare output directory\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Load optional ground truth annotations\n    gt_all = parse_optional_gt_file(input_path)\n    have_gt = gt_all is not None\n    map_acc = MAPAccumulator() if have_gt else None\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n\n    # Open video\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"Error: Cannot open input video:\", input_path)\n        return\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        print(\"Error: Cannot open output video for writing:\", output_path)\n        cap.release()\n        return\n\n    frame_index = 0\n    running_map = None\n    t0 = time.time()\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Preprocess\n            inp = preprocess_frame(frame, input_shape, input_dtype)\n\n            # Inference\n            interpreter.set_tensor(input_details[0]['index'], inp)\n            interpreter.invoke()\n\n            # Extract outputs robustly\n            outs = [interpreter.get_tensor(d['index']) for d in output_details]\n\n            boxes = None\n            classes = None\n            scores = None\n            num = None\n            # Identify outputs by shape heuristics\n            for a in outs:\n                if a.size == 1:\n                    num = int(np.round(float(a.reshape(-1)[0])))\n                elif a.ndim == 3 and a.shape[0] == 1 and a.shape[2] == 4:\n                    boxes = a[0]\n                elif a.ndim == 2 and a.shape[0] == 1:\n                    # Defer deciding between classes and scores\n                    pass\n\n            # Find the two [1, N] tensors for classes and scores\n            two_d = [a for a in outs if (a.ndim == 2 and a.shape[0] == 1 and a.size > 1)]\n            if len(two_d) >= 2:\n                a0 = two_d[0][0]\n                a1 = two_d[1][0]\n                max0 = float(np.max(a0)) if a0.size > 0 else 0.0\n                max1 = float(np.max(a1)) if a1.size > 0 else 0.0\n                # Scores are usually in [0,1], classes usually > 1\n                if max0 <= 1.0 and max1 > 1.0:\n                    scores = a0\n                    classes = a1\n                elif max1 <= 1.0 and max0 > 1.0:\n                    scores = a1\n                    classes = a0\n                else:\n                    # Fallback by dtype: classes often float but larger values; pick based on means\n                    if float(np.mean(a0)) < float(np.mean(a1)):\n                        scores, classes = a0, a1\n                    else:\n                        scores, classes = a1, a0\n            elif len(two_d) == 1:\n                # Some models return only boxes and scores without classes; fallback class id 0\n                scores = two_d[0][0]\n                classes = np.zeros_like(scores)\n\n            if boxes is None or scores is None or classes is None:\n                # Cannot parse outputs; skip drawing for this frame\n                writer.write(frame)\n                frame_index += 1\n                continue\n\n            if num is None or num <= 0 or num > boxes.shape[0]:\n                num = boxes.shape[0]\n\n            # Postprocess detections: filter by score and convert to pixel coords\n            filtered = []\n            for i in range(num):\n                score = float(scores[i])\n                if score < confidence_threshold:\n                    continue\n                cls_id = int(classes[i])\n                y_min, x_min, y_max, x_max = boxes[i]\n                # boxes are normalized [0,1]; convert to pixel coords\n                top = int(clamp(y_min, 0.0, 1.0) * frame_h)\n                left = int(clamp(x_min, 0.0, 1.0) * frame_w)\n                bottom = int(clamp(y_max, 0.0, 1.0) * frame_h)\n                right = int(clamp(x_max, 0.0, 1.0) * frame_w)\n                # Clamp and fix ordering if necessary\n                t = clamp(min(top, bottom), 0, frame_h - 1)\n                b = clamp(max(top, bottom), 0, frame_h - 1)\n                l = clamp(min(left, right), 0, frame_w - 1)\n                r = clamp(max(left, right), 0, frame_w - 1)\n                filtered.append((cls_id, score, [t, l, b, r]))\n\n            # Apply per-class NMS\n            final_dets = []\n            if filtered:\n                # Group by class\n                by_class = {}\n                for cls_id, score, box in filtered:\n                    by_class.setdefault(cls_id, []).append((score, box))\n                for cls_id, dets in by_class.items():\n                    if not dets:\n                        continue\n                    boxes_cls = [b for (_, b) in dets]\n                    scores_cls = [s for (s, _) in dets]\n                    keep_idx = nms_per_class(boxes_cls, scores_cls, iou_threshold=0.5)\n                    for idx in keep_idx:\n                        final_dets.append((cls_id, scores_cls[idx], boxes_cls[idx]))\n            else:\n                final_dets = []\n\n            # Update running mAP if GT available\n            if have_gt:\n                gt_frame_entries = gt_all.get(frame_index, [])\n                gt_by_class = {}\n                for cls, box in gt_frame_entries:\n                    gt_by_class.setdefault(cls, []).append(box)\n                # Add GT counts first (for frames processed so far)\n                map_acc.add_ground_truths(gt_by_class)\n                # Convert detections to dict by class with (score, box)\n                det_by_class = {}\n                for cls_id, score, box in final_dets:\n                    det_by_class.setdefault(cls_id, []).append((score, box))\n                # Update matches\n                map_acc.add_detections(det_by_class, gt_by_class, iou_thresh=0.5)\n                running_map = map_acc.compute_map()\n\n            # Draw detections\n            for cls_id, score, box in final_dets:\n                t, l, b, r = box\n                color = (0, 255, 0)\n                cv2.rectangle(frame, (l, t), (r, b), color, 2)\n                # Resolve label text\n                label_txt = None\n                if 0 <= cls_id < len(labels):\n                    label_txt = labels[cls_id]\n                else:\n                    # Some label files have a \"background\" at index 0; try offset by 1\n                    if 0 <= cls_id + 1 < len(labels):\n                        label_txt = labels[cls_id + 1]\n                if label_txt is None:\n                    label_txt = f\"class_{cls_id}\"\n                caption = f\"{label_txt}: {score:.2f}\"\n                # Put label\n                y_text = max(0, t - 10)\n                cv2.putText(frame, caption, (l, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n\n            # Put mAP text\n            if have_gt:\n                if running_map is None:\n                    map_text = \"mAP: N/A (insufficient GT)\"\n                else:\n                    map_text = f\"mAP: {running_map:.3f}\"\n            else:\n                map_text = \"mAP: N/A (no ground truth)\"\n            cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (10, 220, 255), 2, cv2.LINE_AA)\n\n            # Write frame\n            writer.write(frame)\n            frame_index += 1\n\n    finally:\n        cap.release()\n        writer.release()\n        elapsed = time.time() - t0\n        if frame_index > 0 and elapsed > 0:\n            print(f\"Processed {frame_index} frames in {elapsed:.2f}s ({frame_index/elapsed:.2f} FPS).\")\n        if have_gt:\n            final_map = map_acc.compute_map()\n            if final_map is None:\n                print(\"Final mAP: N/A (no ground truth annotations were found).\")\n            else:\n                print(f\"Final mAP over processed frames: {final_map:.4f}\")\n        print(\"Output saved to:\", output_path)\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 149.478,
      "total_cost": 0.08889625,
      "observations": [
        {
          "id": "time-12-34-49-160172_chatcmpl-D6DSjQDbsnWTlUca3CoEUTkLVtwgu",
          "traceId": "f1cbe71d",
          "type": "GENERATION",
          "name": "f1cb_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:34:49.160000+00:00",
          "endTime": "2026-02-06T10:37:18.638000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08889625,
          "latency": 149478.0,
          "costDetails": {
            "total": 0.08889625
          },
          "totalTokens": 9251,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 8838,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:34:49.445Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:37:30.132Z"
    },
    {
      "id": "cc2a5772",
      "timestamp": "2026-02-06T10:31:48.060000+00:00",
      "name": "cc2a_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# =========================\n# Utilities\n# =========================\ndef load_labels(path):\n    \"\"\"\n    Load labels from a label map file.\n    Supports:\n      - Simple list (each line a label; may include '???' for background)\n      - PBTXT-like format with 'item { id: X name: 'label' }'\n    Returns a dict: {id(int): name(str)}\n    \"\"\"\n    labels = {}\n    if not os.path.exists(path):\n        return labels\n    with open(path, 'r', encoding='utf-8') as f:\n        content = f.read()\n    # Detect PBTXT-like structure\n    if 'item' in content and 'id' in content and 'name' in content:\n        # Simple manual parse without regex\n        lines = [ln.strip() for ln in content.splitlines()]\n        current_id = None\n        current_name = None\n        for ln in lines:\n            if ln.startswith('item'):\n                current_id = None\n                current_name = None\n            if 'id' in ln and ':' in ln:\n                # Extract digits for id\n                digits = ''.join([ch for ch in ln if ch.isdigit()])\n                if digits:\n                    try:\n                        current_id = int(digits)\n                    except Exception:\n                        current_id = None\n            if 'name' in ln and ':' in ln:\n                # Try to extract between single or double quotes\n                name = None\n                if \"'\" in ln:\n                    try:\n                        name = ln.split(\"'\", 2)[1]\n                    except Exception:\n                        name = None\n                if name is None and '\"' in ln:\n                    try:\n                        name = ln.split('\"', 2)[1]\n                    except Exception:\n                        name = None\n                current_name = name\n            if ln.endswith('}') and current_id is not None and current_name is not None:\n                labels[current_id] = current_name\n                current_id = None\n                current_name = None\n        if labels:\n            return labels\n    # Fallback: simple list\n    lines = [ln.strip() for ln in content.splitlines() if ln.strip()]\n    for idx, name in enumerate(lines):\n        labels[idx] = name\n    return labels\n\ndef get_interpreter(model_path):\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    return interpreter\n\ndef preprocess_frame(frame, input_shape, input_dtype):\n    \"\"\"\n    Resize and convert color to match model input.\n    input_shape expected as (1, height, width, channels) or similar.\n    \"\"\"\n    if len(input_shape) == 4:\n        height, width = int(input_shape[1]), int(input_shape[2])\n    elif len(input_shape) == 3:\n        height, width = int(input_shape[0]), int(input_shape[1])\n    else:\n        raise ValueError(\"Unexpected input tensor shape: {}\".format(input_shape))\n    # Convert BGR to RGB\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (width, height))\n    input_data = resized\n    if input_dtype == np.float32:\n        input_data = (input_data.astype(np.float32) / 255.0).astype(np.float32)\n    else:\n        input_data = input_data.astype(input_dtype)\n    # Add batch dimension if needed\n    if len(input_shape) == 4:\n        input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef run_inference(interpreter, input_data):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Try to map outputs by typical SSD tensors: boxes, classes, scores, num\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n    for out in output_details:\n        idx = out['index']\n        tensor = interpreter.get_tensor(idx)\n        shape = tensor.shape\n        # Heuristics based on shapes and dtypes\n        if tensor.ndim == 3 and shape[-1] == 4:\n            boxes = tensor\n        elif tensor.ndim == 2 and (tensor.shape[-1] > 1) and np.issubdtype(tensor.dtype, np.floating):\n            # Could be classes or scores; check value ranges\n            if np.all((tensor >= 0.0) & (tensor <= 1.0)):\n                scores = tensor\n            else:\n                classes = tensor\n        elif tensor.ndim == 2 and np.issubdtype(tensor.dtype, np.integer):\n            classes = tensor\n        elif tensor.size == 1:\n            num = int(np.squeeze(tensor).tolist())\n    # Some models return [1, N, 4], [1, N], [1, N], [1]\n    if boxes is not None and boxes.ndim == 3:\n        boxes = boxes[0]\n    if classes is not None and classes.ndim == 2:\n        classes = classes[0]\n    if scores is not None and scores.ndim == 2:\n        scores = scores[0]\n    if num is None and boxes is not None:\n        num = boxes.shape[0]\n    return boxes, classes, scores, num\n\ndef denormalize_boxes(boxes, frame_width, frame_height):\n    \"\"\"\n    Convert normalized ymin, xmin, ymax, xmax to pixel coordinates [xmin, ymin, xmax, ymax]\n    \"\"\"\n    pixel_boxes = []\n    for b in boxes:\n        ymin, xmin, ymax, xmax = float(b[0]), float(b[1]), float(b[2]), float(b[3])\n        x1 = max(0, min(frame_width - 1, int(xmin * frame_width)))\n        y1 = max(0, min(frame_height - 1, int(ymin * frame_height)))\n        x2 = max(0, min(frame_width - 1, int(xmax * frame_width)))\n        y2 = max(0, min(frame_height - 1, int(ymax * frame_height)))\n        pixel_boxes.append([x1, y1, x2, y2])\n    return pixel_boxes\n\ndef draw_detections(frame, boxes, classes, scores, labels, conf_thr):\n    h, w = frame.shape[:2]\n    count = 0\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score < conf_thr:\n            continue\n        count += 1\n        cls_id = int(classes[i]) if classes is not None else -1\n        label = labels.get(cls_id, str(cls_id))\n        # color derived from class id\n        color = ((37 * (cls_id + 1)) % 255, (17 * (cls_id + 2)) % 255, (29 * (cls_id + 3)) % 255)\n        # box is normalized; convert to pixel\n        ymin, xmin, ymax, xmax = boxes[i]\n        x1 = int(max(0, min(w - 1, xmin * w)))\n        y1 = int(max(0, min(h - 1, ymin * h)))\n        x2 = int(max(0, min(w - 1, xmax * w)))\n        y2 = int(max(0, min(h - 1, ymax * h)))\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        caption = \"{}: {:.2f}\".format(label, score)\n        (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), color, -1)\n        cv2.putText(frame, caption, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n    return count\n\ndef iou(box_a, box_b):\n    \"\"\"\n    box: [xmin, ymin, xmax, ymax]\n    \"\"\"\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n    inter_x1 = ax1 if ax1 > bx1 else bx1\n    inter_y1 = ay1 if ay1 > by1 else by1\n    inter_x2 = ax2 if ax2 < bx2 else bx2\n    inter_y2 = ay2 if ay2 < by2 else by2\n    iw = inter_x2 - inter_x1 + 1\n    ih = inter_y2 - inter_y1 + 1\n    if iw <= 0 or ih <= 0:\n        return 0.0\n    inter = iw * ih\n    area_a = (ax2 - ax1 + 1) * (ay2 - ay1 + 1)\n    area_b = (bx2 - bx1 + 1) * (by2 - by1 + 1)\n    union = area_a + area_b - inter\n    if union <= 0:\n        return 0.0\n    return float(inter) / float(union)\n\ndef compute_map(predictions_by_class, ground_truth_by_class, iou_threshold=0.5):\n    \"\"\"\n    Compute mAP across classes with available ground truth.\n    predictions_by_class: {class_id: [(image_id, score, [xmin, ymin, xmax, ymax]), ...]}\n    ground_truth_by_class: {class_id: {image_id: [[xmin, ymin, xmax, ymax], ...], ...}, ...}\n    Returns mAP (float). If no GT is available, returns 0.0.\n    \"\"\"\n    ap_list = []\n    for cls_id in predictions_by_class:\n        preds = predictions_by_class.get(cls_id, [])\n        gts_dict = ground_truth_by_class.get(cls_id, {})\n        total_gts = 0\n        for img_id in gts_dict:\n            total_gts += len(gts_dict[img_id])\n        # Skip classes with no GT\n        if total_gts == 0:\n            continue\n        # Prepare matched flags for GTs\n        matched = {}\n        for img_id in gts_dict:\n            matched[img_id] = [False] * len(gts_dict[img_id])\n        # Sort predictions by descending score\n        preds_sorted = sorted(preds, key=lambda x: x[1], reverse=True)\n        tps = []\n        fps = []\n        for (img_id, score, box_p) in preds_sorted:\n            if img_id in gts_dict:\n                gt_boxes = gts_dict[img_id]\n                gt_matched_flags = matched[img_id]\n            else:\n                gt_boxes = []\n                gt_matched_flags = []\n            best_iou = 0.0\n            best_gt_idx = -1\n            for gi, gt_box in enumerate(gt_boxes):\n                if gt_matched_flags[gi]:\n                    continue\n                ov = iou(box_p, gt_box)\n                if ov > best_iou:\n                    best_iou = ov\n                    best_gt_idx = gi\n            if best_iou >= iou_threshold and best_gt_idx >= 0:\n                gt_matched_flags[best_gt_idx] = True\n                tps.append(1.0)\n                fps.append(0.0)\n            else:\n                tps.append(0.0)\n                fps.append(1.0)\n        if len(tps) == 0:\n            ap_list.append(0.0)\n            continue\n        tp_cum = np.cumsum(np.array(tps, dtype=np.float32))\n        fp_cum = np.cumsum(np.array(fps, dtype=np.float32))\n        recalls = tp_cum / float(total_gts)\n        precisions = tp_cum / np.maximum(tp_cum + fp_cum, 1e-9)\n        # Compute AP using 101-point interpolation\n        recall_levels = np.linspace(0.0, 1.0, 101)\n        precision_interpolated = []\n        for r in recall_levels:\n            mask = recalls >= r\n            if np.any(mask):\n                precision_interpolated.append(np.max(precisions[mask]))\n            else:\n                precision_interpolated.append(0.0)\n        ap = float(np.mean(np.array(precision_interpolated, dtype=np.float32)))\n        ap_list.append(ap)\n    if len(ap_list) == 0:\n        return 0.0\n    return float(np.mean(np.array(ap_list, dtype=np.float32)))\n\ndef ensure_dir_for_file(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\n# =========================\n# Main application\n# =========================\ndef main():\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Setup interpreter\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(\"Model file not found: {}\".format(model_path))\n    interpreter = get_interpreter(model_path)\n    input_details = interpreter.get_input_details()\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n\n    # Setup video IO\n    if not os.path.exists(input_path):\n        raise FileNotFoundError(\"Input video file not found: {}\".format(input_path))\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(\"Failed to open video: {}\".format(input_path))\n\n    # Try to read first frame to get properties reliably\n    ret, first_frame = cap.read()\n    if not ret or first_frame is None:\n        cap.release()\n        raise RuntimeError(\"Failed to read frames from video: {}\".format(input_path))\n\n    height, width = first_frame.shape[:2]\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 1e-3:\n        fps = 30.0\n\n    ensure_dir_for_file(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(\"Failed to open VideoWriter for: {}\".format(output_path))\n\n    # mAP bookkeeping\n    predictions_by_class = {}  # {cls_id: [(frame_idx, score, [x1,y1,x2,y2]), ...]}\n    ground_truth_by_class = {}  # No ground truth provided; remains empty\n    frame_idx = 0\n\n    # Process first frame then loop remaining\n    frames_to_process = [first_frame]\n\n    def process_frame(frame, frame_idx):\n        nonlocal predictions_by_class\n        # Prepare input\n        input_data = preprocess_frame(frame, input_shape, input_dtype)\n        # Inference\n        boxes, classes, scores, num = run_inference(interpreter, input_data)\n        if boxes is None or scores is None:\n            # Cannot proceed if outputs missing\n            det_count = 0\n            map_val = compute_map(predictions_by_class, ground_truth_by_class, iou_threshold=0.5)\n            cv2.putText(frame, \"mAP: {:.3f}\".format(map_val), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2, cv2.LINE_AA)\n            return frame, det_count\n        n = int(num) if num is not None else len(scores)\n        n = min(n, len(scores), len(boxes))\n        # Draw detections and accumulate for mAP with pixel boxes\n        det_count = 0\n        h, w = frame.shape[:2]\n        for i in range(n):\n            sc = float(scores[i])\n            if sc < confidence_threshold:\n                continue\n            det_count += 1\n            cls_id = int(classes[i]) if classes is not None else -1\n            ymin, xmin, ymax, xmax = boxes[i]\n            x1 = int(max(0, min(w - 1, xmin * w)))\n            y1 = int(max(0, min(h - 1, ymin * h)))\n            x2 = int(max(0, min(w - 1, xmax * w)))\n            y2 = int(max(0, min(h - 1, ymax * h)))\n            # Accumulate prediction\n            if cls_id not in predictions_by_class:\n                predictions_by_class[cls_id] = []\n            predictions_by_class[cls_id].append((frame_idx, sc, [x1, y1, x2, y2]))\n        # Draw on frame\n        draw_detections(frame, boxes[:n], classes[:n] if classes is not None else None, scores[:n], labels, confidence_threshold)\n        # Compute running mAP (will be 0.0 without GT)\n        map_val = compute_map(predictions_by_class, ground_truth_by_class, iou_threshold=0.5)\n        cv2.putText(frame, \"mAP: {:.3f}\".format(map_val), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (50, 220, 50), 2, cv2.LINE_AA)\n        return frame, det_count\n\n    total_frames = 0\n    total_dets = 0\n    t0 = time.time()\n\n    # Process first frame\n    frame_out, dets = process_frame(frames_to_process[0], frame_idx)\n    writer.write(frame_out)\n    total_frames += 1\n    total_dets += dets\n    frame_idx += 1\n\n    # Process remaining frames\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_out, dets = process_frame(frame, frame_idx)\n        writer.write(frame_out)\n        total_frames += 1\n        total_dets += dets\n        frame_idx += 1\n\n    # Cleanup\n    cap.release()\n    writer.release()\n    elapsed = time.time() - t0\n\n    # Final mAP over processed video (will be 0.0 if no GT provided)\n    final_map = compute_map(predictions_by_class, ground_truth_by_class, iou_threshold=0.5)\n\n    print(\"Processing complete.\")\n    print(\"Input video: {}\".format(input_path))\n    print(\"Output video: {}\".format(output_path))\n    print(\"Frames processed: {}\".format(total_frames))\n    print(\"Total detections (score >= {}): {}\".format(confidence_threshold, total_dets))\n    print(\"Final mAP (IoU=0.5): {:.3f}\".format(final_map))\n    if total_frames > 0 and elapsed > 0:\n        print(\"Average FPS: {:.2f}\".format(total_frames / elapsed))\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 148.711,
      "total_cost": 0.09073625,
      "observations": [
        {
          "id": "time-12-31-48-730682_chatcmpl-D6DPpCHnFk53CcuMfxS3SVudynS4L",
          "traceId": "cc2a5772",
          "type": "GENERATION",
          "name": "cc2a_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:31:48.730000+00:00",
          "endTime": "2026-02-06T10:34:17.441000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.09073625,
          "latency": 148711.0,
          "costDetails": {
            "total": 0.09073625
          },
          "totalTokens": 9435,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 9022,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:31:48.623Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:34:28.864Z"
    },
    {
      "id": "daf123bc",
      "timestamp": "2026-02-06T10:29:59.076000+00:00",
      "name": "daf1_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# Per instruction: use ai_edge_litert Interpreter\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n\ndef load_labels(path):\n    labels = []\n    if not os.path.exists(path):\n        return labels\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            # Support formats like \"0 person\", \"0: person\", or just \"person\"\n            if \":\" in line:\n                parts = line.split(\":\", 1)\n                name = parts[1].strip()\n                labels.append(name)\n            elif line[:1].isdigit():\n                parts = line.split(maxsplit=1)\n                if len(parts) == 2 and parts[0].isdigit():\n                    labels.append(parts[1].strip())\n                else:\n                    labels.append(line.strip())\n            else:\n                labels.append(line.strip())\n    return labels\n\n\ndef get_label_name(class_id, labels):\n    if not labels:\n        return str(class_id)\n    # Handle off-by-one depending on label map (some start with '???')\n    if 0 <= class_id < len(labels):\n        name = labels[class_id]\n        if name == \"???\":\n            # Try class_id+1\n            if 0 <= class_id + 1 < len(labels):\n                return labels[class_id + 1]\n        return name\n    elif 0 <= class_id - 1 < len(labels):\n        return labels[class_id - 1]\n    return str(class_id)\n\n\ndef prepare_interpreter(model_path):\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef get_io_details(interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    # Assume single input\n    in_det = input_details[0]\n    in_index = in_det[\"index\"]\n    in_dtype = in_det[\"dtype\"]\n    in_shape = in_det[\"shape\"]  # [1, height, width, 3]\n    return in_index, in_dtype, in_shape, output_details\n\n\ndef preprocess_frame(frame, input_shape, input_dtype):\n    # input_shape: [1, height, width, 3]\n    h, w = int(input_shape[1]), int(input_shape[2])\n    # Convert BGR->RGB and resize\n    resized = cv2.resize(frame, (w, h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(rgb, axis=0)\n    if input_dtype == np.float32:\n        input_data = input_data.astype(np.float32) / 255.0\n    elif input_dtype == np.uint8:\n        input_data = input_data.astype(np.uint8)\n    else:\n        # Fallback: cast to required dtype without normalization\n        input_data = input_data.astype(input_dtype)\n    return input_data\n\n\ndef parse_outputs(interpreter, output_details):\n    # Attempt to identify boxes, classes, scores, num_detections from output_details\n    boxes_idx = None\n    classes_idx = None\n    scores_idx = None\n    num_idx = None\n\n    for i, det in enumerate(output_details):\n        shape = det.get(\"shape\", [])\n        dtype = det.get(\"dtype\", np.float32)\n        size = int(np.prod(shape)) if len(shape) > 0 else 1\n\n        if len(shape) >= 2 and shape[-1] == 4:\n            boxes_idx = i\n        elif size == 1:\n            num_idx = i\n        else:\n            # Heuristic: scores are float, classes often float/int but we will assign after scores\n            if dtype == np.float32:\n                # prefer scores as float\n                if scores_idx is None:\n                    scores_idx = i\n            else:\n                if classes_idx is None:\n                    classes_idx = i\n\n    # If classes_idx not found but we have two float outputs, pick the non-box, non-num one left\n    if classes_idx is None:\n        for i, det in enumerate(output_details):\n            if i in (boxes_idx, scores_idx, num_idx):\n                continue\n            classes_idx = i\n            break\n\n    def get_tensor_by_idx(idx):\n        if idx is None:\n            return None\n        return interpreter.get_tensor(output_details[idx][\"index\"])\n\n    boxes = get_tensor_by_idx(boxes_idx)\n    classes = get_tensor_by_idx(classes_idx)\n    scores = get_tensor_by_idx(scores_idx)\n    num = get_tensor_by_idx(num_idx)\n\n    # Normalize shapes to common format\n    # Expected shapes: boxes [1, N, 4]; classes [1, N]; scores [1, N]; num [1]\n    if boxes is not None and boxes.ndim == 2 and boxes.shape[-1] == 4:\n        boxes = np.expand_dims(boxes, 0)\n    if classes is not None and classes.ndim == 1:\n        classes = np.expand_dims(classes, 0)\n    if scores is not None and scores.ndim == 1:\n        scores = np.expand_dims(scores, 0)\n    if num is not None and num.size == 1:\n        num = float(num.flatten()[0])\n    elif scores is not None:\n        num = scores.shape[1]\n    else:\n        num = 0\n\n    # Convert to usable numpy arrays\n    boxes = boxes if boxes is not None else np.zeros((1, 0, 4), dtype=np.float32)\n    classes = classes if classes is not None else np.zeros((1, 0), dtype=np.float32)\n    scores = scores if scores is not None else np.zeros((1, 0), dtype=np.float32)\n\n    return boxes, classes, scores, int(num)\n\n\ndef draw_detections(frame, boxes, classes, scores, labels, threshold, running_map=None):\n    h, w = frame.shape[:2]\n    for i in range(boxes.shape[1]):\n        score = float(scores[0, i])\n        if score < threshold:\n            continue\n        cls_id = int(classes[0, i])\n        label = get_label_name(cls_id, labels)\n        ymin, xmin, ymax, xmax = boxes[0, i]\n        x1 = max(0, min(w - 1, int(xmin * w)))\n        y1 = max(0, min(h - 1, int(ymin * h)))\n        x2 = max(0, min(w - 1, int(xmax * w)))\n        y2 = max(0, min(h - 1, int(ymax * h)))\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        caption = f\"{label}: {score*100:.1f}%\"\n        (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - baseline - 4), (x1 + tw + 4, y1), color, -1)\n        cv2.putText(frame, caption, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Overlay mAP (proxy) on frame if provided\n    if running_map is not None:\n        text = f\"mAP: {running_map:.3f}\"\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n        pad = 6\n        cv2.rectangle(frame, (10, 10), (10 + tw + 2 * pad, 10 + th + 2 * pad), (0, 0, 0), -1)\n        cv2.putText(frame, text, (10 + pad, 10 + th + pad), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n\n\ndef main():\n    # Ensure output directory exists\n    out_dir = os.path.dirname(output_path)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Setup TFLite interpreter\n    interpreter = prepare_interpreter(model_path)\n    in_index, in_dtype, in_shape, out_details = get_io_details(interpreter)\n\n    # Video IO\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"Error: cannot open input video: {input_path}\")\n        return\n\n    # Read first frame to setup writer\n    ret, frame = cap.read()\n    if not ret or frame is None:\n        print(\"Error: failed to read the first frame from input video.\")\n        cap.release()\n        return\n\n    height, width = frame.shape[:2]\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0:\n        fps = 30.0  # fallback if FPS not available\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        print(f\"Error: cannot open output video for writing: {output_path}\")\n        cap.release()\n        return\n\n    # Processing loop\n    total_det_conf = 0.0  # For proxy mAP: sum of confidences of detections above threshold\n    total_det_count = 0   # For proxy mAP: count of detections above threshold\n\n    # Process the already-read first frame then continue\n    frames_processed = 0\n    start_time = time.time()\n\n    def process_and_write(curr_frame):\n        nonlocal total_det_conf, total_det_count\n        input_data = preprocess_frame(curr_frame, in_shape, in_dtype)\n        interpreter.set_tensor(in_index, input_data)\n        interpreter.invoke()\n        boxes, classes, scores, num = parse_outputs(interpreter, out_details)\n\n        # Update proxy mAP accumulators\n        if scores.size > 0:\n            valid = scores[0] >= confidence_threshold\n            if np.any(valid):\n                total_det_conf += float(np.sum(scores[0][valid]))\n                total_det_count += int(np.sum(valid))\n\n        # Compute running proxy mAP\n        map_proxy = (total_det_conf / total_det_count) if total_det_count > 0 else 0.0\n\n        # Draw and write\n        draw_detections(curr_frame, boxes, classes, scores, labels, confidence_threshold, running_map=map_proxy)\n        writer.write(curr_frame)\n\n    process_and_write(frame)\n    frames_processed += 1\n\n    while True:\n        ret, frame = cap.read()\n        if not ret or frame is None:\n            break\n        process_and_write(frame)\n        frames_processed += 1\n\n    elapsed = time.time() - start_time\n    cap.release()\n    writer.release()\n\n    # Final summary\n    fps_proc = frames_processed / elapsed if elapsed > 0 else 0.0\n    map_proxy_final = (total_det_conf / total_det_count) if total_det_count > 0 else 0.0\n    print(f\"Processed {frames_processed} frames in {elapsed:.2f}s ({fps_proc:.2f} FPS).\")\n    print(f\"Saved annotated video to: {output_path}\")\n    print(f\"mAP (proxy without ground truth): {map_proxy_final:.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 76.621,
      "total_cost": 0.05595625,
      "observations": [
        {
          "id": "time-12-29-59-684130_chatcmpl-D6DO3SiVMYdwGbJSVrbVxTFnrFipQ",
          "traceId": "daf123bc",
          "type": "GENERATION",
          "name": "daf1_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:29:59.684000+00:00",
          "endTime": "2026-02-06T10:31:16.305000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05595625,
          "latency": 76621.0,
          "costDetails": {
            "total": 0.05595625
          },
          "totalTokens": 5957,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 5544,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:29:59.638Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:31:27.795Z"
    },
    {
      "id": "777c558f",
      "timestamp": "2026-02-06T10:26:35.333000+00:00",
      "name": "777c_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# Per guideline: use the AI Edge LiteRT interpreter\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# CONFIGURATION PARAMETERS\n# =========================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"  # Read a single video file from the given input_path\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Output processed video\nCONFIDENCE_THRESHOLD = 0.5  # Threshold for displaying detections\n\n\ndef load_labels(label_path):\n    \"\"\"\n    Load labels from the provided label map.\n    Supports \"id label\" or \"label\" per line formats.\n    Returns a dict mapping class_id (int) -> label (str).\n    \"\"\"\n    labels = {}\n    if not os.path.isfile(label_path):\n        return labels\n    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n        for idx, line in enumerate(f):\n            line = line.strip()\n            if not line:\n                continue\n            # Try \"id label\" format first\n            parts = line.split(maxsplit=1)\n            if len(parts) == 2 and parts[0].isdigit():\n                class_id = int(parts[0])\n                label = parts[1].strip()\n                labels[class_id] = label\n            else:\n                # Fallback: assume simple \"label\" with implicit index\n                labels[idx] = line\n    return labels\n\n\ndef ensure_dir_for_file(file_path):\n    \"\"\"Ensure the parent directory exists for the given file path.\"\"\"\n    parent = os.path.dirname(os.path.abspath(file_path))\n    if parent and not os.path.exists(parent):\n        os.makedirs(parent, exist_ok=True)\n\n\ndef preprocess_frame(frame_bgr, input_shape, input_dtype):\n    \"\"\"\n    Preprocess a BGR frame to match the model input requirements.\n    - Resize to model's expected (H, W)\n    - Convert BGR to RGB\n    - Normalize if float input\n    - Add batch dimension\n    Returns the prepared input tensor.\n    \"\"\"\n    _, in_h, in_w, in_c = input_shape  # Expect shape [1, H, W, C]\n    resized = cv2.resize(frame_bgr, (in_w, in_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n    if input_dtype == np.float32:\n        input_data = rgb.astype(np.float32) / 255.0\n    else:\n        # Assume uint8 quantized\n        input_data = rgb.astype(np.uint8)\n\n    # Add batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n\ndef identify_detection_tensors(interpreter):\n    \"\"\"\n    Identify and fetch detection outputs from the interpreter in a robust way.\n    Returns a dict with keys: 'boxes', 'classes', 'scores', 'count'\n      - boxes: (N, 4) array with [ymin, xmin, ymax, xmax] normalized to [0,1] (typical SSD output)\n      - classes: (N,) array (float or int). If unavailable, returns None.\n      - scores: (N,) float array. If unavailable, returns None.\n      - count: int number of detections (if provided), else deduced from shapes\n    This function avoids assumptions about output tensor order and guards against None values.\n    \"\"\"\n    outputs = interpreter.get_output_details()\n    arrays = []\n    meta = []\n    for od in outputs:\n        name = (od.get(\"name\") or \"\").lower()\n        tensor = interpreter.get_tensor(od[\"index\"])\n        arr = np.squeeze(tensor)  # remove leading dims like [1, ...]\n        arrays.append(arr)\n        meta.append({\"name\": name, \"shape\": arr.shape, \"dtype\": arr.dtype})\n\n    boxes = None\n    classes = None\n    scores = None\n    count = None\n\n    # Pass 1: Identify by name hints\n    for arr, m in zip(arrays, meta):\n        name = m[\"name\"]\n        shape = m[\"shape\"]\n        if isinstance(shape, tuple) and len(shape) == 2 and shape[1] == 4 and (\"box\" in name or \"bbox\" in name or \"loc\" in name or \":0\" in name):\n            boxes = arr\n        elif len(np.atleast_1d(arr).shape) == 1 and (\"class\" in name or \"label\" in name or \":1\" in name):\n            classes = arr\n        elif len(np.atleast_1d(arr).shape) == 1 and (\"score\" in name or \"confidence\" in name or \"prob\" in name or \":2\" in name):\n            scores = arr\n        elif (np.issubdtype(m[\"dtype\"], np.integer) and ((\"num\" in name) or (\"count\" in name) or name.endswith(\":3\"))):\n            try:\n                count = int(np.array(arr).flatten()[0])\n            except Exception:\n                pass\n\n    # Pass 2: Fallback by shape if still missing\n    if boxes is None:\n        for arr, m in zip(arrays, meta):\n            shape = m[\"shape\"]\n            if isinstance(shape, tuple) and len(shape) == 2 and shape[1] == 4:\n                boxes = arr\n                break\n\n    # Determine N (number of candidates)\n    N = 0\n    if boxes is not None and len(boxes.shape) == 2 and boxes.shape[1] == 4:\n        N = boxes.shape[0]\n    elif scores is not None and np.ndim(scores) == 1:\n        N = scores.shape[0]\n    elif classes is not None and np.ndim(classes) == 1:\n        N = classes.shape[0]\n\n    if count is None:\n        # Some models don't provide a separate count tensor\n        count = int(N)\n\n    # If classes or scores not identified by name, try to infer by shape relative to N\n    if classes is None:\n        for arr, m in zip(arrays, meta):\n            if np.ndim(arr) == 1 and arr.shape[0] == N and (np.issubdtype(m[\"dtype\"], np.integer) or np.issubdtype(m[\"dtype\"], np.floating)):\n                # Avoid picking scores if already set\n                if scores is not None and np.allclose(arr, scores):\n                    continue\n                classes = arr\n                break\n\n    if scores is None:\n        # Pick a 1D float array of length N as scores\n        for arr, m in zip(arrays, meta):\n            if np.ndim(arr) == 1 and arr.shape[0] == N and np.issubdtype(m[\"dtype\"], np.floating):\n                if classes is not None and np.allclose(arr, classes):\n                    continue\n                scores = arr\n                break\n\n    # Normalize dtypes and shapes\n    if boxes is not None:\n        boxes = np.array(boxes, dtype=np.float32).reshape(-1, 4)\n    if classes is not None:\n        classes = np.array(classes)\n    if scores is not None:\n        scores = np.array(scores, dtype=np.float32).reshape(-1)\n\n    # Safety cap: ensure count does not exceed available lengths\n    if boxes is not None:\n        count = min(count, boxes.shape[0])\n    if classes is not None:\n        count = min(count, classes.shape[0])\n    if scores is not None:\n        count = min(count, scores.shape[0])\n\n    return {\"boxes\": boxes, \"classes\": classes, \"scores\": scores, \"count\": int(count)}\n\n\ndef build_detections(output_dict, frame_w, frame_h, confidence_threshold, labels):\n    \"\"\"\n    Build a list of detection dicts from model outputs.\n    Each detection dict contains:\n      - 'box': (x1, y1, x2, y2) in pixel coordinates\n      - 'score': float confidence\n      - 'class_id': int id (or -1 if unknown)\n      - 'label': string label\n    This function avoids the previous crash by guarding None outputs.\n    \"\"\"\n    boxes = output_dict.get(\"boxes\", None)\n    classes = output_dict.get(\"classes\", None)\n    scores = output_dict.get(\"scores\", None)\n    count = int(output_dict.get(\"count\", 0))\n\n    detections = []\n\n    if scores is None or boxes is None or count <= 0:\n        return detections  # Nothing to show\n\n    # Ensure shapes\n    count = min(count, boxes.shape[0], scores.shape[0])\n\n    for i in range(count):\n        score = float(scores[i])\n        if score < confidence_threshold:\n            continue\n\n        # SSD boxes are typically [ymin, xmin, ymax, xmax] normalized to [0,1]\n        ymin, xmin, ymax, xmax = boxes[i].tolist()\n\n        # Convert to pixel coordinates\n        x1 = int(max(0, xmin) * frame_w)\n        y1 = int(max(0, ymin) * frame_h)\n        x2 = int(min(1.0, xmax) * frame_w)\n        y2 = int(min(1.0, ymax) * frame_h)\n\n        # Guard for classes being None\n        if classes is not None and i < classes.shape[0]:\n            try:\n                class_id = int(classes[i])\n            except Exception:\n                # Some models provide float class IDs\n                class_id = int(float(classes[i]))\n        else:\n            class_id = -1\n\n        # Label resolution\n        if class_id in (labels or {}):\n            label_str = labels[class_id]\n        elif class_id >= 0:\n            label_str = f\"id_{class_id}\"\n        else:\n            label_str = \"N/A\"\n\n        detections.append({\n            \"box\": (x1, y1, x2, y2),\n            \"score\": score,\n            \"class_id\": class_id,\n            \"label\": label_str\n        })\n\n    return detections\n\n\ndef draw_detections(frame, detections, map_value=None, fps=None):\n    \"\"\"\n    Draw rectangles and label texts for each detection.\n    Optionally overlay mAP and FPS texts.\n    \"\"\"\n    for det in detections:\n        x1, y1, x2, y2 = det[\"box\"]\n        label = det[\"label\"]\n        score = det[\"score\"]\n        color = (0, 255, 0)  # Green box\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n        text = f\"{label}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(0, y1 - 10)\n        cv2.rectangle(frame, (x1, y_text - th - baseline), (x1 + tw, y_text + baseline // 2), color, -1)\n        cv2.putText(frame, text, (x1, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Overlay metrics\n    overlay_y = 20\n    if map_value is not None:\n        cv2.putText(frame, f\"mAP (proxy): {map_value:.3f}\", (10, overlay_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50, 200, 255), 2, cv2.LINE_AA)\n        overlay_y += 22\n    if fps is not None:\n        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, overlay_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50, 200, 255), 2, cv2.LINE_AA)\n\n\ndef compute_proxy_map(per_class_scores):\n    \"\"\"\n    Compute a proxy mAP from accumulated detection confidences per class.\n    Note: True mAP requires ground truth. Since only a video input is provided (no annotations),\n    we compute a simple proxy: average of mean confidences across classes that had detections.\n    \"\"\"\n    class_means = []\n    for cls_id, scores in per_class_scores.items():\n        if scores:  # non-empty\n            class_means.append(float(np.mean(scores)))\n    if not class_means:\n        return 0.0\n    return float(np.mean(class_means))\n\n\ndef main():\n    # 1) Setup: Load interpreter, allocate tensors, load labels, open input video\n    if not os.path.isfile(MODEL_PATH):\n        raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize interpreter\n    num_threads = max(1, min(4, os.cpu_count() or 1))\n    interpreter = Interpreter(model_path=MODEL_PATH, num_threads=num_threads)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    if not input_details:\n        raise RuntimeError(\"Interpreter has no input details.\")\n    input_index = input_details[0][\"index\"]\n    input_shape = input_details[0][\"shape\"]\n    input_dtype = input_details[0][\"dtype\"]\n\n    # Open input video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # sensible default for Raspberry Pi\n\n    # Prepare output writer\n    ensure_dir_for_file(OUTPUT_PATH)\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    out_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_w, frame_h))\n    if not out_writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output writer: {OUTPUT_PATH}\")\n\n    # 2-4) Loop: preprocess, inference, postprocess, draw, compute proxy mAP, save\n    per_class_scores = {}  # class_id -> list of confidences\n    prev_time = time.time()\n    avg_fps = None\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess\n        input_data = preprocess_frame(frame, input_shape, input_dtype)\n\n        # Inference\n        interpreter.set_tensor(input_index, input_data)\n        t0 = time.time()\n        interpreter.invoke()\n        t1 = time.time()\n\n        # Extract detections robustly (fixes prior NoneType issue)\n        output_dict = identify_detection_tensors(interpreter)\n        detections = build_detections(output_dict, frame_w, frame_h, CONFIDENCE_THRESHOLD, labels)\n\n        # Accumulate scores for proxy mAP\n        for det in detections:\n            cls = det[\"class_id\"]\n            if cls not in per_class_scores:\n                per_class_scores[cls] = []\n            per_class_scores[cls].append(det[\"score\"])\n        proxy_map = compute_proxy_map(per_class_scores)\n\n        # FPS smoothing\n        inst_fps = 1.0 / max(1e-6, (t1 - t0))\n        if avg_fps is None:\n            avg_fps = inst_fps\n        else:\n            avg_fps = 0.9 * avg_fps + 0.1 * inst_fps\n\n        # Draw and write\n        draw_detections(frame, detections, map_value=proxy_map, fps=avg_fps)\n        out_writer.write(frame)\n\n    # Cleanup\n    cap.release()\n    out_writer.release()\n\n    # Final log\n    print(f\"Processing complete.\")\n    print(f\"Input video:  {INPUT_PATH}\")\n    print(f\"Output video: {OUTPUT_PATH}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 173.819,
      "total_cost": 0.12624625,
      "observations": [
        {
          "id": "time-12-26-35-973237_chatcmpl-D6DKmK57BPgdfPEVor2UPLWRzpSlg",
          "traceId": "777c558f",
          "type": "GENERATION",
          "name": "777c_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:26:35.973000+00:00",
          "endTime": "2026-02-06T10:27:59.339000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05793625,
          "latency": 83366.0,
          "costDetails": {
            "total": 0.05793625
          },
          "totalTokens": 6155,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 5742,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-28-00-195590_chatcmpl-D6DM8G4lYkoc8TrUrSoAcVPjm7Vq0",
          "traceId": "777c558f",
          "type": "GENERATION",
          "name": "777c_psg_gen_attempt#2",
          "startTime": "2026-02-06T10:28:00.195000+00:00",
          "endTime": "2026-02-06T10:29:29.792000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06831,
          "latency": 89597.0,
          "costDetails": {
            "total": 0.06831
          },
          "totalTokens": 7475,
          "environment": "default",
          "promptTokens": 736,
          "completionTokens": 6739,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:26:35.897Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:29:40.194Z"
    },
    {
      "id": "7367e42e",
      "timestamp": "2026-02-06T10:24:07.440000+00:00",
      "name": "7367_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# Configuration parameters (provided)\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5  # for drawing/final results\n\n# Helper: ensure output directory exists\ndef ensure_parent_dir(path):\n    parent = os.path.dirname(path)\n    if parent and not os.path.exists(parent):\n        os.makedirs(parent, exist_ok=True)\n\n# Load labels from file\ndef load_labels(label_path):\n    labels = []\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\n# Prepare input tensor from frame according to interpreter input details\ndef prepare_input(frame_bgr, input_details):\n    # Determine input shape and dtype\n    height, width = input_details[0]['shape'][1], input_details[0]['shape'][2]\n    dtype = input_details[0]['dtype']\n    quant = input_details[0].get('quantization', (0.0, 0))\n    scale, zero_point = quant if isinstance(quant, (tuple, list)) else (0.0, 0)\n\n    # Convert BGR to RGB and resize\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (width, height), interpolation=cv2.INTER_LINEAR)\n\n    if dtype == np.uint8:\n        if scale and scale > 0:\n            # Quantize if quantization params provided\n            input_data = np.clip(resized.astype(np.float32) / scale + zero_point, 0, 255).astype(np.uint8)\n        else:\n            input_data = resized.astype(np.uint8)\n    else:\n        # float32 path\n        input_data = resized.astype(np.float32) / 255.0\n\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n# Retrieve model outputs robustly\ndef get_detection_outputs(interpreter, output_details):\n    # Attempt to map by names if available, else by shape heuristics\n    tensors = {}\n    for od in output_details:\n        name = od.get('name', '').lower()\n        data = interpreter.get_tensor(od['index'])\n        if 'box' in name:\n            tensors['boxes'] = data\n        elif 'score' in name:\n            tensors['scores'] = data\n        elif 'class' in name:\n            tensors['classes'] = data\n        elif 'num' in name or 'count' in name:\n            tensors['num'] = data\n\n    # Fallback using shapes if names were not informative\n    if 'boxes' not in tensors or 'scores' not in tensors or 'classes' not in tensors or 'num' not in tensors:\n        outs = {od['index']: interpreter.get_tensor(od['index']) for od in output_details}\n        # boxes: 3D with last dim=4\n        for k, v in outs.items():\n            if isinstance(v, np.ndarray) and v.ndim == 3 and v.shape[-1] == 4:\n                tensors['boxes'] = v\n        # scores/classes: 2D [1, N]\n        candidates_2d = [v for v in outs.values() if isinstance(v, np.ndarray) and v.ndim == 2]\n        if len(candidates_2d) >= 2:\n            # Heuristic: classes are closer to integer values\n            a, b = candidates_2d[0], candidates_2d[1]\n            def is_int_like(x):\n                return np.mean(np.abs(x - np.round(x))) < 1e-3\n            if is_int_like(a) and not is_int_like(b):\n                tensors['classes'] = a\n                tensors['scores'] = b\n            elif is_int_like(b) and not is_int_like(a):\n                tensors['classes'] = b\n                tensors['scores'] = a\n            else:\n                # If both look similar, choose by name if exists, else default order\n                tensors.setdefault('scores', a)\n                tensors.setdefault('classes', b)\n        # num: 1D [1]\n        for v in outs.values():\n            if isinstance(v, np.ndarray) and v.ndim == 1 and v.shape[0] == 1:\n                tensors['num'] = v\n\n    # Final safety checks\n    boxes = tensors.get('boxes', None)\n    classes = tensors.get('classes', None)\n    scores = tensors.get('scores', None)\n    num = tensors.get('num', None)\n    if boxes is None or classes is None or scores is None or num is None:\n        raise RuntimeError(\"Could not parse TFLite detection outputs.\")\n\n    # Squeeze to remove batch dimension\n    boxes = np.squeeze(boxes)\n    classes = np.squeeze(classes)\n    scores = np.squeeze(scores)\n    num = int(np.squeeze(num).astype(np.int32))\n    return boxes, classes, scores, num\n\n# Convert normalized box [ymin, xmin, ymax, xmax] to absolute [xmin, ymin, xmax, ymax] in pixels and clamp\ndef denormalize_and_clamp(box, img_w, img_h):\n    ymin, xmin, ymax, xmax = box\n    xmin_abs = max(0, min(int(xmin * img_w), img_w - 1))\n    xmax_abs = max(0, min(int(xmax * img_w), img_w - 1))\n    ymin_abs = max(0, min(int(ymin * img_h), img_h - 1))\n    ymax_abs = max(0, min(int(ymax * img_h), img_h - 1))\n    # Ensure proper ordering\n    if xmax_abs < xmin_abs: xmin_abs, xmax_abs = xmax_abs, xmin_abs\n    if ymax_abs < ymin_abs: ymin_abs, ymax_abs = ymax_abs, ymin_abs\n    return [xmin_abs, ymin_abs, xmax_abs, ymax_abs]\n\n# Compute IoU between a single box and an array of boxes\ndef iou_with_many(box, boxes):\n    # box, boxes expected as [xmin, ymin, xmax, ymax]\n    if boxes.size == 0:\n        return np.zeros((0,), dtype=np.float32)\n    xA = np.maximum(box[0], boxes[:, 0])\n    yA = np.maximum(box[1], boxes[:, 1])\n    xB = np.minimum(box[2], boxes[:, 2])\n    yB = np.minimum(box[3], boxes[:, 3])\n\n    interW = np.maximum(0, xB - xA + 1)\n    interH = np.maximum(0, yB - yA + 1)\n    interArea = interW * interH\n\n    boxArea = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n    boxesArea = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n\n    unionArea = boxArea + boxesArea - interArea + 1e-9\n    return interArea / unionArea\n\n# Greedy Non-Maximum Suppression\ndef nms(boxes, scores, iou_threshold=0.5):\n    if boxes.shape[0] == 0:\n        return np.array([], dtype=np.int32)\n    idxs = scores.argsort()[::-1]\n    selected = []\n    while idxs.size > 0:\n        i = idxs[0]\n        selected.append(i)\n        if idxs.size == 1:\n            break\n        rest = idxs[1:]\n        ious = iou_with_many(boxes[i], boxes[rest])\n        keep = rest[ious <= iou_threshold]\n        idxs = keep\n    return np.array(selected, dtype=np.int32)\n\n# Compute AP for a single class given GT boxes and predicted boxes with scores\ndef compute_ap(gt_boxes, pred_boxes, pred_scores, iou_thresh=0.5):\n    # gt_boxes: (G, 4), pred_boxes: (P, 4), pred_scores: (P,)\n    G = gt_boxes.shape[0]\n    if G == 0:\n        return None  # undefined; will be skipped in mAP\n    if pred_boxes.shape[0] == 0:\n        return 0.0\n\n    # Sort predictions by score desc\n    order = pred_scores.argsort()[::-1]\n    pred_boxes = pred_boxes[order]\n    pred_scores = pred_scores[order]\n\n    matched_gt = np.zeros((G,), dtype=np.uint8)\n    tp = np.zeros((pred_boxes.shape[0],), dtype=np.float32)\n    fp = np.zeros((pred_boxes.shape[0],), dtype=np.float32)\n\n    for i in range(pred_boxes.shape[0]):\n        ious = iou_with_many(pred_boxes[i], gt_boxes)\n        best_idx = int(np.argmax(ious)) if ious.size > 0 else -1\n        best_iou = ious[best_idx] if ious.size > 0 else 0.0\n        if best_iou >= iou_thresh and matched_gt[best_idx] == 0:\n            tp[i] = 1.0\n            matched_gt[best_idx] = 1\n        else:\n            fp[i] = 1.0\n\n    cum_tp = np.cumsum(tp)\n    cum_fp = np.cumsum(fp)\n    recalls = cum_tp / (G + 1e-9)\n    precisions = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n\n    # Precision envelope\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n    for i in range(mpre.size - 1, 0, -1):\n        if mpre[i - 1] < mpre[i]:\n            mpre[i - 1] = mpre[i]\n    # Integrate AP as area under curve\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n    return float(ap)\n\n# Compute mAP across classes (skipping classes with no GT)\ndef compute_map(gt_by_class, pred_by_class, iou_thresh=0.5):\n    aps = []\n    for cls_id in gt_by_class.keys():\n        gt_list = gt_by_class.get(cls_id, [])\n        pred_list = pred_by_class.get(cls_id, [])\n        if len(gt_list) == 0:\n            continue\n        gt_boxes = np.array(gt_list, dtype=np.float32).reshape(-1, 4)\n        if len(pred_list) == 0:\n            aps.append(0.0)\n            continue\n        pred_boxes = np.array([p[1] for p in pred_list], dtype=np.float32).reshape(-1, 4)\n        pred_scores = np.array([p[0] for p in pred_list], dtype=np.float32).reshape(-1,)\n        ap = compute_ap(gt_boxes, pred_boxes, pred_scores, iou_thresh=iou_thresh)\n        if ap is not None:\n            aps.append(ap)\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\n# Deterministic color for a class id\ndef class_color(cls_id):\n    # Simple hashing to BGR\n    r = (37 * (cls_id + 1)) % 255\n    g = (17 * (cls_id + 1)) % 255\n    b = (29 * (cls_id + 1)) % 255\n    return int(b), int(g), int(r)\n\ndef main():\n    # Setup\n    ensure_parent_dir(OUTPUT_PATH)\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Video IO\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(\"Failed to open input video: %s\" + INPUT_PATH)\n    in_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, float(fps), (in_w, in_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(\"Failed to open output video for writing: %s\" + OUTPUT_PATH)\n\n    # Aggregators for proxy mAP (GT via NMS-filtered detections; Predictions = raw detections)\n    gt_by_class = {}    # cls_id -> list of boxes\n    pred_by_class = {}  # cls_id -> list of (score, box)\n\n    # Inference loop\n    frame_idx = 0\n    t0 = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_idx += 1\n\n        # Prepare input\n        input_data = prepare_input(frame, input_details)\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        # Inference\n        interpreter.invoke()\n\n        # Get outputs\n        boxes_norm, classes_raw, scores_raw, num = get_detection_outputs(interpreter, output_details)\n\n        # Collect detections\n        # Use a lower threshold for prediction collection (for PR curve)\n        raw_thresh = 0.05\n        preds_per_class_boxes = {}\n        preds_per_class_scores = {}\n\n        drawn_boxes = []  # final boxes to draw after NMS at CONFIDENCE_THRESHOLD\n        drawn_scores = []\n        drawn_classes = []\n\n        # Make sure we iterate only over 'num'\n        det_count = min(num, boxes_norm.shape[0], scores_raw.shape[0], classes_raw.shape[0])\n        for i in range(det_count):\n            score = float(scores_raw[i])\n            cls_id = int(classes_raw[i])\n            # Map to label index: many TFLite SSD labelmaps have '???' at 0, so add 1\n            mapped_cls_id = cls_id  # store original numeric id for metrics\n            # Denormalize box\n            abs_box = denormalize_and_clamp(boxes_norm[i], in_w, in_h)  # [xmin, ymin, xmax, ymax]\n\n            # Collect raw predictions per class for mAP computation\n            if score >= raw_thresh:\n                if mapped_cls_id not in preds_per_class_boxes:\n                    preds_per_class_boxes[mapped_cls_id] = []\n                    preds_per_class_scores[mapped_cls_id] = []\n                preds_per_class_boxes[mapped_cls_id].append(abs_box)\n                preds_per_class_scores[mapped_cls_id].append(score)\n\n            # Collect for final drawing if above main threshold\n            if score >= CONFIDENCE_THRESHOLD:\n                drawn_boxes.append(abs_box)\n                drawn_scores.append(score)\n                drawn_classes.append(mapped_cls_id)\n\n        # Update prediction aggregator\n        for c in preds_per_class_boxes.keys():\n            if c not in pred_by_class:\n                pred_by_class[c] = []\n            for pb, ps in zip(preds_per_class_boxes[c], preds_per_class_scores[c]):\n                pred_by_class[c].append((ps, pb))\n\n        # Build GT via NMS on drawn detections per class\n        gt_this_frame_by_class = {}\n        if len(drawn_boxes) > 0:\n            drawn_boxes_arr = np.array(drawn_boxes, dtype=np.float32)\n            drawn_scores_arr = np.array(drawn_scores, dtype=np.float32)\n            drawn_classes_arr = np.array(drawn_classes, dtype=np.int32)\n            # Per class NMS\n            for c in np.unique(drawn_classes_arr):\n                mask = (drawn_classes_arr == c)\n                c_boxes = drawn_boxes_arr[mask]\n                c_scores = drawn_scores_arr[mask]\n                keep_idx = nms(c_boxes, c_scores, iou_threshold=0.5)\n                if keep_idx.size > 0:\n                    c_kept = c_boxes[keep_idx]\n                    gt_this_frame_by_class[c] = c_kept\n        # Update GT aggregator\n        for c, c_gt_boxes in gt_this_frame_by_class.items():\n            if c not in gt_by_class:\n                gt_by_class[c] = []\n            for b in c_gt_boxes:\n                gt_by_class[c].append(b.tolist())\n\n        # Compute running mAP (proxy) across all frames so far\n        map_val = compute_map(gt_by_class, pred_by_class, iou_thresh=0.5)\n\n        # Draw final (NMS-filtered) boxes and labels on frame\n        # We already have NMS per class results in gt_this_frame_by_class; draw those\n        for c, c_gt_boxes in gt_this_frame_by_class.items():\n            color = class_color(c)\n            for b in c_gt_boxes:\n                x1, y1, x2, y2 = int(b[0]), int(b[1]), int(b[2]), int(b[3])\n                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n                # Choose the highest score among raw predictions overlapping this GT for label score display\n                # Find best-matching prediction for score display\n                scores_for_c = preds_per_class_scores.get(c, [])\n                boxes_for_c = preds_per_class_boxes.get(c, [])\n                best_score = 0.0\n                if boxes_for_c:\n                    boxes_for_c_arr = np.array(boxes_for_c, dtype=np.float32)\n                    ious = iou_with_many([x1, y1, x2, y2], boxes_for_c_arr)\n                    if ious.size > 0:\n                        j = int(np.argmax(ious))\n                        best_score = float(scores_for_c[j])\n                # Label text\n                # Use label mapping with +1 offset if appropriate\n                lbl_idx = c + 1 if (c + 1) < len(labels) else c\n                lbl_text = labels[lbl_idx] if lbl_idx < len(labels) else f\"id:{c}\"\n                text = f\"{lbl_text} {best_score*100:.1f}%\"\n                cv2.putText(frame, text, (x1, max(0, y1 - 7)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n\n        # Overlay mAP\n        cv2.putText(frame, f\"mAP: {map_val*100:.2f}%\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 220, 0), 2, cv2.LINE_AA)\n\n        # Write frame\n        writer.write(frame)\n\n    # Cleanup\n    cap.release()\n    writer.release()\n\n    total_time = time.time() - t0\n    final_map = compute_map(gt_by_class, pred_by_class, iou_thresh=0.5)\n    print(\"Processing complete.\")\n    print(f\"Frames processed: {frame_idx}\")\n    print(f\"Elapsed time: {total_time:.2f}s\")\n    print(f\"Proxy mAP (detections vs. NMS-filtered detections) @ IoU=0.5: {final_map:.4f}\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 108.118,
      "total_cost": 0.08834625,
      "observations": [
        {
          "id": "time-12-24-08-065750_chatcmpl-D6DIOCjqy3CKTy2A24zQCJOIZRSm1",
          "traceId": "7367e42e",
          "type": "GENERATION",
          "name": "7367_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:24:08.065000+00:00",
          "endTime": "2026-02-06T10:25:56.183000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08834625,
          "latency": 108118.0,
          "costDetails": {
            "total": 0.08834625
          },
          "totalTokens": 9196,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 8783,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:24:08.008Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:26:15.740Z"
    },
    {
      "id": "7baf60cc",
      "timestamp": "2026-02-06T10:22:28.816000+00:00",
      "name": "7baf_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# Per guideline, use ai_edge_litert Interpreter\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# CONFIGURATION PARAMETERS\n# =========================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"  # Read a single video file from the given input_path\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Output video with rectangles, labels, and mAP\nCONFIDENCE_THRESHOLD = 0.5\n\n# =========================\n# UTILITY FUNCTIONS\n# =========================\ndef load_labels(label_path):\n    # Assumes one label per line. If a background label like \"???\", keep it so indices match.\n    labels = []\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            labels.append(line)\n    return labels\n\ndef preprocess_frame(frame_bgr, input_height, input_width, input_dtype):\n    # Convert BGR to RGB, resize to model input, add batch dimension\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (input_width, input_height))\n    if input_dtype == np.uint8:\n        input_data = np.expand_dims(resized.astype(np.uint8), axis=0)\n    else:\n        # Float32: normalize to [0,1]\n        input_data = np.expand_dims(resized.astype(np.float32) / 255.0, axis=0)\n    return input_data\n\ndef get_detections(interpreter, threshold, frame_w, frame_h, labels):\n    # TFLite SSD output order: boxes, classes, scores, num\n    out_details = interpreter.get_output_details()\n    boxes = interpreter.get_tensor(out_details[0]['index'])[0]   # [N,4] in [ymin, xmin, ymax, xmax], normalized\n    classes = interpreter.get_tensor(out_details[1]['index'])[0] # [N]\n    scores = interpreter.get_tensor(out_details[2]['index'])[0]  # [N]\n    num = int(interpreter.get_tensor(out_details[3]['index'])[0])\n\n    results = []\n    for i in range(num):\n        score = float(scores[i])\n        if score < threshold:\n            continue\n        ymin, xmin, ymax, xmax = boxes[i]\n        # Convert to absolute pixel coordinates and clip\n        x1 = max(0, int(xmin * frame_w))\n        y1 = max(0, int(ymin * frame_h))\n        x2 = min(frame_w - 1, int(xmax * frame_w))\n        y2 = min(frame_h - 1, int(ymax * frame_h))\n        cls_id = int(classes[i])  # Typically 0-based or 1-based depending on label file\n        label = labels[cls_id] if 0 <= cls_id < len(labels) else f\"id_{cls_id}\"\n        results.append((x1, y1, x2, y2, cls_id, label, score))\n    return results\n\ndef draw_detections(frame_bgr, detections, mAP_value):\n    # Draw boxes and labels; overlay mAP\n    for (x1, y1, x2, y2, cls_id, label, score) in detections:\n        color = ((37 * (cls_id + 1)) % 255, (17 * (cls_id + 1)) % 255, (29 * (cls_id + 1)) % 255)\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        text = f\"{label}: {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(0, y1 - 8)\n        cv2.rectangle(frame_bgr, (x1, y_text - th - 4), (x1 + tw + 2, y_text + 2), color, -1)\n        cv2.putText(frame_bgr, text, (x1 + 1, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Draw mAP at top-left\n    map_text = f\"mAP: {mAP_value:.3f}\"\n    cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)\n\ndef safe_video_props(cap):\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 1e-3:\n        fps = 30.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    if width <= 0 or height <= 0:\n        # Fallback probe using first frame\n        ret, frame = cap.read()\n        if not ret:\n            return 30.0, 640, 480, None\n        height, width = frame.shape[:2]\n        return fps, width, height, frame\n    return fps, width, height, None\n\n# =========================\n# MAIN APPLICATION LOGIC\n# =========================\ndef main():\n    # 1) Setup: load interpreter, allocate tensors, load labels, open input video\n    if not os.path.isfile(MODEL_PATH):\n        raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n    if not os.path.isfile(LABEL_PATH):\n        raise FileNotFoundError(f\"Label file not found: {LABEL_PATH}\")\n    if not os.path.isfile(INPUT_PATH):\n        raise FileNotFoundError(f\"Input video not found: {INPUT_PATH}\")\n\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize interpreter (use available CPU cores on Raspberry Pi 4B)\n    num_threads = max(1, (os.cpu_count() or 4))\n    interpreter = Interpreter(model_path=MODEL_PATH, num_threads=num_threads)\n    interpreter.allocate_tensors()\n\n    in_details = interpreter.get_input_details()\n    in_index = in_details[0]['index']\n    in_shape = in_details[0]['shape']  # [1, H, W, C]\n    in_dtype = in_details[0]['dtype']\n    input_h, input_w = int(in_shape[1]), int(in_shape[2])\n\n    # Open video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    fps, frame_w, frame_h, first_frame = safe_video_props(cap)\n\n    # 2-pass approach:\n    #   Pass 1: Run inference per frame, collect detections, and accumulate scores to compute mAP proxy.\n    #   Pass 2: Reopen video, draw detections and final mAP, write to output.\n\n    detections_per_frame = []\n    all_scores = []\n    frame_idx = 0\n\n    # If we probed and consumed first frame, handle it\n    frames_to_process = []\n    if first_frame is not None:\n        frames_to_process.append(first_frame)\n\n    # Read remaining frames in pass 1\n    while True:\n        if first_frame is None:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frames_to_process.append(frame)\n        # Process accumulated frames (to avoid storing all video frames in memory, we process immediately)\n        for frame in frames_to_process:\n            # 2) Preprocessing\n            input_data = preprocess_frame(frame, input_h, input_w, in_dtype)\n            interpreter.set_tensor(in_index, input_data)\n\n            # 3) Inference\n            t0 = time.time()\n            interpreter.invoke()\n            _ = time.time() - t0  # inference_time (not strictly required to display)\n\n            # 4) Collect detections for this frame\n            dets = get_detections(interpreter, CONFIDENCE_THRESHOLD, frame_w, frame_h, labels)\n            detections_per_frame.append(dets)\n            # Accumulate scores for proxy mAP computation\n            for d in dets:\n                all_scores.append(d[6])  # score at index 6\n\n            frame_idx += 1\n        frames_to_process = []\n        first_frame = None\n\n    cap.release()\n\n    # Compute proxy mAP: mean of detection confidences across all detections above threshold\n    # Note: True mAP requires ground-truth annotations to compute precision/recall vs IoU.\n    # Here we provide a proxy metric due to unavailable ground truth.\n    if len(all_scores) > 0:\n        mAP_value = float(np.mean(np.array(all_scores, dtype=np.float32)))\n    else:\n        mAP_value = 0.0\n\n    # Pass 2: Draw and save the video with rectangles, labels, and mAP text\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        writer.release()\n        raise RuntimeError(f\"Failed to reopen input video for rendering: {INPUT_PATH}\")\n\n    out_index = 0\n    while True:\n        ret, frame = cap.read()\n        if not ret or out_index >= len(detections_per_frame):\n            break\n        dets = detections_per_frame[out_index]\n        draw_detections(frame, dets, mAP_value)\n        writer.write(frame)\n        out_index += 1\n\n    cap.release()\n    writer.release()\n\n    # Optional console summary\n    print(f\"Processed {out_index} frames.\")\n    print(f\"Detections above threshold: {len(all_scores)}\")\n    print(f\"mAP (proxy mean confidence): {mAP_value:.4f}\")\n    print(f\"Saved output video to: {OUTPUT_PATH}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 68.853,
      "total_cost": 0.05769625,
      "observations": [
        {
          "id": "time-12-22-29-440873_chatcmpl-D6DGnGFqXJqScRw9rA28b6sqEVqKe",
          "traceId": "7baf60cc",
          "type": "GENERATION",
          "name": "7baf_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:22:29.440000+00:00",
          "endTime": "2026-02-06T10:23:38.293000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05769625,
          "latency": 68853.0,
          "costDetails": {
            "total": 0.05769625
          },
          "totalTokens": 6131,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 5718,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:22:29.374Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:23:47.692Z"
    },
    {
      "id": "da485475",
      "timestamp": "2026-02-06T10:20:44.994000+00:00",
      "name": "da48_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport cv2\nimport numpy as np\nfrom ai_edge_litert.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(path):\n    labels = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef prepare_input(image_bgr, input_shape, input_dtype, quant_params):\n    # input_shape: [1, h, w, c]\n    _, ih, iw, _ = input_shape\n    # Convert to RGB and resize to model input\n    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(image_rgb, (iw, ih), interpolation=cv2.INTER_LINEAR)\n    input_tensor = resized.astype(np.float32)\n    if input_dtype == np.float32:\n        # Normalize to [0,1]\n        input_tensor = input_tensor / 255.0\n    else:\n        # Quantized input (uint8) with scale/zero_point\n        scale, zero_point = quant_params if quant_params is not None else (1.0, 0)\n        if scale and scale > 0:\n            input_tensor = np.round(input_tensor / scale + zero_point).astype(np.uint8)\n        else:\n            input_tensor = input_tensor.astype(np.uint8)\n    # Add batch dimension\n    input_tensor = np.expand_dims(input_tensor, axis=0)\n    return input_tensor\n\ndef extract_output(interpreter):\n    # Attempt to robustly fetch boxes, classes, scores, num\n    output_details = interpreter.get_output_details()\n    tensors = {}\n    # Retrieve all tensors first\n    for od in output_details:\n        name = od.get('name', '').lower()\n        data = interpreter.get_tensor(od['index'])\n        tensors[name] = data\n\n    # Heuristic mapping by names\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    def find_by_keyword(keyword):\n        for name, data in tensors.items():\n            if keyword in name:\n                return data\n        return None\n\n    boxes = find_by_keyword('box')\n    if boxes is None:\n        boxes = find_by_keyword('bbox')\n    classes = find_by_keyword('class')\n    scores = find_by_keyword('score')\n    num = find_by_keyword('num')\n\n    # Fallback to positional assumptions\n    if boxes is None or classes is None or scores is None:\n        # Positional guess: [boxes, classes, scores, num]\n        # Sort by rank/shape if names not informative\n        ods = interpreter.get_output_details()\n        arrs = [interpreter.get_tensor(od['index']) for od in ods]\n        # Find boxes: tensor with shape [1, N, 4]\n        for a in arrs:\n            if isinstance(a, np.ndarray) and a.ndim == 3 and a.shape[-1] == 4:\n                boxes = a\n                break\n        # Find num: tensor with shape [1] or scalar-like\n        for a in arrs:\n            if isinstance(a, np.ndarray) and a.size == 1 and a.ndim in (0, 1):\n                num = a\n                break\n        # Remaining two of shape [1, N] -> classes and scores (float32)\n        candidates = [a for a in arrs if isinstance(a, np.ndarray) and a.ndim == 2 and a.shape[0] == 1]\n        if len(candidates) >= 2:\n            # Heuristic: scores are [0,1] floats\n            cand0, cand1 = candidates[0], candidates[1]\n            def looks_like_scores(x):\n                return x.dtype.kind == 'f' and np.all(x <= 1.0001) and np.all(x >= -0.0001)\n            if looks_like_scores(cand0) and not looks_like_scores(cand1):\n                scores, classes = cand0, cand1\n            elif looks_like_scores(cand1) and not looks_like_scores(cand0):\n                scores, classes = cand1, cand0\n            else:\n                # Default: assume order scores then classes\n                scores, classes = cand0, cand1\n\n    # Squeeze leading batch if present\n    if isinstance(boxes, np.ndarray) and boxes.ndim == 3:\n        boxes = boxes[0]\n    if isinstance(classes, np.ndarray) and classes.ndim == 2:\n        classes = classes[0]\n    if isinstance(scores, np.ndarray) and scores.ndim == 2:\n        scores = scores[0]\n    if isinstance(num, np.ndarray):\n        num = int(num.flatten()[0])\n\n    # Final safeguards\n    if boxes is None or classes is None or scores is None or num is None:\n        raise RuntimeError(\"Failed to parse model outputs (boxes/classes/scores/num).\")\n\n    # Classes may come as float; cast to int for indexing\n    classes = classes.astype(np.int32, copy=False)\n    return boxes, classes, scores, num\n\ndef compute_map_proxy(conf_by_class):\n    # Proxy mAP: average of mean confidences per class that had at least one detection\n    # Note: This is a placeholder proxy since no ground-truth annotations are provided.\n    per_class_avgs = []\n    for _, confs in conf_by_class.items():\n        if len(confs) > 0:\n            per_class_avgs.append(float(np.mean(confs)))\n    if len(per_class_avgs) == 0:\n        return 0.0\n    return float(np.mean(per_class_avgs))\n\ndef get_color_for_class(class_id):\n    # Deterministic color per class id\n    rng = np.random.RandomState(class_id * 9973 + 12345)\n    color = tuple(int(x) for x in rng.randint(0, 255, size=3))\n    return color\n\ndef draw_labelled_box(frame, box, label_text, color, thickness=2):\n    x1, y1, x2, y2 = box\n    cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n    # Text background\n    (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n    th_full = th + baseline\n    x2_bg = min(frame.shape[1] - 1, x1 + tw + 6)\n    y2_bg = max(0, y1 - th_full - 4)\n    cv2.rectangle(frame, (x1, y1 - th_full - 6), (x2_bg, y1), color, -1)\n    cv2.putText(frame, label_text, (x1 + 3, y1 - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\ndef main():\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Load labels\n    labels = load_labels(label_path)\n    label_offset = 1 if len(labels) > 0 and labels[0].strip().lower() == '???' else 0\n\n    # Initialize TFLite Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']  # [1, h, w, c]\n    input_dtype = input_details[0]['dtype']\n    quant_params = input_details[0].get('quantization', None)\n    if isinstance(quant_params, (list, tuple)) and len(quant_params) == 2:\n        pass\n    else:\n        quant_params = None\n\n    # Video IO setup\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    in_fps = cap.get(cv2.CAP_PROP_FPS)\n    if not in_fps or in_fps <= 0:\n        in_fps = 30.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Use mp4v for .mp4 output\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, in_fps, (width, height))\n    if not out.isOpened():\n        raise RuntimeError(f\"Failed to open output video for writing: {output_path}\")\n\n    # Statistics\n    conf_by_class = {}  # class_id -> list of confidences (above threshold)\n    infer_times = []\n    total_frames = 0\n\n    # Processing loop\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        total_frames += 1\n\n        # Prepare input\n        input_tensor = prepare_input(frame_bgr, input_shape, input_dtype, quant_params)\n\n        # Set input and run inference\n        interpreter.set_tensor(input_index, input_tensor)\n        t0 = time.time()\n        interpreter.invoke()\n        t1 = time.time()\n        infer_times.append(t1 - t0)\n\n        # Extract and postprocess outputs\n        try:\n            boxes, classes, scores, num = extract_output(interpreter)\n        except Exception as e:\n            # If output parsing fails, write frame untouched but continue\n            cv2.putText(frame_bgr, f\"Inference output error: {e}\", (10, 30),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2, cv2.LINE_AA)\n            out.write(frame_bgr)\n            continue\n\n        # Draw detections\n        for i in range(min(num, boxes.shape[0], scores.shape[0], classes.shape[0])):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n\n            cls_id_raw = int(classes[i])\n            cls_id = cls_id_raw + (label_offset)\n            # Map cls_id to label index space\n            label_idx = cls_id\n            if label_idx < 0 or label_idx >= len(labels):\n                label_text = f\"id:{cls_id_raw} {score:.2f}\"\n            else:\n                label_text = f\"{labels[label_idx]} {score:.2f}\"\n\n            # Box coordinates are normalized ymin, xmin, ymax, xmax\n            y_min, x_min, y_max, x_max = boxes[i]\n            x1 = max(0, min(width - 1, int(x_min * width)))\n            y1 = max(0, min(height - 1, int(y_min * height)))\n            x2 = max(0, min(width - 1, int(x_max * width)))\n            y2 = max(0, min(height - 1, int(y_max * height)))\n            # Ensure proper ordering\n            x1, x2 = min(x1, x2), max(x1, x2)\n            y1, y2 = min(y1, y2), max(y1, y2)\n\n            color = get_color_for_class(cls_id_raw)\n            draw_labelled_box(frame_bgr, (x1, y1, x2, y2), label_text, color, thickness=2)\n\n            # Update proxy mAP stats\n            if cls_id_raw not in conf_by_class:\n                conf_by_class[cls_id_raw] = []\n            conf_by_class[cls_id_raw].append(score)\n\n        # Compute and overlay proxy mAP and FPS\n        map_proxy = compute_map_proxy(conf_by_class)\n        avg_infer = np.mean(infer_times) if len(infer_times) > 0 else 0.0\n        fps_est = 1.0 / avg_infer if avg_infer > 0 else 0.0\n\n        overlay_text_1 = f\"Frames: {total_frames}  mAP(proxy): {map_proxy:.3f}\"\n        overlay_text_2 = f\"FPS(est): {fps_est:.1f}  Threshold: {confidence_threshold:.2f}\"\n        cv2.putText(frame_bgr, overlay_text_1, (10, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n        cv2.putText(frame_bgr, overlay_text_2, (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n\n        out.write(frame_bgr)\n\n    # Release resources\n    cap.release()\n    out.release()\n\n    # Final stats\n    final_map_proxy = compute_map_proxy(conf_by_class)\n    avg_infer = float(np.mean(infer_times)) if len(infer_times) > 0 else 0.0\n    print(\"Processing complete.\")\n    print(f\"Total frames: {total_frames}\")\n    print(f\"Average inference time per frame: {avg_infer*1000:.2f} ms\")\n    print(f\"Estimated FPS: {1.0/avg_infer if avg_infer>0 else 0.0:.2f}\")\n    print(f\"Proxy mAP (no ground-truth available): {final_map_proxy:.3f}\")\n    print(f\"Output saved to: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 70.812,
      "total_cost": 0.06305625,
      "observations": [
        {
          "id": "time-12-20-45-589084_chatcmpl-D6DF7bxUaPmMYsIeP0HiDKRbL9Wew",
          "traceId": "da485475",
          "type": "GENERATION",
          "name": "da48_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:20:45.589000+00:00",
          "endTime": "2026-02-06T10:21:56.401000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06305625,
          "latency": 70812.0,
          "costDetails": {
            "total": 0.06305625
          },
          "totalTokens": 6667,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 6254,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:20:45.576Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:22:08.225Z"
    },
    {
      "id": "59800a99",
      "timestamp": "2026-02-06T10:19:11.391000+00:00",
      "name": "5980_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration Parameters\n# =========================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"         # Read a single video file from the given input_path\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Output the video file with rectangles, labels, and mAP\nCONF_THRESHOLD = 0.5\n\n\ndef load_labels(path):\n    labels = []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            name = line.strip()\n            if name:\n                # Handle potential \"index name\" format or plain name\n                # If the line has an index and a name, split and keep the name part.\n                parts = name.split(maxsplit=1)\n                if len(parts) == 2 and parts[0].isdigit():\n                    labels.append(parts[1])\n                else:\n                    labels.append(name)\n    return labels\n\n\ndef get_input_size_and_dtype(interpreter):\n    input_details = interpreter.get_input_details()\n    if not input_details:\n        raise RuntimeError(\"No input details found in the interpreter.\")\n    input_shape = input_details[0][\"shape\"]\n    # Expect shape [1, height, width, channels]\n    if len(input_shape) != 4:\n        raise RuntimeError(f\"Unexpected input tensor shape: {input_shape}\")\n    height, width = int(input_shape[1]), int(input_shape[2])\n    dtype = input_details[0][\"dtype\"]\n    return width, height, dtype\n\n\ndef preprocess_frame(frame_bgr, input_w, input_h, input_dtype):\n    # Convert BGR to RGB for most TFLite detection models\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (input_w, input_h), interpolation=cv2.INTER_LINEAR)\n    if input_dtype == np.float32:\n        tensor = resized.astype(np.float32) / 255.0\n    else:\n        # For uint8/other types, pass as is (TFLite quantization parameters will be used internally)\n        tensor = resized.astype(input_dtype)\n    tensor = np.expand_dims(tensor, axis=0)  # [1, H, W, C]\n    return tensor\n\n\ndef run_inference(interpreter, input_tensor):\n    input_details = interpreter.get_input_details()\n    interpreter.set_tensor(input_details[0][\"index\"], input_tensor)\n    interpreter.invoke()\n    output_details = interpreter.get_output_details()\n\n    # Typical SSD MobileNet v1 TFLite outputs:\n    # 0: detection_boxes [1, num, 4] (ymin, xmin, ymax, xmax) normalized\n    # 1: detection_classes [1, num]\n    # 2: detection_scores [1, num]\n    # 3: num_detections [1]\n    # However, order may vary; we'll infer by tensor shapes.\n    boxes = classes = scores = num = None\n    for od in output_details:\n        out = interpreter.get_tensor(od[\"index\"])\n        if out.ndim == 3 and out.shape[-1] == 4:\n            boxes = out[0]\n        elif out.ndim == 2:\n            # Could be classes or scores depending on dtype\n            if out.dtype in (np.float32, np.float64):\n                # assume scores\n                scores = out[0].astype(np.float32)\n            else:\n                classes = out[0].astype(np.int32)\n        elif out.size == 1:\n            num = int(np.squeeze(out).astype(np.int32))\n\n    # Fallback if num_detections not present\n    if num is None and scores is not None:\n        num = len(scores)\n\n    # Trim arrays to num\n    if boxes is not None:\n        boxes = boxes[:num]\n    if classes is not None:\n        classes = classes[:num]\n    if scores is not None:\n        scores = scores[:num]\n\n    return boxes, classes, scores, num\n\n\ndef label_for_class(class_id, labels):\n    # Try direct index\n    if 0 <= class_id < len(labels):\n        return labels[class_id]\n    # Many TFLite SSD models are 1-based in label files (first entry '???')\n    if 0 <= (class_id + 1) < len(labels):\n        return labels[class_id + 1]\n    return f\"id:{class_id}\"\n\n\ndef draw_detections(frame_bgr, boxes, classes, scores, labels, conf_thr):\n    h, w = frame_bgr.shape[:2]\n    drawn = 0\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score < conf_thr:\n            continue\n        ymin, xmin, ymax, xmax = boxes[i]\n        # Convert from normalized [0,1] to absolute pixels\n        x1 = int(max(0, xmin) * w)\n        y1 = int(max(0, ymin) * h)\n        x2 = int(min(1.0, xmax) * w)\n        y2 = int(min(1.0, ymax) * h)\n\n        cls_id = int(classes[i]) if classes is not None else -1\n        cls_name = label_for_class(cls_id, labels) if labels else f\"id:{cls_id}\"\n\n        color = (0, 200, 0)  # Green\n        thickness = 2\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, thickness)\n\n        label_text = f\"{cls_name}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        # Draw filled rectangle for text background\n        y_text = max(0, y1 - th - 4)\n        cv2.rectangle(frame_bgr, (x1, y_text), (x1 + tw + 4, y_text + th + baseline + 4), color, -1)\n        cv2.putText(\n            frame_bgr, label_text, (x1 + 2, y_text + th + 2),\n            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA\n        )\n        drawn += 1\n    return drawn\n\n\ndef main():\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize TFLite interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n    input_w, input_h, input_dtype = get_input_size_and_dtype(interpreter)\n\n    # Open input video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0:\n        fps = 30.0  # fallback\n\n    # Setup video writer\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    # For runtime stats and proxy mAP calculation (no ground-truth available)\n    proxy_scores_all = []  # Collect all detection scores above threshold\n    frame_index = 0\n    t0_total = time.time()\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_index += 1\n\n            # Preprocess\n            input_tensor = preprocess_frame(frame, input_w, input_h, input_dtype)\n\n            # Inference\n            t0 = time.time()\n            boxes, classes, scores, num = run_inference(interpreter, input_tensor)\n            infer_time_ms = (time.time() - t0) * 1000.0\n\n            # Draw detections\n            if boxes is None or scores is None:\n                # No outputs; write frame as-is\n                display = frame.copy()\n                map_text = \"mAP (proxy): N/A\"\n                cv2.putText(display, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n                writer.write(display)\n                continue\n\n            display = frame.copy()\n            # Accumulate scores above threshold for a proxy \"mAP\" in absence of ground truth\n            above_thr = [float(s) for s in scores if s >= CONF_THRESHOLD]\n            proxy_scores_all.extend(above_thr)\n\n            _ = draw_detections(display, boxes, classes, scores, labels, CONF_THRESHOLD)\n\n            # Compute running proxy mAP (mean of detection scores above threshold)\n            if proxy_scores_all:\n                map_proxy = float(np.mean(proxy_scores_all))\n                map_text = f\"mAP (proxy): {map_proxy:.3f}\"\n            else:\n                map_text = \"mAP (proxy): N/A\"\n\n            # Overlay info: proxy mAP and inference time\n            cv2.putText(display, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (50, 170, 50), 2, cv2.LINE_AA)\n            perf_text = f\"Infer: {infer_time_ms:.1f} ms\"\n            cv2.putText(display, perf_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (60, 60, 220), 2, cv2.LINE_AA)\n\n            writer.write(display)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    total_time = time.time() - t0_total\n    # Final proxy mAP summary in console\n    if proxy_scores_all:\n        final_map_proxy = float(np.mean(proxy_scores_all))\n        print(f\"Finished. Frames: {frame_index}, Time: {total_time:.2f}s, Proxy mAP: {final_map_proxy:.4f}\")\n    else:\n        print(f\"Finished. Frames: {frame_index}, Time: {total_time:.2f}s, Proxy mAP: N/A (no detections above threshold)\")\n\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 61.937,
      "total_cost": 0.05598625,
      "observations": [
        {
          "id": "time-12-19-11-983856_chatcmpl-D6DDcTlFmnpDkpBhM9vyTkAMB7t2i",
          "traceId": "59800a99",
          "type": "GENERATION",
          "name": "5980_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:19:11.983000+00:00",
          "endTime": "2026-02-06T10:20:13.920000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05598625,
          "latency": 61937.0,
          "costDetails": {
            "total": 0.05598625
          },
          "totalTokens": 5960,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 5547,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:19:11.953Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:20:25.140Z"
    },
    {
      "id": "e1993021",
      "timestamp": "2026-02-06T10:16:33.457000+00:00",
      "name": "e199_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5  # for drawing/filtering final detections\nIOU_THRESHOLD = 0.5         # for matching/NMS\nMIN_SCORE_FOR_MAP = 0.05    # keep low to accumulate predictions for proxy mAP\n\ndef load_labels(label_path):\n    labels = {}\n    with open(label_path, 'r', encoding='utf-8') as f:\n        lines = [l.strip() for l in f.readlines() if l.strip()]\n    # Common TFLite label files have '???' as label 0; skip if present\n    if lines and lines[0].startswith('???'):\n        lines = lines[1:]\n    for i, name in enumerate(lines):\n        labels[i] = name\n    return labels\n\ndef preprocess_frame(frame, input_details):\n    ih, iw = input_details['shape'][1], input_details['shape'][2]\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (iw, ih))\n    tensor = None\n    if input_details['dtype'] == np.float32:\n        tensor = np.expand_dims(resized.astype(np.float32) / 255.0, axis=0)\n    else:\n        # uint8 (quantized) - pass as-is\n        tensor = np.expand_dims(resized.astype(np.uint8), axis=0)\n    return tensor\n\ndef set_input_tensor(interpreter, input_details, input_data):\n    interpreter.set_tensor(input_details['index'], input_data)\n\ndef get_output_tensors(interpreter, output_details):\n    outputs = []\n    for od in output_details:\n        outputs.append(interpreter.get_tensor(od['index']))\n    # Expecting 4 outputs: boxes, classes, scores, count (typical SSD MobileNet V1)\n    # Try to infer by shapes/dtypes if ordering is unknown\n    boxes = classes = scores = num = None\n    for out in outputs:\n        out_shape = out.shape\n        out_dtype = out.dtype\n        if len(out_shape) == 2 and out_shape[1] == 4:\n            boxes = out\n        elif len(out_shape) == 2 and out_shape[1] > 1 and out_dtype in (np.float32, np.uint8):\n            # could be classes or scores depending on dtype\n            # classes is usually float32 but integral-like values\n            # scores are float32 in [0,1]\n            # Heuristic: if values are mostly integers within a small range, it's classes\n            arr = out[0]\n            if out_dtype == np.float32 and np.all(np.mod(arr, 1) == 0):\n                classes = out\n            else:\n                # Try to distinguish by value range\n                if np.max(arr) <= 1.0:\n                    scores = out\n                else:\n                    classes = out\n        elif len(out_shape) == 1 or (len(out_shape) == 2 and out_shape[1] == 1):\n            num = out\n    # Fallback to common TFLite ordering: [boxes, classes, scores, num]\n    if boxes is None or classes is None or scores is None:\n        try:\n            boxes, classes, scores, num = outputs\n        except Exception:\n            pass\n    return boxes, classes, scores, num\n\ndef denorm_box_to_pixels(box, frame_w, frame_h):\n    # TFLite SSD outputs boxes as [ymin, xmin, ymax, xmax] normalized [0,1]\n    ymin, xmin, ymax, xmax = box\n    xmin = int(max(0, min(frame_w - 1, xmin * frame_w)))\n    xmax = int(max(0, min(frame_w - 1, xmax * frame_w)))\n    ymin = int(max(0, min(frame_h - 1, ymin * frame_h)))\n    ymax = int(max(0, min(frame_h - 1, ymax * frame_h)))\n    return [xmin, ymin, xmax, ymax]\n\ndef iou(box_a, box_b):\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n    inter_w = max(0, inter_x2 - inter_x1)\n    inter_h = max(0, inter_y2 - inter_y1)\n    inter_area = inter_w * inter_h\n    area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)\n    area_b = max(0, bx2 - bx1) * max(0, by2 - by1)\n    union = area_a + area_b - inter_area\n    if union <= 0:\n        return 0.0\n    return inter_area / union\n\ndef nms(boxes, scores, iou_thresh=0.5):\n    if len(boxes) == 0:\n        return []\n    idxs = list(np.argsort(scores)[::-1])\n    keep = []\n    while idxs:\n        current = idxs.pop(0)\n        keep.append(current)\n        idxs = [i for i in idxs if iou(boxes[current], boxes[i]) < iou_thresh]\n    return keep\n\ndef build_proxy_ground_truth(all_detections, num_classes, confidence_threshold=0.5, iou_thresh=0.5):\n    # Returns gt_by_class: {class_id: {frame_idx: [boxes...]}}\n    gt_by_class = {c: {} for c in range(num_classes)}\n    for frame_idx, frame_dets in enumerate(all_detections):\n        # Group by class\n        by_class = {}\n        for det in frame_dets:\n            c = det['class_id']\n            s = det['score']\n            if s < confidence_threshold:\n                continue\n            by_class.setdefault(c, {'boxes': [], 'scores': []})\n            by_class[c]['boxes'].append(det['box'])\n            by_class[c]['scores'].append(s)\n        # NMS per class to get proxy GT boxes\n        for c, data in by_class.items():\n            boxes = data['boxes']\n            scores = data['scores']\n            keep_idx = nms(boxes, scores, iou_thresh=iou_thresh)\n            kept_boxes = [boxes[i] for i in keep_idx]\n            if kept_boxes:\n                gt_by_class[c].setdefault(frame_idx, [])\n                gt_by_class[c][frame_idx].extend(kept_boxes)\n    return gt_by_class\n\ndef compute_map_proxy(all_detections, num_classes, iou_thresh=0.5, gt_conf_thresh=0.5, min_pred_score=0.05):\n    # Build proxy ground-truth using NMS on confident detections\n    gt_by_class = build_proxy_ground_truth(all_detections, num_classes, confidence_threshold=gt_conf_thresh, iou_thresh=iou_thresh)\n\n    ap_list = []\n\n    for c in range(num_classes):\n        # Build predictions list across frames for this class\n        preds = []\n        for frame_idx, frame_dets in enumerate(all_detections):\n            for det in frame_dets:\n                if det['class_id'] == c and det['score'] >= min_pred_score:\n                    preds.append({'frame_idx': frame_idx, 'score': det['score'], 'box': det['box']})\n        if len(preds) == 0:\n            continue\n\n        # Total number of GT boxes for this class\n        total_gt = sum(len(lst) for lst in gt_by_class.get(c, {}).values())\n        if total_gt == 0:\n            # No GT proxies for this class; skip from mAP\n            continue\n\n        # Sort predictions by descending score\n        preds.sort(key=lambda x: x['score'], reverse=True)\n\n        # For each frame, create 'used' flags for GT boxes\n        used_flags = {}\n        for fidx, gts in gt_by_class.get(c, {}).items():\n            used_flags[fidx] = [False] * len(gts)\n\n        tp = np.zeros(len(preds), dtype=np.float32)\n        fp = np.zeros(len(preds), dtype=np.float32)\n\n        for i, p in enumerate(preds):\n            fidx = p['frame_idx']\n            pred_box = p['box']\n            gts = gt_by_class.get(c, {}).get(fidx, [])\n            best_iou = 0.0\n            best_idx = -1\n            for j, gt_box in enumerate(gts):\n                if used_flags[fidx][j]:\n                    continue\n                iou_val = iou(pred_box, gt_box)\n                if iou_val > best_iou:\n                    best_iou = iou_val\n                    best_idx = j\n            if best_iou >= iou_thresh and best_idx >= 0:\n                tp[i] = 1.0\n                used_flags[fidx][best_idx] = True\n            else:\n                fp[i] = 1.0\n\n        # Cumulative precision-recall\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        recall = cum_tp / max(1, total_gt)\n        precision = cum_tp / np.maximum(1, (cum_tp + cum_fp))\n\n        # Compute AP using precision envelope method\n        mrec = np.concatenate(([0.0], recall, [1.0]))\n        mpre = np.concatenate(([0.0], precision, [0.0]))\n        for i in range(len(mpre) - 2, -1, -1):\n            mpre[i] = max(mpre[i], mpre[i + 1])\n        # Sum over recall steps where it changes\n        idx = np.where(mrec[1:] != mrec[:-1])[0]\n        ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n        ap_list.append(ap)\n\n    if len(ap_list) == 0:\n        return None\n    return float(np.mean(ap_list))\n\ndef main():\n    # Ensure output directory exists\n    out_dir = os.path.dirname(OUTPUT_PATH)\n    if out_dir and not os.path.isdir(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n    num_classes = max(labels.keys()) + 1 if labels else 0\n\n    # Load TFLite interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()\n\n    # Open input video (single file)\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(\"Error: Cannot open input video:\", INPUT_PATH)\n        return\n\n    # Read video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 1e-2:\n        fps = 30.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # First pass: run inference and store detections per frame\n    all_detections = []  # list of lists; one per frame\n    frame_idx = 0\n    t0 = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        input_tensor = preprocess_frame(frame, input_details)\n        set_input_tensor(interpreter, input_details, input_tensor)\n        interpreter.invoke()\n        boxes_out, classes_out, scores_out, num_out = get_output_tensors(interpreter, output_details)\n\n        # Normalize shapes\n        if boxes_out is None or classes_out is None or scores_out is None:\n            print(\"Error: Unexpected TFLite model outputs.\")\n            cap.release()\n            return\n\n        # Squeeze to remove batch dimension\n        boxes = np.squeeze(boxes_out)\n        classes = np.squeeze(classes_out).astype(np.int32)\n        scores = np.squeeze(scores_out).astype(np.float32)\n\n        # Some models provide num_detections\n        if num_out is not None:\n            n = int(np.squeeze(num_out))\n            boxes = boxes[:n]\n            classes = classes[:n]\n            scores = scores[:n]\n\n        frame_dets = []\n        for i in range(len(scores)):\n            score = float(scores[i])\n            cls_id = int(classes[i])\n            # Clip class id to labels range if necessary\n            if labels and cls_id not in labels:\n                # If class id exceeds label mapping, skip\n                continue\n            # Convert normalized box to pixel coords\n            box_norm = boxes[i]\n            xmin, ymin, xmax, ymax = denorm_box_to_pixels(box_norm, width, height)\n            # Sanity check\n            if xmax <= xmin or ymax <= ymin:\n                continue\n            frame_dets.append({\n                'class_id': cls_id,\n                'score': score,\n                'box': [xmin, ymin, xmax, ymax]\n            })\n\n        all_detections.append(frame_dets)\n        frame_idx += 1\n\n    cap.release()\n    elapsed = time.time() - t0\n    if frame_idx > 0:\n        print(f\"Inference pass complete: {frame_idx} frames in {elapsed:.2f}s ({frame_idx / max(1e-6, elapsed):.2f} FPS)\")\n\n    # Compute proxy mAP across the video\n    map_score = compute_map_proxy(\n        all_detections=all_detections,\n        num_classes=num_classes,\n        iou_thresh=IOU_THRESHOLD,\n        gt_conf_thresh=CONFIDENCE_THRESHOLD,\n        min_pred_score=MIN_SCORE_FOR_MAP\n    )\n\n    # Second pass: draw detections and save output video with mAP overlay\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(\"Error: Cannot reopen input video for writing:\", INPUT_PATH)\n        return\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        print(\"Error: Cannot open output video for writing:\", OUTPUT_PATH)\n        cap.release()\n        return\n\n    frame_index = 0\n    overlay_text = f\"mAP: {'N/A' if map_score is None else f'{map_score:.3f}'}\"\n    text_pos = (10, 25)\n    label_font = cv2.FONT_HERSHEY_SIMPLEX\n\n    while True:\n        ret, frame = cap.read()\n        if not ret or frame_index >= len(all_detections):\n            break\n\n        dets = all_detections[frame_index]\n        # Filter for drawing using confidence threshold\n        draw_dets = [d for d in dets if d['score'] >= CONFIDENCE_THRESHOLD]\n\n        for det in draw_dets:\n            xmin, ymin, xmax, ymax = det['box']\n            cls_id = det['class_id']\n            score = det['score']\n            label = labels.get(cls_id, str(cls_id)) if labels else str(cls_id)\n\n            # Draw rectangle\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n\n            # Prepare label text\n            text = f\"{label}: {score:.2f}\"\n            (tw, th), bl = cv2.getTextSize(text, label_font, 0.5, 1)\n            # Draw filled rectangle behind text for readability\n            cv2.rectangle(frame, (xmin, max(0, ymin - th - 6)), (xmin + tw + 4, ymin), (0, 255, 0), -1)\n            cv2.putText(frame, text, (xmin + 2, max(0, ymin - 4)), label_font, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n        # Draw mAP overlay\n        cv2.putText(frame, overlay_text, text_pos, label_font, 0.7, (50, 200, 255), 2, cv2.LINE_AA)\n\n        writer.write(frame)\n        frame_index += 1\n\n    cap.release()\n    writer.release()\n\n    print(\"Output saved to:\", OUTPUT_PATH)\n    if map_score is None:\n        print(\"mAP: N/A (no sufficient proxy ground-truth could be derived)\")\n    else:\n        print(f\"mAP (proxy, GT via high-conf NMS): {map_score:.3f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 125.29,
      "total_cost": 0.09222625,
      "observations": [
        {
          "id": "time-12-16-34-005545_chatcmpl-D6DB4J5M5yC56U1xtm0QmhUuDWOzj",
          "traceId": "e1993021",
          "type": "GENERATION",
          "name": "e199_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:16:34.005000+00:00",
          "endTime": "2026-02-06T10:18:39.295000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.09222625,
          "latency": 125290.0,
          "costDetails": {
            "total": 0.09222625
          },
          "totalTokens": 9584,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 9171,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:16:34.038Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:18:50.781Z"
    },
    {
      "id": "8724bffb",
      "timestamp": "2026-02-06T10:14:49.554000+00:00",
      "name": "8724_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Configuration parameters\n# =========================\nMODEL_PATH = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH = \"data/object_detection/sheeps.mp4\"   # Read a single video file from the given input_path\nOUTPUT_PATH = \"results/object_detection/test_results/sheeps_detections.mp4\"  # Save video with rectangles, labels, and mAP\nCONFIDENCE_THRESHOLD = 0.5\n\n\ndef load_labels(label_path):\n    labels = {}\n    with open(label_path, \"r\", encoding=\"utf-8\") as f:\n        for idx, line in enumerate(f):\n            name = line.strip()\n            if name:\n                labels[idx] = name\n    return labels\n\n\ndef make_dirs_for_file(file_path):\n    out_dir = os.path.dirname(os.path.abspath(file_path))\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n\ndef preprocess_frame(frame_bgr, input_w, input_h, input_dtype):\n    # Convert BGR (OpenCV) to RGB (most models expect RGB)\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (input_w, input_h), interpolation=cv2.INTER_LINEAR)\n\n    if input_dtype == np.float32:\n        inp = resized.astype(np.float32) / 255.0\n    else:\n        # Assume uint8 quantized model\n        inp = resized.astype(np.uint8)\n    # Add batch dimension\n    inp = np.expand_dims(inp, axis=0)\n    return inp\n\n\ndef parse_outputs(interpreter):\n    \"\"\"\n    Robustly extract detection outputs: boxes, classes, scores, num_detections\n    Works with common SSD Mobilenet TFLite postprocess outputs.\n    Returns:\n        boxes: (N,4) in [ymin, xmin, ymax, xmax] normalized\n        classes: (N,) float/int class ids\n        scores: (N,) float scores\n        num_detections: int\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    # Fetch all outputs once\n    tensors = []\n    for od in output_details:\n        data = interpreter.get_tensor(od[\"index\"])\n        tensors.append((od, data))\n\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # First try by name\n    for od, data in tensors:\n        name = od.get(\"name\", \"\").lower()\n        if \"box\" in name:\n            boxes = np.squeeze(data, axis=0)\n        elif \"score\" in name:\n            scores = np.squeeze(data, axis=0)\n        elif \"class\" in name:\n            classes = np.squeeze(data, axis=0)\n        elif \"num\" in name:\n            num_det = int(np.squeeze(data))\n\n    # Fallback by shape/content if name-based failed\n    if boxes is None or scores is None or classes is None or num_det is None:\n        # Identify boxes: shape (1, N, 4)\n        for od, data in tensors:\n            if data.ndim == 3 and data.shape[-1] == 4:\n                boxes = data[0]\n\n        cand = []\n        single = []\n        for od, data in tensors:\n            if od.get(\"name\", \"\").lower().find(\"box\") >= 0:\n                continue\n            if data.ndim == 2 and data.shape[0] == 1:\n                cand.append(np.squeeze(data, axis=0))\n            if data.size == 1:\n                single.append(int(np.squeeze(data)))\n\n        # Among cand, identify scores as the one largely in [0,1]\n        if len(cand) >= 2:\n            conf_idx = None\n            for i, arr in enumerate(cand):\n                if arr.dtype.kind == \"f\":\n                    # fraction of values in [0,1]\n                    frac_in_01 = np.mean((arr >= 0.0) & (arr <= 1.0))\n                    if frac_in_01 > 0.8:\n                        conf_idx = i\n                        break\n            if conf_idx is not None:\n                scores = cand[conf_idx]\n                classes = cand[1 - conf_idx]\n            else:\n                # Default assign\n                scores = cand[0]\n                classes = cand[1]\n        if num_det is None:\n            num_det = single[0] if single else (scores.shape[0] if scores is not None else 0)\n\n    # Ensure proper shapes/types\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.float32)\n\n    # Clip num_det to available arrays\n    n = int(num_det) if isinstance(num_det, (int, np.integer)) else int(np.squeeze(num_det))\n    n = min(n, boxes.shape[0], scores.shape[0], classes.shape[0])\n    return boxes[:n], classes[:n], scores[:n], n\n\n\nclass MAPAggregator:\n    \"\"\"\n    Simple aggregator to compute a running mAP-like metric across frames.\n    Here, AP per class is approximated as the mean confidence of detections\n    above threshold for that class. mAP is the mean of per-class APs.\n    \"\"\"\n    def __init__(self, threshold):\n        self.threshold = threshold\n        self.class_scores = {}  # class_id -> list of scores\n\n    def update(self, classes, scores):\n        for c, s in zip(classes, scores):\n            if s >= self.threshold:\n                cid = int(c)\n                if cid not in self.class_scores:\n                    self.class_scores[cid] = []\n                self.class_scores[cid].append(float(s))\n\n    def compute_map(self):\n        if not self.class_scores:\n            return 0.0\n        aps = []\n        for _, vals in self.class_scores.items():\n            if len(vals) == 0:\n                continue\n            aps.append(float(np.mean(vals)))\n        if not aps:\n            return 0.0\n        return float(np.mean(aps))\n\n\ndef draw_detections(frame_bgr, detections, labels, map_value):\n    h, w = frame_bgr.shape[:2]\n    boxes, classes, scores = detections\n\n    for box, cls_id_f, score in zip(boxes, classes, scores):\n        if score < CONFIDENCE_THRESHOLD:\n            continue\n        ymin, xmin, ymax, xmax = box\n        left = max(0, min(w - 1, int(xmin * w)))\n        right = max(0, min(w - 1, int(xmax * w)))\n        top = max(0, min(h - 1, int(ymin * h)))\n        bottom = max(0, min(h - 1, int(ymax * h)))\n\n        cls_id = int(cls_id_f)\n        # Safe label fetch\n        label_text = labels.get(cls_id, f\"id:{cls_id}\")\n        color = (0, 255, 0)  # green boxes\n        cv2.rectangle(frame_bgr, (left, top), (right, bottom), color, 2)\n\n        text = f\"{label_text} {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame_bgr, (left, top - th - baseline - 3), (left + tw + 4, top), color, -1)\n        cv2.putText(frame_bgr, text, (left + 2, top - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Overlay mAP value\n    map_text = f\"mAP: {map_value:.3f}\"\n    (tw, th), baseline = cv2.getTextSize(map_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n    cv2.rectangle(frame_bgr, (10, 10), (10 + tw + 10, 10 + th + baseline + 10), (255, 255, 255), -1)\n    cv2.putText(frame_bgr, map_text, (15, 10 + th), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2, cv2.LINE_AA)\n\n\ndef main():\n    # 1. Setup: Interpreter, labels, video I/O\n    if not os.path.isfile(MODEL_PATH):\n        raise FileNotFoundError(f\"Model file not found: {MODEL_PATH}\")\n    if not os.path.isfile(LABEL_PATH):\n        raise FileNotFoundError(f\"Label file not found: {LABEL_PATH}\")\n    if not os.path.isfile(INPUT_PATH):\n        raise FileNotFoundError(f\"Input video not found: {INPUT_PATH}\")\n\n    labels = load_labels(LABEL_PATH)\n\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()[0]\n    input_idx = input_details[\"index\"]\n    in_dtype = input_details[\"dtype\"]\n    # Expect shape [1, height, width, 3]\n    in_h = int(input_details[\"shape\"][1])\n    in_w = int(input_details[\"shape\"][2])\n\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    make_dirs_for_file(OUTPUT_PATH)\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video writer: {OUTPUT_PATH}\")\n\n    map_agg = MAPAggregator(CONFIDENCE_THRESHOLD)\n\n    frame_count = 0\n    t0 = time.time()\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_count += 1\n\n            # 2. Preprocessing\n            inp = preprocess_frame(frame, in_w, in_h, in_dtype)\n\n            # 3. Inference\n            interpreter.set_tensor(input_idx, inp)\n            interpreter.invoke()\n\n            boxes, classes, scores, n = parse_outputs(interpreter)\n\n            # Update mAP aggregator with current detections\n            map_agg.update(classes, scores)\n            current_map = map_agg.compute_map()\n\n            # 4. Output handling: draw and write frame\n            draw_detections(frame, (boxes, classes, scores), labels, current_map)\n            writer.write(frame)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    elapsed = time.time() - t0\n    avg_fps = frame_count / elapsed if elapsed > 0 else 0.0\n    final_map = map_agg.compute_map()\n    print(f\"Processed {frame_count} frames in {elapsed:.2f}s (avg {avg_fps:.2f} FPS)\")\n    print(f\"Final mAP: {final_map:.4f}\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 71.283,
      "total_cost": 0.06559625,
      "observations": [
        {
          "id": "time-12-14-50-175842_chatcmpl-D6D9OQQ512qA3cGCPapBLv5h2xBHv",
          "traceId": "8724bffb",
          "type": "GENERATION",
          "name": "8724_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:14:50.175000+00:00",
          "endTime": "2026-02-06T10:16:01.458000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06559625,
          "latency": 71283.0,
          "costDetails": {
            "total": 0.06559625
          },
          "totalTokens": 6921,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 6508,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:14:50.116Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:16:12.794Z"
    },
    {
      "id": "8846c177",
      "timestamp": "2026-02-06T10:13:10.722000+00:00",
      "name": "8846_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =======================\n# Configuration Parameters\n# =======================\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# =======================\n# Utility Functions\n# =======================\ndef load_labels(path):\n    labels = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            l = line.strip()\n            if l:\n                labels.append(l)\n    return labels\n\ndef setup_interpreter(model_path):\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Assume a single input tensor\n    in_det = input_details[0]\n    input_index = in_det['index']\n    input_shape = in_det['shape']\n    # Expected: [1, height, width, 3]\n    if len(input_shape) != 4 or input_shape[-1] != 3:\n        raise RuntimeError(f\"Unexpected model input shape: {input_shape}\")\n    input_height, input_width = int(input_shape[1]), int(input_shape[2])\n    input_dtype = in_det['dtype']\n\n    return interpreter, input_index, input_height, input_width, input_dtype, output_details\n\ndef preprocess_frame(frame_bgr, input_width, input_height, input_dtype):\n    # Convert BGR to RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (input_width, input_height), interpolation=cv2.INTER_LINEAR)\n    if input_dtype == np.float32:\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        input_data = resized.astype(np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef run_inference(interpreter, input_index, input_tensor, output_details):\n    interpreter.set_tensor(input_index, input_tensor)\n    interpreter.invoke()\n\n    # Try the common output order for TFLite detection models\n    try:\n        boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # [N, 4] in ymin, xmin, ymax, xmax (normalized)\n        classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(np.int32)  # [N]\n        scores = interpreter.get_tensor(output_details[2]['index'])[0]  # [N]\n        num = int(interpreter.get_tensor(output_details[3]['index'])[0])  # scalar\n    except Exception:\n        # Fallback heuristic if order differs\n        outs = [np.squeeze(interpreter.get_tensor(od['index'])) for od in output_details]\n        boxes, classes, scores, num = None, None, None, None\n        for o in outs:\n            if o.ndim == 2 and o.shape[-1] == 4:\n                boxes = o\n            elif o.ndim == 1 and o.dtype in (np.float32, np.float64):\n                # Could be scores or classes; pick later\n                if scores is None:\n                    scores = o\n                else:\n                    # whichever has more unique integers is likely classes after cast\n                    cand = o\n                    if len(np.unique(cand.astype(np.int32))) > len(np.unique(scores.astype(np.int32))):\n                        classes = cand.astype(np.int32)\n                    else:\n                        scores = cand\n            elif np.isscalar(o) or (o.ndim == 0):\n                num = int(o)\n        if boxes is None or classes is None or scores is None or num is None:\n            raise RuntimeError(\"Unable to parse model outputs.\")\n    return boxes, classes, scores, num\n\ndef get_color_for_class(class_id):\n    # Deterministic pseudo-color based on class_id\n    # Map class_id into a BGR tuple\n    r = (37 * (class_id + 1)) % 255\n    g = (17 * (class_id + 7)) % 255\n    b = (29 * (class_id + 13)) % 255\n    return int(b), int(g), int(r)\n\ndef draw_detections(frame_bgr, boxes, classes, scores, num, labels, threshold, accum_scores_by_class):\n    h, w = frame_bgr.shape[:2]\n    drawn = 0\n    num = min(num, boxes.shape[0], scores.shape[0], classes.shape[0])\n    for i in range(num):\n        score = float(scores[i])\n        if score < threshold:\n            continue\n        cls_id = int(classes[i])\n        ymin, xmin, ymax, xmax = boxes[i]\n        # Convert normalized coords to pixel ints (clip to frame)\n        x1 = max(0, min(w - 1, int(xmin * w)))\n        y1 = max(0, min(h - 1, int(ymin * h)))\n        x2 = max(0, min(w - 1, int(xmax * w)))\n        y2 = max(0, min(h - 1, int(ymax * h)))\n\n        color = get_color_for_class(cls_id)\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n\n        label_text = labels[cls_id] if 0 <= cls_id < len(labels) else f\"id:{cls_id}\"\n        text = f\"{label_text}: {score:.2f}\"\n        # Draw background rectangle for text\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        ty1 = max(0, y1 - th - 6)\n        cv2.rectangle(frame_bgr, (x1, ty1), (x1 + tw + 4, ty1 + th + 4), color, thickness=-1)\n        cv2.putText(frame_bgr, text, (x1 + 2, ty1 + th + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n        # Accumulate scores per class for proxy mAP\n        if cls_id not in accum_scores_by_class:\n            accum_scores_by_class[cls_id] = []\n        accum_scores_by_class[cls_id].append(score)\n        drawn += 1\n    return drawn\n\ndef compute_proxy_map(accum_scores_by_class):\n    # Proxy mAP: mean of average confidences per detected class.\n    # This is NOT a true mAP (no ground truth available).\n    if not accum_scores_by_class:\n        return None\n    per_class_means = []\n    for cls_id, scores in accum_scores_by_class.items():\n        if len(scores) > 0:\n            per_class_means.append(float(np.mean(scores)))\n    if not per_class_means:\n        return None\n    return float(np.mean(per_class_means))\n\ndef ensure_dir_for_file(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\n# =======================\n# Main Pipeline\n# =======================\ndef main():\n    print(\"TFLite object detection - starting\")\n    print(f\"Model: {model_path}\")\n    print(f\"Labels: {label_path}\")\n    print(f\"Input video: {input_path}\")\n    print(f\"Output video: {output_path}\")\n    print(\"Note: mAP shown is a proxy (mean confidence of detections per class) as no ground truth is provided.\")\n\n    labels = load_labels(label_path)\n    # Common label maps include '???' at index 0; we keep it, since classes often are 1-based\n\n    interpreter, input_index, in_h, in_w, in_dtype, output_details = setup_interpreter(model_path)\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"ERROR: Unable to open input video: {input_path}\")\n        return\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    ensure_dir_for_file(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h), True)\n    if not writer.isOpened():\n        print(f\"ERROR: Unable to create output video: {output_path}\")\n        cap.release()\n        return\n\n    accum_scores_by_class = {}\n    processed = 0\n    t0 = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        input_tensor = preprocess_frame(frame, in_w, in_h, in_dtype)\n        boxes, classes, scores, num = run_inference(interpreter, input_index, input_tensor, output_details)\n\n        # Draw detections and update accumulators\n        draw_detections(frame, boxes, classes, scores, num, labels, confidence_threshold, accum_scores_by_class)\n\n        # Compute proxy mAP so far and overlay on frame\n        proxy_map = compute_proxy_map(accum_scores_by_class)\n        map_text = f\"mAP*: {proxy_map:.3f}\" if proxy_map is not None else \"mAP*: N/A\"\n        cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (20, 20, 230), 2, cv2.LINE_AA)\n\n        writer.write(frame)\n        processed += 1\n        if processed % 30 == 0:\n            elapsed = time.time() - t0\n            speed = processed / elapsed if elapsed > 0 else 0.0\n            if total_frames > 0:\n                print(f\"Processed {processed}/{total_frames} frames at {speed:.2f} FPS\")\n            else:\n                print(f\"Processed {processed} frames at {speed:.2f} FPS\")\n\n    cap.release()\n    writer.release()\n\n    total_time = time.time() - t0\n    final_map = compute_proxy_map(accum_scores_by_class)\n    print(\"========== Summary ==========\")\n    print(f\"Frames processed: {processed}\")\n    print(f\"Total time: {total_time:.2f}s, Avg FPS: {processed / total_time if total_time > 0 else 0.0:.2f}\")\n    if final_map is None:\n        print(\"Final mAP*: N/A (no detections or no ground truth available)\")\n    else:\n        print(f\"Final mAP*: {final_map:.4f}\")\n    print(\"* mAP* is a proxy metric (mean confidence of detections per class).\")\n    print(f\"Saved annotated video to: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_psg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 66.231,
      "total_cost": 0.05624625,
      "observations": [
        {
          "id": "time-12-13-11-396741_chatcmpl-D6D7nvePjrFLTubullYmBfIr8sOVY",
          "traceId": "8846c177",
          "type": "GENERATION",
          "name": "8846_psg_gen_attempt#1",
          "startTime": "2026-02-06T10:13:11.396000+00:00",
          "endTime": "2026-02-06T10:14:17.627000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05624625,
          "latency": 66231.0,
          "costDetails": {
            "total": 0.05624625
          },
          "totalTokens": 5986,
          "environment": "default",
          "promptTokens": 413,
          "completionTokens": 5573,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T10:13:11.482Z",
      "environment": "default",
      "updatedAt": "2026-02-06T10:14:29.708Z"
    }
  ],
  "meta": {
    "total_items": 30
  }
}