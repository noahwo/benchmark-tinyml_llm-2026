{
  "data": [
    {
      "id": "bf0274fe",
      "timestamp": "2026-02-19T10:50:49.843000+00:00",
      "name": "bf02_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_img, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Convert normalized coordinates to pixel values\n            top_left = (int(box[0] * frame_width), int(box[1] * frame_height))\n            bottom_right = (int(box[2] * frame_width), int(box[3] * frame_height))\n\n            # Draw rectangle and label text\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            label_text = f\"{label}: {score:.2f}\"\n            cv2.putText(frame, label_text, (top_left[0], top_left[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes on the frame\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write the processed frame to output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.203,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-50-49-844741_chatcmpl-a33d762d-f66a-4eca-aea0-03718fb86d2b",
          "traceId": "bf0274fe",
          "type": "GENERATION",
          "name": "bf02_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:50:49.844000+00:00",
          "endTime": "2026-02-19T10:51:14.741000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24897.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-51-22-245841_chatcmpl-d7ded43d-a115-40be-9fc0-228e99e59928",
          "traceId": "bf0274fe",
          "type": "GENERATION",
          "name": "bf02_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:51:22.245000+00:00",
          "endTime": "2026-02-19T10:51:45.624000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23379.0,
          "promptTokens": 611,
          "completionTokens": 870,
          "totalTokens": 1481,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-51-53-650424_chatcmpl-1511f4d6-f7a2-4fb8-97f7-aeb99a3b0815",
          "traceId": "bf0274fe",
          "type": "GENERATION",
          "name": "bf02_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:51:53.650000+00:00",
          "endTime": "2026-02-19T10:52:17.047000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23397.0,
          "promptTokens": 662,
          "completionTokens": 874,
          "totalTokens": 1536,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:50:50.366Z",
      "updatedAt": "2026-02-19T10:53:15.313Z",
      "environment": "default"
    },
    {
      "id": "b2bec059",
      "timestamp": "2026-02-19T10:48:02.172000+00:00",
      "name": "b2be_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 88.557,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-48-02-173482_chatcmpl-07a2e796-c0ae-43ba-b250-81021b548d28",
          "traceId": "b2bec059",
          "type": "GENERATION",
          "name": "b2be_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:48:02.173000+00:00",
          "endTime": "2026-02-19T10:48:27.998000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25825.0,
          "promptTokens": 474,
          "completionTokens": 937,
          "totalTokens": 1411,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-48-35-734420_chatcmpl-d1930d2d-85e4-4277-bd26-3d58e230d02e",
          "traceId": "b2bec059",
          "type": "GENERATION",
          "name": "b2be_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:48:35.734000+00:00",
          "endTime": "2026-02-19T10:48:59.190000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23456.0,
          "promptTokens": 610,
          "completionTokens": 873,
          "totalTokens": 1483,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-49-07-459432_chatcmpl-09f6d8d8-dfd7-4e0c-84e6-9354622ca40f",
          "traceId": "b2bec059",
          "type": "GENERATION",
          "name": "b2be_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:49:07.459000+00:00",
          "endTime": "2026-02-19T10:49:30.730000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23271.0,
          "promptTokens": 662,
          "completionTokens": 869,
          "totalTokens": 1531,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:48:02.694Z",
      "updatedAt": "2026-02-19T10:50:28.428Z",
      "environment": "default"
    },
    {
      "id": "3716c0fa",
      "timestamp": "2026-02-19T10:45:19.507000+00:00",
      "name": "3716_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_img, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 85.499,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-45-19-508442_chatcmpl-082a64f6-77d2-4442-ba78-d5e5aa492782",
          "traceId": "3716c0fa",
          "type": "GENERATION",
          "name": "3716_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:45:19.508000+00:00",
          "endTime": "2026-02-19T10:45:44.400000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24892.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-45-52-026637_chatcmpl-d9a16f65-ddda-4ec4-82f4-502c6ab5a00c",
          "traceId": "3716c0fa",
          "type": "GENERATION",
          "name": "3716_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:45:52.026000+00:00",
          "endTime": "2026-02-19T10:46:15.225000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23199.0,
          "promptTokens": 610,
          "completionTokens": 864,
          "totalTokens": 1474,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-46-23-307025_chatcmpl-964b21d8-1fc0-4c11-963f-a13a75931dda",
          "traceId": "3716c0fa",
          "type": "GENERATION",
          "name": "3716_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:46:23.307000+00:00",
          "endTime": "2026-02-19T10:46:45.007000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21700.0,
          "promptTokens": 662,
          "completionTokens": 811,
          "totalTokens": 1473,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:45:20.029Z",
      "updatedAt": "2026-02-19T10:47:42.917Z",
      "environment": "default"
    },
    {
      "id": "fb249eee",
      "timestamp": "2026-02-19T10:42:31.920000+00:00",
      "name": "fb24_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame with detection to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 88.566,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-42-31-921469_chatcmpl-42f8ef0c-9877-4d97-8e3a-29696d9934e8",
          "traceId": "fb249eee",
          "type": "GENERATION",
          "name": "fb24_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:42:31.921000+00:00",
          "endTime": "2026-02-19T10:42:56.819000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24898.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-43-04-548776_chatcmpl-e8c81b2f-06f9-4e8b-b4e1-b0c97e85e1ad",
          "traceId": "fb249eee",
          "type": "GENERATION",
          "name": "fb24_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:43:04.548000+00:00",
          "endTime": "2026-02-19T10:43:28.497000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23949.0,
          "promptTokens": 608,
          "completionTokens": 892,
          "totalTokens": 1500,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-43-36-670151_chatcmpl-08785262-c703-4da8-9190-ad4f7afcbb05",
          "traceId": "fb249eee",
          "type": "GENERATION",
          "name": "fb24_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:43:36.670000+00:00",
          "endTime": "2026-02-19T10:44:00.487000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23817.0,
          "promptTokens": 662,
          "completionTokens": 890,
          "totalTokens": 1552,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:42:32.440Z",
      "updatedAt": "2026-02-19T10:44:58.657Z",
      "environment": "default"
    },
    {
      "id": "61b58e3a",
      "timestamp": "2026-02-19T10:39:46.439000+00:00",
      "name": "61b5_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.29,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-39-46-440256_chatcmpl-d843d6c2-6949-48df-b13d-68bc6d405190",
          "traceId": "61b58e3a",
          "type": "GENERATION",
          "name": "61b5_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:39:46.440000+00:00",
          "endTime": "2026-02-19T10:40:11.342000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24902.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-40-18-976041_chatcmpl-101e81e7-d47a-4887-9431-8e3f33bfb446",
          "traceId": "61b58e3a",
          "type": "GENERATION",
          "name": "61b5_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:40:18.976000+00:00",
          "endTime": "2026-02-19T10:40:42.426000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23450.0,
          "promptTokens": 609,
          "completionTokens": 873,
          "totalTokens": 1482,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-40-50-565264_chatcmpl-dd61af7d-215f-4c3d-a20b-c365af070eff",
          "traceId": "61b58e3a",
          "type": "GENERATION",
          "name": "61b5_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:40:50.565000+00:00",
          "endTime": "2026-02-19T10:41:13.730000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23165.0,
          "promptTokens": 661,
          "completionTokens": 866,
          "totalTokens": 1527,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:39:46.958Z",
      "updatedAt": "2026-02-19T10:42:11.865Z",
      "environment": "default"
    },
    {
      "id": "e538f202",
      "timestamp": "2026-02-19T10:37:01.773000+00:00",
      "name": "e538_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_img, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Convert normalized coordinates to pixel values\n            top_left = (int(box[0] * frame_width), int(box[1] * frame_height))\n            bottom_right = (int(box[2] * frame_width), int(box[3] * frame_height))\n\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            label_text = f\"{label}: {score:.2f}\"\n            cv2.putText(frame, label_text, (top_left[0], top_left[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    draw_detection_boxes(frame, boxes, classes, scores)\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 85.986,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-37-01-775007_chatcmpl-0328ba2f-19a1-4f6d-8f6e-969e91a8f78e",
          "traceId": "e538f202",
          "type": "GENERATION",
          "name": "e538_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:37:01.775000+00:00",
          "endTime": "2026-02-19T10:37:26.572000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24797.0,
          "promptTokens": 474,
          "completionTokens": 931,
          "totalTokens": 1405,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-37-34-273304_chatcmpl-f1264c23-e159-4396-838d-08d411514be9",
          "traceId": "e538f202",
          "type": "GENERATION",
          "name": "e538_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:37:34.273000+00:00",
          "endTime": "2026-02-19T10:37:57.484000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23211.0,
          "promptTokens": 608,
          "completionTokens": 864,
          "totalTokens": 1472,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-38-05-795607_chatcmpl-0b90a493-7eb8-47e2-8755-9b1a5c30ca77",
          "traceId": "e538f202",
          "type": "GENERATION",
          "name": "e538_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:38:05.795000+00:00",
          "endTime": "2026-02-19T10:38:27.761000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21966.0,
          "promptTokens": 661,
          "completionTokens": 821,
          "totalTokens": 1482,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:37:02.294Z",
      "updatedAt": "2026-02-19T10:39:25.977Z",
      "environment": "default"
    },
    {
      "id": "8b7c6b37",
      "timestamp": "2026-02-19T10:34:16.364000+00:00",
      "name": "8b7c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.31,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-34-16-366096_chatcmpl-688aedcd-0b90-4746-9d73-679e40a08695",
          "traceId": "8b7c6b37",
          "type": "GENERATION",
          "name": "8b7c_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:34:16.366000+00:00",
          "endTime": "2026-02-19T10:34:41.283000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24917.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-34-49-072921_chatcmpl-bdb1d585-6af2-4197-86fb-6f315b50b9e8",
          "traceId": "8b7c6b37",
          "type": "GENERATION",
          "name": "8b7c_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:34:49.072000+00:00",
          "endTime": "2026-02-19T10:35:12.225000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23153.0,
          "promptTokens": 609,
          "completionTokens": 862,
          "totalTokens": 1471,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-35-20-533622_chatcmpl-c1d5931e-e735-4774-9621-db3439e22b27",
          "traceId": "8b7c6b37",
          "type": "GENERATION",
          "name": "8b7c_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:35:20.533000+00:00",
          "endTime": "2026-02-19T10:35:43.676000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23143.0,
          "promptTokens": 662,
          "completionTokens": 865,
          "totalTokens": 1527,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:34:16.884Z",
      "updatedAt": "2026-02-19T10:36:41.799Z",
      "environment": "default"
    },
    {
      "id": "26c62127",
      "timestamp": "2026-02-19T10:31:30.758000+00:00",
      "name": "26c6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_img, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Convert normalized coordinates to pixel values\n            top_left = (int(box[0] * frame_width), int(box[1] * frame_height))\n            bottom_right = (int(box[2] * frame_width), int(box[3] * frame_height))\n\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            label_text = f\"{label}: {score:.2f}\"\n            cv2.putText(frame, label_text, (top_left[0], top_left[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    draw_detection_boxes(frame, boxes, classes, scores)\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 86.69,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-31-30-759076_chatcmpl-cfef8b9b-7769-419b-9203-79aed3c56c7d",
          "traceId": "26c62127",
          "type": "GENERATION",
          "name": "26c6_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:31:30.759000+00:00",
          "endTime": "2026-02-19T10:31:55.664000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24905.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-32-03-438808_chatcmpl-9fb2d781-9cdf-47d8-8a26-11f19ce40022",
          "traceId": "26c62127",
          "type": "GENERATION",
          "name": "26c6_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:32:03.438000+00:00",
          "endTime": "2026-02-19T10:32:27.347000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23909.0,
          "promptTokens": 611,
          "completionTokens": 890,
          "totalTokens": 1501,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-32-35-475256_chatcmpl-4cbf8085-cf8b-4e90-917c-fdf38f4ea22a",
          "traceId": "26c62127",
          "type": "GENERATION",
          "name": "26c6_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:32:35.475000+00:00",
          "endTime": "2026-02-19T10:32:57.449000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21974.0,
          "promptTokens": 662,
          "completionTokens": 821,
          "totalTokens": 1483,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:31:31.278Z",
      "updatedAt": "2026-02-19T10:33:55.529Z",
      "environment": "default"
    },
    {
      "id": "9b43ed10",
      "timestamp": "2026-02-19T10:28:46.389000+00:00",
      "name": "9b43_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_img, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 85.607,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-28-46-391214_chatcmpl-a2d727e8-3e6f-4ee8-b178-d83f4f97c6bd",
          "traceId": "9b43ed10",
          "type": "GENERATION",
          "name": "9b43_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:28:46.391000+00:00",
          "endTime": "2026-02-19T10:29:11.289000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24898.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-29-19-007371_chatcmpl-1429abd8-887a-407e-b6c1-3111125349e4",
          "traceId": "9b43ed10",
          "type": "GENERATION",
          "name": "9b43_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:29:19.007000+00:00",
          "endTime": "2026-02-19T10:29:42.392000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23385.0,
          "promptTokens": 611,
          "completionTokens": 871,
          "totalTokens": 1482,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-29-50-509426_chatcmpl-1110b2aa-978f-4e39-b3a4-0d250271805a",
          "traceId": "9b43ed10",
          "type": "GENERATION",
          "name": "9b43_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:29:50.509000+00:00",
          "endTime": "2026-02-19T10:30:11.998000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21489.0,
          "promptTokens": 662,
          "completionTokens": 803,
          "totalTokens": 1465,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:28:46.910Z",
      "updatedAt": "2026-02-19T10:31:10.342Z",
      "environment": "default"
    },
    {
      "id": "c33d2dfe",
      "timestamp": "2026-02-19T10:26:01.758000+00:00",
      "name": "c33d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_img, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 86.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-26-01-760130_chatcmpl-66d04cc0-7915-42dd-9208-78ca5f900cee",
          "traceId": "c33d2dfe",
          "type": "GENERATION",
          "name": "c33d_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:26:01.760000+00:00",
          "endTime": "2026-02-19T10:26:26.657000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24897.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-26-34-451267_chatcmpl-4ee41aba-d33f-49dc-9b6d-f47303b3dd41",
          "traceId": "c33d2dfe",
          "type": "GENERATION",
          "name": "c33d_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:26:34.451000+00:00",
          "endTime": "2026-02-19T10:26:57.661000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23210.0,
          "promptTokens": 611,
          "completionTokens": 864,
          "totalTokens": 1475,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-27-05-767845_chatcmpl-10826767-0f66-4d4f-acf9-a84c4b069c0d",
          "traceId": "c33d2dfe",
          "type": "GENERATION",
          "name": "c33d_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:27:05.767000+00:00",
          "endTime": "2026-02-19T10:27:27.760000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21993.0,
          "promptTokens": 663,
          "completionTokens": 822,
          "totalTokens": 1485,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:26:02.280Z",
      "updatedAt": "2026-02-19T10:28:25.555Z",
      "environment": "default"
    },
    {
      "id": "1ff805f9",
      "timestamp": "2026-02-19T10:23:16.239000+00:00",
      "name": "1ff8_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame with detection to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.177,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-23-16-240653_chatcmpl-02366bc5-8117-4dcd-9b44-91ac394c17c2",
          "traceId": "1ff805f9",
          "type": "GENERATION",
          "name": "1ff8_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:23:16.240000+00:00",
          "endTime": "2026-02-19T10:23:41.038000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24798.0,
          "promptTokens": 474,
          "completionTokens": 931,
          "totalTokens": 1405,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-23-48-814316_chatcmpl-8ed74e7a-7dd4-4992-abad-e0855eb8ca73",
          "traceId": "1ff805f9",
          "type": "GENERATION",
          "name": "1ff8_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:23:48.814000+00:00",
          "endTime": "2026-02-19T10:24:12.028000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23214.0,
          "promptTokens": 610,
          "completionTokens": 864,
          "totalTokens": 1474,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-24-20-211178_chatcmpl-cfddeabc-dc77-42de-8212-deee39001706",
          "traceId": "1ff805f9",
          "type": "GENERATION",
          "name": "1ff8_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:24:20.211000+00:00",
          "endTime": "2026-02-19T10:24:43.417000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23206.0,
          "promptTokens": 660,
          "completionTokens": 867,
          "totalTokens": 1527,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:23:16.762Z",
      "updatedAt": "2026-02-19T10:25:41.683Z",
      "environment": "default"
    },
    {
      "id": "2f4efae1",
      "timestamp": "2026-02-19T10:20:30.655000+00:00",
      "name": "2f4e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.152,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-20-30-657020_chatcmpl-b5517694-2203-49e0-b67f-2c26597acc29",
          "traceId": "2f4efae1",
          "type": "GENERATION",
          "name": "2f4e_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:20:30.657000+00:00",
          "endTime": "2026-02-19T10:20:55.554000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24897.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-21-03-393930_chatcmpl-3d6a8fef-78e7-44b7-9b8b-b0e7a7114400",
          "traceId": "2f4efae1",
          "type": "GENERATION",
          "name": "2f4e_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:21:03.393000+00:00",
          "endTime": "2026-02-19T10:21:26.608000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23215.0,
          "promptTokens": 610,
          "completionTokens": 864,
          "totalTokens": 1474,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-21-34-671205_chatcmpl-39d6e63d-22e0-485e-bea7-c86e50c83e8b",
          "traceId": "2f4efae1",
          "type": "GENERATION",
          "name": "2f4e_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:21:34.671000+00:00",
          "endTime": "2026-02-19T10:21:57.809000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23138.0,
          "promptTokens": 662,
          "completionTokens": 865,
          "totalTokens": 1527,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:20:31.178Z",
      "updatedAt": "2026-02-19T10:22:55.886Z",
      "environment": "default"
    },
    {
      "id": "34fb40e7",
      "timestamp": "2026-02-19T10:17:47.191000+00:00",
      "name": "34fb_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the image\n    preprocessed_image = preprocess_image(frame)\n    \n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], [preprocessed_image])\n    \n    # Run inference\n    interpreter.invoke()\n    \n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    \n    # Draw detection boxes and labels\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    # Write the frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 85.554,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-17-47-191833_chatcmpl-86089b91-b41e-4cae-aea3-5af263c3b6ac",
          "traceId": "34fb40e7",
          "type": "GENERATION",
          "name": "34fb_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:17:47.191000+00:00",
          "endTime": "2026-02-19T10:18:11.987000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24796.0,
          "promptTokens": 474,
          "completionTokens": 931,
          "totalTokens": 1405,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-18-19-655152_chatcmpl-51616af6-ca15-475c-80a0-290253a5398e",
          "traceId": "34fb40e7",
          "type": "GENERATION",
          "name": "34fb_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:18:19.655000+00:00",
          "endTime": "2026-02-19T10:18:43.047000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23392.0,
          "promptTokens": 610,
          "completionTokens": 871,
          "totalTokens": 1481,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-18-51-145145_chatcmpl-97cdcd85-7b5d-4f15-9ac3-7eb595ab7cb8",
          "traceId": "34fb40e7",
          "type": "GENERATION",
          "name": "34fb_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:18:51.145000+00:00",
          "endTime": "2026-02-19T10:19:12.745000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21600.0,
          "promptTokens": 663,
          "completionTokens": 807,
          "totalTokens": 1470,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:17:47.712Z",
      "updatedAt": "2026-02-19T10:20:10.593Z",
      "environment": "default"
    },
    {
      "id": "8d169bcd",
      "timestamp": "2026-02-19T10:15:02.361000+00:00",
      "name": "8d16_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_img, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n    return input_data\n\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    draw_boxes(frame, boxes, classes, scores)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 86.032,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-15-02-363040_chatcmpl-d309dadc-3343-4671-9b94-a0959ab7785a",
          "traceId": "8d169bcd",
          "type": "GENERATION",
          "name": "8d16_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:15:02.363000+00:00",
          "endTime": "2026-02-19T10:15:27.273000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24910.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-15-34-937032_chatcmpl-30f8d4e1-92e5-4067-a578-466b96205904",
          "traceId": "8d169bcd",
          "type": "GENERATION",
          "name": "8d16_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:15:34.937000+00:00",
          "endTime": "2026-02-19T10:15:58.139000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23202.0,
          "promptTokens": 611,
          "completionTokens": 864,
          "totalTokens": 1475,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-16-06-562409_chatcmpl-d270a3fb-5b06-4cf7-9b4d-5bad4c9e1703",
          "traceId": "8d169bcd",
          "type": "GENERATION",
          "name": "8d16_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:16:06.562000+00:00",
          "endTime": "2026-02-19T10:16:28.395000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21833.0,
          "promptTokens": 662,
          "completionTokens": 816,
          "totalTokens": 1478,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:15:02.885Z",
      "updatedAt": "2026-02-19T10:17:26.116Z",
      "environment": "default"
    },
    {
      "id": "6babe18e",
      "timestamp": "2026-02-19T10:12:16.907000+00:00",
      "name": "6bab_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.702,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-12-16-908817_chatcmpl-cd347746-5489-4ce6-949c-a8a30e37c046",
          "traceId": "6babe18e",
          "type": "GENERATION",
          "name": "6bab_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:12:16.908000+00:00",
          "endTime": "2026-02-19T10:12:41.806000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24898.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-12-49-497475_chatcmpl-efdd08fb-c98e-4f34-a8a0-0ceea162998f",
          "traceId": "6babe18e",
          "type": "GENERATION",
          "name": "6bab_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:12:49.497000+00:00",
          "endTime": "2026-02-19T10:13:13.315000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23818.0,
          "promptTokens": 610,
          "completionTokens": 887,
          "totalTokens": 1497,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-13-21-467198_chatcmpl-096e4979-b056-4ba4-bf2a-8bd726d509ce",
          "traceId": "6babe18e",
          "type": "GENERATION",
          "name": "6bab_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:13:21.467000+00:00",
          "endTime": "2026-02-19T10:13:44.610000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23143.0,
          "promptTokens": 662,
          "completionTokens": 865,
          "totalTokens": 1527,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:12:17.431Z",
      "updatedAt": "2026-02-19T10:14:42.858Z",
      "environment": "default"
    },
    {
      "id": "7fc5c442",
      "timestamp": "2026-02-19T10:09:31.369000+00:00",
      "name": "7fc5_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.854,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-09-31-371244_chatcmpl-ad329ac7-5499-4975-8260-429d055c276e",
          "traceId": "7fc5c442",
          "type": "GENERATION",
          "name": "7fc5_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:09:31.371000+00:00",
          "endTime": "2026-02-19T10:09:56.271000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24900.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-10-03-993446_chatcmpl-831303b8-93b7-4ec7-81a6-6d2e3505831e",
          "traceId": "7fc5c442",
          "type": "GENERATION",
          "name": "7fc5_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:10:03.993000+00:00",
          "endTime": "2026-02-19T10:10:27.932000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23939.0,
          "promptTokens": 611,
          "completionTokens": 891,
          "totalTokens": 1502,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-10-35-983459_chatcmpl-43aaf824-dd28-4f6f-87f2-cf6739013194",
          "traceId": "7fc5c442",
          "type": "GENERATION",
          "name": "7fc5_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:10:35.983000+00:00",
          "endTime": "2026-02-19T10:10:59.225000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23242.0,
          "promptTokens": 662,
          "completionTokens": 869,
          "totalTokens": 1531,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:09:31.896Z",
      "updatedAt": "2026-02-19T10:11:57.081Z",
      "environment": "default"
    },
    {
      "id": "2ec0b787",
      "timestamp": "2026-02-19T10:06:47.987000+00:00",
      "name": "2ec0_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_img, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 85.594,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-06-47-989258_chatcmpl-8b6f2760-ab38-4a34-a948-8bacc31ed89c",
          "traceId": "2ec0b787",
          "type": "GENERATION",
          "name": "2ec0_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:06:47.989000+00:00",
          "endTime": "2026-02-19T10:07:12.891000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24902.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-07-20-564999_chatcmpl-d3bdaa96-0d1b-40fa-b851-ab5fcbeacd5b",
          "traceId": "2ec0b787",
          "type": "GENERATION",
          "name": "2ec0_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:07:20.564000+00:00",
          "endTime": "2026-02-19T10:07:43.767000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23203.0,
          "promptTokens": 611,
          "completionTokens": 864,
          "totalTokens": 1475,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-07-51-939687_chatcmpl-58053c28-4ba8-4846-aaf4-9e005f3843d8",
          "traceId": "2ec0b787",
          "type": "GENERATION",
          "name": "2ec0_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:07:51.939000+00:00",
          "endTime": "2026-02-19T10:08:13.583000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21644.0,
          "promptTokens": 663,
          "completionTokens": 809,
          "totalTokens": 1472,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:06:48.517Z",
      "updatedAt": "2026-02-19T10:09:11.404Z",
      "environment": "default"
    },
    {
      "id": "c2ab0f07",
      "timestamp": "2026-02-19T10:03:58.495000+00:00",
      "name": "c2ab_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.363,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-03-58-496842_chatcmpl-d442ae12-c1c2-44ed-8328-57217a5860bb",
          "traceId": "c2ab0f07",
          "type": "GENERATION",
          "name": "c2ab_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:03:58.496000+00:00",
          "endTime": "2026-02-19T10:04:23.581000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25085.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-04-31-327418_chatcmpl-495f9fe1-15fe-479f-a132-c2ecdbbaae7c",
          "traceId": "c2ab0f07",
          "type": "GENERATION",
          "name": "c2ab_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:04:31.327000+00:00",
          "endTime": "2026-02-19T10:04:54.529000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23202.0,
          "promptTokens": 610,
          "completionTokens": 864,
          "totalTokens": 1474,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-05-02-607747_chatcmpl-3d5690c9-cfcd-4fde-b4da-92ba0bf72831",
          "traceId": "c2ab0f07",
          "type": "GENERATION",
          "name": "c2ab_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:05:02.607000+00:00",
          "endTime": "2026-02-19T10:05:25.859000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23252.0,
          "promptTokens": 662,
          "completionTokens": 869,
          "totalTokens": 1531,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:03:59.027Z",
      "updatedAt": "2026-02-19T10:06:27.733Z",
      "environment": "default"
    },
    {
      "id": "1e788582",
      "timestamp": "2026-02-19T10:01:13.947000+00:00",
      "name": "1e78_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the image\n    preprocessed_image = preprocess_image(frame)\n    \n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], [preprocessed_image])\n    \n    # Run inference\n    interpreter.invoke()\n    \n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    \n    # Draw detection boxes and labels\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    # Write the frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.453,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-01-13-948439_chatcmpl-f30e0241-8307-4558-9702-10fd3c0a086b",
          "traceId": "1e788582",
          "type": "GENERATION",
          "name": "1e78_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T10:01:13.948000+00:00",
          "endTime": "2026-02-19T10:01:38.852000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24904.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-01-46-562266_chatcmpl-6f29400a-52d3-4eb5-9d55-4bf99c895a37",
          "traceId": "1e788582",
          "type": "GENERATION",
          "name": "1e78_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T10:01:46.562000+00:00",
          "endTime": "2026-02-19T10:02:11.471000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24909.0,
          "promptTokens": 611,
          "completionTokens": 928,
          "totalTokens": 1539,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-02-19-794261_chatcmpl-66dd6a2e-f381-4186-bfe2-4cffa6cd881e",
          "traceId": "1e788582",
          "type": "GENERATION",
          "name": "1e78_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T10:02:19.794000+00:00",
          "endTime": "2026-02-19T10:02:41.401000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21607.0,
          "promptTokens": 663,
          "completionTokens": 807,
          "totalTokens": 1470,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T10:01:14.482Z",
      "updatedAt": "2026-02-19T10:03:39.251Z",
      "environment": "default"
    },
    {
      "id": "7fa220a0",
      "timestamp": "2026-02-19T09:58:26.475000+00:00",
      "name": "7fa2_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 89.39,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-58-26-476787_chatcmpl-a3172292-3839-42b7-ad5b-cc8cdb4a4a9e",
          "traceId": "7fa220a0",
          "type": "GENERATION",
          "name": "7fa2_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:58:26.476000+00:00",
          "endTime": "2026-02-19T09:58:51.492000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25016.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-58-59-329685_chatcmpl-ddd8665b-9e28-42f9-861f-cfb3a03e3e3c",
          "traceId": "7fa220a0",
          "type": "GENERATION",
          "name": "7fa2_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:58:59.329000+00:00",
          "endTime": "2026-02-19T09:59:24.508000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25179.0,
          "promptTokens": 610,
          "completionTokens": 938,
          "totalTokens": 1548,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-59-32-656120_chatcmpl-5a2efc0c-91ee-45c7-a338-ff542207bf94",
          "traceId": "7fa220a0",
          "type": "GENERATION",
          "name": "7fa2_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:59:32.656000+00:00",
          "endTime": "2026-02-19T09:59:55.866000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23210.0,
          "promptTokens": 662,
          "completionTokens": 867,
          "totalTokens": 1529,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:58:27.011Z",
      "updatedAt": "2026-02-19T10:00:53.733Z",
      "environment": "default"
    },
    {
      "id": "ed3125e8",
      "timestamp": "2026-02-19T09:55:40.054000+00:00",
      "name": "ed31_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.66,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-55-40-055830_chatcmpl-c6e600f1-7a12-4b66-8bf5-4bc14968d4ad",
          "traceId": "ed3125e8",
          "type": "GENERATION",
          "name": "ed31_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:55:40.055000+00:00",
          "endTime": "2026-02-19T09:56:04.956000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24901.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-56-12-585034_chatcmpl-1dbd9832-4bba-4690-8ba2-75f77a850b66",
          "traceId": "ed3125e8",
          "type": "GENERATION",
          "name": "ed31_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:56:12.585000+00:00",
          "endTime": "2026-02-19T09:56:35.992000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23407.0,
          "promptTokens": 611,
          "completionTokens": 871,
          "totalTokens": 1482,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-56-44-451549_chatcmpl-6d5cfb57-80f6-4e6d-8798-35e92a9b9ba8",
          "traceId": "ed3125e8",
          "type": "GENERATION",
          "name": "ed31_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:56:44.451000+00:00",
          "endTime": "2026-02-19T09:57:07.715000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23264.0,
          "promptTokens": 663,
          "completionTokens": 869,
          "totalTokens": 1532,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:55:40.590Z",
      "updatedAt": "2026-02-19T09:58:05.800Z",
      "environment": "default"
    },
    {
      "id": "32c98127",
      "timestamp": "2026-02-19T09:52:52.627000+00:00",
      "name": "32c9_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 89.109,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-52-52-628308_chatcmpl-cdaf0593-1301-447b-9d14-ae9d1990b6c4",
          "traceId": "32c98127",
          "type": "GENERATION",
          "name": "32c9_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:52:52.628000+00:00",
          "endTime": "2026-02-19T09:53:17.523000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24895.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-53-25-279624_chatcmpl-bbb47a42-f89d-4a42-832b-c786a8533d79",
          "traceId": "32c98127",
          "type": "GENERATION",
          "name": "32c9_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:53:25.279000+00:00",
          "endTime": "2026-02-19T09:53:50.487000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25208.0,
          "promptTokens": 610,
          "completionTokens": 939,
          "totalTokens": 1549,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-53-58-524371_chatcmpl-269b124a-04c7-42bd-b600-9aff9ce32649",
          "traceId": "32c98127",
          "type": "GENERATION",
          "name": "32c9_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:53:58.524000+00:00",
          "endTime": "2026-02-19T09:54:21.737000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23213.0,
          "promptTokens": 662,
          "completionTokens": 867,
          "totalTokens": 1529,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:52:53.162Z",
      "updatedAt": "2026-02-19T09:55:19.320Z",
      "environment": "default"
    },
    {
      "id": "1573ee73",
      "timestamp": "2026-02-19T09:50:06.133000+00:00",
      "name": "1573_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_img, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Convert normalized coordinates to pixel values\n            top_left = (int(box[0] * frame_width), int(box[1] * frame_height))\n            bottom_right = (int(box[2] * frame_width), int(box[3] * frame_height))\n\n            # Draw rectangle and label text\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (top_left[0], top_left[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes on the frame\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write the processed frame to output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.448,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-50-06-134282_chatcmpl-956510b5-a623-4c37-a188-204e5499a5f0",
          "traceId": "1573ee73",
          "type": "GENERATION",
          "name": "1573_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:50:06.134000+00:00",
          "endTime": "2026-02-19T09:50:31.037000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24903.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-50-38-713818_chatcmpl-9ff4b217-e15c-457e-9b18-56bde19ef422",
          "traceId": "1573ee73",
          "type": "GENERATION",
          "name": "1573_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:50:38.713000+00:00",
          "endTime": "2026-02-19T09:51:01.917000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23204.0,
          "promptTokens": 610,
          "completionTokens": 864,
          "totalTokens": 1474,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-51-10-078260_chatcmpl-66e8d071-b53f-48db-8dd2-a44552e20af8",
          "traceId": "1573ee73",
          "type": "GENERATION",
          "name": "1573_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:51:10.078000+00:00",
          "endTime": "2026-02-19T09:51:33.582000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23504.0,
          "promptTokens": 663,
          "completionTokens": 878,
          "totalTokens": 1541,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:50:06.667Z",
      "updatedAt": "2026-02-19T09:52:31.942Z",
      "environment": "default"
    },
    {
      "id": "2f1cd093",
      "timestamp": "2026-02-19T09:47:20.620000+00:00",
      "name": "2f1c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.116,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-47-20-621749_chatcmpl-53727da2-bd1b-4b01-94df-c5d1031f40b5",
          "traceId": "2f1cd093",
          "type": "GENERATION",
          "name": "2f1c_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:47:20.621000+00:00",
          "endTime": "2026-02-19T09:47:45.531000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24910.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-47-53-241538_chatcmpl-d5c7819c-ac88-45f4-91f0-1e2e9c410574",
          "traceId": "2f1cd093",
          "type": "GENERATION",
          "name": "2f1c_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:47:53.241000+00:00",
          "endTime": "2026-02-19T09:48:16.454000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23213.0,
          "promptTokens": 610,
          "completionTokens": 864,
          "totalTokens": 1474,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-48-24-492495_chatcmpl-0c449e66-3830-425d-a6b9-9d114d21e898",
          "traceId": "2f1cd093",
          "type": "GENERATION",
          "name": "2f1c_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:48:24.492000+00:00",
          "endTime": "2026-02-19T09:48:47.737000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23245.0,
          "promptTokens": 661,
          "completionTokens": 869,
          "totalTokens": 1530,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:47:21.156Z",
      "updatedAt": "2026-02-19T09:49:45.568Z",
      "environment": "default"
    },
    {
      "id": "2274e3a9",
      "timestamp": "2026-02-19T09:44:36.188000+00:00",
      "name": "2274_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the image\n    preprocessed_image = preprocess_image(frame)\n    \n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], [preprocessed_image])\n    \n    # Run inference\n    interpreter.invoke()\n    \n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    \n    # Draw detection boxes and labels\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    # Write the frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 86.299,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-44-36-190133_chatcmpl-c2bc7e27-203d-42a7-b192-3158dbd7a65c",
          "traceId": "2274e3a9",
          "type": "GENERATION",
          "name": "2274_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:44:36.190000+00:00",
          "endTime": "2026-02-19T09:45:01.105000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24915.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-45-08-793013_chatcmpl-eff6f2ba-f7e1-4027-87a5-a1981d055373",
          "traceId": "2274e3a9",
          "type": "GENERATION",
          "name": "2274_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:45:08.793000+00:00",
          "endTime": "2026-02-19T09:45:32.752000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23959.0,
          "promptTokens": 609,
          "completionTokens": 892,
          "totalTokens": 1501,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-45-40-883189_chatcmpl-56893567-a66a-4261-ae51-f9289c32d309",
          "traceId": "2274e3a9",
          "type": "GENERATION",
          "name": "2274_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:45:40.883000+00:00",
          "endTime": "2026-02-19T09:46:02.489000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21606.0,
          "promptTokens": 663,
          "completionTokens": 807,
          "totalTokens": 1470,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:44:36.721Z",
      "updatedAt": "2026-02-19T09:47:00.488Z",
      "environment": "default"
    },
    {
      "id": "d1b4a647",
      "timestamp": "2026-02-19T09:41:49.684000+00:00",
      "name": "d1b4_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed image data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 88.327,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-41-49-685534_chatcmpl-2b93541e-2a1e-41f2-842c-c2e845d671a6",
          "traceId": "d1b4a647",
          "type": "GENERATION",
          "name": "d1b4_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:41:49.685000+00:00",
          "endTime": "2026-02-19T09:42:14.594000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24909.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-42-22-299178_chatcmpl-2a948ba0-e506-4a28-a30c-6a35d4566048",
          "traceId": "d1b4a647",
          "type": "GENERATION",
          "name": "d1b4_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:42:22.299000+00:00",
          "endTime": "2026-02-19T09:42:46.382000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24083.0,
          "promptTokens": 610,
          "completionTokens": 874,
          "totalTokens": 1484,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-42-54-565034_chatcmpl-a5d5fbdb-9199-4344-82f4-44de96d24221",
          "traceId": "d1b4a647",
          "type": "GENERATION",
          "name": "d1b4_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:42:54.565000+00:00",
          "endTime": "2026-02-19T09:43:18.012000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23447.0,
          "promptTokens": 662,
          "completionTokens": 866,
          "totalTokens": 1528,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:41:50.222Z",
      "updatedAt": "2026-02-19T09:44:16.123Z",
      "environment": "default"
    },
    {
      "id": "a8eb2a19",
      "timestamp": "2026-02-19T09:39:04.258000+00:00",
      "name": "a8eb_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame with detection to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.087,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-39-04-258808_chatcmpl-6fec25eb-9611-481b-93c5-ce8cf4dd0ab5",
          "traceId": "a8eb2a19",
          "type": "GENERATION",
          "name": "a8eb_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:39:04.258000+00:00",
          "endTime": "2026-02-19T09:39:29.165000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24907.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-39-36-836558_chatcmpl-e0cf8f51-4a1b-4a61-970c-7479ae1314a3",
          "traceId": "a8eb2a19",
          "type": "GENERATION",
          "name": "a8eb_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:39:36.836000+00:00",
          "endTime": "2026-02-19T09:39:59.355000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22519.0,
          "promptTokens": 609,
          "completionTokens": 838,
          "totalTokens": 1447,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-40-07-515932_chatcmpl-da3fdc53-e3da-44d2-9098-091f4d331149",
          "traceId": "a8eb2a19",
          "type": "GENERATION",
          "name": "a8eb_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:40:07.515000+00:00",
          "endTime": "2026-02-19T09:40:31.345000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23830.0,
          "promptTokens": 661,
          "completionTokens": 890,
          "totalTokens": 1551,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:39:04.791Z",
      "updatedAt": "2026-02-19T09:41:29.041Z",
      "environment": "default"
    },
    {
      "id": "959cdf4e",
      "timestamp": "2026-02-19T09:36:18.822000+00:00",
      "name": "959c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop to process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set tensor to point to the preprocessed data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.264,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-36-18-823511_chatcmpl-eccf60a5-b75f-40e6-91ce-94f125654f6f",
          "traceId": "959cdf4e",
          "type": "GENERATION",
          "name": "959c_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:36:18.823000+00:00",
          "endTime": "2026-02-19T09:36:43.715000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24892.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-36-51-462723_chatcmpl-ea4afff6-9e9b-4a5e-bec3-722fd646bf30",
          "traceId": "959cdf4e",
          "type": "GENERATION",
          "name": "959c_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:36:51.462000+00:00",
          "endTime": "2026-02-19T09:37:14.906000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23444.0,
          "promptTokens": 611,
          "completionTokens": 873,
          "totalTokens": 1484,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-37-22-937403_chatcmpl-4bd3fa93-191b-473e-94a3-f67065d0ebbf",
          "traceId": "959cdf4e",
          "type": "GENERATION",
          "name": "959c_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:37:22.937000+00:00",
          "endTime": "2026-02-19T09:37:46.087000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23150.0,
          "promptTokens": 660,
          "completionTokens": 865,
          "totalTokens": 1525,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:36:19.354Z",
      "updatedAt": "2026-02-19T09:38:44.174Z",
      "environment": "default"
    },
    {
      "id": "8552d540",
      "timestamp": "2026-02-19T09:33:33.298000+00:00",
      "name": "8552_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_img, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is UINT8\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.364,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-33-33-300273_chatcmpl-890bd2dd-4aef-4a3d-9f2d-8c3946df54dc",
          "traceId": "8552d540",
          "type": "GENERATION",
          "name": "8552_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:33:33.300000+00:00",
          "endTime": "2026-02-19T09:33:58.238000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24938.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-34-05-899542_chatcmpl-6d957ecb-e4ae-4e5f-8c67-f1ac032b0081",
          "traceId": "8552d540",
          "type": "GENERATION",
          "name": "8552_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:34:05.899000+00:00",
          "endTime": "2026-02-19T09:34:31.118000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25219.0,
          "promptTokens": 610,
          "completionTokens": 939,
          "totalTokens": 1549,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-34-39-320753_chatcmpl-c0e861cc-9094-4bd5-b0e8-7aef3dad1a01",
          "traceId": "8552d540",
          "type": "GENERATION",
          "name": "8552_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:34:39.320000+00:00",
          "endTime": "2026-02-19T09:35:00.664000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21344.0,
          "promptTokens": 662,
          "completionTokens": 797,
          "totalTokens": 1459,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:33:33.827Z",
      "updatedAt": "2026-02-19T09:35:58.601Z",
      "environment": "default"
    },
    {
      "id": "2d69fffb",
      "timestamp": "2026-02-19T09:30:47.903000+00:00",
      "name": "2d69_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_img = np.array(resized_img, dtype=np.uint8)\n    return normalized_img\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the image\n    preprocessed_image = preprocess_image(frame)\n    \n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], [preprocessed_image])\n    \n    # Run inference\n    interpreter.invoke()\n    \n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    \n    # Draw detection boxes and labels\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    # Write the frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_12d3_tpusg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.562,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-30-47-929101_chatcmpl-3e6f3ac0-6739-4bc6-a07b-ebddb557b330",
          "traceId": "2d69fffb",
          "type": "GENERATION",
          "name": "2d69_tpusg_gen_attempt#1",
          "startTime": "2026-02-19T09:30:47.929000+00:00",
          "endTime": "2026-02-19T09:31:12.826000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24897.0,
          "promptTokens": 474,
          "completionTokens": 935,
          "totalTokens": 1409,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-31-22-580837_chatcmpl-f8313b8a-4705-4f39-a276-176b0957a6ad",
          "traceId": "2d69fffb",
          "type": "GENERATION",
          "name": "2d69_tpusg_gen_attempt#2",
          "startTime": "2026-02-19T09:31:22.580000+00:00",
          "endTime": "2026-02-19T09:31:45.799000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23219.0,
          "promptTokens": 610,
          "completionTokens": 864,
          "totalTokens": 1474,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-31-53-894334_chatcmpl-fd2b582c-e9d0-4c4d-826e-bcf17995e53c",
          "traceId": "2d69fffb",
          "type": "GENERATION",
          "name": "2d69_tpusg_gen_attempt#3",
          "startTime": "2026-02-19T09:31:53.894000+00:00",
          "endTime": "2026-02-19T09:32:15.491000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {
            "temperature": "0.1",
            "top_p": "0.3"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21597.0,
          "promptTokens": 663,
          "completionTokens": 807,
          "totalTokens": 1470,
          "environment": "default",
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-19T09:30:48.432Z",
      "updatedAt": "2026-02-19T09:33:13.327Z",
      "environment": "default"
    }
  ],
  "meta": {
    "total_items": 30
  }
}