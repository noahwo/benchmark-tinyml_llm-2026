{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595ec5e2",
   "metadata": {},
   "source": [
    "# Export valid sketches from raw_*.json files in raw_export\n",
    "This notebook extracts code snippets stored in the `sketch` field where `status == \"success\"` and writes them as `.py` files under a new sibling folder `exported_valid_code`.\n",
    "\n",
    "Outline:\n",
    "- Import dependencies\n",
    "- Resolve input/output paths\n",
    "- Enumerate raw JSON files\n",
    "- Parse and filter successful sketches\n",
    "- Write extracted code to files\n",
    "- Quick verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc20391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import glob\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a16c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dir: /home/han/Projects/benchmark-tinyml_llm-2026/langfuse_export/2026/02.22_abla-l2-p-qw32\n",
      "Dataset suffix: abla-l2-p-qw32\n",
      "Raw dir: /home/han/Projects/benchmark-tinyml_llm-2026/langfuse_export/2026/02.22_abla-l2-p-qw32/raw_export\n",
      "Export root: /home/han/Projects/benchmark-tinyml_llm-2026/langfuse_export/2026/02.22_abla-l2-p-qw32/exported_valid_code\n",
      "Subfolders (precreated):\n",
      " - /home/han/Projects/benchmark-tinyml_llm-2026/langfuse_export/2026/02.22_abla-l2-p-qw32/exported_valid_code/sg\n",
      " - /home/han/Projects/benchmark-tinyml_llm-2026/langfuse_export/2026/02.22_abla-l2-p-qw32/exported_valid_code/psg\n",
      " - /home/han/Projects/benchmark-tinyml_llm-2026/langfuse_export/2026/02.22_abla-l2-p-qw32/exported_valid_code/tpusg\n"
     ]
    }
   ],
   "source": [
    "# Resolve input and output paths\n",
    "# Derive base_dir relative to the notebook location to keep things portable.\n",
    "# If run from the ipynb folder, base_dir is its parent; otherwise use CWD.\n",
    "cwd = Path.cwd().resolve()\n",
    "base_dir = cwd.parent if cwd.name == 'ipynb' else cwd\n",
    "raw_dir = base_dir / 'raw_export'\n",
    "export_dir = base_dir / 'exported_valid_code'\n",
    "\n",
    "# Derive a dataset suffix from the folder name (drop leading number prefix if present)\n",
    "name_parts = base_dir.name.split('_', 1)\n",
    "dataset_suffix = name_parts[1] if len(name_parts) > 1 else base_dir.name\n",
    "# Sanitize suffix for filenames\n",
    "_dataset_clean = ''.join(ch if ch.isalnum() or ch in {'-', '_'} else '_' for ch in dataset_suffix).strip('_')\n",
    "dataset_suffix = _dataset_clean or 'dataset'\n",
    "\n",
    "# Pre-create known prefix subfolders (sg, psg, tpusg); fallback created on demand\n",
    "prefix_dirs = [export_dir / 'sg', export_dir / 'psg', export_dir / 'tpusg']\n",
    "for d in prefix_dirs:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Base dir: {base_dir}')\n",
    "print(f'Dataset suffix: {dataset_suffix}')\n",
    "print(f'Raw dir: {raw_dir}')\n",
    "print(f'Export root: {export_dir}')\n",
    "print('Subfolders (precreated):')\n",
    "for d in prefix_dirs:\n",
    "    print(' -', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3dded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 raw files\n",
      " - raw_qwen2.5-coder:32b_12d3_psg_batch.json\n",
      " - raw_qwen2.5-coder:32b_12d3_sg_batch.json\n",
      " - raw_qwen2.5-coder:32b_12d3_tpusg_batch.json\n"
     ]
    }
   ],
   "source": [
    "# Enumerate raw JSON files\n",
    "raw_files = sorted(raw_dir.glob('raw_*.json'))\n",
    "print(f'Found {len(raw_files)} raw files')\n",
    "for f in raw_files:\n",
    "    print(' -', f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e703b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 60 sketches with status==\"success\"\n"
     ]
    }
   ],
   "source": [
    "# Parse and filter successful sketches\n",
    "\n",
    "def safe_name(text: str) -> str:\n",
    "    # Keep alphanumerics, dash, underscore; replace others with underscore\n",
    "    cleaned = ''.join(ch if ch.isalnum() or ch in {'-', '_'} else '_' for ch in text)\n",
    "    return cleaned.strip('_') or 'unknown_item'\n",
    "\n",
    "def detect_prefix(filename: str) -> str:\n",
    "    name = filename.lower()\n",
    "    if 'tpusg' in name:\n",
    "        return 'tpusg'\n",
    "    if 'psg' in name:\n",
    "        return 'psg'\n",
    "    if 'sg' in name:\n",
    "        return 'sg'\n",
    "    return 'unknown_item'\n",
    "\n",
    "records = []\n",
    "for fpath in raw_files:\n",
    "    prefix = detect_prefix(fpath.name)\n",
    "    try:\n",
    "        with open(fpath, 'r', encoding='utf-8') as f:\n",
    "            payload = json.load(f)\n",
    "    except Exception as exc:\n",
    "        print(f'Failed to load {fpath.name}: {exc}')\n",
    "        continue\n",
    "\n",
    "    items: List[Dict[str, Any]] = payload.get('data', []) if isinstance(payload, dict) else []\n",
    "    for entry in items:\n",
    "        output = entry.get('output') if isinstance(entry, dict) else {}\n",
    "        if not isinstance(output, dict):\n",
    "            continue\n",
    "        status = output.get('status')\n",
    "        if not status or str(status).lower() != 'success':\n",
    "            continue\n",
    "        sketch = output.get('sketch')\n",
    "        if not sketch:\n",
    "            continue\n",
    "        rec_id = entry.get('id') or entry.get('name') or fpath.stem\n",
    "        records.append({\n",
    "            'source_file': fpath.name,\n",
    "            'id': rec_id,\n",
    "            'sketch': sketch,\n",
    "            'prefix': prefix,\n",
    "        })\n",
    "\n",
    "print(f'Collected {len(records)} sketches with status==\"success\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2eb67ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 processed CSV files\n",
      "Success count comparison (records vs CSV):\n",
      " tpusg: records=30 csv_success=30\n",
      " psg: records=30 csv_success=30\n",
      " sg: records=0 csv_success=0\n",
      " unknown_item: records=0 csv_success=0\n",
      "\n",
      "Per-file CSV success counts:\n",
      " - processed_data/qwen2.5-coder:32b_12d3/clean_qwen2.5-coder:32b_12d3_psg_batch.csv [psg] success=30/30\n",
      " - processed_data/qwen2.5-coder:32b_12d3/clean_qwen2.5-coder:32b_12d3_sg_batch.csv [sg] success=0/30\n",
      " - processed_data/qwen2.5-coder:32b_12d3/clean_qwen2.5-coder:32b_12d3_tpusg_batch.csv [tpusg] success=30/30\n"
     ]
    }
   ],
   "source": [
    "# Schema check: compare exported success counts with processed_data CSVs\n",
    "processed_dir = base_dir / 'processed_data'\n",
    "csv_files = sorted(processed_dir.rglob('*.csv'))\n",
    "\n",
    "print(f'Found {len(csv_files)} processed CSV files')\n",
    "\n",
    "record_counts = Counter([r['prefix'] for r in records])\n",
    "aggregate_csv_success = Counter()\n",
    "per_file_stats = []\n",
    "\n",
    "def prefix_from_csv_name(name: str) -> str:\n",
    "    n = name.lower()\n",
    "    if 'tpusg' in n:\n",
    "        return 'tpusg'\n",
    "    if 'psg' in n:\n",
    "        return 'psg'\n",
    "    if 'sg' in n:\n",
    "        return 'sg'\n",
    "    return 'unknown_item'\n",
    "\n",
    "for path in csv_files:\n",
    "    prefix = prefix_from_csv_name(path.name)\n",
    "    total_rows = 0\n",
    "    success_rows = 0\n",
    "    try:\n",
    "        with open(path, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                total_rows += 1\n",
    "                if str(row.get('status', '')).lower() == 'success':\n",
    "                    success_rows += 1\n",
    "    except Exception as exc:\n",
    "        print(f'Failed to read {path}: {exc}')\n",
    "        continue\n",
    "    aggregate_csv_success[prefix] += success_rows\n",
    "    per_file_stats.append((path, prefix, success_rows, total_rows))\n",
    "\n",
    "print('Success count comparison (records vs CSV):')\n",
    "for prefix in ['tpusg', 'psg', 'sg', 'unknown_item']:\n",
    "    print(f\" {prefix}: records={record_counts.get(prefix, 0)} csv_success={aggregate_csv_success.get(prefix, 0)}\")\n",
    "\n",
    "print('\\nPer-file CSV success counts:')\n",
    "for path, prefix, success_rows, total_rows in per_file_stats:\n",
    "    try:\n",
    "        rel = path.relative_to(base_dir)\n",
    "    except Exception:\n",
    "        rel = path\n",
    "    print(f' - {rel} [{prefix}] success={success_rows}/{total_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdc27fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files accounted for under /home/han/Projects/benchmark-tinyml_llm-2026/langfuse_export/2026/02.22_abla-l2-p-qw32/exported_valid_code: 60\n"
     ]
    }
   ],
   "source": [
    "# Write extracted code to files (idempotent on reruns)\n",
    "written = []\n",
    "for idx, rec in enumerate(records, start=1):\n",
    "    safe_id = safe_name(str(rec['id']))\n",
    "    prefix = rec.get('prefix') or 'unknown_item'\n",
    "    out_dir = export_dir / prefix\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    base_name = f'{prefix}_{safe_id}_{dataset_suffix}' if safe_id else f'{prefix}_{idx}_{dataset_suffix}'\n",
    "    ext = '.ino' if prefix == 'sg' else '.py'\n",
    "    out_path = out_dir / f'{base_name}{ext}'\n",
    "\n",
    "    sketch_content = rec['sketch']\n",
    "    if out_path.exists():\n",
    "        try:\n",
    "            with open(out_path, 'r', encoding='utf-8') as fr:\n",
    "                existing = fr.read()\n",
    "            if existing == sketch_content:\n",
    "                written.append(out_path)\n",
    "                continue\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    with open(out_path, 'w', encoding='utf-8') as fw:\n",
    "        fw.write(sketch_content)\n",
    "    written.append(out_path)\n",
    "\n",
    "print(f'Files accounted for under {export_dir}: {len(written)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1eaa9a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace mapping check:\n",
      " CSV success traces: 60\n",
      " Exported files (parsed): 60\n",
      " Missing exports for CSV success traces: 0\n",
      " Extra exports without CSV success rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Trace-to-export mapping check\n",
    "# Ensure every CSV success trace has a matching exported file and vice versa\n",
    "\n",
    "# Handles filenames like sg_02cd0534_abla-l2-gpt5.ino or sg_02cd0534_abla-l2-gpt5_1.ino\n",
    "# and psg/tpusg *.py equivalents.\n",
    "def parse_trace_from_filename(path: Path):\n",
    "    stem = path.stem\n",
    "    parts = stem.split('_')\n",
    "    if len(parts) < 3:\n",
    "        return None, None\n",
    "    prefix = parts[0]\n",
    "    # Handle optional numeric dedup suffix at end\n",
    "    tail_parts = parts[1:]\n",
    "    if tail_parts[-1].isdigit():\n",
    "        tail_parts = tail_parts[:-1]\n",
    "    # Expect the dataset suffix at the end\n",
    "    if tail_parts and tail_parts[-1] == dataset_suffix:\n",
    "        tail_parts = tail_parts[:-1]\n",
    "    if not tail_parts:\n",
    "        return None, None\n",
    "    trace = '_'.join(tail_parts)\n",
    "    return prefix, trace\n",
    "\n",
    "# CSV success traces\n",
    "csv_success_traces = set()\n",
    "for path in csv_files:\n",
    "    prefix = prefix_from_csv_name(path.name)\n",
    "    try:\n",
    "        with open(path, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                if str(row.get('status', '')).lower() == 'success':\n",
    "                    trace = str(row.get('trace_id', '')).strip()\n",
    "                    if trace:\n",
    "                        csv_success_traces.add((prefix, trace))\n",
    "    except Exception as exc:\n",
    "        print(f'Failed to scan {path} for trace_id: {exc}')\n",
    "\n",
    "# Records (success sketches parsed from raw_*)\n",
    "record_traces = set((r['prefix'], safe_name(str(r['id']))) for r in records)\n",
    "\n",
    "# Written files\n",
    "written_traces = set()\n",
    "for p in written:\n",
    "    prefix, trace = parse_trace_from_filename(p)\n",
    "    if prefix and trace:\n",
    "        written_traces.add((prefix, trace))\n",
    "\n",
    "missing_exports = csv_success_traces - written_traces\n",
    "extra_exports = written_traces - csv_success_traces\n",
    "\n",
    "print('Trace mapping check:')\n",
    "print(f' CSV success traces: {len(csv_success_traces)}')\n",
    "print(f' Exported files (parsed): {len(written_traces)}')\n",
    "print(f' Missing exports for CSV success traces: {len(missing_exports)}')\n",
    "print(f' Extra exports without CSV success rows: {len(extra_exports)}')\n",
    "\n",
    "if missing_exports:\n",
    "    print('\\nMissing (prefix, trace_id):')\n",
    "    for item in sorted(missing_exports):\n",
    "        print(' -', item)\n",
    "\n",
    "if extra_exports:\n",
    "    print('\\nExtra exports not found in CSV success:')\n",
    "    for item in sorted(extra_exports):\n",
    "        print(' -', item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03fa20f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example files:\n",
      " - exported_valid_code/psg/psg_270ded12_abla-l2-p-qw32.py\n",
      " - exported_valid_code/psg/psg_0927dd53_abla-l2-p-qw32.py\n",
      " - exported_valid_code/psg/psg_c6fcfd2d_abla-l2-p-qw32.py\n",
      " - exported_valid_code/psg/psg_ac94d0af_abla-l2-p-qw32.py\n",
      " - exported_valid_code/psg/psg_04238335_abla-l2-p-qw32.py\n",
      "\n",
      "Preview of psg_270ded12_abla-l2-p-qw32.py:\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "import cv2\n",
      "from ai_edge_litert.interpreter import Interpreter\n",
      "\n",
      "# Configuration parameters\n",
      "model_path = \"models/ssd-mobilenet_v1/detect.tflite\"\n",
      "label_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\n",
      "input_path = \"data/object_detection/sheeps.mp4\"\n"
     ]
    }
   ],
   "source": [
    "# Quick verification of exports\n",
    "from itertools import islice\n",
    "\n",
    "print('Example files:')\n",
    "for p in written[:5]:\n",
    "    try:\n",
    "        rel = p.relative_to(base_dir)\n",
    "    except Exception:\n",
    "        rel = p\n",
    "    print(' -', rel)\n",
    "\n",
    "if written:\n",
    "    sample = written[0]\n",
    "    print(f\"\\nPreview of {sample.name}:\")\n",
    "    with open(sample, 'r', encoding='utf-8') as f:\n",
    "        for line in islice(f, 10):\n",
    "            print(line.rstrip())\n",
    "else:\n",
    "    print('No files written.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
