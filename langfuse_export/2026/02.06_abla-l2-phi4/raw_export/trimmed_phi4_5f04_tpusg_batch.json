{
  "data": [
    {
      "id": "98d31651",
      "timestamp": "2026-02-07T03:38:31.549000+00:00",
      "name": "98d3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\nedgetpu_delegate_path = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n# Load labels from label map file\ndef load_labels(label_file):\n    with open(label_file, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        return {i: line.strip() for i, line in enumerate(lines)}\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate(edgetpu_delegate_path)]\n)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninput_shape = input_details[0]['shape']\nheight, width = input_shape[1], input_shape[2]\n\nlabels = load_labels(label_path)\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing: Resize and normalize image\n    img_resized = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(img_resized, axis=0).astype(np.uint8)\n\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Get output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    # Post-processing: Draw detection boxes with labels\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw rectangle and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), 2)\n            object_name = labels[int(classes[i])] if int(classes[i]) in labels else 'N/A'\n            label = f\"{object_name}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n    # Write the frame with detection boxes to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\n\nprint(\"Video processing completed.\")"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 33.909,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-38-31-550325_chatcmpl-7b4710a3-7b03-4a91-9d9c-8a075aea7776",
          "traceId": "98d31651",
          "type": "GENERATION",
          "name": "98d3_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:38:31.550000+00:00",
          "endTime": "2026-02-07T03:38:45.919000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14369.0,
          "completionTokens": 1024,
          "totalTokens": 1478,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-38-53-675338_chatcmpl-d8caf8bc-6521-4fda-807f-2596013e3d98",
          "traceId": "98d31651",
          "type": "GENERATION",
          "name": "98d3_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:38:53.675000+00:00",
          "endTime": "2026-02-07T03:39:05.459000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11784.0,
          "completionTokens": 862,
          "totalTokens": 1484,
          "promptTokens": 622,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:40:03.861Z",
      "createdAt": "2026-02-07T03:38:32.075Z",
      "environment": "default"
    },
    {
      "id": "e0dbe511",
      "timestamp": "2026-02-07T03:36:22.635000+00:00",
      "name": "e0db_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport os\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter with EdgeTPU\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Video capture and writer setup\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Preprocessing function\ndef preprocess(frame):\n    input_shape = input_details[0]['shape']\n    image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    image_resized = cv2.resize(image, (input_shape[1], input_shape[2]))\n    image_normalized = image_resized / 255.0\n    return np.expand_dims(image_normalized, axis=0).astype(np.uint8)\n\n# Inference function\ndef infer(frame):\n    interpreter.set_tensor(input_details[0]['index'], preprocess(frame))\n    interpreter.invoke()\n    \n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n    \n    return boxes, classes, scores\n\n# Function to draw bounding boxes and labels\ndef draw_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {int(scores[i] * 100)}%'\n            cv2.putText(frame, label, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\n# Main processing loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    boxes, classes, scores = infer(frame)\n    draw_boxes(frame, boxes, classes, scores)\n    \n    out.write(frame)\n\ncap.release()\nout.release()\n\nprint(\"Processing complete. Output saved to:\", output_path)"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 50.335,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-36-22-637156_chatcmpl-4c80c68a-1ca2-4429-954b-b6b3b7d508fc",
          "traceId": "e0dbe511",
          "type": "GENERATION",
          "name": "e0db_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:36:22.637000+00:00",
          "endTime": "2026-02-07T03:36:33.797000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11160.0,
          "completionTokens": 818,
          "totalTokens": 1272,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-36-41-405348_chatcmpl-605bbb83-ddcc-4031-96cf-efc7816d0ec4",
          "traceId": "e0dbe511",
          "type": "GENERATION",
          "name": "e0db_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:36:41.405000+00:00",
          "endTime": "2026-02-07T03:36:53.217000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11812.0,
          "completionTokens": 850,
          "totalTokens": 1403,
          "promptTokens": 553,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-37-01-466468_chatcmpl-036936fd-4a55-4929-9022-9136233e15e0",
          "traceId": "e0dbe511",
          "type": "GENERATION",
          "name": "e0db_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:37:01.466000+00:00",
          "endTime": "2026-02-07T03:37:12.972000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11506.0,
          "completionTokens": 848,
          "totalTokens": 1474,
          "promptTokens": 626,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:38:10.893Z",
      "createdAt": "2026-02-07T03:36:23.157Z",
      "environment": "default"
    },
    {
      "id": "655a7a01",
      "timestamp": "2026-02-07T03:34:52.125000+00:00",
      "name": "655a_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return [line.strip() for line in f.readlines()]\n\nlabels = load_labels(label_path)\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocessing function\ndef preprocess_frame(frame):\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[2], input_shape[1]))\n    input_data = np.expand_dims(frame_resized, axis=0).astype(np.uint8)\n    return input_data\n\n# Inference function\ndef run_inference(input_data):\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n    \n    return boxes, classes, scores\n\n# Output handling and drawing function\ndef draw_detections(frame, boxes, classes, scores):\n    height, width, _ = frame.shape\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            \n            # Draw rectangle and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n# Open input video\ncap = cv2.VideoCapture(input_path)\noutput_video = None\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the frame\n    input_data = preprocess_frame(frame)\n\n    # Run inference\n    boxes, classes, scores = run_inference(input_data)\n\n    # Draw detections on the frame\n    draw_detections(frame, boxes, classes, scores)\n    \n    # Initialize video writer if not already done\n    if output_video is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        height, width, _ = frame.shape\n        output_video = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    # Write the processed frame to the output video\n    output_video.write(frame)\n\n# Release resources\ncap.release()\noutput_video.release()\n\nprint(\"Processing complete. Output saved to:\", output_path)"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 12.524,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-34-52-126538_chatcmpl-6f402ae8-b377-41ec-874e-5de3050c05c7",
          "traceId": "655a7a01",
          "type": "GENERATION",
          "name": "655a_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:34:52.126000+00:00",
          "endTime": "2026-02-07T03:35:04.650000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12524.0,
          "completionTokens": 914,
          "totalTokens": 1368,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:36:02.303Z",
      "createdAt": "2026-02-07T03:34:52.647Z",
      "environment": "default"
    },
    {
      "id": "b7b91ae6",
      "timestamp": "2026-02-07T03:32:40.267000+00:00",
      "name": "b7b9_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the TFLite interpreter with EdgeTPU\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Video capture and writer setup\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"Error: Could not open video {input_path}\")\n    exit(1)\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\nout = cv2.VideoWriter(\n    output_path,\n    cv2.VideoWriter_fourcc(*'mp4v'),\n    fps,\n    (frame_width, frame_height)\n)\n\n# Preprocessing function\ndef preprocess_frame(frame):\n    input_shape = input_details[0]['shape']\n    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    image_resized = cv2.resize(image_rgb, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(image_resized, axis=0).astype(np.uint8)\n    return input_data\n\n# Inference function\ndef infer(interpreter, frame):\n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    interpreter.invoke()\n\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    return boxes, classes, scores\n\n# Process each frame in the video\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    boxes, classes, scores = infer(interpreter, frame)\n    \n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            \n            object_name = labels[int(classes[i])]\n            label = f\"{object_name}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n    \n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\n\nprint(f\"Processed video saved to {output_path}\")"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 52.877,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-32-40-268298_chatcmpl-469dd000-149a-4f97-ae94-1f72a87fcac0",
          "traceId": "b7b91ae6",
          "type": "GENERATION",
          "name": "b7b9_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:32:40.268000+00:00",
          "endTime": "2026-02-07T03:32:52.114000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11846.0,
          "completionTokens": 880,
          "totalTokens": 1334,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-32-59-821480_chatcmpl-3935396c-eede-4c38-b460-1db1b9d82e6d",
          "traceId": "b7b91ae6",
          "type": "GENERATION",
          "name": "b7b9_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:32:59.821000+00:00",
          "endTime": "2026-02-07T03:33:12.166000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12345.0,
          "completionTokens": 885,
          "totalTokens": 1446,
          "promptTokens": 561,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-33-20-529229_chatcmpl-21f40135-0a7a-4ab6-ab52-cee30c3500b1",
          "traceId": "b7b91ae6",
          "type": "GENERATION",
          "name": "b7b9_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:33:20.529000+00:00",
          "endTime": "2026-02-07T03:33:33.145000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12616.0,
          "completionTokens": 937,
          "totalTokens": 1562,
          "promptTokens": 625,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:34:31.664Z",
      "createdAt": "2026-02-07T03:32:40.789Z",
      "environment": "default"
    },
    {
      "id": "b0143601",
      "timestamp": "2026-02-07T03:30:30.589000+00:00",
      "name": "b014_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\ndef main():\n    # Configuration parameters\n    model_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\n    label_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\n    input_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\n    output_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n    confidence_threshold = 0.5\n\n    # Load labels\n    with open(label_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\n\n    # Initialize TFLite interpreter with EdgeTPU delegate\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Read video\n    cap = cv2.VideoCapture(input_path)\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess the input image\n        input_frame = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n        input_frame = np.expand_dims(input_frame, axis=0)\n        input_frame = (np.float32(input_frame) - 127.5) / 127.5\n\n        # Set the tensor\n        interpreter.set_tensor(input_details[0]['index'], input_frame.astype(np.float32))\n\n        # Run inference\n        interpreter.invoke()\n\n        # Process output\n        boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n        classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n        scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n        for i in range(len(scores)):\n            if scores[i] >= confidence_threshold:\n                ymin, xmin, ymax, xmax = boxes[i]\n                (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                              ymin * frame_height, ymax * frame_height)\n                left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n\n                cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n                label = f'{labels[int(classes[i])]}: {int(scores[i]*100)}%'\n                cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        # Write the frame with detection boxes\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_dd05b510_1770435125.py\", line 76, in <module>\n    main()\n  File \"script_dd05b510_1770435125.py\", line 48, in main\n    interpreter.set_tensor(input_details[0]['index'], input_frame.astype(np.float32))\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor ."
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 103.713,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-30-30-590777_chatcmpl-1cd34eec-7904-424e-800a-67aa5cb6ee81",
          "traceId": "b0143601",
          "type": "GENERATION",
          "name": "b014_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:30:30.590000+00:00",
          "endTime": "2026-02-07T03:30:44.809000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14219.0,
          "completionTokens": 1029,
          "totalTokens": 1483,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-30-52-893332_chatcmpl-0586d846-f56a-4bc7-8d9f-ba50832d0279",
          "traceId": "b0143601",
          "type": "GENERATION",
          "name": "b014_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:30:52.893000+00:00",
          "endTime": "2026-02-07T03:31:04.427000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11534.0,
          "completionTokens": 845,
          "totalTokens": 1472,
          "promptTokens": 627,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-31-13-041277_chatcmpl-2e331911-6924-49f3-af6d-2e4a785d40ce",
          "traceId": "b0143601",
          "type": "GENERATION",
          "name": "b014_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:31:13.041000+00:00",
          "endTime": "2026-02-07T03:31:24.391000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11350.0,
          "completionTokens": 843,
          "totalTokens": 1467,
          "promptTokens": 624,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-31-32-660828_chatcmpl-86dbdd14-3646-4e06-9e2e-af73973d615d",
          "traceId": "b0143601",
          "type": "GENERATION",
          "name": "b014_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T03:31:32.660000+00:00",
          "endTime": "2026-02-07T03:31:44.976000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12316.0,
          "completionTokens": 911,
          "totalTokens": 1570,
          "promptTokens": 659,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-31-53-239753_chatcmpl-e3112063-257d-4e1f-a0ba-c5ebc106dc90",
          "traceId": "b0143601",
          "type": "GENERATION",
          "name": "b014_tpusg_gen_attempt#5",
          "startTime": "2026-02-07T03:31:53.239000+00:00",
          "endTime": "2026-02-07T03:32:05.845000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12606.0,
          "completionTokens": 926,
          "totalTokens": 1579,
          "promptTokens": 653,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "dc492a40-9b20-4983-a0f6-0a5c12df966a",
          "traceId": "b0143601",
          "type": "SPAN",
          "name": "error_b0_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-07T03:32:14.303000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_dd05b510_1770435125.py\", line 76, in <module>\n    main()\n  File \"script_dd05b510_1770435125.py\", line 48, in main\n    interpreter.set_tensor(input_details[0]['index'], input_frame.astype(np.float32))\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor .",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "updatedAt": "2026-02-07T03:32:19.870Z",
      "createdAt": "2026-02-07T03:30:31.110Z",
      "environment": "default"
    },
    {
      "id": "186c9440",
      "timestamp": "2026-02-07T03:28:18.538000+00:00",
      "name": "186c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Setup and load TFLite interpreter\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels from label map file\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\ndef preprocess_frame(frame):\n    # Resize frame to model's expected input size and convert it to UINT8 type\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0).astype(np.uint8)\n    return input_data\n\ndef draw_detections(frame, detections):\n    for detection in detections:\n        ymin, xmin, ymax, xmax, score, class_id = detection\n        if score >= confidence_threshold:\n            label = f\"{labels[int(class_id)]}: {int(score * 100)}%\"\n            start_point = (int(xmin * frame.shape[1]), int(ymin * frame.shape[0]))\n            end_point = (int(xmax * frame.shape[1]), int(ymax * frame.shape[0]))\n            cv2.rectangle(frame, start_point, end_point, (255, 0, 0), 2)\n            cv2.putText(frame, label, start_point, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n# Open input and output video files\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the frame\n    input_data = preprocess_frame(frame)\n\n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Get output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    # Collect detections\n    detections = []\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            detections.append((ymin, xmin, ymax, xmax, scores[i], classes[i]))\n\n    # Draw detections on the frame\n    draw_detections(frame, detections)\n\n    # Write the processed frame to output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Video processing complete. Output saved at:\", output_path)"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 53.445,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-28-18-540641_chatcmpl-c41fd6b8-a006-44a8-88b7-e41a3d429836",
          "traceId": "186c9440",
          "type": "GENERATION",
          "name": "186c_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:28:18.540000+00:00",
          "endTime": "2026-02-07T03:28:30.671000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12131.0,
          "completionTokens": 876,
          "totalTokens": 1330,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-28-38-983266_chatcmpl-2d9284a8-532c-459a-ad1e-2b70c322f07f",
          "traceId": "186c9440",
          "type": "GENERATION",
          "name": "186c_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:28:38.983000+00:00",
          "endTime": "2026-02-07T03:28:52.412000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13429.0,
          "completionTokens": 986,
          "totalTokens": 1611,
          "promptTokens": 625,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-29-00-598385_chatcmpl-58dc3617-f483-4d3e-9e45-1f49dd916aed",
          "traceId": "186c9440",
          "type": "GENERATION",
          "name": "186c_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:29:00.598000+00:00",
          "endTime": "2026-02-07T03:29:11.985000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11387.0,
          "completionTokens": 847,
          "totalTokens": 1473,
          "promptTokens": 626,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:30:09.772Z",
      "createdAt": "2026-02-07T03:28:19.060Z",
      "environment": "default"
    },
    {
      "id": "82606e8f",
      "timestamp": "2026-02-07T03:26:09.726000+00:00",
      "name": "8260_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\ndef load_labels(path):\n    p = os.path.dirname(path)\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = (l.strip() for l in f.readlines())\n        return {i: l for i, l in enumerate(lines)}\n\n# Load TFLite model and allocate tensors\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Get input shape\n_, height, width, _ = input_details[0]['shape']\n\ndef preprocess_frame(frame):\n    # Resize frame to model input size and normalize pixel values\n    frame_resized = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(frame_resized, axis=0).astype(np.float32)\n    return input_data\n\ndef postprocess(output_data):\n    # Extract detection results\n    boxes = output_data[0][0]['detection_boxes'][0].tolist()\n    classes = output_data[0][0]['detection_classes'][0].tolist()\n    scores = output_data[0][0]['detection_scores'][0].tolist()\n\n    return boxes, classes, scores\n\ndef draw_detections(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            \n            left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n\n            label = f'{labels[int(classes[i])]}: {int(scores[i]*100)}%'\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (left, top - round(1.5*label_size[1])), \n                          (left + round(1.5*label_size[0]), top + base_line), \n                          (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (left, top), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\ndef process_video():\n    cap = cv2.VideoCapture(input_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        input_data = preprocess_frame(frame)\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        interpreter.invoke()\n\n        boxes, classes, scores = postprocess(interpreter.get_all_outputs())\n\n        draw_detections(frame, boxes, classes, scores)\n\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    process_video()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_dd6650fe_1770434863.py\", line 93, in <module>\n    process_video()\n  File \"script_dd6650fe_1770434863.py\", line 79, in process_video\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor ."
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 102.262,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-26-09-727724_chatcmpl-6215914d-d120-4f72-aaf9-0f5e78e9dbb6",
          "traceId": "82606e8f",
          "type": "GENERATION",
          "name": "8260_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:26:09.727000+00:00",
          "endTime": "2026-02-07T03:26:20.050000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10323.0,
          "completionTokens": 749,
          "totalTokens": 1203,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-26-27-669989_chatcmpl-1cf65299-3eae-4b35-a9e5-fbd6971c87a2",
          "traceId": "82606e8f",
          "type": "GENERATION",
          "name": "8260_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:26:27.669000+00:00",
          "endTime": "2026-02-07T03:26:40.459000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12790.0,
          "completionTokens": 948,
          "totalTokens": 1514,
          "promptTokens": 566,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-26-48-848627_chatcmpl-b8dcb926-0d50-4d26-8b04-8718aa6c087e",
          "traceId": "82606e8f",
          "type": "GENERATION",
          "name": "8260_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:26:48.848000+00:00",
          "endTime": "2026-02-07T03:27:01.455000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12607.0,
          "completionTokens": 913,
          "totalTokens": 1534,
          "promptTokens": 621,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-27-09-763255_chatcmpl-629d987c-4812-4a4f-b4ec-7b6e0c867c5c",
          "traceId": "82606e8f",
          "type": "GENERATION",
          "name": "8260_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T03:27:09.763000+00:00",
          "endTime": "2026-02-07T03:27:22.259000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12496.0,
          "completionTokens": 927,
          "totalTokens": 1590,
          "promptTokens": 663,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-27-30-529476_chatcmpl-a90e0787-836d-4514-adcf-29ba19acb14a",
          "traceId": "82606e8f",
          "type": "GENERATION",
          "name": "8260_tpusg_gen_attempt#5",
          "startTime": "2026-02-07T03:27:30.529000+00:00",
          "endTime": "2026-02-07T03:27:43.773000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13244.0,
          "completionTokens": 985,
          "totalTokens": 1602,
          "promptTokens": 617,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "6c777ccc-769f-44ff-b559-f6ea22ee6f44",
          "traceId": "82606e8f",
          "type": "SPAN",
          "name": "error_82_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-07T03:27:51.989000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_dd6650fe_1770434863.py\", line 93, in <module>\n    process_video()\n  File \"script_dd6650fe_1770434863.py\", line 79, in process_video\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor .",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "updatedAt": "2026-02-07T03:27:57.481Z",
      "createdAt": "2026-02-07T03:26:10.247Z",
      "environment": "default"
    },
    {
      "id": "cd778d6f",
      "timestamp": "2026-02-07T03:23:07.084000+00:00",
      "name": "cd77_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\ndef set_input_tensor(interpreter, image):\n    tensor_index = interpreter.get_input_details()[0]['index']\n    input_shape = interpreter.get_input_details()[0]['shape']\n    image = cv2.resize(image, (input_shape[1], input_shape[2]))\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = np.expand_dims(image, axis=0)\n    # Normalize to [0, 255] and convert to UINT8\n    input_tensor = image.astype(np.uint8)\n    interpreter.set_tensor(tensor_index, input_tensor)\n\ndef get_output(interpreter, score_threshold):\n    boxes = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])[0]\n    classes = interpreter.get_tensor(interpreter.get_output_details()[1]['index'])[0]\n    scores = interpreter.get_tensor(interpreter.get_output_details()[2]['index'])[0]\n\n    results = []\n    for i in range(len(scores)):\n        if scores[i] >= score_threshold:\n            result = {\n                'bounding_box': boxes[i],\n                'class_id': classes[i],\n                'score': scores[i]\n            }\n            results.append(result)\n    return results\n\ndef draw_detections(image, detections, labels):\n    for detection in detections:\n        bbox = detection['bounding_box']\n        class_id = int(detection['class_id'])\n        score = detection['score']\n\n        # Denormalize bounding box\n        ymin, xmin, ymax, xmax = bbox\n        (left, right, top, bottom) = (xmin * image.shape[1], xmax * image.shape[1],\n                                      ymin * image.shape[0], ymax * image.shape[0])\n\n        cv2.rectangle(image, (int(left), int(top)), (int(right), int(bottom)),\n                      (10, 255, 0), thickness=4)\n        \n        label = f\"{labels[class_id]}: {int(score * 100)}%\"\n        label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n        top = max(top, label_size[1])\n        cv2.rectangle(image, (int(left), int(top - round(1.5 * label_size[1]))),\n                      (int(left + round(1.5 * label_size[0])), int(top + base_line)),\n                      (255, 255, 255), cv2.FILLED)\n        \n        cv2.putText(image, label, (int(left), int(top)), cv2.FONT_HERSHEY_SIMPLEX,\n                    0.7, (0, 0, 0), thickness=2)\n\ndef main():\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize interpreter with EdgeTPU delegate\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Open video files\n    cap = cv2.VideoCapture(input_path)\n    width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n    # Define the codec and create VideoWriter object\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Set input tensor\n        set_input_tensor(interpreter, frame)\n\n        # Run inference\n        interpreter.invoke()\n\n        # Get results\n        detections = get_output(interpreter, confidence_threshold)\n\n        # Draw detection boxes with labels\n        draw_detections(frame, detections, labels)\n\n        # Write the frame with detections to output video file\n        out.write(frame)\n\n    cap.release()\n    out.release()\n    cv2.destroyAllWindows()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 103.083,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-23-07-085443_chatcmpl-89066079-70c3-49cc-882d-264c452f0771",
          "traceId": "cd778d6f",
          "type": "GENERATION",
          "name": "cd77_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:23:07.085000+00:00",
          "endTime": "2026-02-07T03:23:18.807000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11722.0,
          "completionTokens": 843,
          "totalTokens": 1297,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-23-26-606587_chatcmpl-d80a103e-7ff7-49fd-9532-48584793b4c3",
          "traceId": "cd778d6f",
          "type": "GENERATION",
          "name": "cd77_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:23:26.606000+00:00",
          "endTime": "2026-02-07T03:23:40.422000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13816.0,
          "completionTokens": 1011,
          "totalTokens": 1643,
          "promptTokens": 632,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-23-48-692610_chatcmpl-45dc3c8a-12fc-42f0-aa05-833d62fba489",
          "traceId": "cd778d6f",
          "type": "GENERATION",
          "name": "cd77_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:23:48.692000+00:00",
          "endTime": "2026-02-07T03:24:00.832000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12140.0,
          "completionTokens": 906,
          "totalTokens": 1500,
          "promptTokens": 594,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-24-09-130228_chatcmpl-faa2abc5-3292-4912-9a64-88b8ccebd582",
          "traceId": "cd778d6f",
          "type": "GENERATION",
          "name": "cd77_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T03:24:09.130000+00:00",
          "endTime": "2026-02-07T03:24:25.258000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16128.0,
          "completionTokens": 1189,
          "totalTokens": 1883,
          "promptTokens": 694,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-24-33-485103_chatcmpl-183882d1-3e35-4e95-94ac-ac4d3d18bc3c",
          "traceId": "cd778d6f",
          "type": "GENERATION",
          "name": "cd77_tpusg_gen_attempt#5",
          "startTime": "2026-02-07T03:24:33.485000+00:00",
          "endTime": "2026-02-07T03:24:50.168000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16683.0,
          "completionTokens": 1211,
          "totalTokens": 1909,
          "promptTokens": 698,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:25:48.776Z",
      "createdAt": "2026-02-07T03:23:07.605Z",
      "environment": "default"
    },
    {
      "id": "c16476f5",
      "timestamp": "2026-02-07T03:20:58.535000+00:00",
      "name": "c164_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninput_shape = input_details[0]['shape']\ninput_index = input_details[0]['index']\n\n# Preprocessing function for the video frames\ndef preprocess_frame(frame, target_shape):\n    frame_resized = cv2.resize(frame, (target_shape[1], target_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0).astype(np.uint8)\n    return input_data\n\n# Read and process video\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_data = preprocess_frame(frame, input_shape)\n\n    # Run inference\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\n    # Extract output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (255, 0, 0), 2)\n            object_name = labels[int(classes[i])]\n            label = f'{object_name}: {scores[i]:.2f}'\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (int(left), int(top) - round(1.5 * label_size[1])), \n                          (int(left) + round(1.5 * label_size[0]), int(top + base_line)), \n                          (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(top) + base_line), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Write the frame with bounding boxes\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# Note: Calculating mAP requires a more complex setup and is not included in this script."
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 49.167,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-20-58-536030_chatcmpl-caaac37d-44f8-41d1-88c1-9d6658e4aff2",
          "traceId": "c16476f5",
          "type": "GENERATION",
          "name": "c164_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:20:58.536000+00:00",
          "endTime": "2026-02-07T03:21:07.743000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 9207.0,
          "completionTokens": 657,
          "totalTokens": 1111,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-21-15-365883_chatcmpl-b309b322-d6e2-46a5-928f-cf0b0ab600c5",
          "traceId": "c16476f5",
          "type": "GENERATION",
          "name": "c164_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:21:15.365000+00:00",
          "endTime": "2026-02-07T03:21:28.281000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12916.0,
          "completionTokens": 957,
          "totalTokens": 1540,
          "promptTokens": 583,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-21-36-511962_chatcmpl-950a0199-edfa-4f78-b6f9-fdeabd1ee19b",
          "traceId": "c16476f5",
          "type": "GENERATION",
          "name": "c164_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:21:36.511000+00:00",
          "endTime": "2026-02-07T03:21:47.703000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11192.0,
          "completionTokens": 834,
          "totalTokens": 1460,
          "promptTokens": 626,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:22:46.272Z",
      "createdAt": "2026-02-07T03:20:59.055Z",
      "environment": "default"
    },
    {
      "id": "fb4557bf",
      "timestamp": "2026-02-07T03:18:29.644000+00:00",
      "name": "fb45_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport os\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels from file\ndef load_labels(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    return {i: line.strip() for i, line in enumerate(lines)}\n\nlabels = load_labels(label_path)\n\n# Initialize TFLite interpreter for EdgeTPU\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninput_shape = input_details[0]['shape']\ninput_index = input_details[0]['index']\n\n# Prepare the video capture and writer objects\ncap = cv2.VideoCapture(input_path)\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\noutput_size = (int(width * 0.5), int(height * 0.5))  # Adjusted for processing\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, output_size)\n\ndef preprocess_frame(frame):\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0).astype(np.uint8)\n    return input_data\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the frame\n    input_data = preprocess_frame(frame)\n\n    # Run inference\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\n    # Get output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    h, w, _ = frame.shape\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * w, xmax * w, ymin * h, ymax * h)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            object_name = labels[int(classes[i])]\n            label = f'{object_name}: {int(scores[i] * 100)}%'\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (int(left), int(top - round(1.5 * label_size[1]))),\n                          (int(left + label_size[0]), int(top + base_line)), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(top)),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Resize frame for output video\n    resized_frame = cv2.resize(frame, output_size)\n    \n    # Write the processed frame to the output video file\n    out.write(resized_frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Processing complete. The output is saved at:\", output_path)"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 73.027,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-18-29-646005_chatcmpl-96b6a51f-17c2-410f-aa09-73a3f5326a74",
          "traceId": "fb4557bf",
          "type": "GENERATION",
          "name": "fb45_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:18:29.646000+00:00",
          "endTime": "2026-02-07T03:18:42.227000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12581.0,
          "completionTokens": 880,
          "totalTokens": 1334,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-18-50-049389_chatcmpl-8f00bd12-6b8e-4c03-9be0-98017596f514",
          "traceId": "fb4557bf",
          "type": "GENERATION",
          "name": "fb45_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:18:50.049000+00:00",
          "endTime": "2026-02-07T03:19:00.781000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10732.0,
          "completionTokens": 793,
          "totalTokens": 1362,
          "promptTokens": 569,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-19-09-237383_chatcmpl-397aec90-2874-4e8d-b5d5-b458e0ef0946",
          "traceId": "fb4557bf",
          "type": "GENERATION",
          "name": "fb45_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:19:09.237000+00:00",
          "endTime": "2026-02-07T03:19:21.239000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12002.0,
          "completionTokens": 888,
          "totalTokens": 1514,
          "promptTokens": 626,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-19-29-459327_chatcmpl-39e10f66-355d-42b3-b583-33dddf5d150d",
          "traceId": "fb4557bf",
          "type": "GENERATION",
          "name": "fb45_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T03:19:29.459000+00:00",
          "endTime": "2026-02-07T03:19:42.673000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13214.0,
          "completionTokens": 982,
          "totalTokens": 1608,
          "promptTokens": 626,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:20:37.922Z",
      "createdAt": "2026-02-07T03:18:30.167Z",
      "environment": "default"
    },
    {
      "id": "78c4fedd",
      "timestamp": "2026-02-07T03:16:17.132000+00:00",
      "name": "78c4_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels from label map\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the interpreter with EdgeTPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninput_size = input_details[0]['shape'][1:3]\ninput_tensor_index = input_details[0]['index']\n\n# Open video capture\ncap = cv2.VideoCapture(input_path)\nout = None\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the frame\n    img = cv2.resize(frame, (input_size[1], input_size[0]))\n    img = np.expand_dims(img, axis=0).astype(np.uint8)\n    \n    # Perform inference\n    interpreter.set_tensor(input_tensor_index, img)\n    interpreter.invoke()\n    \n    # Retrieve detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n    \n    h, w, _ = frame.shape\n    \n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * w, xmax * w, ymin * h, ymax * h)\n            \n            # Draw bounding box\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (10, 255, 0), 2)\n            \n            # Draw label\n            object_name = labels[int(classes[i])]\n            label = f'{object_name}: {int(scores[i]*100)}%'\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (int(left), int(top - label_size[1])), (int(left + label_size[0]), int(top + base_line)), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(top)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Initialize video writer\n    if out is None:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n    \n    out.write(frame)\n\ncap.release()\nout.release()\n\nprint(\"Processing complete and output saved to:\", output_path)"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 52.947,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-16-17-134094_chatcmpl-3aa2ce2d-c83f-4666-b856-9e7d23901c76",
          "traceId": "78c4fedd",
          "type": "GENERATION",
          "name": "78c4_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:16:17.134000+00:00",
          "endTime": "2026-02-07T03:16:29.256000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12122.0,
          "completionTokens": 835,
          "totalTokens": 1289,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-16-37-097791_chatcmpl-e7695aff-ff7d-49b0-85a6-d6225ad648a1",
          "traceId": "78c4fedd",
          "type": "GENERATION",
          "name": "78c4_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:16:37.097000+00:00",
          "endTime": "2026-02-07T03:16:48.530000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11433.0,
          "completionTokens": 841,
          "totalTokens": 1420,
          "promptTokens": 579,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-16-56-974697_chatcmpl-2b1f4462-66ea-4f47-83e6-a36f6e713d3b",
          "traceId": "78c4fedd",
          "type": "GENERATION",
          "name": "78c4_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:16:56.974000+00:00",
          "endTime": "2026-02-07T03:17:10.081000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13107.0,
          "completionTokens": 961,
          "totalTokens": 1551,
          "promptTokens": 590,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:18:08.357Z",
      "createdAt": "2026-02-07T03:16:17.656Z",
      "environment": "default"
    },
    {
      "id": "732fd66a",
      "timestamp": "2026-02-07T03:13:31.400000+00:00",
      "name": "732f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\ndef main():\n    # Configuration parameters\n    model_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\n    label_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\n    input_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\n    output_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n    confidence_threshold = 0.5\n\n    # Load labels\n    with open(label_path, 'r') as f:\n        labels = [line.strip() for line in f.readlines()]\n\n    # Initialize the TFLite interpreter with EdgeTPU support\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Get video capture and writer\n    cap = cv2.VideoCapture(input_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    # Process video\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocessing: Resize and convert to UINT8\n        input_shape = input_details[0]['shape']\n        image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n        image_np = np.expand_dims(image_resized, axis=0).astype(np.uint8)\n\n        # Inference\n        interpreter.set_tensor(input_details[0]['index'], image_np)\n        interpreter.invoke()\n\n        # Output handling: Get detection results\n        boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n        classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n        scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence of detected objects\n\n        for i in range(len(scores)):\n            if scores[i] > confidence_threshold:\n                ymin, xmin, ymax, xmax = boxes[i]\n                (left, right, top, bottom) = (xmin * width, xmax * width,\n                                              ymin * height, ymax * height)\n                cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n\n                object_name = labels[int(classes[i])]\n                label = f\"{object_name}: {int(scores[i]*100)}%\"\n                labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n                top = max(top, labelSize[1])\n                cv2.rectangle(frame, (int(left), int(top) - round(1.5*labelSize[1])),\n                              (int(left)+round(1.5*labelSize[0]), int(top+baseLine)),\n                              (255, 255, 255), cv2.FILLED)\n                cv2.putText(frame, label, (int(left), int(top) + baseLine),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n        # Write the frame with detections\n        out.write(frame)\n        cv2.imshow('Frame', frame)\n\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n\n    cap.release()\n    out.release()\n    cv2.destroyAllWindows()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 86.21,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-13-31-401820_chatcmpl-f9b436d3-a336-4e3c-83fa-5606b3012018",
          "traceId": "732fd66a",
          "type": "GENERATION",
          "name": "732f_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:13:31.401000+00:00",
          "endTime": "2026-02-07T03:13:47.251000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15850.0,
          "completionTokens": 1104,
          "totalTokens": 1558,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-13-55-052177_chatcmpl-75f24b3f-3725-4b1b-99ae-3de62d62eca9",
          "traceId": "732fd66a",
          "type": "GENERATION",
          "name": "732f_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:13:55.052000+00:00",
          "endTime": "2026-02-07T03:14:13.267000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18215.0,
          "completionTokens": 1328,
          "totalTokens": 1965,
          "promptTokens": 637,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-14-21-649375_chatcmpl-dd347a5a-9daa-41ca-a5d8-5924567629d1",
          "traceId": "732fd66a",
          "type": "GENERATION",
          "name": "732f_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:14:21.649000+00:00",
          "endTime": "2026-02-07T03:14:36.540000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14891.0,
          "completionTokens": 1098,
          "totalTokens": 1754,
          "promptTokens": 656,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-14-44-752480_chatcmpl-62f8a14b-e8b5-42d5-9faf-34ce2be35344",
          "traceId": "732fd66a",
          "type": "GENERATION",
          "name": "732f_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T03:14:44.752000+00:00",
          "endTime": "2026-02-07T03:14:57.611000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12859.0,
          "completionTokens": 946,
          "totalTokens": 1599,
          "promptTokens": 653,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:15:56.154Z",
      "createdAt": "2026-02-07T03:13:31.924Z",
      "environment": "default"
    },
    {
      "id": "12d17c7d",
      "timestamp": "2026-02-07T03:11:19.067000+00:00",
      "name": "12d1_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport os\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Open the input video file\ncap = cv2.VideoCapture(input_path)\nwidth, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_shape = input_details[0]['shape']\n    image = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image = np.expand_dims(image, axis=0)\n\n    # Convert to UINT8\n    image = image.astype(np.uint8)\n\n    # Set the tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], image)\n    interpreter.invoke()\n\n    # Retrieve output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    # Loop over all detections and draw detection box if confidence is above threshold\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            text = f'{labels[int(classes[i])]}: {int(scores[i] * 100)}%'\n            cv2.putText(frame, text, (int(left), int(top) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame with detections\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 52.361,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-11-19-069072_chatcmpl-28bf9711-80c5-4652-a56b-7a53c21c9ba2",
          "traceId": "12d17c7d",
          "type": "GENERATION",
          "name": "12d1_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:11:19.069000+00:00",
          "endTime": "2026-02-07T03:11:32.286000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13217.0,
          "completionTokens": 925,
          "totalTokens": 1379,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-11-40-210158_chatcmpl-9128874b-eb7c-4012-a361-075bff58f775",
          "traceId": "12d17c7d",
          "type": "GENERATION",
          "name": "12d1_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:11:40.210000+00:00",
          "endTime": "2026-02-07T03:11:50.957000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10747.0,
          "completionTokens": 792,
          "totalTokens": 1355,
          "promptTokens": 563,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-11-59-140458_chatcmpl-29b5ea29-cb74-4564-ae6a-8f883a54f1b7",
          "traceId": "12d17c7d",
          "type": "GENERATION",
          "name": "12d1_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:11:59.140000+00:00",
          "endTime": "2026-02-07T03:12:11.430000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12290.0,
          "completionTokens": 904,
          "totalTokens": 1531,
          "promptTokens": 627,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:13:09.929Z",
      "createdAt": "2026-02-07T03:11:19.589Z",
      "environment": "default"
    },
    {
      "id": "de5e50c8",
      "timestamp": "2026-02-07T03:08:25.218000+00:00",
      "name": "de5e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\ndef load_labels(path):\n    p = open(path, \"rb\")\n    result = {}\n    for idx, pair in enumerate(p.read().decode(\"utf-8\").strip().split(\"\\n\")):\n        key, val = pair.split(maxsplit=1)\n        result[int(key)] = val.strip()\n    return result\n\ndef set_input_tensor(interpreter, image):\n    tensor_index = interpreter.get_input_details()[0]['index']\n    input_shape = interpreter.get_input_details()[0]['shape']\n    # Resize and normalize the image\n    image = cv2.resize(image, (input_shape[1], input_shape[2]))\n    image = np.expand_dims(image, axis=0)\n    image = (image / 255.0).astype(np.uint8)  # Ensure UINT8 format for EdgeTPU\n    interpreter.set_tensor(tensor_index, image)\n\ndef get_output(interpreter, score_threshold):\n    boxes = interpreter.get_tensor(\n        interpreter.get_output_details()[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(\n        interpreter.get_output_details()[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(\n        interpreter.get_output_details()[2]['index'])[0]  # Confidence of detected objects\n    # Filter out detections with low confidence\n    valid_detections = []\n    for i in range(len(scores)):\n        if scores[i] >= score_threshold:\n            valid_detections.append((boxes[i], classes[i], scores[i]))\n    return valid_detections\n\ndef main():\n    # Load TFLite model and allocate tensors.\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n\n    _, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n\n    # Open the video file\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"Error opening video stream or file\")\n        return\n\n    # Get video properties\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Define the codec and create VideoWriter object\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess the image\n        set_input_tensor(interpreter, frame)\n\n        # Run inference\n        interpreter.invoke()\n\n        # Get results\n        detections = get_output(interpreter, confidence_threshold)\n\n        # Draw boxes and labels on the frame\n        for detection in detections:\n            bbox, class_id, score = detection\n            ymin, xmin, ymax, xmax = bbox\n\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (10, 255, 0), 2)\n\n            label = f\"{labels[int(class_id)]}: {int(score*100)}%\"\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.putText(frame, label, (int(left), int(top-5)), cv2.FONT_HERSHEY_SIMPLEX,\n                        0.7, (255, 255, 255), 2)\n\n        # Write the frame into the output video file\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 95.091,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-08-25-219453_chatcmpl-8aefbea4-8360-4630-ae3b-42c200b25a29",
          "traceId": "de5e50c8",
          "type": "GENERATION",
          "name": "de5e_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:08:25.219000+00:00",
          "endTime": "2026-02-07T03:08:37.715000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12496.0,
          "completionTokens": 862,
          "totalTokens": 1316,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-08-45-854985_chatcmpl-2da224a6-d952-4759-8f6f-a82941f8c1a2",
          "traceId": "de5e50c8",
          "type": "GENERATION",
          "name": "de5e_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:08:45.854000+00:00",
          "endTime": "2026-02-07T03:08:55.920000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10066.0,
          "completionTokens": 731,
          "totalTokens": 1356,
          "promptTokens": 625,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-09-04-087117_chatcmpl-10edca06-6f86-4c6f-ac81-0efb938252f9",
          "traceId": "de5e50c8",
          "type": "GENERATION",
          "name": "de5e_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:09:04.087000+00:00",
          "endTime": "2026-02-07T03:09:15.985000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11898.0,
          "completionTokens": 886,
          "totalTokens": 1447,
          "promptTokens": 561,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-09-24-323147_chatcmpl-b785fd9f-4aee-4c54-b77c-cfef3e00e6a5",
          "traceId": "de5e50c8",
          "type": "GENERATION",
          "name": "de5e_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T03:09:24.323000+00:00",
          "endTime": "2026-02-07T03:09:37.748000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13425.0,
          "completionTokens": 991,
          "totalTokens": 1616,
          "promptTokens": 625,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-09-46-174978_chatcmpl-e88d86cc-06e2-4c4e-92f7-0436ebc7490a",
          "traceId": "de5e50c8",
          "type": "GENERATION",
          "name": "de5e_tpusg_gen_attempt#5",
          "startTime": "2026-02-07T03:09:46.174000+00:00",
          "endTime": "2026-02-07T03:10:00.310000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14136.0,
          "completionTokens": 1034,
          "totalTokens": 1697,
          "promptTokens": 663,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:10:57.940Z",
      "createdAt": "2026-02-07T03:08:25.741Z",
      "environment": "default"
    },
    {
      "id": "b9486fb9",
      "timestamp": "2026-02-07T03:06:09.402000+00:00",
      "name": "b948_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\ndef load_video(input_path):\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise IOError(\"Cannot open video file\")\n    return cap\n\ndef preprocess_frame(frame, input_size):\n    # Resize and normalize the frame\n    resized_frame = cv2.resize(frame, (input_size[1], input_size[0]))\n    normalized_frame = np.expand_dims(resized_frame, axis=0).astype(np.float32)\n    normalized_frame /= 255.0  # Normalize to [0,1]\n    return normalized_frame\n\ndef detect_objects(interpreter, frame):\n    input_size = interpreter.get_input_details()[0]['shape'][1:3]\n    preprocessed_frame = preprocess_frame(frame, input_size)\n\n    # Set the tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], (preprocessed_frame * 255).astype(np.uint8))\n    interpreter.invoke()\n\n    # Retrieve detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    return boxes, classes, scores\n\ndef draw_detections(frame, boxes, classes, scores):\n    height, width, _ = frame.shape\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            left, right, top, bottom = int(xmin * width), int(xmax * width), int(ymin * height), int(ymax * height)\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n\n            class_id = int(classes[i])\n            label = f\"{labels[class_id]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\ndef compute_map():\n    # Placeholder for mAP computation logic\n    pass\n\ndef main():\n    cap = load_video(input_path)\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        boxes, classes, scores = detect_objects(interpreter, frame)\n        draw_detections(frame, boxes, classes, scores)\n\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\n    compute_map()  # Compute mAP after processing all frames\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 57.454,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-06-09-404131_chatcmpl-eb2b6208-c9fa-43a8-8a4a-99fef52c5d4e",
          "traceId": "b9486fb9",
          "type": "GENERATION",
          "name": "b948_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:06:09.404000+00:00",
          "endTime": "2026-02-07T03:06:25.343000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15939.0,
          "completionTokens": 1103,
          "totalTokens": 1557,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-06-33-112655_chatcmpl-dd8106f5-f604-45ce-93b7-3510fe5867aa",
          "traceId": "b9486fb9",
          "type": "GENERATION",
          "name": "b948_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:06:33.112000+00:00",
          "endTime": "2026-02-07T03:06:46.390000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13278.0,
          "completionTokens": 982,
          "totalTokens": 1537,
          "promptTokens": 555,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-06-54-578699_chatcmpl-ee6b2620-f3db-4c1a-9ca9-e56f1b51f291",
          "traceId": "b9486fb9",
          "type": "GENERATION",
          "name": "b948_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:06:54.578000+00:00",
          "endTime": "2026-02-07T03:07:06.858000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12280.0,
          "completionTokens": 906,
          "totalTokens": 1565,
          "promptTokens": 659,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:08:05.479Z",
      "createdAt": "2026-02-07T03:06:09.930Z",
      "environment": "default"
    },
    {
      "id": "fd2a2a38",
      "timestamp": "2026-02-07T03:03:57.996000+00:00",
      "name": "fd2a_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\ndef load_labels(label_path):\n    \"\"\"Load labels from file.\"\"\"\n    with open(label_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    return {i: line.strip() for i, line in enumerate(lines)}\n\ndef preprocess_frame(frame, input_shape):\n    \"\"\"Preprocess frame to fit model requirements.\"\"\"\n    height, width, _ = frame.shape\n    img = cv2.resize(frame, (input_shape[1], input_shape[0]))\n    img = np.expand_dims(img, axis=0)\n    return img\n\ndef postprocess_results(results, labels, confidence_threshold):\n    \"\"\"Process the inference results and draw boxes with labels.\"\"\"\n    detection_boxes = results[0]\n    detection_classes = results[1]\n    detection_scores = results[2]\n\n    for i in range(len(detection_scores)):\n        if detection_scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = detection_boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            object_id = int(detection_classes[i])\n            label = labels.get(object_id, 'N/A')\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            text = f'{label}: {detection_scores[i]:.2f}'\n            cv2.putText(frame, text, (int(left), int(top) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\ndef main():\n    # Configuration parameters\n    model_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\n    label_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\n    input_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\n    output_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n    confidence_threshold = 0.5\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize interpreter\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n\n    # Get input and output details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    _, height, width, _ = input_details[0]['shape']\n\n    # Open video files\n    cap = cv2.VideoCapture(input_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess the frame\n        input_data = preprocess_frame(frame, interpreter.get_input_details()[0]['shape'][1:3])\n\n        # Set input tensor and run inference\n        interpreter.set_tensor(input_details[0]['index'], (input_data / 255.0).astype(np.float32))\n        interpreter.invoke()\n\n        # Extract results\n        results = [interpreter.get_tensor(output['index']) for output in output_details]\n        postprocess_results(results, labels, confidence_threshold)\n\n        # Write the frame with detections\n        out.write(frame)\n\n    cap.release()\n    out.release()\n    cv2.destroyAllWindows()\n\nif __name__ == \"__main__\":\n    main()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_cc089971_1770433533.py\", line 88, in <module>\n    main()\n  File \"script_cc089971_1770433533.py\", line 73, in main\n    interpreter.set_tensor(input_details[0]['index'], (input_data / 255.0).astype(np.float32))\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor ."
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 103.811,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-03-57-996855_chatcmpl-cfccb246-07fd-4824-a1e4-0aeae01a5419",
          "traceId": "fd2a2a38",
          "type": "GENERATION",
          "name": "fd2a_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:03:57.996000+00:00",
          "endTime": "2026-02-07T03:04:11.536000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13540.0,
          "completionTokens": 910,
          "totalTokens": 1364,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-04-19-287217_chatcmpl-6f21c32b-f066-4b76-90c4-a64e6dbafe7a",
          "traceId": "fd2a2a38",
          "type": "GENERATION",
          "name": "fd2a_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:04:19.287000+00:00",
          "endTime": "2026-02-07T03:04:31.643000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12356.0,
          "completionTokens": 907,
          "totalTokens": 1479,
          "promptTokens": 572,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-04-39-835782_chatcmpl-f6f0d8f0-30ff-4230-9868-3605eabded8a",
          "traceId": "fd2a2a38",
          "type": "GENERATION",
          "name": "fd2a_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T03:04:39.835000+00:00",
          "endTime": "2026-02-07T03:04:50.690000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10855.0,
          "completionTokens": 796,
          "totalTokens": 1424,
          "promptTokens": 628,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-04-59-063392_chatcmpl-05359013-b296-4949-be2f-32b968b311a8",
          "traceId": "fd2a2a38",
          "type": "GENERATION",
          "name": "fd2a_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T03:04:59.063000+00:00",
          "endTime": "2026-02-07T03:05:13.263000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14200.0,
          "completionTokens": 1037,
          "totalTokens": 1665,
          "promptTokens": 628,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-05-21-721204_chatcmpl-d4e7be72-9cea-4f73-8b19-d3a5e771fa05",
          "traceId": "fd2a2a38",
          "type": "GENERATION",
          "name": "fd2a_tpusg_gen_attempt#5",
          "startTime": "2026-02-07T03:05:21.721000+00:00",
          "endTime": "2026-02-07T03:05:33.573000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11852.0,
          "completionTokens": 877,
          "totalTokens": 1536,
          "promptTokens": 659,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "9a8c7b3a-0b8d-4af0-afec-13d9bd0b3246",
          "traceId": "fd2a2a38",
          "type": "SPAN",
          "name": "error_fd_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-07T03:05:41.807000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_cc089971_1770433533.py\", line 88, in <module>\n    main()\n  File \"script_cc089971_1770433533.py\", line 73, in main\n    interpreter.set_tensor(input_details[0]['index'], (input_data / 255.0).astype(np.float32))\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor .",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "updatedAt": "2026-02-07T03:05:47.168Z",
      "createdAt": "2026-02-07T03:03:58.519Z",
      "environment": "default"
    },
    {
      "id": "e392d87e",
      "timestamp": "2026-02-07T03:02:08.302000+00:00",
      "name": "e392_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels from the label map file\ndef load_labels(label_file):\n    with open(label_file, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    return {i: line.strip() for i, line in enumerate(lines)}\n\nlabels = load_labels(label_path)\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing: Resize and convert the image to required format\n    input_frame = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_frame = np.expand_dims(input_frame, axis=0).astype(np.uint8)\n\n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_frame)\n    interpreter.invoke()\n\n    # Extract output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw bounding box and label on the image\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            object_name = labels[int(classes[i])] if int(classes[i]) in labels else 'N/A'\n            label = f'{object_name}: {int(scores[i]*100)}%'\n            cv2.putText(frame, label, (int(left), int(top-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n    # Write the frame with detection boxes\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\n\nprint(\"Video processing complete. Output saved to:\", output_path)"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 32.535,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-02-08-304605_chatcmpl-13922e6b-ab5b-4553-8cfd-9fccfff6e7d9",
          "traceId": "e392d87e",
          "type": "GENERATION",
          "name": "e392_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:02:08.304000+00:00",
          "endTime": "2026-02-07T03:02:21.552000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13248.0,
          "completionTokens": 911,
          "totalTokens": 1365,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-05-02-29-311079_chatcmpl-f1ef9477-1d10-4dd9-8c62-d357875d4073",
          "traceId": "e392d87e",
          "type": "GENERATION",
          "name": "e392_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T03:02:29.311000+00:00",
          "endTime": "2026-02-07T03:02:40.839000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11528.0,
          "completionTokens": 824,
          "totalTokens": 1446,
          "promptTokens": 622,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:03:38.580Z",
      "createdAt": "2026-02-07T03:02:08.828Z",
      "environment": "default"
    },
    {
      "id": "35b0cf0d",
      "timestamp": "2026-02-07T03:00:36.802000+00:00",
      "name": "35b0_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        return [line.strip() for line in f.readlines()]\n\ndef set_input_tensor(interpreter, image):\n    tensor_index = interpreter.get_input_details()[0]['index']\n    input_shape = interpreter.get_input_details()[0]['shape']\n    image = cv2.resize(image, (input_shape[1], input_shape[2]))\n    image = np.expand_dims(image, axis=0)\n    interpreter.set_tensor(tensor_index, image)\n\ndef get_output(interpreter, score_threshold):\n    boxes = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(interpreter.get_output_details()[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(interpreter.get_output_details()[2]['index'])[0]  # Confidence of detected objects\n    \n    results = []\n    for i in range(len(scores)):\n        if scores[i] >= score_threshold:\n            result = {\n                'bounding_box': boxes[i],\n                'class_id': classes[i],\n                'score': scores[i]\n            }\n            results.append(result)\n    return results\n\ndef draw_detections(image, detections, labels):\n    for detection in detections:\n        ymin, xmin, ymax, xmax = detection['bounding_box']\n        class_id = int(detection['class_id'])\n        score = detection['score']\n\n        left, right, top, bottom = int(xmin * image.shape[1]), int(xmax * image.shape[1]), int(ymin * image.shape[0]), int(ymax * image.shape[0])\n        \n        cv2.rectangle(image, (left, top), (right, bottom), color=(0, 255, 0), thickness=2)\n        label = f\"{labels[class_id]}: {int(score * 100)}%\"\n        cv2.putText(image, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\ndef main():\n    model_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\n    label_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\n    input_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\n    output_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n    confidence_threshold = 0.5\n\n    labels = load_labels(label_path)\n\n    # Setup\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n\n    input_size = tuple(interpreter.get_input_details()[0]['shape'][1:3])\n\n    # Preprocessing\n    cap = cv2.VideoCapture(input_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        set_input_tensor(interpreter, frame)\n        interpreter.invoke()\n\n        detections = get_output(interpreter, confidence_threshold)\n\n        draw_detections(frame, detections, labels)\n        \n        out.write(frame)\n\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 13.021,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-05-00-36-803758_chatcmpl-c1e9fec4-0646-467d-bcdd-c5d07377afd6",
          "traceId": "35b0cf0d",
          "type": "GENERATION",
          "name": "35b0_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T03:00:36.803000+00:00",
          "endTime": "2026-02-07T03:00:49.824000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13021.0,
          "completionTokens": 873,
          "totalTokens": 1327,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:01:48.058Z",
      "createdAt": "2026-02-07T03:00:37.350Z",
      "environment": "default"
    },
    {
      "id": "c749384b",
      "timestamp": "2026-02-07T02:58:01.524000+00:00",
      "name": "c749_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels from label file\ndef load_labels(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return [line.strip() for line in f.readlines()]\n\nlabels = load_labels(label_path)\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate(\"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\")]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninput_shape = input_details[0]['shape']\ninput_index = input_details[0]['index']\n\n# Open video files\ncap = cv2.VideoCapture(input_path)\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nout = cv2.VideoWriter(\n    output_path,\n    cv2.VideoWriter_fourcc(*'mp4v'),\n    fps,\n    (frame_width, frame_height)\n)\n\n# Process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess input image\n    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    img_resized = cv2.resize(img_rgb, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(img_resized, axis=0)\n\n    # Set tensor type to UINT8 for the input\n    input_data_uint8 = input_data.astype(np.uint8)\n    interpreter.set_tensor(input_index, input_data_uint8)\n\n    # Run inference\n    start_time = time.time()\n    interpreter.invoke()\n    end_time = time.time()\n\n    # Extract output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    # Loop over all detections and draw detection box if confidence is above minimum threshold\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            class_id = int(classes[i])\n            label = f'{labels[class_id]}: {int(scores[i] * 100)}%'\n            \n            # Draw a bounding box rectangle and label on the image\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (10, 255, 0), 2)\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(int(top), label_size[1])\n            cv2.rectangle(frame, (int(left), int(top - label_size[1])), \n                          (int(left + label_size[0]), int(top + base_line)), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(top)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Write the frame with detection boxes\n    out.write(frame)\n    print(f\"Processed frame in {end_time - start_time:.4f} seconds\")\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Video processing completed and saved to:\", output_path)"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 75.831,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-58-01-525392_chatcmpl-df1b6bbd-b82e-443b-855c-edc0672a6556",
          "traceId": "c749384b",
          "type": "GENERATION",
          "name": "c749_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:58:01.525000+00:00",
          "endTime": "2026-02-07T02:58:15.387000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13862.0,
          "completionTokens": 924,
          "totalTokens": 1378,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-58-23-171496_chatcmpl-f1fa0344-8d72-42d6-bde3-20bde1545a87",
          "traceId": "c749384b",
          "type": "GENERATION",
          "name": "c749_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:58:23.171000+00:00",
          "endTime": "2026-02-07T02:58:35.762000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12591.0,
          "completionTokens": 864,
          "totalTokens": 1427,
          "promptTokens": 563,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-58-43-938951_chatcmpl-455299b6-4fd4-451e-b975-54c604fea06b",
          "traceId": "c749384b",
          "type": "GENERATION",
          "name": "c749_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T02:58:43.938000+00:00",
          "endTime": "2026-02-07T02:58:55.701000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11763.0,
          "completionTokens": 842,
          "totalTokens": 1469,
          "promptTokens": 627,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-59-03-856957_chatcmpl-2f025b46-39f9-4ee2-83ce-2ee09abe8791",
          "traceId": "c749384b",
          "type": "GENERATION",
          "name": "c749_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T02:59:03.856000+00:00",
          "endTime": "2026-02-07T02:59:17.356000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13500.0,
          "completionTokens": 996,
          "totalTokens": 1624,
          "promptTokens": 628,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T03:00:15.708Z",
      "createdAt": "2026-02-07T02:58:02.047Z",
      "environment": "default"
    },
    {
      "id": "ed1bf196",
      "timestamp": "2026-02-07T02:56:10.779000+00:00",
      "name": "ed1b_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport os\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels from label map file\ndef load_labels(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        return [line.strip() for line in lines]\n\n# Initialize the TFLite interpreter\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from the given path\ncap = cv2.VideoCapture(input_path)\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nlabels = load_labels(label_path)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the input image\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(frame_rgb, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    img = np.expand_dims(img, axis=0).astype(np.uint8)\n\n    # Perform inference\n    interpreter.set_tensor(input_details[0]['index'], img)\n    interpreter.invoke()\n\n    # Process output\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw rectangle and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {int(scores[i] * 100)}%'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    # Write the frame with detections\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# Note: mAP calculation is not implemented here as it requires multiple frames and ground truth data."
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 32.221,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-56-10-780096_chatcmpl-7669b343-f6ae-45fb-90ba-928c80c1f395",
          "traceId": "ed1bf196",
          "type": "GENERATION",
          "name": "ed1b_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:56:10.780000+00:00",
          "endTime": "2026-02-07T02:56:23.001000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12221.0,
          "completionTokens": 868,
          "totalTokens": 1322,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-56-31-157119_chatcmpl-44c26737-50a5-4677-866f-76dc528af5d9",
          "traceId": "ed1bf196",
          "type": "GENERATION",
          "name": "ed1b_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:56:31.157000+00:00",
          "endTime": "2026-02-07T02:56:43.001000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11844.0,
          "completionTokens": 852,
          "totalTokens": 1478,
          "promptTokens": 626,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:57:41.133Z",
      "createdAt": "2026-02-07T02:56:11.301Z",
      "environment": "default"
    },
    {
      "id": "423bfa9e",
      "timestamp": "2026-02-07T02:54:38.335000+00:00",
      "name": "423b_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\nimport cv2\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels from the label map\ndef load_labels(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n        return [line.strip() for line in lines]\n\nlabels = load_labels(label_path)\n\n# Setup: Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninput_shape = input_details[0]['shape']\ninput_index = input_details[0]['index']\nbox_output_index = output_details[0]['index']\nclass_output_index = output_details[1]['index']\n\n# Preprocessing: Read video\ncap = cv2.VideoCapture(input_path)\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Helper functions\ndef set_input_tensor(interpreter, image):\n    tensor_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']\n    image = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(image, axis=0)\n    interpreter.set_tensor(tensor_index, input_data)\n\ndef get_output(interpreter):\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n    return boxes, classes, scores\n\n# Inference loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    set_input_tensor(interpreter, frame)\n    start_time = time.time()\n    interpreter.invoke()\n    inference_time = time.time() - start_time\n\n    # Get the results\n    boxes, classes, scores = get_output(interpreter)\n\n    # Draw bounding boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width, ymin * height, ymax * height)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n\n            object_name = labels[int(classes[i])]\n            label = f\"{object_name}: {int(scores[i]*100)}%\"\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, label_size[1])\n            cv2.rectangle(frame, (int(left), int(top - round(1.5 * label_size[1]))),\n                          (int(left + round(1.5 * label_size[0])), int(top + base_line)),\n                          (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(top)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\n\n# Placeholder for mAP calculation (requires additional implementation)\nprint(\"mAP calculation is not implemented in this script.\")"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 13.874,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-54-38-336481_chatcmpl-4c21748e-adb1-4509-82ec-9c13f50002fe",
          "traceId": "423bfa9e",
          "type": "GENERATION",
          "name": "423b_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:54:38.336000+00:00",
          "endTime": "2026-02-07T02:54:52.210000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13874.0,
          "completionTokens": 990,
          "totalTokens": 1444,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:55:50.510Z",
      "createdAt": "2026-02-07T02:54:38.858Z",
      "environment": "default"
    },
    {
      "id": "275c389a",
      "timestamp": "2026-02-07T02:51:44.574000+00:00",
      "name": "275c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\ndef load_model():\n    # Load TFLite model and allocate tensors with EdgeTPU delegate\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n    return interpreter\n\ndef preprocess_frame(frame, input_size):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_size[1], input_size[0]))\n    input_data = np.expand_dims(frame_resized, axis=0).astype(np.float32)\n    return input_data\n\ndef set_input_tensor(interpreter, image):\n    tensor_index = interpreter.get_input_details()[0]['index']\n    input_tensor = interpreter.tensor(tensor_index)()[0]\n    input_tensor[:, :] = image\n\ndef get_output_boxes(interpreter, score_threshold):\n    boxes = interpreter.get_output_details()[0]['index']\n    classes = interpreter.get_output_details()[1]['index']\n    scores = interpreter.get_output_details()[2]['index']\n\n    output_data = [\n        np.squeeze(interpreter.get_tensor(boxes)),\n        np.squeeze(interpreter.get_tensor(classes)).astype(np.int32),\n        np.squeeze(interpreter.get_tensor(scores))\n    ]\n\n    detection_boxes = []\n    for i in range(len(output_data[0])):\n        if output_data[2][i] >= score_threshold:\n            box = tuple(output_data[0][i].tolist())\n            class_id = int(output_data[1][i])\n            score = float(output_data[2][i])\n            detection_boxes.append((box, class_id, score))\n    return detection_boxes\n\ndef draw_boxes(frame, boxes):\n    for box, class_id, score in boxes:\n        ymin, xmin, ymax, xmax = box\n        (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                     ymin * frame.shape[0], ymax * frame.shape[0])\n        cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (10, 255, 0), 2)\n        label = f\"{labels[class_id]}: {int(score*100)}%\"\n        cv2.putText(frame, label, (int(left), int(top-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\ndef main():\n    # Setup\n    interpreter = load_model()\n    input_size = tuple(interpreter.get_input_details()[0]['shape'][1:3])\n\n    # Video capture and writer setup\n    cap = cv2.VideoCapture(input_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocessing\n        input_data = preprocess_frame(frame, input_size)\n\n        # Inference\n        set_input_tensor(interpreter, input_data)\n        interpreter.invoke()\n\n        # Output handling\n        boxes = get_output_boxes(interpreter, confidence_threshold)\n        draw_boxes(frame, boxes)\n\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 95.581,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-51-44-575716_chatcmpl-32297a26-3e08-4305-919f-41c5ac7351f6",
          "traceId": "275c389a",
          "type": "GENERATION",
          "name": "275c_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:51:44.575000+00:00",
          "endTime": "2026-02-07T02:51:58.762000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14187.0,
          "completionTokens": 985,
          "totalTokens": 1439,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-52-06-750073_chatcmpl-5112ddad-9062-4c9a-b700-8420ec8b3a4b",
          "traceId": "275c389a",
          "type": "GENERATION",
          "name": "275c_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:52:06.750000+00:00",
          "endTime": "2026-02-07T02:52:18.040000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11290.0,
          "completionTokens": 827,
          "totalTokens": 1396,
          "promptTokens": 569,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-52-25-784815_chatcmpl-ba8c2100-7631-463e-9b38-664f4d573ebe",
          "traceId": "275c389a",
          "type": "GENERATION",
          "name": "275c_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T02:52:25.784000+00:00",
          "endTime": "2026-02-07T02:52:38.238000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12454.0,
          "completionTokens": 906,
          "totalTokens": 1460,
          "promptTokens": 554,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-52-46-410188_chatcmpl-34191c7a-4781-4783-9fc2-93c404ba8a25",
          "traceId": "275c389a",
          "type": "GENERATION",
          "name": "275c_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T02:52:46.410000+00:00",
          "endTime": "2026-02-07T02:52:58.871000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12461.0,
          "completionTokens": 909,
          "totalTokens": 1572,
          "promptTokens": 663,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-53-07-131138_chatcmpl-ac58819c-a18c-41b2-98d0-c2d9bac92997",
          "traceId": "275c389a",
          "type": "GENERATION",
          "name": "275c_tpusg_gen_attempt#5",
          "startTime": "2026-02-07T02:53:07.131000+00:00",
          "endTime": "2026-02-07T02:53:20.156000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13025.0,
          "completionTokens": 966,
          "totalTokens": 1582,
          "promptTokens": 616,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:54:18.372Z",
      "createdAt": "2026-02-07T02:51:45.098Z",
      "environment": "default"
    },
    {
      "id": "be907f31",
      "timestamp": "2026-02-07T02:49:31.915000+00:00",
      "name": "be90_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Setup TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels from label map file\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing: Resize and normalize input image\n    input_image = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_image = np.expand_dims(input_image, axis=0)\n    \n    # Convert to UINT8 as expected by the model\n    input_data = np.uint8((input_image / 255.0) * 255)\n\n    # Set tensor and perform inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Get detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            # Scale box to original image size\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width, ymin * height, ymax * height)\n            \n            # Draw bounding box and label on frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            object_name = labels[int(classes[i])]\n            label = f\"{object_name}: {int(scores[i] * 100)}%\"\n            cv2.putText(frame, label, (int(left), int(top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    # Write the frame with detections\n    out.write(frame)\n    \n# Release resources\ncap.release()\nout.release()\n\nprint(\"Processing complete. Output saved to:\", output_path)"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 53.841,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-49-31-916336_chatcmpl-49ec09bf-8353-4e57-9476-1e8938bf8456",
          "traceId": "be907f31",
          "type": "GENERATION",
          "name": "be90_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:49:31.916000+00:00",
          "endTime": "2026-02-07T02:49:44.944000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13028.0,
          "completionTokens": 907,
          "totalTokens": 1361,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-49-52-530050_chatcmpl-ee455852-6b13-4354-a999-4f4a2ad5d01b",
          "traceId": "be907f31",
          "type": "GENERATION",
          "name": "be90_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:49:52.530000+00:00",
          "endTime": "2026-02-07T02:50:04.348000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11818.0,
          "completionTokens": 856,
          "totalTokens": 1430,
          "promptTokens": 574,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-50-12-860223_chatcmpl-c0dea3a2-305c-42ce-8f06-05db6efe629a",
          "traceId": "be907f31",
          "type": "GENERATION",
          "name": "be90_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T02:50:12.860000+00:00",
          "endTime": "2026-02-07T02:50:25.757000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12897.0,
          "completionTokens": 926,
          "totalTokens": 1552,
          "promptTokens": 626,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:51:24.618Z",
      "createdAt": "2026-02-07T02:49:32.439Z",
      "environment": "default"
    },
    {
      "id": "3e9f5b9d",
      "timestamp": "2026-02-07T02:47:39.763000+00:00",
      "name": "3e9f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\ndef load_tflite_model(model_file):\n    interpreter = Interpreter(\n        model_path=model_file,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n    return interpreter\n\n# Setup\ninterpreter = load_tflite_model(model_path)\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocessing function\ndef preprocess_frame(frame):\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n    return input_data\n\n# Inference function\ndef infer(interpreter, frame):\n    input_data = preprocess_frame(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Extract output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence of detected objects\n    \n    return boxes, classes, scores\n\n# Output handling function\ndef draw_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (10, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {int(scores[i]*100)}%\"\n            cv2.putText(frame, label, (int(left), int(top-5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    boxes, classes, scores = infer(interpreter, frame)\n    draw_boxes(frame, boxes, classes, scores)\n\n    out.write(frame)  # Save the output frame\n\ncap.release()\nout.release()\n\n# Note: mAP calculation is a complex task and requires more context like ground truth data. It's not implemented here."
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 32.432,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-47-39-765219_chatcmpl-65eba819-2aff-4f8c-917c-e8ea779aee53",
          "traceId": "3e9f5b9d",
          "type": "GENERATION",
          "name": "3e9f_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:47:39.765000+00:00",
          "endTime": "2026-02-07T02:47:53.719000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13954.0,
          "completionTokens": 955,
          "totalTokens": 1409,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-48-01-541091_chatcmpl-126a1ce1-efdf-4127-a14e-b4f1ae90bd12",
          "traceId": "3e9f5b9d",
          "type": "GENERATION",
          "name": "3e9f_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:48:01.541000+00:00",
          "endTime": "2026-02-07T02:48:12.197000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10656.0,
          "completionTokens": 783,
          "totalTokens": 1348,
          "promptTokens": 565,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:49:10.407Z",
      "createdAt": "2026-02-07T02:47:40.288Z",
      "environment": "default"
    },
    {
      "id": "ac1a2941",
      "timestamp": "2026-02-07T02:45:03.037000+00:00",
      "name": "ac1a_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\ndef load_labels(path):\n    \"\"\"Load the labels from a label file.\"\"\"\n    with open(path, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\ndef preprocess_frame(frame, input_shape):\n    \"\"\"Preprocess the frame to match model's input requirements.\"\"\"\n    image = cv2.resize(frame, (input_shape[1], input_shape[0]))\n    image = np.expand_dims(image, axis=0)\n    return image\n\ndef set_input_tensor(interpreter, image):\n    \"\"\"Set the input tensor.\"\"\"\n    interpreter.set_tensor(\n        interpreter.get_input_details()[0]['index'],\n        image.astype(np.uint8))\n\ndef get_output(interpreter, score_threshold):\n    \"\"\"Return detection results from the model that are above the threshold.\"\"\"\n    boxes = np.squeeze(interpreter.get_tensor(interpreter.get_output_details()[0]['index'])[0])\n    classes = np.squeeze(interpreter.get_tensor(interpreter.get_output_details()[1]['index'])[0])\n    scores = np.squeeze(interpreter.get_tensor(interpreter.get_output_details()[2]['index'])[0])\n    \n    results = []\n    for i in range(len(scores)):\n        if scores[i] >= score_threshold:\n            result = {\n                'bounding_box': boxes[i],\n                'class_id': classes[i],\n                'score': scores[i]\n            }\n            results.append(result)\n    return results\n\ndef draw_detections(frame, results, labels):\n    \"\"\"Draw the detection boxes with labels on the frame.\"\"\"\n    for obj in results:\n        ymin, xmin, ymax, xmax = obj['bounding_box']\n        bbox_height = (ymax - ymin) * frame.shape[0]\n        bbox_width = (xmax - xmin) * frame.shape[1]\n\n        cv2.rectangle(frame,\n                      (int(xmin * frame.shape[1]), int(ymin * frame.shape[0])),\n                      (int(xmax * frame.shape[1]), int(ymax * frame.shape[0])), \n                      (10, 255, 0), 2)\n\n        label = f'{labels[int(obj[\"class_id\"])]}: {round(obj[\"score\"], 3)}'\n        cv2.putText(frame, label, \n                    (int(xmin * frame.shape[1]), int(ymin * frame.shape[0] - 10)), \n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n\ndef main():\n    model_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\n    label_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\n    input_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\n    output_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n    confidence_threshold = 0.5\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize TFLite interpreter with EdgeTPU\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n    \n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Get video capture\n    cap = cv2.VideoCapture(input_path)\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n    # Define the codec and create VideoWriter object to save the video\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess the frame\n        input_data = preprocess_frame(frame, input_details[0]['shape'][1:3])\n        \n        # Perform inference\n        set_input_tensor(interpreter, input_data)\n        interpreter.invoke()\n\n        # Get results and draw detections\n        results = get_output(interpreter, confidence_threshold)\n        draw_detections(frame, results, labels)\n\n        # Write the frame with detections to output video\n        out.write(frame)\n\n    cap.release()\n    out.release()\n    cv2.destroyAllWindows()\n\nif __name__ == '__main__':\n    main()"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 77.447,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-45-03-038969_chatcmpl-44d482e8-dce5-4b18-ab93-57ea77f177df",
          "traceId": "ac1a2941",
          "type": "GENERATION",
          "name": "ac1a_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:45:03.038000+00:00",
          "endTime": "2026-02-07T02:45:16.054000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13016.0,
          "completionTokens": 921,
          "totalTokens": 1375,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-45-23-717605_chatcmpl-d03b69f2-6ffb-4f2a-b8d4-04ae748f0c0d",
          "traceId": "ac1a2941",
          "type": "GENERATION",
          "name": "ac1a_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:45:23.717000+00:00",
          "endTime": "2026-02-07T02:45:36.452000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12735.0,
          "completionTokens": 928,
          "totalTokens": 1610,
          "promptTokens": 682,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-45-44-752656_chatcmpl-ce38901c-91d4-481a-9091-adf54c868b99",
          "traceId": "ac1a2941",
          "type": "GENERATION",
          "name": "ac1a_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T02:45:44.752000+00:00",
          "endTime": "2026-02-07T02:45:58.283000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13531.0,
          "completionTokens": 995,
          "totalTokens": 1685,
          "promptTokens": 690,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-46-06-475627_chatcmpl-0a642a1b-be19-40be-a25f-259953fe53d7",
          "traceId": "ac1a2941",
          "type": "GENERATION",
          "name": "ac1a_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T02:46:06.475000+00:00",
          "endTime": "2026-02-07T02:46:20.485000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14010.0,
          "completionTokens": 1029,
          "totalTokens": 1680,
          "promptTokens": 651,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:47:19.401Z",
      "createdAt": "2026-02-07T02:45:03.563Z",
      "environment": "default"
    },
    {
      "id": "c4673846",
      "timestamp": "2026-02-07T02:42:47.208000+00:00",
      "name": "c467_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\ndef load_labels(label_path):\n    with open(label_path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\ndef preprocess_input(image, input_size):\n    image_resized = cv2.resize(image, (input_size[1], input_size[0]))\n    input_data = np.expand_dims(image_resized, axis=0).astype(np.uint8)\n    return input_data\n\ndef draw_boxes(image, boxes, classes, scores, labels, confidence_threshold):\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * image.shape[1], xmax * image.shape[1],\n                                          ymin * image.shape[0], ymax * image.shape[0])\n            cv2.rectangle(image, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(image, label, (int(left), int(top) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\ndef main():\n    # Configuration parameters\n    model_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\n    label_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\n    input_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\n    output_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n    confidence_threshold = 0.5\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize the TFLite interpreter with EdgeTPU support\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n\n    # Get input and output details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    input_size = input_details[0]['shape'][1:3]\n\n    # Open video files\n    cap = cv2.VideoCapture(input_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess input\n        input_data = preprocess_input(frame, input_size)\n\n        # Set input tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        # Run inference\n        interpreter.invoke()\n\n        # Extract output data\n        boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n        classes = interpreter.get_tensor(output_details[1]['index'])[0]\n        scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n        # Draw bounding boxes and labels on the frame\n        draw_boxes(frame, boxes, classes, scores, labels, confidence_threshold)\n\n        # Write the processed frame to the output video\n        out.write(frame)\n\n    # Release resources\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 57.941,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-42-47-209574_chatcmpl-a4ee78c4-5046-4fc4-8233-7e389292d47e",
          "traceId": "c4673846",
          "type": "GENERATION",
          "name": "c467_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:42:47.209000+00:00",
          "endTime": "2026-02-07T02:43:02.950000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15741.0,
          "completionTokens": 1084,
          "totalTokens": 1538,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-43-10-810714_chatcmpl-b01bfd53-3614-4e04-af94-e4ac81da5911",
          "traceId": "c4673846",
          "type": "GENERATION",
          "name": "c467_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:43:10.810000+00:00",
          "endTime": "2026-02-07T02:43:25.809000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14999.0,
          "completionTokens": 1090,
          "totalTokens": 1752,
          "promptTokens": 662,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-43-34-103756_chatcmpl-e9d9badd-4779-4dcd-b448-a461c5399db8",
          "traceId": "c4673846",
          "type": "GENERATION",
          "name": "c467_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T02:43:34.103000+00:00",
          "endTime": "2026-02-07T02:43:45.150000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11047.0,
          "completionTokens": 813,
          "totalTokens": 1464,
          "promptTokens": 651,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:44:42.909Z",
      "createdAt": "2026-02-07T02:42:47.736Z",
      "environment": "default"
    },
    {
      "id": "e71bfe33",
      "timestamp": "2026-02-07T02:40:57.590000+00:00",
      "name": "e71b_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport os\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(path):\n    with open(path, 'r') as file:\n        return [line.strip() for line in file.readlines()]\n\nlabels = load_labels(label_path)\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocessing function for video frames\ndef preprocess_frame(frame, input_shape):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_shape[1], input_shape[0]))\n    return np.expand_dims(frame_resized, axis=0)\n\n# Postprocessing function to draw detection boxes\ndef draw_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = tuple(boxes[i])\n            left, right, top, bottom = int(xmin * frame.shape[1]), int(xmax * frame.shape[1]), int(ymin * frame.shape[0]), int(ymax * frame.shape[0])\n\n            cv2.rectangle(frame, (left, top), (right, bottom), (10, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {int(scores[i]*100)}%'\n            cv2.putText(frame, label, (left, top-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n# Open video files\ninput_video = cv2.VideoCapture(input_path)\noutput_video = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), \n                               input_video.get(cv2.CAP_PROP_FPS),\n                               (int(input_video.get(cv2.CAP_PROP_FRAME_WIDTH)),\n                                int(input_video.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n\n# Process each frame\nwhile True:\n    ret, frame = input_video.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    preprocessed_frame = preprocess_frame(frame, interpreter.get_input_details()[0]['shape'][1:3])\n    \n    # Perform inference\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n    interpreter.invoke()\n\n    # Get output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    # Draw detection boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to output video\n    output_video.write(frame)\n\n# Release resources\ninput_video.release()\noutput_video.release()\n\nprint(\"Video processing completed and saved at:\", output_path)"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 30.667,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-40-57-591675_chatcmpl-fa9cb7ce-4745-4dfb-ad5d-504bde1df47a",
          "traceId": "e71bfe33",
          "type": "GENERATION",
          "name": "e71b_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:40:57.591000+00:00",
          "endTime": "2026-02-07T02:41:09.127000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11536.0,
          "completionTokens": 800,
          "totalTokens": 1254,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-41-16-863125_chatcmpl-ecf8af41-a8e5-4636-8044-e16ba430d8be",
          "traceId": "e71bfe33",
          "type": "GENERATION",
          "name": "e71b_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:41:16.863000+00:00",
          "endTime": "2026-02-07T02:41:28.258000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11395.0,
          "completionTokens": 842,
          "totalTokens": 1405,
          "promptTokens": 563,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:42:25.795Z",
      "createdAt": "2026-02-07T02:40:58.115Z",
      "environment": "default"
    },
    {
      "id": "1e578093",
      "timestamp": "2026-02-07T02:37:59.665000+00:00",
      "name": "1e57_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return [line.strip() for line in f.readlines()]\n\ndef set_input_tensor(interpreter, image):\n    \"\"\"Preprocess the input image and set it to the input tensor.\"\"\"\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    height, width = input_details[0]['shape'][1:3]\n    \n    # Resize and normalize the image\n    image = cv2.resize(image, (width, height))\n    input_data = np.expand_dims(image, axis=0)\n    input_data = (input_data / 255.0).astype(np.uint8)  # Normalize to [0, 1] and convert to UINT8\n    \n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\ndef get_output(interpreter):\n    \"\"\"Return detection results from the inference.\"\"\"\n    output_details = interpreter.get_output_details()\n    \n    boxes = np.squeeze(interpreter.get_tensor(output_details[0]['index']))\n    classes = np.squeeze(interpreter.get_tensor(output_details[1]['index']))\n    scores = np.squeeze(interpreter.get_tensor(output_details[2]['index']))\n    \n    return boxes, classes, scores\n\ndef draw_detections(image, boxes, classes, scores, labels):\n    \"\"\"Draw bounding box and label on the image.\"\"\"\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * image.shape[1], xmax * image.shape[1],\n                                          ymin * image.shape[0], ymax * image.shape[0])\n            \n            cv2.rectangle(image, (int(left), int(top)), (int(right), int(bottom)), (10, 255, 0), 2)\n            label = '%s: %d%%' % (labels[int(classes[i])], int(scores[i] * 100))\n            cv2.putText(image, label, (int(left), int(top) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 255, 0), 2)\n\ndef main():\n    # Load labels\n    labels = load_labels(label_path)\n    \n    # Initialize the interpreter with EdgeTPU delegate\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    interpreter.allocate_tensors()\n    \n    # Open video files\n    cap = cv2.VideoCapture(input_path)\n    out = None\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        set_input_tensor(interpreter, frame)\n        interpreter.invoke()\n        \n        boxes, classes, scores = get_output(interpreter)\n        draw_detections(frame, boxes, classes, scores, labels)\n        \n        # Initialize video writer\n        if out is None:\n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n            out = cv2.VideoWriter(output_path, fourcc, 30.0, (frame.shape[1], frame.shape[0]))\n        \n        out.write(frame)\n    \n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 99.132,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-37-59-667114_chatcmpl-906d1a99-e266-45ba-94d0-b46d18a214bc",
          "traceId": "1e578093",
          "type": "GENERATION",
          "name": "1e57_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:37:59.667000+00:00",
          "endTime": "2026-02-07T02:38:13.470000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13803.0,
          "completionTokens": 948,
          "totalTokens": 1402,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-38-21-227010_chatcmpl-c397dd6f-2857-4954-a891-b90cad5aaf38",
          "traceId": "1e578093",
          "type": "GENERATION",
          "name": "1e57_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:38:21.227000+00:00",
          "endTime": "2026-02-07T02:38:36.046000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14819.0,
          "completionTokens": 1075,
          "totalTokens": 1693,
          "promptTokens": 618,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-38-44-120323_chatcmpl-843161b1-2e2e-45e8-b633-f500b03dc7d0",
          "traceId": "1e578093",
          "type": "GENERATION",
          "name": "1e57_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T02:38:44.120000+00:00",
          "endTime": "2026-02-07T02:38:55.830000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11710.0,
          "completionTokens": 865,
          "totalTokens": 1491,
          "promptTokens": 626,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-39-03-998381_chatcmpl-207ad1bf-be63-479f-aba1-2a4027449826",
          "traceId": "1e578093",
          "type": "GENERATION",
          "name": "1e57_tpusg_gen_attempt#4",
          "startTime": "2026-02-07T02:39:03.998000+00:00",
          "endTime": "2026-02-07T02:39:17.658000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13660.0,
          "completionTokens": 985,
          "totalTokens": 1579,
          "promptTokens": 594,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-39-25-801335_chatcmpl-17eec388-3771-4f9e-8a97-f988cb51b34c",
          "traceId": "1e578093",
          "type": "GENERATION",
          "name": "1e57_tpusg_gen_attempt#5",
          "startTime": "2026-02-07T02:39:25.801000+00:00",
          "endTime": "2026-02-07T02:39:38.799000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12998.0,
          "completionTokens": 960,
          "totalTokens": 1651,
          "promptTokens": 691,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:40:36.497Z",
      "createdAt": "2026-02-07T02:38:00.261Z",
      "environment": "default"
    },
    {
      "id": "0262222b",
      "timestamp": "2026-02-07T02:35:47.012000+00:00",
      "name": "0262_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport cv2\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load Labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup Interpreter with EdgeTPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read Video\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_frame = cv2.resize(frame, (input_details[0]['shape'][2], input_details[0]['shape'][1]))\n    input_data = np.expand_dims(input_frame, axis=0).astype(np.uint8)\n\n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    start_time = time.time()\n    interpreter.invoke()\n    inference_time = time.time() - start_time\n\n    # Output handling\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            # Draw bounding box\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n\n            # Put label\n            object_name = labels[int(classes[i])] if classes[i] >= 0 else 'N/A'\n            label = f'{object_name}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\n    # Write frame to output video\n    out.write(frame)\n    print(f'Frame processed in {inference_time:.4f} seconds')\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\n# Note: mAP calculation is not included here as it requires ground truth data and additional implementation."
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 55.071,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-35-47-013913_chatcmpl-f4a1b8b7-77a7-4281-b3c2-b893a4d712fa",
          "traceId": "0262222b",
          "type": "GENERATION",
          "name": "0262_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:35:47.013000+00:00",
          "endTime": "2026-02-07T02:36:00.854000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13841.0,
          "completionTokens": 967,
          "totalTokens": 1421,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-36-09-291818_chatcmpl-a6f2ddca-8feb-4d36-acc9-ad4ea37c0114",
          "traceId": "0262222b",
          "type": "GENERATION",
          "name": "0262_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:36:09.291000+00:00",
          "endTime": "2026-02-07T02:36:22.398000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13107.0,
          "completionTokens": 960,
          "totalTokens": 1585,
          "promptTokens": 625,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-36-30-558371_chatcmpl-457afc3f-1f9d-4a4e-b2c0-ad891ca4fd15",
          "traceId": "0262222b",
          "type": "GENERATION",
          "name": "0262_tpusg_gen_attempt#3",
          "startTime": "2026-02-07T02:36:30.558000+00:00",
          "endTime": "2026-02-07T02:36:42.084000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11526.0,
          "completionTokens": 847,
          "totalTokens": 1477,
          "promptTokens": 630,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:37:40.228Z",
      "createdAt": "2026-02-07T02:35:47.538Z",
      "environment": "default"
    },
    {
      "id": "9ef633a9",
      "timestamp": "2026-02-07T02:33:57.257000+00:00",
      "name": "9ef6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\nimport os\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup the TFLite interpreter with EdgeTPU\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\ninput_shape = input_details[0]['shape']\nheight, width, channels = input_shape[1], input_shape[2], input_shape[3]\ninput_index = input_details[0]['index']\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nfourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\nout = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    input_data = cv2.resize(frame_rgb, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Normalize and convert to UINT8\n    input_data = (input_data / 255.0).astype(np.uint8)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_index, input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    # Loop over all detections and draw detection box if confidence is above threshold\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n\n            cv2.rectangle(frame, (left, top), (right, bottom), (10, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {int(scores[i] * 100)}%\"\n            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (10, 255, 0), 2)\n\n    # Write the frame with detections\n    out.write(frame)\n\ncap.release()\nout.release()\n\n# Note: mAP computation is not included in this script as it requires a separate dataset of ground truth annotations."
      },
      "session_id": "phi4_5f04_tpusg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "phi4:latest",
        "tpu_sketch_generator"
      ],
      "latency": 32.376,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-04-33-57-259504_chatcmpl-815098f6-d09f-42ab-8691-530e12a728b5",
          "traceId": "9ef633a9",
          "type": "GENERATION",
          "name": "9ef6_tpusg_gen_attempt#1",
          "startTime": "2026-02-07T02:33:57.259000+00:00",
          "endTime": "2026-02-07T02:34:09.043000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11784.0,
          "completionTokens": 855,
          "totalTokens": 1309,
          "promptTokens": 454,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-34-17-564193_chatcmpl-5793276e-d044-453c-bc62-43b76643d065",
          "traceId": "9ef633a9",
          "type": "GENERATION",
          "name": "9ef6_tpusg_gen_attempt#2",
          "startTime": "2026-02-07T02:34:17.564000+00:00",
          "endTime": "2026-02-07T02:34:29.635000+00:00",
          "model": "phi4:latest",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12071.0,
          "completionTokens": 879,
          "totalTokens": 1503,
          "promptTokens": 624,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-07T02:35:26.020Z",
      "createdAt": "2026-02-07T02:33:57.783Z",
      "environment": "default"
    }
  ],
  "meta": {
    "total_items": 30
  }
}