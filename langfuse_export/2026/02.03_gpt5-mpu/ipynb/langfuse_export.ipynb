{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://langfuse.com/docs/query-traces\n",
    "import os\n",
    "import json\n",
    "from langfuse import Langfuse\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "LOCAL_HOST = True\n",
    "\n",
    "\"\"\"Define session_id\"\"\"\n",
    "# session_id=\"qwen2.5-coder_f4d4_dp_batch\"\n",
    "session_id_list = [\n",
    "    # \"qwen2.5-coder:32b_4e11_tpsg_batch\",\n",
    "    \"gpt-5-2025-08-07_54c5_tpusg_batch\",\n",
    "    \"gpt-5-2025-08-07_54c5_psg_batch\",\n",
    "    \"gpt-5-2025-08-07_54c5_sg_batch\",\n",
    "\n",
    "    \n",
    "    \"gpt-5-2025-08-07_40f1_sg_batch\",\n",
    "    \"gpt-5-2025-08-07_40f1_tpusg_batch\",\n",
    "    \"gpt-5-2025-08-07_40f1_psg_batch\"\n",
    "]\n",
    "\n",
    "\n",
    "\"\"\"Define paths\"\"\"\n",
    " \n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    " \n",
    "# date = os.path.basename(parent_dir)\n",
    "tex_dir = os.path.join(parent_dir, \"tex\")\n",
    "processed_data_dir = os.path.join(parent_dir, \"processed_data\")\n",
    "raw_export_dir = os.path.join(parent_dir, \"raw_export\")\n",
    "ipynb_dir = os.path.join(parent_dir, \"ipynb\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Export raw data\n",
    "\n",
    "Langfuse added a limit of 20 API invocations per minute. https://langfuse.com/faq/all/api-limits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching traces for session gpt-5-2025-08-07_54c5_tpusg_batch...\n",
      "Fetching observation data for time-23-16-38-578876_chatcmpl-D5eWl3t2ZpA3C980lljV9dfKVfCmb...\n",
      "Fetching observation data for time-23-18-51-160685_chatcmpl-D5eYt1up9LfjngVuGpq5rMycFGHvI...\n",
      "Fetching observation data for time-23-20-45-759788_chatcmpl-D5eake28i4vsFZO5paVPfYvM1aTFL...\n",
      "Fetching observation data for time-23-22-12-037929_chatcmpl-D5ec8OR1Ww3XYHC8PgWEVwd3c5ULy...\n",
      "Fetching observation data for time-23-23-31-686185_chatcmpl-D5edQH22UlzCDoLuFR8uVBGuqSNsQ...\n",
      "Fetching observation data for a581b2b6-5ba4-407a-8d3a-ed6514c08f32...\n",
      "Fetching observation data for time-23-12-56-244970_chatcmpl-D5eTABU9uk8Ci8kSGpP2ECAg8Flme...\n",
      "Fetching observation data for time-23-08-44-374639_chatcmpl-D5eP62mPyRAxpv7vO8Wj7MJSQeahy...\n",
      "Fetching observation data for time-23-05-12-743715_chatcmpl-D5eLh6mr49jzV1CZQkDCu7U74fxju...\n",
      "Fetching observation data for time-23-00-22-740017_chatcmpl-D5eH1e7zZsYtF6BAiK1ahvG4eNqib...\n",
      "Fetching observation data for time-22-56-22-900146_chatcmpl-D5eD9fFTa8poOiWm66QMEK2bpHLC8...\n",
      "Fetching observation data for time-22-52-28-143578_chatcmpl-D5e9MJkxvF22vDjcawcOUiVg0z5K0...\n",
      "Fetching observation data for time-22-48-41-056374_chatcmpl-D5e5hF65rhEenpfcirA4IWHztLmH4...\n",
      "Fetching observation data for time-22-44-10-696523_chatcmpl-D5e1Lvc3yCJIHS5OxElfzdPnk9MnV...\n",
      "Fetching observation data for time-22-40-17-012986_chatcmpl-D5dxZDXIJ2sYlIRTseoEZeBxuczeE...\n",
      "Raw JSON saved to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/raw_gpt-5-2025-08-07_54c5_tpusg_batch.json\n",
      "Fetching traces for session gpt-5-2025-08-07_54c5_psg_batch...\n",
      "Fetching observation data for time-00-05-40-634874_chatcmpl-D5fIDlpy9dS3MiGQQmWCvDUJIRw4g...\n",
      "Fetching observation data for time-00-01-32-680059_chatcmpl-D5fEDcmm7sIHXS4e0OrqOZTrpGOVt...\n",
      "Fetching observation data for time-23-58-13-697434_chatcmpl-D5fAzA80arqQ3LHJazXetFn4Occ9D...\n",
      "Fetching observation data for time-23-47-28-742136_chatcmpl-D5f0bReS3ApREiNslEkPcZDo7i6je...\n",
      "Fetching observation data for time-23-43-15-864634_chatcmpl-D5ewWlYpGLmSrKG2dehUmxxeayBQH...\n",
      "Fetching observation data for time-23-39-25-845993_chatcmpl-D5esownDeMC1tG88jNKT6zYncUG0f...\n",
      "Fetching observation data for time-23-36-37-136594_chatcmpl-D5eq5768YM8NHy5XqZwNTBXOTLhjg...\n",
      "Fetching observation data for time-23-32-35-325816_chatcmpl-D5emB23tQDeYrXluRK46hJURb92Dm...\n",
      "Fetching observation data for time-23-28-23-579511_chatcmpl-D5ei8KuKtnvDcOQyVx1aCtvEY0Wbr...\n",
      "Fetching observation data for time-23-24-58-384874_chatcmpl-D5eeo04mG2Q6SSW6RI16GkxTVI2W8...\n",
      "Raw JSON saved to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/raw_gpt-5-2025-08-07_54c5_psg_batch.json\n",
      "Fetching traces for session gpt-5-2025-08-07_54c5_sg_batch...\n",
      "Fetching observation data for time-22-34-59-862047_chatcmpl-D5dsSWjYcLR4TcjAwkP8BltiwjTRl...\n",
      "Fetching observation data for time-22-36-02-921328_chatcmpl-D5dtTSVv2s5JnFjYlqD0Uwo8aJyVi...\n",
      "Fetching observation data for time-22-37-53-896823_chatcmpl-D5dvGdoPxIXVV33KyuZB4hEgfuwup...\n",
      "Fetching observation data for time-22-29-44-634175_chatcmpl-D5dnNEGzWpY6mKfOgzMnznjXjQkPE...\n",
      "Fetching observation data for time-22-30-51-867914_chatcmpl-D5doSs1Z7QvEhB9l7RhdGc3Qct9SD...\n",
      "Fetching observation data for time-22-32-52-340486_chatcmpl-D5dqOmeS2QJpqVbu8DVQICsugmTi1...\n",
      "Fetching observation data for time-22-24-35-700864_chatcmpl-D5diOowZjSRfu2bIZaNc1KSvwBZ0r...\n",
      "Fetching observation data for time-22-25-38-979695_chatcmpl-D5djPO7zMFlNFbJOW6Kg0NV9AnZt6...\n",
      "Fetching observation data for time-22-27-38-551965_chatcmpl-D5dlLvYh3K5frMdOTsAbfAXP7IXT7...\n",
      "Fetching observation data for time-22-18-38-749628_chatcmpl-D5dcd3d0dY2oXbQFkGa4OywIzuhS5...\n",
      "Fetching observation data for time-22-20-10-379585_chatcmpl-D5de6aqQApxR9RQAgI64nH82sbcEK...\n",
      "Fetching observation data for time-22-22-08-875509_chatcmpl-D5dg1bYNsVQR01UttTVPOfeiHKPGC...\n",
      "Fetching observation data for time-22-13-00-942269_chatcmpl-D5dXBQK8VlvOR1MdUzgUayHsaYy6Y...\n",
      "Fetching observation data for time-22-14-44-998052_chatcmpl-D5dYrtkJJUIlTbFvu8lPweNkv1BxC...\n",
      "Fetching observation data for time-22-16-17-270562_chatcmpl-D5daL0a5o55xfnZHWA5d8EebU7DCR...\n",
      "Fetching observation data for time-22-08-13-072365_chatcmpl-D5dSXQvO7yOHCVrG59aUTX77otowc...\n",
      "Fetching observation data for time-22-09-13-594678_chatcmpl-D5dTWUQYoYEf1o5GDWD2achWpoGjI...\n",
      "Fetching observation data for time-22-11-00-617459_chatcmpl-D5dVEMraEQr1uUz334ic7cq3WwGUA...\n",
      "Fetching observation data for time-22-03-32-062232_chatcmpl-D5dO0D2U08thhRJ9TqdeXlWMVlbWe...\n",
      "Fetching observation data for time-22-04-34-450080_chatcmpl-D5dP0LUKvjLgHY2oB5RQZ0Wm41flt...\n",
      "Fetching observation data for time-22-06-15-374569_chatcmpl-D5dQd6Rp4bFDwBJ6qAzyH0UySDQRy...\n",
      "Fetching observation data for time-21-58-04-961953_chatcmpl-D5dIk1S5UoTGjepkcGndA9rCnxoUJ...\n",
      "Fetching observation data for time-21-59-02-018396_chatcmpl-D5dJfMZpRW22C0GICmsfNQzLo8ToR...\n",
      "Fetching observation data for time-22-00-28-158813_chatcmpl-D5dL2dc66oLHIGUEEjVzP0RtHyT1x...\n",
      "Fetching observation data for time-22-01-36-722197_chatcmpl-D5dM9DnBxgK0yMDwAsngx30KfL8NK...\n",
      "Fetching observation data for time-21-51-48-039811_chatcmpl-D5dCekkXdSQVH8bst7mPZGBuN4kqR...\n",
      "Fetching observation data for time-21-53-02-017396_chatcmpl-D5dDqxprkgcumcyP3Hkb73ofbK1NG...\n",
      "Fetching observation data for time-21-54-16-879406_chatcmpl-D5dF3UJNBjrSQQzftUQKKrNEFzWEF...\n",
      "Fetching observation data for time-21-55-45-268705_chatcmpl-D5dGU20xOroqDzNgy15hY5n4rGDyt...\n",
      "Fetching observation data for time-21-45-27-165126_chatcmpl-D5d6W1GbB7W22nY5UYoEtOkEiDaUc...\n",
      "Fetching observation data for time-21-46-22-459841_chatcmpl-D5d7P56WUplR3iAJFOeusy4sWVYTL...\n",
      "Fetching observation data for time-21-48-53-783944_chatcmpl-D5d9rRCFZKq19aDCaHPXY4qfjm8zq...\n",
      "Raw JSON saved to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/raw_gpt-5-2025-08-07_54c5_sg_batch.json\n",
      "Fetching traces for session gpt-5-2025-08-07_40f1_sg_batch...\n",
      "Fetching observation data for time-02-14-47-317498_chatcmpl-D5hJ908NwcxymEJrFdookxJEvMQC1...\n",
      "Fetching observation data for time-02-16-02-583665_chatcmpl-D5hKNl866JY1h6oNr96yq5GpthxUa...\n",
      "Fetching observation data for time-02-18-05-172626_chatcmpl-D5hMM2qUdDpeHHjureup1Dx9lpwhD...\n",
      "Fetching observation data for time-02-19-20-991326_chatcmpl-D5hNZLG5ioT6pXwanfRj5IDEhvjAg...\n",
      "Fetching observation data for time-02-09-13-294064_chatcmpl-D5hDlogcYDzT38zbyzaqxj2Cp7jQJ...\n",
      "Fetching observation data for time-02-09-54-144441_chatcmpl-D5hEQGEl9Uuv39LL8ATwaSxNip5iB...\n",
      "Fetching observation data for time-02-12-07-593459_chatcmpl-D5hGZBsyvNucprFvq4Ms1U2sdsU5h...\n",
      "Fetching observation data for time-02-03-58-739785_chatcmpl-D5h8gApmfONJmx3f5Y9vlYTK03477...\n",
      "Fetching observation data for time-02-04-47-474218_chatcmpl-D5h9T9dRL88eLxmaqxM6pszKmxGhs...\n",
      "Fetching observation data for time-02-06-52-163496_chatcmpl-D5hBUgbiFD0Pl8rRuuHWsd7tFkhV6...\n",
      "Fetching observation data for time-01-59-13-606519_chatcmpl-D5h45pgJ0LnQ3J78enah1fXXICQkr...\n",
      "Fetching observation data for time-02-00-03-059772_chatcmpl-D5h4th9ni1qMk3Y9skRuSMPF9qdpa...\n",
      "Fetching observation data for time-01-54-33-804599_chatcmpl-D5gzaf3mN93aI14IdGYQzxR2WpdXq...\n",
      "Fetching observation data for time-01-55-31-007812_chatcmpl-D5h0VW7xeXGBIPKcbKWBkXakbyKLK...\n",
      "Fetching observation data for time-01-56-58-067503_chatcmpl-D5h1uV91260UXUzlZYiA49FgI4q0W...\n",
      "Fetching observation data for time-01-47-31-892142_chatcmpl-D5gsm0fFxaworEwv9dtWiPD4UNOyR...\n",
      "Fetching observation data for time-01-48-23-991279_chatcmpl-D5gtcDShu6CUw1cWK9ptdKO4LDp3K...\n",
      "Fetching observation data for time-01-50-39-660246_chatcmpl-D5gvoXrUIbJTQVv1bFeBw0SgZJGSj...\n",
      "Fetching observation data for time-01-51-54-366974_chatcmpl-D5gx0PVmyXo6rcuFCln4auLshhgwn...\n",
      "Fetching observation data for time-01-41-32-921258_chatcmpl-D5gmz3NjnaHCejqv5jpqHU8Rz89dM...\n",
      "Fetching observation data for time-01-42-36-648509_chatcmpl-D5go064DdNSlNzXoXrsJvP8hBnRPP...\n",
      "Fetching observation data for time-01-44-33-393803_chatcmpl-D5gptSphlvkGpy8TWM0If3tl9CON7...\n",
      "Fetching observation data for time-01-34-04-839781_chatcmpl-D5gfl7iGMrJpSVKJoJJ3pve3YfKaR...\n",
      "Fetching observation data for time-01-35-02-651027_chatcmpl-D5gghk8LETqo7mrT8xYD0tAsBLjQ4...\n",
      "Fetching observation data for time-01-37-01-865107_chatcmpl-D5gictUTqdNQdL8BHdZHGabm6Y3PW...\n",
      "Fetching observation data for time-01-38-07-210143_chatcmpl-D5gjgZjygXHsr1wI6uWPAyeDEiDGg...\n",
      "Fetching observation data for time-01-28-28-860381_chatcmpl-D5gaL3Uy1M5aLInpXwbMDBtjy1PVk...\n",
      "Fetching observation data for time-01-29-37-702278_chatcmpl-D5gbRsGogXy1fxSKrvW9ug6klux4J...\n",
      "Fetching observation data for time-01-31-49-585453_chatcmpl-D5gdaI2mc0SkgxoENrFPtzUUbQFBg...\n",
      "Fetching observation data for time-01-21-45-795752_chatcmpl-D5gTq75cGqr3jLeITv719jn23156s...\n",
      "Fetching observation data for time-01-22-57-927564_chatcmpl-D5gV0yFRkYbGrIFb9MLLnQy0vUTOp...\n",
      "Fetching observation data for time-01-26-05-936126_chatcmpl-D5gY2fiSAWHErtgV2WrXVo1uUEb60...\n",
      "Fetching observation data for time-01-14-11-421172_chatcmpl-D5gMV19voem8wtEiUaKu7CE7WkDKo...\n",
      "Fetching observation data for time-01-15-21-609014_chatcmpl-D5gNdyd0Or2cUztCSoHdOU3ettra8...\n",
      "Fetching observation data for time-01-16-43-223712_chatcmpl-D5gOxcYAMMzsuTnSBIpBbM6gJ6IXZ...\n",
      "Fetching observation data for time-01-17-30-278976_chatcmpl-D5gPih9wWpnpxOR8fZEpBDQZ7Wf7a...\n",
      "Fetching observation data for time-01-18-49-239744_chatcmpl-D5gQz7vGPA61VMo0pgmUN4zeJQ6Hm...\n",
      "Fetching observation data for time-01-06-55-910590_chatcmpl-D5gFUjgswOv3QAs5a69QMcs4sREYr...\n",
      "Fetching observation data for time-01-08-03-691521_chatcmpl-D5gGa6fulibTKgVwISyWOQ6VTd37N...\n",
      "Fetching observation data for time-01-10-06-379307_chatcmpl-D5gIYNPOObyFVmXXVF96gDN4f1wZw...\n",
      "Fetching observation data for time-01-11-43-376988_chatcmpl-D5gK7WZlEpDUTA69dZRlowW4ftsJr...\n",
      "Fetching observation data for time-00-59-24-662246_chatcmpl-D5g8D4XnCsk9WORPxD8lbLt3WbdJ7...\n",
      "Fetching observation data for time-01-00-44-641244_chatcmpl-D5g9VvOan4Q0Eomo8GLCK45SPBICf...\n",
      "Fetching observation data for time-01-03-28-856690_chatcmpl-D5gC9b2zWoXS3CAfZoaoFT1kQyZzr...\n",
      "Fetching observation data for time-00-51-05-906207_chatcmpl-D5g0Aq61OKgOYQUk84p4coD5qjwjE...\n",
      "Fetching observation data for time-00-52-47-958947_chatcmpl-D5g1oru53kW97svdzA3w8q6HUi8eq...\n",
      "Fetching observation data for time-00-55-35-546991_chatcmpl-D5g4VREJO3pF6KbTjF2fbshjCCnSy...\n",
      "Fetching observation data for time-00-44-26-209551_chatcmpl-D5fti0sh0baFDt1JeTeL0mvmmPfB5...\n",
      "Fetching observation data for time-00-45-45-891373_chatcmpl-D5fv09FmKUV4kh9pfQ9jzjzBJkSiU...\n",
      "Fetching observation data for time-00-48-25-282398_chatcmpl-D5fxZ9vYZYspOApFsWa9HmWkYM0L5...\n",
      "Fetching observation data for time-00-39-19-012739_chatcmpl-D5folG8fQOo32inHNbeqAIMjeyjH6...\n",
      "Fetching observation data for time-00-40-23-965355_chatcmpl-D5fpo3Sz3w6WxCNlrofVx8yCvqJoh...\n",
      "Fetching observation data for time-00-42-15-829498_chatcmpl-D5frcCcLZMIIlKAWkb5q86LBX3fLx...\n",
      "Fetching observation data for time-00-34-42-259463_chatcmpl-D5fkJ2iadlNZ4TtPXNX33rZwxQKhy...\n",
      "Fetching observation data for time-00-35-35-835639_chatcmpl-D5flAT7A0l9Qq0XP10miauPdboA1l...\n",
      "Fetching observation data for time-00-29-39-319514_chatcmpl-D5ffPLvh8EJGzSpTny0xZeQPWszMI...\n",
      "Fetching observation data for time-00-30-31-635770_chatcmpl-D5fgFbs1C2CsJBR0322Bxo7KJfkUW...\n",
      "Fetching observation data for time-00-32-18-797415_chatcmpl-D5fhz1woX5UvWawAsAxe4ypaPrq7G...\n",
      "Fetching observation data for time-00-22-51-426063_chatcmpl-D5fYq2Imji9htlYfhSFESDj44Bwky...\n",
      "Fetching observation data for time-00-23-54-471717_chatcmpl-D5fZrzIwEaJriAtgvsMGuSeuY9nfd...\n",
      "Fetching observation data for time-00-26-01-004658_chatcmpl-D5fbtFmxDNNbSyJ2J7owFunD6xg13...\n",
      "Fetching observation data for time-00-27-20-347672_chatcmpl-D5fdAfSdYTHZmaHDvBMBTEVofy6uo...\n",
      "Fetching observation data for time-00-16-53-794828_chatcmpl-D5fT43X93muwOwpumjjQaF7zEWPBl...\n",
      "Fetching observation data for time-00-17-57-657463_chatcmpl-D5fU6s9JVAJDQj4xiBEpnGqDqd3Gz...\n",
      "Fetching observation data for time-00-20-22-235126_chatcmpl-D5fWQ7xIEPwLdfqYIVUJmRr1iEKFf...\n",
      "Raw JSON saved to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/raw_gpt-5-2025-08-07_40f1_sg_batch.json\n",
      "Fetching traces for session gpt-5-2025-08-07_40f1_tpusg_batch...\n",
      "Fetching observation data for time-03-35-00-902206_chatcmpl-D5iYnsyAQYKYSU7Srhq0BA6NJFhMJ...\n",
      "Fetching observation data for time-03-32-23-168980_chatcmpl-D5iWFZmnK0W6f1nzfeWY9PQGXQNwU...\n",
      "Fetching observation data for time-03-29-18-471286_chatcmpl-D5iTH9OEfxru4yJ8QwnEz3vrjnioa...\n",
      "Fetching observation data for time-03-21-04-393245_chatcmpl-D5iLIHYjHaKIhPL8mbvjcXfzoBjUx...\n",
      "Fetching observation data for time-03-23-19-537318_chatcmpl-D5iNTpn4RFiKtUVObl8PTPWs7wL5k...\n",
      "Fetching observation data for time-03-24-34-460999_chatcmpl-D5iOgTdteYRqBbIXVBt0mU9pUtVWr...\n",
      "Fetching observation data for time-03-25-55-126866_chatcmpl-D5iPzWC7XbrY8hsKKEnwFJERfAQYM...\n",
      "Fetching observation data for time-03-28-02-132832_chatcmpl-D5iS2POdJUJmtHOX2aoj7cqC5zELT...\n",
      "Fetching observation data for 22b1f348-77d6-4caa-ba59-7a92b4d605da...\n",
      "Fetching observation data for time-03-14-40-766012_chatcmpl-D5iF7KZmFk43kAMk91mq6KuT2Bx8B...\n",
      "Fetching observation data for time-03-16-10-682694_chatcmpl-D5iGZSQLLMNnWhi6K4BJ3C84rdjUf...\n",
      "Fetching observation data for time-03-17-01-523034_chatcmpl-D5iHOHAIwoBA1BblECZrSKTRCobMz...\n",
      "Fetching observation data for time-03-17-59-925917_chatcmpl-D5iIK2q43LEmOHO68eHJ1jKLTIW6l...\n",
      "Fetching observation data for time-03-19-48-260473_chatcmpl-D5iK4rHwSybaUoM1FlxlTmqK0YXAI...\n",
      "Fetching observation data for d28fb659-8dc8-447e-aeb0-b4ffbf2963c1...\n",
      "Fetching observation data for time-03-11-37-928093_chatcmpl-D5iCBGSOc8BXKuPX0pH5mHjLjn3mG...\n",
      "Fetching observation data for time-03-09-05-303488_chatcmpl-D5i9hJVEaoQVF5cc1L3pJxl5MRb9D...\n",
      "Fetching observation data for time-03-06-50-583266_chatcmpl-D5i7X7pyOIClwJJOemsK0XD6HyxQW...\n",
      "Fetching observation data for time-03-02-53-948361_chatcmpl-D5i3i3WkOL6vXxV21YZjLjqIDwpf9...\n",
      "Fetching observation data for time-03-00-21-488004_chatcmpl-D5i1FKQNycJujCXmuJ8gMGuW777pH...\n",
      "Fetching observation data for time-02-57-47-864348_chatcmpl-D5hym22T7xrlFGiaZfNtJukU5mS2K...\n",
      "Fetching observation data for time-02-54-12-985868_chatcmpl-D5hvJlFquF67mmvR5tUjC5MCqfa5R...\n",
      "Fetching observation data for time-02-51-08-206635_chatcmpl-D5hsKUzgMnMmsyYSOePqYkaLGJ1Tw...\n",
      "Fetching observation data for time-02-45-54-469552_chatcmpl-D5hnHHccRsb8QF322QQn8H4baOUBq...\n",
      "Fetching observation data for time-02-48-46-440587_chatcmpl-D5hq3odUn8Nc1HcMXuubRIMEUFESa...\n",
      "Fetching observation data for time-02-42-24-624416_chatcmpl-D5hjt7Ns6fubcCkmJZ3Y3965go9hg...\n",
      "Fetching observation data for time-02-35-21-829761_chatcmpl-D5hd4Z6OhFj6LymUk3f6mfl4LwUZF...\n",
      "Fetching observation data for time-02-37-45-583771_chatcmpl-D5hfNVqq5MCh07OlcexakAOBzSCSW...\n",
      "Fetching observation data for time-02-39-06-722368_chatcmpl-D5hghEcu4KSAduvcgC0VLOdYdMuIX...\n",
      "Fetching observation data for time-02-40-13-098725_chatcmpl-D5hhlK1yLGk3lFrjEWFh0skl4zsWh...\n",
      "Fetching observation data for time-02-31-55-014277_chatcmpl-D5hZj0gmvPMpyzhqoFz7W8lGR3q9W...\n",
      "Fetching observation data for time-02-29-15-221726_chatcmpl-D5hX9Q2Dld0n5kLLr9xoucKiPmXAp...\n",
      "Fetching observation data for time-02-25-44-722151_chatcmpl-D5hTlf4xzgvayP3IxqecrdwPXv19F...\n",
      "Fetching observation data for time-02-21-46-914521_chatcmpl-D5hPvuToH4DghMynfuN9LHfsEC1S0...\n",
      "Raw JSON saved to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/raw_gpt-5-2025-08-07_40f1_tpusg_batch.json\n",
      "Fetching traces for session gpt-5-2025-08-07_40f1_psg_batch...\n",
      "Fetching observation data for time-04-42-19-984368_chatcmpl-D5jbwOk4HYHvC8a9mTgS7jZEcfoEH...\n",
      "Fetching observation data for time-04-40-04-183827_chatcmpl-D5jZkPDT4D9fTzlPgoo7SRokgnMoi...\n",
      "Fetching observation data for time-04-37-26-378605_chatcmpl-D5jXCmeXxXRh45h4JbbW3IvPZgFrw...\n",
      "Fetching observation data for time-04-28-39-774022_chatcmpl-D5jOiA2gVsIiILLxkqOXalEaD7nfI...\n",
      "Fetching observation data for time-04-31-56-121752_chatcmpl-D5jRs5TTVX1RHLK5738DyR66fdV9T...\n",
      "Fetching observation data for time-04-33-12-528612_chatcmpl-D5jT6SoimeoMr5oxVSnirqcY6iQTl...\n",
      "Fetching observation data for time-04-34-32-983832_chatcmpl-D5jUPf1SjlGVwxdfbqJBFWRxToN5h...\n",
      "Fetching observation data for time-04-35-56-360477_chatcmpl-D5jVkfSL0HyYwLZcXIY4wKZBW3pQr...\n",
      "Fetching observation data for a1c5c464-ab33-4343-8c89-aafcafa97e49...\n",
      "Fetching observation data for time-04-25-32-982126_chatcmpl-D5jLhmGFHa7rX9BhdPVlBncljUloe...\n",
      "Fetching observation data for time-04-21-22-236150_chatcmpl-D5jHeewaipLywU02jTxsLpzNnSEuq...\n",
      "Fetching observation data for time-04-18-38-606798_chatcmpl-D5jF1q8rIZbmgphTokyz7FMi48fMg...\n",
      "Fetching observation data for time-04-16-00-904357_chatcmpl-D5jCTpwCYIg9AJ64G0t3g08hTTW56...\n",
      "Fetching observation data for time-04-11-54-173665_chatcmpl-D5j8ULheKFSbnrm0KTe7km4POXHpQ...\n",
      "Fetching observation data for time-04-08-57-390580_chatcmpl-D5j5dM88TN3GU2Z1cTuYvcWF6R4t5...\n",
      "Fetching observation data for time-04-06-16-615760_chatcmpl-D5j328xg8OKwiRTMoLVVTXB7V1qCi...\n",
      "Fetching observation data for time-04-03-53-785960_chatcmpl-D5j0kYaW03aQrOxw9TYfnBGRRhXfo...\n",
      "Fetching observation data for time-04-00-23-005104_chatcmpl-D5ixLjHcnUMWrLE1Hni03q4ukyqLu...\n",
      "Fetching observation data for time-03-56-25-314518_chatcmpl-D5itVJ4fht9Vj0LEnXzpyGQGdcW07...\n",
      "Fetching observation data for time-03-53-11-527701_chatcmpl-D5iqNmLh0RIbPlPXvhPgdKJViqSEG...\n",
      "Fetching observation data for time-03-50-32-635415_chatcmpl-D5inoF8cxMvgmqGpSe6eTQtY4voJZ...\n",
      "Fetching observation data for time-03-47-26-850401_chatcmpl-D5ikpNMiLXAJXPpZ6XhtZ2Y1tzNCT...\n",
      "Fetching observation data for time-03-43-19-002293_chatcmpl-D5igp5ALVLwlsiyxHNXohyabemN7I...\n",
      "Fetching observation data for time-03-45-05-161078_chatcmpl-D5iiXD4FcxtxCJu2N2dC0YBzW0vXp...\n",
      "Fetching observation data for time-03-40-35-815355_chatcmpl-D5ieCLUEfKFLNe1qKTML8JSNWa5uG...\n",
      "Fetching observation data for time-03-37-34-030522_chatcmpl-D5ibGpxSsZG1cpdyI72sGpYDr0yLF...\n",
      "Raw JSON saved to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/raw_gpt-5-2025-08-07_40f1_psg_batch.json\n"
     ]
    }
   ],
   "source": [
    "# ALTERNATIVE TO 2.\n",
    "import os\n",
    "import json\n",
    "from time import sleep\n",
    "from langfuse import Langfuse\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LANGFUSE_SERVICE_PUBLIC_KEY = \"pk-lf-559a2c0f-ee29-4c32-944c-bf73b5f0ce28\"\n",
    "# LANGFUSE_SERVICE_SECRET_KEY = \"sk-lf-75f8bf7f-a5db-4756-b0dd-d758a2a292c8\"\n",
    "# LANGFUSE_SERVICE_HOST = \"https://langfuse.hann.fi\"\n",
    "\n",
    "\n",
    "if LOCAL_HOST:\n",
    "    langfuse = Langfuse(\n",
    "        secret_key=\"sk-lf-75f8bf7f-a5db-4756-b0dd-d758a2a292c8\",\n",
    "        public_key=\"pk-lf-559a2c0f-ee29-4c32-944c-bf73b5f0ce28\",\n",
    "        host=\"https://langfuse.hann.fi\",\n",
    "    )\n",
    "else:\n",
    "    langfuse = Langfuse(\n",
    "        secret_key=LANGFUSE_SERVICE_SECRET_KEY,\n",
    "        public_key=LANGFUSE_SERVICE_PUBLIC_KEY,\n",
    "        host=LANGFUSE_SERVICE_HOST,\n",
    "    )\n",
    "\n",
    "API_invok_count = 0\n",
    "query_range_num_run = {\"start\": 0, \"end\": 1}\n",
    "\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def __init__(self, *args, LOCAL_HOST=True, **kwargs):\n",
    "        self.LOCAL_HOST = LOCAL_HOST\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        if hasattr(obj, \"__dict__\"):\n",
    "            data = obj.__dict__.copy()\n",
    "            if \"observations\" in data:\n",
    "                data[\"observations\"] = [\n",
    "                    fetch_observation_data(obs, self.LOCAL_HOST)\n",
    "                    for obs in data[\"observations\"]\n",
    "                ]\n",
    "\n",
    "            return data\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "def fetch_observation_data(observation_id, LOCAL_HOST):\n",
    "    \"\"\"\n",
    "    Fetches observation data from Langfuse and returns its dictionary representation.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching observation data for {observation_id}...\")\n",
    "    global API_invok_count\n",
    "    if API_invok_count >= 0 and not LOCAL_HOST:\n",
    "        print(\"Waiting for 3 seconds to fetch observation data...\")\n",
    "        for _ in tqdm(range(3), desc=\"Progress\", unit=\"s\"):\n",
    "            sleep(1)\n",
    "        API_invok_count = 0\n",
    "\n",
    "    observation_response = langfuse.fetch_observation(observation_id)\n",
    "    API_invok_count += 1\n",
    "\n",
    "    return observation_response.data.dict()\n",
    "\n",
    "\n",
    "def fetch_and_save_complete_data(session_id_list, raw_export_dir, LOCAL_HOST):\n",
    "    \"\"\"\n",
    "    Fetches complete trace data for each session ID and saves it to JSON files.\n",
    "\n",
    "    Parameters:\n",
    "        session_id_list (list): List of session IDs to process.\n",
    "        raw_export_dir (str): Directory path to save raw JSON files.\n",
    "    \"\"\"\n",
    "\n",
    "    def save_complete_data(session_id):\n",
    "        global API_invok_count\n",
    "        if API_invok_count >= 0 and not LOCAL_HOST:\n",
    "            print(\"Waiting for 4 seconds to fetch traces...\")\n",
    "            for _ in tqdm(range(4), desc=\"Progress\", unit=\"s\"):\n",
    "                sleep(1)\n",
    "            API_invok_count = 0\n",
    "\n",
    "        fetch_traces_response = langfuse.fetch_traces(session_id=session_id)\n",
    "        API_invok_count += 1\n",
    "\n",
    "        print(f\"Fetching traces for session {session_id}...\")\n",
    "        # Create directories if they don't exist\n",
    "        os.makedirs(raw_export_dir, exist_ok=True)\n",
    "\n",
    "        # Save complete data to JSON file\n",
    "        # if session_id.startswith(\"da0a\"):\n",
    "        #     session_id = \"phi4_\" + session_id\n",
    "        if \"tpsg\" in session_id:\n",
    "            session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "        else:\n",
    "            session_id_ = session_id\n",
    "            \n",
    "        raw_path = os.path.join(raw_export_dir, f\"raw_{session_id_}.json\")\n",
    "        with open(raw_path, \"w\") as f:\n",
    "            json.dump(fetch_traces_response, f, cls=CustomJSONEncoder, indent=2)\n",
    "\n",
    "        print(f\"Raw JSON saved to: {raw_path}\")\n",
    "\n",
    "    for session_id in session_id_list:\n",
    "        save_complete_data(session_id)\n",
    "\n",
    "\n",
    "fetch_and_save_complete_data(session_id_list, raw_export_dir, LOCAL_HOST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Trim data\n",
    "\n",
    "Here also intercept the runs with fatal errors that need to be excluded from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPAN error_f8_tpusg_failure_signal_tpu_sketch_generator: Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n",
      "  File \"script_70665060_1770240284.py\", line 180, in <module>\n",
      "    resized_rgb = cv2.resize(frame_rgb, (model_width, model_height))\n",
      "TypeError: an integer is required (got type str).\n",
      "Successfully processed and saved trimmed data for session gpt-5-2025-08-07_54c5_tpusg_batch\n",
      "Successfully processed and saved trimmed data for session gpt-5-2025-08-07_54c5_psg_batch\n",
      "Successfully processed and saved trimmed data for session gpt-5-2025-08-07_54c5_sg_batch\n",
      "Successfully processed and saved trimmed data for session gpt-5-2025-08-07_40f1_sg_batch\n",
      "SPAN error_44_tpusg_failure_signal_tpu_sketch_generator: Failed: Max retries reached with error. Last error: [INFO] EdgeTPU delegate loaded via 'libedgetpu.so.1.0'.\n",
      "[INFO] Starting processing on backend 'tflite_runtime'. EdgeTPU active: True\n",
      "[INFO] Input video: /home/mendel/tinyml_autopilot/data//sheeps.mp4\n",
      "[INFO] Output video: /home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\n",
      "[INFO] Confidence threshold: 0.5\n",
      "Traceback (most recent call last):\n",
      "  File \"script_841bee79_1770254938.py\", line 288, in <module>\n",
      "    frame_bgr, model_in_w, model_in_h, floating_model, input_dtype\n",
      "  File \"script_841bee79_1770254938.py\", line 174, in preprocess_frame_bgr_to_input\n",
      "    resized = cv2.resize(frame_rgb, (int(target_w), int(target_h)))\n",
      "ValueError: invalid literal for int() with base 10: 'gpt-5-2025-08-07'.\n",
      "SPAN error_40_tpusg_failure_signal_tpu_sketch_generator: Failed: Max retries reached with error. Last error: INFO: EdgeTPU delegate loaded via 'libedgetpu.so.1.0'.\n",
      "Traceback (most recent call last):\n",
      "  File \"script_601e8243_1770254445.py\", line 219, in <module>\n",
      "    input_data = preprocess_frame(frame)\n",
      "  File \"script_601e8243_1770254445.py\", line 185, in preprocess_frame\n",
      "    resized = cv2.resize(frame_bgr, (model_w, model_h))\n",
      "TypeError: an integer is required (got type str).\n",
      "Successfully processed and saved trimmed data for session gpt-5-2025-08-07_40f1_tpusg_batch\n",
      "SPAN error_db_psg_failure_signal_py_sketch_generator: Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043714_psg_gpt-5-2025-08-07/tmp_20260205043714_psg_gpt-5-2025-08-07.py\", line 374, in <module>\n",
      "    input_data = preprocess_frame(frame)\n",
      "  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043714_psg_gpt-5-2025-08-07/tmp_20260205043714_psg_gpt-5-2025-08-07.py\", line 182, in preprocess_frame\n",
      "    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n",
      "cv2.error: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'resize'\n",
      "> Overload resolution failed:\n",
      ">  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n",
      ">  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n",
      "\n",
      "\n",
      "Successfully processed and saved trimmed data for session gpt-5-2025-08-07_40f1_psg_batch\n",
      "Total 0 traces skipped. They are []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "skipped_traces = []\n",
    "\n",
    "\n",
    "def process_existing_observation(observation):\n",
    "    \"\"\"\n",
    "    Processes an existing observation dictionary by trimming unwanted keys.\n",
    "    \"\"\"\n",
    "    unwanted_observation_keys = [\n",
    "        \"completionStartTime\",\n",
    "        \"metadata\",\n",
    "        \"timeToFirstToken\",\n",
    "        \"createdAt\",\n",
    "        \"usageDetails\",\n",
    "        \"usage\",\n",
    "        \"projectId\",\n",
    "        \"unit\",\n",
    "        \"updatedAt\",\n",
    "        \"version\",\n",
    "        \"parentObservationId\",\n",
    "        \"promptId\",\n",
    "        \"promptName\",\n",
    "        \"promptVersion\",\n",
    "        \"modelId\",\n",
    "        \"inputPrice\",\n",
    "        \"outputPrice\",\n",
    "        \"totalPrice\",\n",
    "        # \"modelParameters\",\n",
    "        \"input\",\n",
    "        \"output\",\n",
    "    ]\n",
    "\n",
    "    # If observation is a dictionary containing observation data\n",
    "    if isinstance(observation, dict):\n",
    "        trimmed_observation = {\n",
    "            k: v for k, v in observation.items() if k not in unwanted_observation_keys\n",
    "        }\n",
    "        return trimmed_observation\n",
    "    return observation\n",
    "\n",
    "\n",
    "def trim_data(data):\n",
    "    \"\"\"\n",
    "    Recursively trims the data structure.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        # Process the current dictionary\n",
    "        unwanted_trace_keys = [\n",
    "            \"release\",\n",
    "            \"version\",\n",
    "            \"user_id\",\n",
    "            \"public\",\n",
    "            \"html_path\",\n",
    "            \"scores\",\n",
    "            \"bookmarked\",\n",
    "            \"projectId\",\n",
    "            \"externalId\",\n",
    "            \"page\",\n",
    "            \"limit\",\n",
    "            \"total_pages\",\n",
    "        ]\n",
    "\n",
    "        # If this is a trace that contains observations, check for fatal errors\n",
    "        if \"observations\" in data:\n",
    "            # Check for SPAN observations with fatal errors before processing\n",
    "            skip_trace = False\n",
    "            for obs in data[\"observations\"]:\n",
    "                if isinstance(obs, dict) and obs.get(\"name\").startswith(\"error\"):\n",
    "                    status_message = obs.get(\"statusMessage\", \"\")\n",
    "                    ob_name = obs.get(\"name\")\n",
    "                    print(f\"SPAN {ob_name}: {status_message}\")\n",
    "\n",
    "                    if \"Fatal error\" in status_message:\n",
    "                        print(f\"Found Fatal error in SPAN observation, skipping trace\")\n",
    "                        skip_trace = True\n",
    "                        skipped_traces.append(data[\"name\"])\n",
    "                        break\n",
    "\n",
    "            if skip_trace:\n",
    "                return None  # Signal to skip this trace\n",
    "\n",
    "        # Create a new dictionary with wanted keys and recursively process values\n",
    "        trimmed_data = {}\n",
    "        for key, value in data.items():\n",
    "            if key not in unwanted_trace_keys:\n",
    "                if key == \"observations\":\n",
    "                    # Special handling for observations\n",
    "                    trimmed_data[key] = [\n",
    "                        process_existing_observation(obs) for obs in value\n",
    "                    ]\n",
    "                elif isinstance(value, (dict, list)):\n",
    "                    # Recursively process nested structures\n",
    "                    trimmed_data[key] = trim_data(value)\n",
    "                else:\n",
    "                    trimmed_data[key] = value\n",
    "\n",
    "        return trimmed_data\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        # Recursively process each item in the list\n",
    "        processed_items = []\n",
    "        for item in data:\n",
    "            processed_item = trim_data(item)\n",
    "            if processed_item is not None:  # Only add items that weren't filtered out\n",
    "                processed_items.append(processed_item)\n",
    "        return processed_items\n",
    "\n",
    "    else:\n",
    "        # Return non-dict, non-list values as is\n",
    "        return data\n",
    "\n",
    "\n",
    "def read_and_trim_data(session_id_list, raw_export_dir, trimmed_export_dir):\n",
    "    \"\"\"\n",
    "    Reads complete data from JSON files, trims the data, and saves the trimmed data to new JSON files.\n",
    "    \"\"\"\n",
    "    os.makedirs(trimmed_export_dir, exist_ok=True)\n",
    "\n",
    "    for session_id in session_id_list:\n",
    "        try:\n",
    "            if session_id.startswith(\"da0a\"):\n",
    "                session_id = \"phi4_\" + session_id\n",
    "            # Read raw data\n",
    "            if \"tpsg\" in session_id:\n",
    "                session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "            else:\n",
    "                session_id_ = session_id\n",
    "            raw_path = os.path.join(raw_export_dir, f\"raw_{session_id_}.json\")\n",
    "            with open(raw_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            # Process and trim the data\n",
    "            trimmed_data = trim_data(data)\n",
    "\n",
    "            # If the entire data was filtered out (unlikely but possible)\n",
    "            if trimmed_data is None:\n",
    "                print(\n",
    "                    f\"All traces in session {session_id} were filtered due to fatal errors\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Save trimmed data\n",
    "            if \"tpsg\" in session_id:\n",
    "                session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "            else:\n",
    "                session_id_ = session_id\n",
    "            trimmed_path = os.path.join(\n",
    "                trimmed_export_dir, f\"trimmed_{session_id_}.json\"\n",
    "            )\n",
    "            with open(trimmed_path, \"w\") as f:\n",
    "                json.dump(trimmed_data, f, indent=2)\n",
    "\n",
    "            print(\n",
    "                f\"Successfully processed and saved trimmed data for session {session_id}\"\n",
    "            )\n",
    "\n",
    "            # Optional: Verify trimming worked\n",
    "            # print(f\"Verifying trimmed data for session {session_id}...\")\n",
    "            # verify_trimming(trimmed_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing session {session_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "def verify_trimming(trimmed_path):\n",
    "    \"\"\"\n",
    "    Verifies that the trimmed data doesn't contain unwanted keys.\n",
    "    \"\"\"\n",
    "    with open(trimmed_path, \"r\") as f:\n",
    "        trimmed_data = json.load(f)\n",
    "\n",
    "    unwanted_keys = [\n",
    "        \"release\",\n",
    "        \"version\",\n",
    "        \"user_id\",\n",
    "        \"public\",\n",
    "        \"html_path\",\n",
    "        \"scores\",\n",
    "        \"bookmarked\",\n",
    "        \"projectId\",\n",
    "        \"externalId\",\n",
    "        \"page\",\n",
    "        \"limit\",\n",
    "        \"total_pages\",\n",
    "        \"completionStartTime\",\n",
    "        \"metadata\",\n",
    "        \"usageDetails\",\n",
    "        \"timeToFirstToken\",\n",
    "        \"createdAt\",\n",
    "        \"completionTokens\",\n",
    "        \"promptTokens\",\n",
    "        \"projectId\",\n",
    "        \"unit\",\n",
    "        \"updatedAt\",\n",
    "        \"version\",\n",
    "        # \"statusMessage\",\n",
    "        \"parentObservationId\",\n",
    "        \"promptId\",\n",
    "        \"promptName\",\n",
    "        \"promptVersion\",\n",
    "        \"modelId\",\n",
    "        \"inputPrice\",\n",
    "        \"outputPrice\",\n",
    "        \"totalPrice\",\n",
    "        \"calculatedInputCost\",\n",
    "        \"calculatedOutputCost\",\n",
    "        \"calculatedTotalCost\",\n",
    "    ]\n",
    "\n",
    "    def check_keys(obj):\n",
    "        if isinstance(obj, dict):\n",
    "            for key in obj.keys():\n",
    "                if key in unwanted_keys:\n",
    "                    print(f\"Warning: Found unwanted key '{key}' in trimmed data\")\n",
    "            for value in obj.values():\n",
    "                check_keys(value)\n",
    "        elif isinstance(obj, list):\n",
    "            for item in obj:\n",
    "                check_keys(item)\n",
    "\n",
    "    check_keys(trimmed_data)\n",
    "    print(\"Verification complete\")\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "read_and_trim_data(session_id_list, raw_export_dir, raw_export_dir)\n",
    "print(f\"Total {len(skipped_traces)} traces skipped. They are {skipped_traces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate CSV files from JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing session gpt-5-2025-08-07_54c5_tpusg_batch, simple id gpt-5-2025-08-07_54c5. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_54c5_tpusg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_tpusg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_tpusg_batch.csv\n",
      "Processing session gpt-5-2025-08-07_54c5_psg_batch, simple id gpt-5-2025-08-07_54c5. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_54c5_psg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_psg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_psg_batch.csv\n",
      "Processing session gpt-5-2025-08-07_54c5_sg_batch, simple id gpt-5-2025-08-07_54c5. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_54c5_sg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_sg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_sg_batch.csv\n",
      "Processing session gpt-5-2025-08-07_40f1_sg_batch, simple id gpt-5-2025-08-07_40f1. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_40f1_sg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_sg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_sg_batch.csv\n",
      "Processing session gpt-5-2025-08-07_40f1_tpusg_batch, simple id gpt-5-2025-08-07_40f1. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_40f1_tpusg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_tpusg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_tpusg_batch.csv\n",
      "Processing session gpt-5-2025-08-07_40f1_psg_batch, simple id gpt-5-2025-08-07_40f1. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_40f1_psg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_psg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_psg_batch.csv\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "\n",
    "def json_to_csv(session_id):\n",
    "    \"\"\"\n",
    "    Convert JSON trace data to CSV format with aggregated metrics.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Identifier for the session to process\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_observation_details(observations, trace_id):\n",
    "        \"\"\"Extract and aggregate metrics from observations\"\"\"\n",
    "        metrics = {\n",
    "            \"status\": None,\n",
    "            \"latency\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"prompt_tokens\": 0,\n",
    "            \"completion_tokens\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"input_cost\": 0,\n",
    "            \"output_cost\": 0,\n",
    "            \"parameters\": set(),\n",
    "        }\n",
    "\n",
    "        # Process GENERATION observations\n",
    "        for obs in (o for o in observations if o[\"type\"] == \"GENERATION\"):\n",
    "            metrics[\"total_tokens\"] += obs[\"totalTokens\"]\n",
    "            metrics[\"prompt_tokens\"] += obs[\"promptTokens\"]\n",
    "            metrics[\"completion_tokens\"] += obs[\"completionTokens\"]\n",
    "            metrics[\"latency\"] += obs[\"latency\"]\n",
    "            for key, value in obs[\"modelParameters\"].items():\n",
    "                metrics[\"parameters\"].add(key + \":\" + value)\n",
    "\n",
    "            # Add costs if present\n",
    "            for cost_type in [\"Total\", \"Input\", \"Output\"]:\n",
    "                key = f\"calculated{cost_type}Cost\"\n",
    "                metric_key = cost_type.lower() + \"_cost\"\n",
    "                if obs.get(key) is not None:\n",
    "                    metrics[metric_key] += obs[key]\n",
    "        if len(metrics[\"parameters\"]) == 0:\n",
    "            metrics[\"parameters\"] = \"N/A\"\n",
    "        # Process SPAN observations for status\n",
    "        status_indicators = [\n",
    "            obs[\"name\"]\n",
    "            for obs in observations\n",
    "            if obs[\"type\"] == \"SPAN\" and \"start_\" not in obs[\"name\"]\n",
    "        ]\n",
    "\n",
    "        #  if later than 2025-05-19, use status_signal_from_output\n",
    "        if datetime.now() > datetime(2025, 5, 19):\n",
    "            pass\n",
    "        else:\n",
    "            # Determine status\n",
    "            success_signals = sum(\"end_\" in name for name in status_indicators)\n",
    "            failure_signals = sum(\n",
    "                \"failure_signal\" in name for name in status_indicators\n",
    "            )\n",
    "\n",
    "            if success_signals + failure_signals > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple status indicators found in trace {trace_id}\"\n",
    "                )\n",
    "\n",
    "            metrics[\"status\"] = (\n",
    "                \"success\"\n",
    "                if success_signals\n",
    "                else \"failure\" if failure_signals else \"unknown\"\n",
    "            )\n",
    "\n",
    "        metrics[\"prompt_cost\"] = metrics.pop(\"input_cost\")\n",
    "        metrics[\"completion_cost\"] = metrics.pop(\"output_cost\")\n",
    "        metrics[\"latency\"] = round(metrics[\"latency\"] / 1000, 2)\n",
    "        return metrics\n",
    "\n",
    "    def cal_time(trace):\n",
    "        time_diff = datetime.fromisoformat(\n",
    "            trace[\"updatedAt\"].replace(\"Z\", \"+00:00\")\n",
    "        ) - datetime.fromisoformat(trace[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n",
    "        seconds_diff = time_diff.total_seconds()\n",
    "        return seconds_diff\n",
    "\n",
    "    try:\n",
    "\n",
    "        if session_id.startswith(\"da0a\"):\n",
    "            session_id = \"phi4_\" + session_id\n",
    "        simple_session_id = session_id.rsplit(\"_\", 2)[0]\n",
    "\n",
    "        \n",
    "        # Load JSON data\n",
    "        if \"tpsg\" in session_id:\n",
    "                session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "        else:\n",
    "                session_id_ = session_id\n",
    "        trimmed_path = os.path.join(raw_export_dir, f\"trimmed_{session_id_}.json\")\n",
    "        print(\n",
    "            f\"Processing session {session_id}, simple id {simple_session_id}. Look for {trimmed_path}\"\n",
    "        )\n",
    "        with open(trimmed_path, \"r\") as file:\n",
    "            traces = json.load(file)[\"data\"]\n",
    "\n",
    "        # Process traces\n",
    "        rows = [\n",
    "            {\n",
    "                \"num_run\": trace[\"metadata\"][\"num_run\"],\n",
    "                \"name\": trace[\"name\"],\n",
    "                \"trace_id\": trace[\"id\"],\n",
    "                \"batch_id\": trace[\"session_id\"],\n",
    "                # \"latency\": cal_time(trace),\n",
    "                # \"latency\": round(trace[\"latency\"], 2),\n",
    "                **extract_observation_details(\n",
    "                    trace[\"observations\"],\n",
    "                    trace[\"id\"],\n",
    "                ),\n",
    "                \"status\": (\n",
    "                    \"failure\"\n",
    "                    if trace[\"output\"][\"status\"].lower() == \"failed\"\n",
    "                    else \"success\"\n",
    "                ),\n",
    "                \"tags\": trace[\"tags\"],\n",
    "                \"timestamp\": int(parser.isoparse(trace[\"timestamp\"]).timestamp()),\n",
    "            }\n",
    "            for trace in traces\n",
    "        ]\n",
    "        # print(rows)\n",
    "        # print(rows)\n",
    "        # Create and save DataFrame\n",
    "        df = pd.DataFrame(rows).sort_values(\"num_run\")\n",
    "\n",
    "        output_dir = os.path.join(processed_data_dir, f\"{simple_session_id}\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"clean_{session_id_}.csv\")\n",
    "\n",
    "        print(output_path)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved CSV to: {output_path}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"FileNotFoundError: For session {session_id} not found. Looked for {trimmed_path}\\nError info: \\n{e}\\n\\nTraceback: {traceback.format_exc()}\"\n",
    "        )\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in input file for session {session_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {session_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "for session_id in session_id_list:\n",
    "    json_to_csv(session_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV Generation with Generation Counts\n",
    "\n",
    "This section creates CSV files similar to the langfuse_export section 3, but adds a column for the number of generation attempts used for each trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sessions: ['gpt-5-2025-08-07_40f1_tpusg_batch', 'gpt-5-2025-08-07_54c5_sg_batch', 'gpt-5-2025-08-07_40f1_psg_batch', 'gpt-5-2025-08-07_40f1_sg_batch', 'gpt-5-2025-08-07_54c5_tpusg_batch', 'gpt-5-2025-08-07_54c5_psg_batch']\n",
      "Looking for raw files in: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export\n",
      "Will save CSV files to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data\n",
      "Processing session gpt-5-2025-08-07_40f1_tpusg_batch, simple id gpt-5-2025-08-07_40f1. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_40f1_tpusg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_tpusg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_tpusg_batch.csv\n",
      "Processing session gpt-5-2025-08-07_54c5_sg_batch, simple id gpt-5-2025-08-07_54c5. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_54c5_sg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_sg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_sg_batch.csv\n",
      "Processing session gpt-5-2025-08-07_40f1_psg_batch, simple id gpt-5-2025-08-07_40f1. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_40f1_psg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_psg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_psg_batch.csv\n",
      "Processing session gpt-5-2025-08-07_40f1_sg_batch, simple id gpt-5-2025-08-07_40f1. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_40f1_sg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_sg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_40f1/clean_gpt-5-2025-08-07_40f1_sg_batch.csv\n",
      "Processing session gpt-5-2025-08-07_54c5_tpusg_batch, simple id gpt-5-2025-08-07_54c5. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_54c5_tpusg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_tpusg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_tpusg_batch.csv\n",
      "Processing session gpt-5-2025-08-07_54c5_psg_batch, simple id gpt-5-2025-08-07_54c5. Look for /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/raw_export/trimmed_gpt-5-2025-08-07_54c5_psg_batch.json\n",
      "/home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_psg_batch.csv\n",
      "Successfully saved CSV to: /home/han/Projects/reference-benchmark-tinyml_llm/langfuse_export/2026/02.03/processed_data/gpt-5-2025-08-07_54c5/clean_gpt-5-2025-08-07_54c5_psg_batch.csv\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "\n",
    "# Setup paths - same as langfuse_export\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "raw_export_dir = os.path.join(parent_dir, \"raw_export\")\n",
    "processed_data_dir = os.path.join(parent_dir, \"processed_data\")\n",
    "\n",
    "\n",
    "# Get session id list from data directory\n",
    "session_id_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(raw_export_dir):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        if \"trimmed_\" in file_path:\n",
    "            session_id = file_path.split('trimmed_')[1].rstrip('.json')\n",
    "            session_id_list.append(session_id)\n",
    "\n",
    "print(f\"Processing sessions: {session_id_list}\")\n",
    "print(f\"Looking for raw files in: {raw_export_dir}\")\n",
    "print(f\"Will save CSV files to: {processed_data_dir}\")\n",
    "\n",
    "\n",
    "def json_to_csv_weighted(session_id):\n",
    "    \"\"\"\n",
    "    Convert JSON trace data to CSV format with aggregated metrics.\n",
    "    Upgraded version that includes generation_count column.\n",
    "\n",
    "    Args:\n",
    "        session_id (str): Identifier for the session to process\n",
    "    \"\"\"\n",
    "\n",
    "    def extract_observation_details(observations, trace_id):\n",
    "        \"\"\"Extract and aggregate metrics from observations\"\"\"\n",
    "        metrics = {\n",
    "            \"status\": None,\n",
    "            \"latency\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"prompt_tokens\": 0,\n",
    "            \"completion_tokens\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"input_cost\": 0,\n",
    "            \"output_cost\": 0,\n",
    "            \"parameters\": set(),\n",
    "            \"generation_count\": 0,  # New field for generation count\n",
    "        }\n",
    "\n",
    "        # Count generations and process GENERATION observations\n",
    "        for obs in (o for o in observations if o[\"type\"] == \"GENERATION\"):\n",
    "            metrics[\"generation_count\"] += 1\n",
    "            metrics[\"total_tokens\"] += obs[\"totalTokens\"]\n",
    "            metrics[\"prompt_tokens\"] += obs[\"promptTokens\"]\n",
    "            metrics[\"completion_tokens\"] += obs[\"completionTokens\"]\n",
    "            metrics[\"latency\"] += obs[\"latency\"]\n",
    "            for key, value in obs[\"modelParameters\"].items():\n",
    "                metrics[\"parameters\"].add(key + \":\" + value)\n",
    "\n",
    "            # Add costs if present\n",
    "            for cost_type in [\"Total\", \"Input\", \"Output\"]:\n",
    "                key = f\"calculated{cost_type}Cost\"\n",
    "                metric_key = cost_type.lower() + \"_cost\"\n",
    "                if obs.get(key) is not None:\n",
    "                    metrics[metric_key] += obs[key]\n",
    "                    \n",
    "        if len(metrics[\"parameters\"]) == 0:\n",
    "            metrics[\"parameters\"] = \"N/A\"\n",
    "            \n",
    "        # Process SPAN observations for status\n",
    "        status_indicators = [\n",
    "            obs[\"name\"]\n",
    "            for obs in observations\n",
    "            if obs[\"type\"] == \"SPAN\" and \"start_\" not in obs[\"name\"]\n",
    "        ]\n",
    "\n",
    "        #  if later than 2025-05-19, use status_signal_from_output\n",
    "        if datetime.now() > datetime(2025, 5, 19):\n",
    "            pass\n",
    "        else:\n",
    "            # Determine status\n",
    "            success_signals = sum(\"end_\" in name for name in status_indicators)\n",
    "            failure_signals = sum(\n",
    "                \"failure_signal\" in name for name in status_indicators\n",
    "            )\n",
    "\n",
    "            if success_signals + failure_signals > 1:\n",
    "                raise ValueError(\n",
    "                    f\"Multiple status indicators found in trace {trace_id}\"\n",
    "                )\n",
    "\n",
    "            metrics[\"status\"] = (\n",
    "                \"success\"\n",
    "                if success_signals\n",
    "                else \"failure\" if failure_signals else \"unknown\"\n",
    "            )\n",
    "\n",
    "        metrics[\"prompt_cost\"] = metrics.pop(\"input_cost\")\n",
    "        metrics[\"completion_cost\"] = metrics.pop(\"output_cost\")\n",
    "        metrics[\"latency\"] = round(metrics[\"latency\"] / 1000, 2)\n",
    "        return metrics\n",
    "\n",
    "    def cal_time(trace):\n",
    "        time_diff = datetime.fromisoformat(\n",
    "            trace[\"updatedAt\"].replace(\"Z\", \"+00:00\")\n",
    "        ) - datetime.fromisoformat(trace[\"createdAt\"].replace(\"Z\", \"+00:00\"))\n",
    "        seconds_diff = time_diff.total_seconds()\n",
    "        return seconds_diff\n",
    "\n",
    "    try:\n",
    "        if session_id.startswith(\"da0a\"):\n",
    "            session_id = \"phi4_\" + session_id\n",
    "        simple_session_id = session_id.rsplit(\"_\", 2)[0]\n",
    "\n",
    "        # Load JSON data\n",
    "        if \"tpsg\" in session_id:\n",
    "            session_id_ = session_id.replace(\"tpsg\", \"tpusg\")\n",
    "        else:\n",
    "            session_id_ = session_id\n",
    "        trimmed_path = os.path.join(raw_export_dir, f\"trimmed_{session_id_}.json\")\n",
    "        print(\n",
    "            f\"Processing session {session_id}, simple id {simple_session_id}. Look for {trimmed_path}\"\n",
    "        )\n",
    "        with open(trimmed_path, \"r\") as file:\n",
    "            traces = json.load(file)[\"data\"]\n",
    "\n",
    "        # Process traces\n",
    "        rows = [\n",
    "            {\n",
    "                \"num_run\": trace[\"metadata\"][\"num_run\"],\n",
    "                \"name\": trace[\"name\"],\n",
    "                \"trace_id\": trace[\"id\"],\n",
    "                \"batch_id\": trace[\"session_id\"],\n",
    "                # \"latency\": cal_time(trace),\n",
    "                # \"latency\": round(trace[\"latency\"], 2),\n",
    "                **extract_observation_details(\n",
    "                    trace[\"observations\"],\n",
    "                    trace[\"id\"],\n",
    "                ),\n",
    "                \"status\": (\n",
    "                    \"failure\"\n",
    "                    if trace[\"output\"][\"status\"].lower() == \"failed\"\n",
    "                    else \"success\"\n",
    "                ),\n",
    "                \"tags\": trace[\"tags\"],\n",
    "                \"timestamp\": int(parser.isoparse(trace[\"timestamp\"]).timestamp()),\n",
    "            }\n",
    "            for trace in traces\n",
    "        ]\n",
    "        \n",
    "        # Create and save DataFrame\n",
    "        df = pd.DataFrame(rows).sort_values(\"num_run\")\n",
    "\n",
    "        output_dir = os.path.join(processed_data_dir, f\"{simple_session_id}\")\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        output_path = os.path.join(output_dir, f\"clean_{session_id_}.csv\")\n",
    "\n",
    "        print(output_path)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved CSV to: {output_path}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(\n",
    "            f\"FileNotFoundError: For session {session_id} not found. Looked for {trimmed_path}\\nError info: \\n{e}\\n\\nTraceback: {traceback.format_exc()}\"\n",
    "        )\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in input file for session {session_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing session {session_id}: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "for session_id in session_id_list:\n",
    "    json_to_csv_weighted(session_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
