{
  "data": [
    {
      "id": "9816caaf",
      "timestamp": "2026-02-04T22:05:40.216000+00:00",
      "name": "9816_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nObject Detection via a video file on Raspberry Pi 4B using a TFLite SSD model.\n\nPhases implemented as per Programming Guideline:\n- Phase 1: Setup (imports, config, labels, interpreter, model I/O details)\n- Phase 2: Input Acquisition & Preprocessing Loop (read video file, preprocess frames)\n- Phase 3: Inference (set input, invoke interpreter)\n- Phase 4: Output Interpretation & Handling Loop (decode detections, apply threshold, clip/scale boxes, draw, compute mAP proxy, write video)\n- Phase 5: Cleanup (release resources)\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports (Interpreter per guideline)\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef load_labels(label_file_path):\n    \"\"\"Load labels from a file into a list (one label per line).\"\"\"\n    labels = []\n    with open(label_file_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line != '':\n                labels.append(line)\n    return labels\n\ndef ensure_dir_for_file(file_path):\n    \"\"\"Ensure directory exists for a given file path.\"\"\"\n    dir_path = os.path.dirname(file_path)\n    if dir_path and not os.path.exists(dir_path):\n        os.makedirs(dir_path, exist_ok=True)\n\ndef get_output_tensors(interpreter, output_details):\n    \"\"\"\n    Retrieve and organize typical SSD MobileNet outputs from TFLite:\n    - boxes: [1, N, 4] (ymin, xmin, ymax, xmax) normalized\n    - classes: [1, N]\n    - scores: [1, N]\n    - num_detections: [1]\n    Returns a dict with keys: 'boxes', 'classes', 'scores', 'num'\n    \"\"\"\n    outputs = {}\n    for od in output_details:\n        tensor = interpreter.get_tensor(od['index'])\n        shape = tensor.shape\n        # Identify by shape patterns\n        if len(shape) == 3 and shape[-1] == 4:\n            outputs['boxes'] = tensor\n        elif len(shape) == 2 and shape[0] == 1 and shape[1] > 1 and tensor.dtype in (np.float32, np.int64, np.int32):\n            # Could be classes or scores; differentiate by dtype/values later if both collide\n            # Temporarily store to decide\n            if 'scores' not in outputs and tensor.dtype == np.float32 and np.all((tensor >= 0.0) & (tensor <= 1.0)):\n                outputs['scores'] = tensor\n            else:\n                outputs['classes'] = tensor\n        elif len(shape) == 1 and shape[0] == 1:\n            outputs['num'] = tensor\n    # Fallback if classes/scores detection overlapped ambiguously\n    if 'scores' not in outputs or 'classes' not in outputs:\n        # Re-extract by checking output_details in order; typical order is boxes, classes, scores, num\n        # We'll attempt a best-effort mapping\n        collected = []\n        for od in output_details:\n            collected.append(interpreter.get_tensor(od['index']))\n        for t in collected:\n            if len(t.shape) == 3 and t.shape[-1] == 4:\n                outputs['boxes'] = t\n        for t in collected:\n            if len(t.shape) == 2 and t.shape[0] == 1 and t.dtype == np.float32 and np.all((t >= 0.0) & (t <= 1.0)):\n                outputs['scores'] = t\n        for t in collected:\n            if len(t.shape) == 2 and t.shape[0] == 1 and t.dtype != np.float32:\n                outputs['classes'] = t\n        for t in collected:\n            if len(t.shape) == 1 and t.shape[0] == 1:\n                outputs['num'] = t\n    return outputs\n\ndef draw_text_with_bg(img, text, org, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=0.5, text_color=(255, 255, 255), bg_color=(0, 0, 0), thickness=1, padding=3):\n    \"\"\"Draw text with a background rectangle for readability.\"\"\"\n    (tw, th), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n    x, y = org\n    cv2.rectangle(img, (x, y - th - 2 * padding), (x + tw + 2 * padding, y + baseline + padding), bg_color, -1)\n    cv2.putText(img, text, (x + padding, y - padding), font, font_scale, text_color, thickness, cv2.LINE_AA)\n\ndef map_class_to_label(class_id_value, labels):\n    \"\"\"\n    Map raw class id from model to text label robustly.\n    Handles common off-by-one between 0-based and 1-based class ids.\n    \"\"\"\n    if labels is None or len(labels) == 0:\n        return f\"id_{int(class_id_value)}\"\n\n    idx = int(class_id_value)\n    # If idx out of range, try idx-1 (common when labels omit background)\n    if 0 <= idx < len(labels):\n        return labels[idx]\n    elif 0 <= (idx - 1) < len(labels):\n        return labels[idx - 1]\n    else:\n        # Clamp as last resort\n        clamped = max(0, min(len(labels) - 1, idx))\n        return labels[clamped]\n\ndef main():\n    # 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')\n\n    # 1.3 Load Labels (if provided/relevant)\n    labels = None\n    if label_path and os.path.exists(label_path):\n        labels = load_labels(label_path)\n    else:\n        labels = []\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    # Assume single input\n    in_idx = input_details[0]['index']\n    input_shape = input_details[0]['shape']  # e.g., [1, 300, 300, 3]\n    input_height, input_width = input_shape[1], input_shape[2]\n    input_dtype = input_details[0]['dtype']\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data (single video file)\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    # Determine video properties; fallbacks if unavailable\n    src_fps = cap.get(cv2.CAP_PROP_FPS)\n    if src_fps is None or src_fps <= 0 or np.isnan(src_fps):\n        src_fps = 25.0  # default fallback\n    # We'll initialize writer after reading first frame to know size\n    writer = None\n    ensure_dir_for_file(output_path)\n\n    # Stats for mAP proxy across classes\n    # We define a proxy mAP (no ground truth available):\n    # For each class: AP_proxy \u2248 frames_with_at_least_one_detection / total_detections_over_threshold\n    # mAP_proxy = mean(AP_proxy across classes that appeared)\n    class_total_detections = {}       # class_name -> total detections counted over threshold\n    class_frames_with_detection = {}  # class_name -> number of frames with at least one detection\n    total_frames_processed = 0\n\n    # Timing (optional)\n    start_time = time.time()\n    frame_index = 0\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        total_frames_processed += 1\n        frame_index += 1\n\n        # Initialize writer once we know frame size\n        if writer is None:\n            h, w = frame_bgr.shape[:2]\n            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n            writer = cv2.VideoWriter(output_path, fourcc, src_fps, (w, h))\n            if not writer.isOpened():\n                cap.release()\n                raise RuntimeError(f\"Failed to open output video writer: {output_path}\")\n\n        # 2.2 Preprocess Data: convert BGR->RGB, resize to model input, create input tensor\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        resized_rgb = cv2.resize(frame_rgb, (input_width, input_height), interpolation=cv2.INTER_LINEAR)\n\n        # Expand dims to [1, H, W, C] and set dtype\n        if input_dtype == np.float32:\n            input_data = resized_rgb.astype(np.float32)\n        else:\n            input_data = resized_rgb.astype(np.uint8)\n        input_data = np.expand_dims(input_data, axis=0)\n\n        # 2.3 Quantization Handling\n        floating_model = (input_dtype == np.float32)\n        if floating_model:\n            # Normalize to [-1, 1] as per guideline\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n        # For quantized models (uint8), no scaling required here.\n\n        # Phase 3: Inference\n        # 3.1 Set Input Tensor\n        interpreter.set_tensor(in_idx, input_data)\n        # 3.2 Run Inference\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling\n        # 4.1 Get Output Tensors\n        outputs = get_output_tensors(interpreter, output_details)\n        boxes = outputs.get('boxes', None)      # shape [1, N, 4]\n        classes = outputs.get('classes', None)  # shape [1, N]\n        scores = outputs.get('scores', None)    # shape [1, N]\n        num = outputs.get('num', None)          # shape [1]\n\n        if boxes is None or classes is None or scores is None:\n            # If outputs cannot be parsed, write original frame and continue\n            writer.write(frame_bgr)\n            continue\n\n        boxes = boxes[0]\n        classes = classes[0]\n        scores = scores[0]\n        if num is not None:\n            try:\n                num_detections = int(np.squeeze(num))\n                num_detections = min(num_detections, boxes.shape[0], classes.shape[0], scores.shape[0])\n            except Exception:\n                num_detections = min(boxes.shape[0], classes.shape[0], scores.shape[0])\n        else:\n            num_detections = min(boxes.shape[0], classes.shape[0], scores.shape[0])\n\n        # 4.2 Interpret Results + 4.3 Post-processing: thresholding, scaling, clipping\n        fh, fw = frame_bgr.shape[:2]\n        frame_class_counts = {}  # class_name -> detections count in this frame (over threshold)\n        for i in range(num_detections):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n\n            # Extract and clip box coordinates (normalized)\n            ymin, xmin, ymax, xmax = boxes[i].tolist()\n            ymin = max(0.0, min(1.0, ymin))\n            xmin = max(0.0, min(1.0, xmin))\n            ymax = max(0.0, min(1.0, ymax))\n            xmax = max(0.0, min(1.0, xmax))\n\n            # Scale to pixel coordinates\n            x1 = int(xmin * fw)\n            y1 = int(ymin * fh)\n            x2 = int(xmax * fw)\n            y2 = int(ymax * fh)\n\n            # Ensure valid non-negative bounds\n            x1 = max(0, min(fw - 1, x1))\n            y1 = max(0, min(fh - 1, y1))\n            x2 = max(0, min(fw - 1, x2))\n            y2 = max(0, min(fh - 1, y2))\n            if x2 <= x1 or y2 <= y1:\n                continue\n\n            # Map class to text\n            class_id = classes[i]\n            label_text = map_class_to_label(class_id, labels) if labels else f\"id_{int(class_id)}\"\n\n            # Update per-frame stats for mAP proxy\n            frame_class_counts[label_text] = frame_class_counts.get(label_text, 0) + 1\n\n            # 4.4 Handle Output: draw boxes and labels on frame\n            color = (0, 255, 0)  # green box\n            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n            caption = f\"{label_text}: {score:.2f}\"\n            draw_text_with_bg(frame_bgr, caption, (x1, max(10, y1 - 5)), font_scale=0.6, text_color=(255, 255, 255), bg_color=(0, 0, 0), thickness=1, padding=3)\n\n        # Update global mAP proxy stats after processing detections in this frame\n        for cname, count_in_frame in frame_class_counts.items():\n            class_total_detections[cname] = class_total_detections.get(cname, 0) + count_in_frame\n            class_frames_with_detection[cname] = class_frames_with_detection.get(cname, 0) + 1\n\n        # Compute current mAP proxy (see definition above)\n        ap_values = []\n        for cname in class_total_detections.keys():\n            total_det = class_total_detections.get(cname, 0)\n            frames_with = class_frames_with_detection.get(cname, 0)\n            if total_det > 0:\n                ap_values.append(frames_with / total_det)\n        mAP_proxy = float(np.mean(ap_values)) if len(ap_values) > 0 else 0.0\n\n        # Overlay current mAP proxy on the frame\n        mAP_text = f\"mAP (proxy): {mAP_proxy:.3f}\"\n        draw_text_with_bg(frame_bgr, mAP_text, (10, 25), font_scale=0.7, text_color=(255, 255, 255), bg_color=(50, 50, 50), thickness=2, padding=4)\n\n        # 4.4 Continue handling: write annotated frame to output video\n        writer.write(frame_bgr)\n\n        # 4.5 Loop continuation is handled by while True with video frames\n\n    # Phase 5: Cleanup\n    if cap is not None:\n        cap.release()\n    if writer is not None:\n        writer.release()\n\n    elapsed = time.time() - start_time\n\n    # Final mAP proxy calculation and reporting\n    final_ap_values = []\n    for cname in class_total_detections.keys():\n        total_det = class_total_detections.get(cname, 0)\n        frames_with = class_frames_with_detection.get(cname, 0)\n        if total_det > 0:\n            final_ap_values.append(frames_with / total_det)\n    final_mAP_proxy = float(np.mean(final_ap_values)) if len(final_ap_values) > 0 else 0.0\n\n    # Print summary\n    print(\"Detection completed.\")\n    print(f\"Input video: {input_path}\")\n    print(f\"Output video: {output_path}\")\n    print(f\"Frames processed: {total_frames_processed}\")\n    print(f\"Elapsed time: {elapsed:.2f} s\")\n    if len(final_ap_values) > 0:\n        print(\"Per-class AP proxy:\")\n        for cname in sorted(class_total_detections.keys()):\n            total_det = class_total_detections.get(cname, 0)\n            frames_with = class_frames_with_detection.get(cname, 0)\n            ap_proxy = (frames_with / total_det) if total_det > 0 else 0.0\n            print(f\"  {cname}: AP_proxy={ap_proxy:.4f} (frames_with={frames_with}, total_detections={total_det})\")\n    print(f\"Final mAP (proxy, no GT): {final_mAP_proxy:.4f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_54c5_psg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 144.581,
      "total_cost": 0.074791,
      "observations": [
        {
          "id": "time-00-05-40-634874_chatcmpl-D5fIDlpy9dS3MiGQQmWCvDUJIRw4g",
          "traceId": "9816caaf",
          "type": "GENERATION",
          "name": "9816_psg_gen_attempt#1",
          "startTime": "2026-02-04T22:05:40.634000+00:00",
          "endTime": "2026-02-04T22:08:05.215000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.074791,
          "latency": 144581.0,
          "totalTokens": 8950,
          "costDetails": {
            "total": 0.074791
          },
          "completionTokens": 7450,
          "promptTokens": 1500,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T22:08:54.979Z",
      "createdAt": "2026-02-04T22:05:40.743Z",
      "environment": "default"
    },
    {
      "id": "92f54528",
      "timestamp": "2026-02-04T22:01:32.245000+00:00",
      "name": "92f5_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n# Application: Object Detection via a video file\n# Target Device: Raspberry Pi 4B\n# Description: Reads a single video file, runs TFLite SSD object detection on each frame,\n#              writes an output video with bounding boxes, labels, and a running heuristic mAP.\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# -------------------------------\n# Phase 1: Setup\n# -------------------------------\n\n# 1.1 Import Interpreter (try ai_edge_litert first as required; provide robust fallbacks)\nInterpreter = None\ninterpreter_source = None\ntry:\n    from ai_edge_litert.interpreter import Interpreter  # literal import per guideline\n    interpreter_source = \"ai_edge_litert\"\nexcept Exception:\n    # Fallbacks to improve robustness on Raspberry Pi environments\n    try:\n        from tflite_runtime.interpreter import Interpreter  # commonly available runtime on Pi\n        interpreter_source = \"tflite_runtime\"\n    except Exception:\n        try:\n            # As a last resort, try the TensorFlow Lite interpreter if TensorFlow is installed\n            from tensorflow.lite.python.interpreter import Interpreter\n            interpreter_source = \"tensorflow.lite\"\n        except Exception as e:\n            raise ImportError(\n                \"Failed to import TFLite Interpreter from ai_edge_litert, tflite_runtime, or tensorflow.lite. \"\n                \"Please ensure one of these is installed.\"\n            ) from e\n\n# 1.2 Paths/Parameters from CONFIGURATION PARAMETERS\nmodel_path  = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path  = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path  = \"data/object_detection/sheeps.mp4\"\noutput_path  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n# 1.3 Load Labels (if provided and relevant)\ndef load_labels(path):\n    labels = []\n    if os.path.isfile(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if len(name) > 0:\n                    labels.append(name)\n    return labels\n\nlabels = load_labels(label_path)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Model input characteristics\ninput_index = input_details[0]['index']\ninput_dtype = input_details[0]['dtype']\ninput_shape = input_details[0]['shape']\n# Expecting NHWC: [1, height, width, channels]\nif len(input_shape) != 4:\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\ninput_height, input_width = int(input_shape[1]), int(input_shape[2])\ninput_channels = int(input_shape[3])\nif input_channels != 3:\n    # Most SSD MobileNet models are 3-channel RGB; enforce here\n    raise ValueError(f\"Expected 3-channel input, got {input_channels}\")\n\n# -------------------------------\n# Helper Functions\n# -------------------------------\n\ndef preprocess_frame(frame_bgr, target_width, target_height, floating_model):\n    # Resize and convert color BGR->RGB as TFLite SSD typically expects RGB\n    resized = cv2.resize(frame_bgr, (target_width, target_height))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    if floating_model:\n        # Normalize to [-1, 1] as per guideline\n        input_data = (np.float32(rgb) - 127.5) / 127.5\n    else:\n        input_data = np.uint8(rgb)\n    # Add batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef find_detection_outputs(output_details_list):\n    # Identify indices for boxes, classes, scores, and num_detections robustly\n    boxes_idx = classes_idx = scores_idx = num_idx = None\n    for i, od in enumerate(output_details_list):\n        shape = od.get('shape', [])\n        dtype = od.get('dtype', None)\n        # Boxes: [..., 4] float32\n        if dtype == np.float32 and len(shape) >= 2 and shape[-1] == 4:\n            boxes_idx = i\n        # Scores: float32 and 2D or 3D with last dim equal to number of detections\n        elif dtype == np.float32 and len(shape) >= 2 and shape[-1] != 4:\n            # Candidate for scores; will disambiguate with classes later\n            if scores_idx is None:\n                scores_idx = i\n        # Classes: int or float, not boxes; prefer non-float32 or name includes 'classes'\n        if ('classes' in od.get('name', '').lower()) or (dtype in [np.int32, np.int64]):\n            classes_idx = i\n        # Num detections: scalar or shape size 1\n        if (len(shape) == 1 and shape[0] == 1) or (len(shape) == 0):\n            # Usually float32\n            if dtype in [np.float32, np.int32, np.int64]:\n                num_idx = i\n    # If classes_idx still None, try to infer: pick any float array similar to scores_idx\n    if classes_idx is None:\n        for i, od in enumerate(output_details_list):\n            if i == boxes_idx or i == scores_idx or i == num_idx:\n                continue\n            shape = od.get('shape', [])\n            if len(shape) >= 2:\n                classes_idx = i\n                break\n    return boxes_idx, classes_idx, scores_idx, num_idx\n\ndef parse_detections(interpreter, output_details_list):\n    # Extract raw outputs\n    boxes_idx, classes_idx, scores_idx, num_idx = find_detection_outputs(output_details_list)\n\n    boxes = interpreter.get_tensor(output_details_list[boxes_idx]['index'])\n    classes = interpreter.get_tensor(output_details_list[classes_idx]['index'])\n    scores = interpreter.get_tensor(output_details_list[scores_idx]['index'])\n    if num_idx is not None:\n        num_det = interpreter.get_tensor(output_details_list[num_idx]['index'])\n    else:\n        num_det = None\n\n    # Squeeze to remove batch dimension\n    boxes = np.squeeze(boxes)\n    classes = np.squeeze(classes)\n    scores = np.squeeze(scores)\n    if num_det is not None:\n        num_det = int(np.squeeze(num_det).astype(np.int32))\n    else:\n        # If not provided, infer from scores length\n        num_det = scores.shape[0]\n\n    # Align shapes to length N\n    N = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    return boxes[:N], classes[:N], scores[:N]\n\ndef clip_box(x1, y1, x2, y2, width, height):\n    x1c = max(0, min(width - 1, int(round(x1))))\n    y1c = max(0, min(height - 1, int(round(y1))))\n    x2c = max(0, min(width - 1, int(round(x2))))\n    y2c = max(0, min(height - 1, int(round(y2))))\n    # Ensure valid ordering\n    if x2c < x1c:\n        x1c, x2c = x2c, x1c\n    if y2c < y1c:\n        y1c, y2c = y2c, y1c\n    return x1c, y1c, x2c, y2c\n\ndef class_id_to_name(cid, labels_list):\n    # Handle both 0-based and 1-based class indexing gracefully\n    name = None\n    if isinstance(cid, float):\n        cid = int(cid)\n    if labels_list:\n        if 0 <= cid < len(labels_list):\n            name = labels_list[cid]\n        elif 0 <= cid - 1 < len(labels_list):\n            name = labels_list[cid - 1]\n    return name if name else f\"class_{int(cid)}\"\n\ndef iou(boxA, boxB):\n    # Boxes: (x1,y1,x2,y2)\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interW = max(0, xB - xA + 1)\n    interH = max(0, yB - yA + 1)\n    interArea = interW * interH\n    areaA = max(0, (boxA[2] - boxA[0] + 1)) * max(0, (boxA[3] - boxA[1] + 1))\n    areaB = max(0, (boxB[2] - boxB[0] + 1)) * max(0, (boxB[3] - boxB[1] + 1))\n    union = areaA + areaB - interArea + 1e-9\n    return interArea / union\n\ndef compute_map_heuristic(gt_per_class, preds_per_class, iou_thresh=0.5):\n    # Heuristic mAP using \"one best detection per class per frame\" as pseudo ground truth.\n    ap_values = []\n    for cls_id in sorted(set(list(gt_per_class.keys()) + list(preds_per_class.keys()))):\n        gts = gt_per_class.get(cls_id, {})\n        preds = preds_per_class.get(cls_id, [])\n        num_gt = len(gts)\n        if num_gt == 0 or len(preds) == 0:\n            # No meaningful AP can be computed; skip class\n            continue\n        # Sort predictions by descending score\n        preds_sorted = sorted(preds, key=lambda x: -x['score'])\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n        matched_frames = set()\n\n        for i, pred in enumerate(preds_sorted):\n            f = pred['frame']\n            box_p = pred['box']\n            if (f in gts) and (f not in matched_frames):\n                if iou(box_p, gts[f]) >= iou_thresh:\n                    tp[i] = 1.0\n                    matched_frames.add(f)\n                else:\n                    fp[i] = 1.0\n            else:\n                fp[i] = 1.0\n\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        recalls = cum_tp / (num_gt + 1e-9)\n        precisions = cum_tp / (cum_tp + cum_fp + 1e-9)\n\n        # VOC-style continuous interpolated AP\n        mrec = np.concatenate(([0.0], recalls, [1.0]))\n        mpre = np.concatenate(([0.0], precisions, [0.0]))\n        for i in range(len(mpre) - 2, -1, -1):\n            mpre[i] = max(mpre[i], mpre[i + 1])\n        idx = np.where(mrec[1:] != mrec[:-1])[0] + 1\n        ap = 0.0\n        for i in idx:\n            ap += (mrec[i] - mrec[i - 1]) * mpre[i]\n        ap_values.append(ap)\n\n    if len(ap_values) == 0:\n        return 0.0\n    return float(np.mean(ap_values))\n\n# -------------------------------\n# Phase 2: Input Acquisition & Preprocessing Loop\n# -------------------------------\n\n# 2.1 Acquire Input Data (single video file from input_path)\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise FileNotFoundError(f\"Failed to open input video: {input_path}\")\n\n# Retrieve input video properties\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or in_fps <= 0.1:\n    in_fps = 30.0  # fallback\nin_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nin_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Prepare output video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(output_path, fourcc, in_fps, (in_width, in_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open output video writer at: {output_path}\")\n\n# Determine floating model\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Structures for heuristic mAP computation\n# gt_per_class: class_id -> {frame_idx: box}\n# preds_per_class: class_id -> [{'score':float, 'frame':int, 'box':(x1,y1,x2,y2)}]\ngt_per_class = {}\npreds_per_class = {}\nframe_index = 0\nrunning_map = 0.0\n\n# -------------------------------\n# Processing Loop\n# -------------------------------\n\nstart_time = time.time()\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break  # End of video\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame(frame, input_width, input_height, floating_model)\n\n    # 2.3 Quantization Handling (handled in preprocess based on floating_model)\n\n    # -------------------------------\n    # Phase 3: Inference\n    # -------------------------------\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\n    # -------------------------------\n    # Phase 4: Output Interpretation & Handling\n    # -------------------------------\n\n    # 4.1 Get Output Tensor(s)\n    det_boxes, det_classes, det_scores = parse_detections(interpreter, output_details)\n\n    # 4.2 Interpret Results\n    h, w = frame.shape[:2]\n    detections_to_draw = []  # list of dicts: {'box':(x1,y1,x2,y2), 'score':float, 'class_id':int, 'label':str}\n    per_class_best = {}  # for heuristic GT: class_id -> {'score':float, 'box':(x1,y1,x2,y2)}\n\n    for i in range(len(det_scores)):\n        score = float(det_scores[i])\n        if score < confidence_threshold:\n            continue\n        # TFLite SSD boxes are [ymin, xmin, ymax, xmax] in normalized coordinates\n        box = det_boxes[i]\n        ymin, xmin, ymax, xmax = float(box[0]), float(box[1]), float(box[2]), float(box[3])\n        # 4.3 Post-processing: coordinate scaling and clipping\n        x1 = int(xmin * w)\n        y1 = int(ymin * h)\n        x2 = int(xmax * w)\n        y2 = int(ymax * h)\n        x1, y1, x2, y2 = clip_box(x1, y1, x2, y2, w, h)\n        if x2 <= x1 or y2 <= y1:\n            continue\n\n        cls_id_raw = det_classes[i]\n        if isinstance(cls_id_raw, (np.floating, float)):\n            cls_id = int(cls_id_raw)\n        else:\n            cls_id = int(cls_id_raw)\n        label_name = class_id_to_name(cls_id, labels)\n\n        det_entry = {'box': (x1, y1, x2, y2), 'score': score, 'class_id': cls_id, 'label': label_name}\n        detections_to_draw.append(det_entry)\n\n        # Update per-class best (highest confidence) for pseudo-GT\n        if cls_id not in per_class_best or score > per_class_best[cls_id]['score']:\n            per_class_best[cls_id] = {'score': score, 'box': (x1, y1, x2, y2)}\n\n        # Add all predictions to preds_per_class for heuristic mAP\n        if cls_id not in preds_per_class:\n            preds_per_class[cls_id] = []\n        preds_per_class[cls_id].append({'score': score, 'frame': frame_index, 'box': (x1, y1, x2, y2)})\n\n    # Update heuristic GT with best per class for current frame\n    for cid, info in per_class_best.items():\n        if cid not in gt_per_class:\n            gt_per_class[cid] = {}\n        gt_per_class[cid][frame_index] = info['box']\n\n    # Compute running heuristic mAP over processed frames so far\n    running_map = compute_map_heuristic(gt_per_class, preds_per_class, iou_thresh=0.5)\n\n    # 4.4 Handle Output: draw bounding boxes, labels, confidences and running mAP\n    for det in detections_to_draw:\n        x1, y1, x2, y2 = det['box']\n        label_text = f\"{det['label']}: {det['score']:.2f}\"\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        # Text background for readability\n        (tw, th), bl = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), (0, 255, 0), -1)\n        cv2.putText(frame, label_text, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Overlay running mAP\n    map_text = f\"mAP@0.5 (heuristic): {running_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 50, 255), 2, cv2.LINE_AA)\n\n    # Write frame to output\n    out_writer.write(frame)\n\n    frame_index += 1\n\n# -------------------------------\n# Phase 5: Cleanup\n# -------------------------------\ncap.release()\nout_writer.release()\nelapsed = time.time() - start_time\n\n# Final console output\nprint(f\"Processed {frame_index} frames in {elapsed:.2f} seconds ({(frame_index / max(elapsed,1e-6)):.2f} FPS).\")\nprint(f\"Output saved to: {output_path}\")\nprint(f\"Final heuristic mAP@0.5 over processed frames: {running_map:.4f}\")\nprint(f\"Interpreter source used: {interpreter_source}\")"
      },
      "session_id": "gpt-5-2025-08-07_54c5_psg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 181.456,
      "total_cost": 0.085085,
      "observations": [
        {
          "id": "time-00-01-32-680059_chatcmpl-D5fEDcmm7sIHXS4e0OrqOZTrpGOVt",
          "traceId": "92f54528",
          "type": "GENERATION",
          "name": "92f5_psg_gen_attempt#1",
          "startTime": "2026-02-04T22:01:32.680000+00:00",
          "endTime": "2026-02-04T22:04:34.136000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.085085,
          "latency": 181456.0,
          "totalTokens": 9821,
          "costDetails": {
            "total": 0.085085
          },
          "completionTokens": 8321,
          "promptTokens": 1500,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T22:05:34.535Z",
      "createdAt": "2026-02-04T22:01:32.774Z",
      "environment": "default"
    },
    {
      "id": "c74d1c76",
      "timestamp": "2026-02-04T21:58:13.314000+00:00",
      "name": "c74d_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file (Raspberry Pi 4B)\nModel: SSD MobileNet V1 (TFLite)\nI/O:\n- Input: Read a single video file from the given input_path\n- Output: Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP (proxy, see console note)\n\nNotes on mAP:\n- Ground truth annotations are not provided, so classical mAP (requiring TP/FP matched to GT) cannot be computed.\n- This script computes a proxy \"precision\" per class per frame using thresholding:\n  TP = detections for a class with score >= threshold\n  FP = detections for a class with score  < threshold\n  Precision(frame, class) = TP / (TP + FP) when at least one detection for that class exists in the frame\n  Per-class precision = mean over frames (where class appears)\n  mAP (proxy) = mean of per-class precisions over classes that appear in the video\n- The live \"mAP (proxy)\" is overlaid on frames and printed at the end.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Import interpreter literally as specified\nfrom ai_edge_litert.interpreter import Interpreter  # Ensure this package is available on the target\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')  # Confidence Threshold\n\ndef ensure_dir_for_file(path: str):\n    \"\"\"Ensure the parent directory for a file path exists.\"\"\"\n    parent = os.path.dirname(os.path.abspath(path))\n    if parent and not os.path.exists(parent):\n        os.makedirs(parent, exist_ok=True)\n\ndef load_labels(label_file: str):\n    \"\"\"Load labels from a file into a list (index -> label).\"\"\"\n    labels = []\n    try:\n        with open(label_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    except Exception as e:\n        print(f\"Warning: Could not read label file '{label_file}': {e}\")\n    return labels\n\ndef clip_bbox(xmin, ymin, xmax, ymax, width, height):\n    \"\"\"Clip bounding box coordinates to image boundaries.\"\"\"\n    xmin = max(0, min(int(xmin), width - 1))\n    xmax = max(0, min(int(xmax), width - 1))\n    ymin = max(0, min(int(ymin), height - 1))\n    ymax = max(0, min(int(ymax), height - 1))\n    return xmin, ymin, xmax, ymax\n\ndef map_detection_outputs(output_details, outputs):\n    \"\"\"\n    Map raw output tensors to (boxes, classes, scores, num) using names or heuristics.\n    Returns a tuple of numpy arrays.\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Attempt by names first\n    for i, detail in enumerate(output_details):\n        name = detail.get('name', '')\n        name_l = name.lower() if isinstance(name, str) else ''\n        val = outputs[i]\n        if 'box' in name_l:\n            boxes = val\n        elif 'score' in name_l or 'scores' in name_l:\n            scores = val\n        elif 'class' in name_l:\n            classes = val\n        elif 'num' in name_l:\n            num = val\n\n    # Heuristic fallback if any are None\n    shapes = [np.shape(o) for o in outputs]\n    # Boxes: look for ...,4 in shape\n    if boxes is None:\n        for o in outputs:\n            shp = np.shape(o)\n            if len(shp) == 3 and shp[-1] == 4:\n                boxes = o\n                break\n    # num: shape length 1 total size 1\n    if num is None:\n        for o in outputs:\n            if np.size(o) == 1:\n                num = o\n                break\n    # classes and scores: both typically shape (1, N)\n    # Try to identify scores by value range [0,1]\n    cand_1xn = [o for o in outputs if len(np.shape(o)) == 2 and np.shape(o)[0] == 1]\n    if scores is None or classes is None:\n        candidate_scores = None\n        candidate_classes = None\n        for o in cand_1xn:\n            arr = o.astype(np.float32)\n            arr_flat = arr.flatten()\n            if arr_flat.size == 0:\n                continue\n            minv, maxv = float(np.min(arr_flat)), float(np.max(arr_flat))\n            # If in [0, 1] range likely scores\n            if 0.0 <= minv and maxv <= 1.0:\n                candidate_scores = o\n            else:\n                candidate_classes = o\n        if scores is None and candidate_scores is not None:\n            scores = candidate_scores\n        if classes is None and candidate_classes is not None:\n            classes = candidate_classes\n    return boxes, classes, scores, num\n\ndef draw_labelled_box(frame, xmin, ymin, xmax, ymax, color, label_text):\n    \"\"\"Draw bounding box with a label.\"\"\"\n    cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n    # Text background\n    (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n    th_box = th + baseline + 4\n    y_text = max(ymin, th_box)\n    cv2.rectangle(frame, (xmin, y_text - th_box), (xmin + tw + 4, y_text), color, -1)\n    cv2.putText(frame, label_text, (xmin + 2, y_text - baseline - 2),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\ndef main():\n    # 1.3 Load Labels (Conditional)\n    labels = load_labels(LABEL_PATH)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Using the first input only (SSD has single input)\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']  # e.g., [1, 300, 300, 3]\n    input_dtype = input_details[0]['dtype']\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: single video file\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: Could not open input video: {INPUT_PATH}\")\n        return\n\n    # Prepare output writer\n    ensure_dir_for_file(OUTPUT_PATH)\n    in_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 1e-3:\n        fps = 30.0  # Fallback if FPS is not detected\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (in_width, in_height))\n    if not writer.isOpened():\n        print(f\"Error: Could not open output video for writing: {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # Setup for stats and timing\n    floating_model = (input_dtype == np.float32)  # 2.3 Quantization Handling\n    target_h, target_w = int(input_shape[1]), int(input_shape[2])\n\n    # For mAP proxy calculation\n    # class_id -> list of per-frame precision values\n    class_prec_history = {}\n    frame_count = 0\n    t0 = time.time()\n\n    print(\"Info: Starting inference on video.\")\n    print(f\"Model: {MODEL_PATH}\")\n    print(f\"Labels: {LABEL_PATH} (loaded {len(labels)} labels)\")\n    print(f\"Input video: {INPUT_PATH}\")\n    print(f\"Output video: {OUTPUT_PATH}\")\n    print(f\"Confidence threshold: {CONF_THRESHOLD}\")\n    print(\"Note: mAP displayed is a proxy precision due to absence of ground truth.\")\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n        frame_draw = frame_bgr.copy()\n        h, w = frame_draw.shape[:2]\n\n        # 2.2 Preprocess Data\n        # - Resize to model input\n        resized = cv2.resize(frame_bgr, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n        # - Convert BGR to RGB as most TFLite models expect RGB\n        input_rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n        input_data = np.expand_dims(input_rgb, axis=0)\n        # Ensure dtype matches model\n        if floating_model:\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n        else:\n            # uint8 model\n            input_data = np.asarray(input_data, dtype=input_dtype)\n\n        # Phase 3: Inference\n        interpreter.set_tensor(input_index, input_data)\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling\n        # 4.1 Get Output Tensors\n        raw_outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n        # 4.2 Interpret Results\n        boxes, classes, scores, num = map_detection_outputs(output_details, raw_outputs)\n\n        # Safety checks and flattening\n        if boxes is None or classes is None or scores is None or num is None:\n            # If mapping failed, skip this frame gracefully\n            cv2.putText(frame_draw, \"Output parsing error\", (10, 30),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n            writer.write(frame_draw)\n            continue\n\n        # Expected shapes: boxes (1, N, 4), classes (1, N), scores (1, N), num (1,)\n        boxes = np.squeeze(boxes, axis=0)\n        classes = np.squeeze(classes, axis=0)\n        scores = np.squeeze(scores, axis=0)\n        try:\n            num_det = int(np.squeeze(num).tolist())\n        except Exception:\n            num_det = int(np.squeeze(num).astype(np.int32))\n\n        # Limit arrays to num_detections length if shapes allow\n        n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n        boxes = boxes[:n]\n        classes = classes[:n]\n        scores = scores[:n]\n\n        # 4.3 Post-processing: thresholding, coordinate scaling, clipping\n        # Determine if boxes are normalized [0,1] or absolute; assume normalized if max <= 1.5\n        normalized = np.max(boxes) <= 1.5\n\n        # Prepare class-wise counts for proxy precision\n        frame_class_counts = {}  # class_id -> total detections in frame (above+below threshold)\n        frame_class_tp = {}      # class_id -> detections above threshold\n\n        # Draw detections\n        for i in range(n):\n            score = float(scores[i])\n            class_id_raw = classes[i]\n            # Many TFLite models output float class IDs; cast to int\n            class_id = int(class_id_raw)\n            frame_class_counts[class_id] = frame_class_counts.get(class_id, 0) + 1\n            if score >= CONF_THRESHOLD:\n                frame_class_tp[class_id] = frame_class_tp.get(class_id, 0) + 1\n\n                ymin, xmin, ymax, xmax = boxes[i]\n                if normalized:\n                    xmin_a = xmin * w\n                    xmax_a = ymax * w  # careful: order is [ymin, xmin, ymax, xmax]\n                    xmax_a = xmax * w\n                    ymin_a = ymin * h\n                    ymax_a = ymax * h\n                else:\n                    xmin_a = xmin\n                    ymin_a = ymin\n                    xmax_a = xmax\n                    ymax_a = ymax\n\n                xmin_c, ymin_c, xmax_c, ymax_c = clip_bbox(xmin_a, ymin_a, xmax_a, ymax_a, w, h)\n\n                # Choose color based on class id\n                color = (37 * (class_id + 1) % 255, 17 * (class_id + 1) % 255, 29 * (class_id + 1) % 255)\n\n                # Label text\n                if 0 <= class_id < len(labels):\n                    label_name = labels[class_id]\n                else:\n                    label_name = f\"id_{class_id}\"\n                label_text = f\"{label_name}: {score:.2f}\"\n\n                draw_labelled_box(frame_draw, xmin_c, ymin_c, xmax_c, ymax_c, color, label_text)\n\n        # Update proxy precision stats\n        for cid, total in frame_class_counts.items():\n            tp = frame_class_tp.get(cid, 0)\n            precision = tp / float(total) if total > 0 else 0.0\n            if cid not in class_prec_history:\n                class_prec_history[cid] = []\n            class_prec_history[cid].append(precision)\n\n        # Compute mAP proxy across seen classes\n        per_class_means = []\n        for cid, plist in class_prec_history.items():\n            if len(plist) > 0:\n                per_class_means.append(float(np.mean(plist)))\n        mAP_proxy = float(np.mean(per_class_means)) if len(per_class_means) > 0 else 0.0\n\n        # 4.4 Handle Output: overlay info and write frame\n        # Show mAP proxy and FPS\n        elapsed = time.time() - t0\n        avg_fps = frame_count / elapsed if elapsed > 0 else 0.0\n        info_text = f\"mAP (proxy): {mAP_proxy:.3f} | FPS: {avg_fps:.1f}\"\n        cv2.putText(frame_draw, info_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX,\n                    0.7, (0, 255, 0), 2, cv2.LINE_AA)\n\n        writer.write(frame_draw)\n\n        # 4.5 Loop Continuation: Next frame until input exhausted\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n\n    # Final summary\n    if frame_count > 0:\n        per_class_means = []\n        for cid, plist in class_prec_history.items():\n            if len(plist) > 0:\n                per_class_means.append(float(np.mean(plist)))\n        final_mAP_proxy = float(np.mean(per_class_means)) if len(per_class_means) > 0 else 0.0\n    else:\n        final_mAP_proxy = 0.0\n\n    print(\"Inference completed.\")\n    print(f\"Processed frames: {frame_count}\")\n    print(f\"Final mAP (proxy): {final_mAP_proxy:.4f}\")\n    print(f\"Result saved to: {OUTPUT_PATH}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_54c5_psg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 141.926,
      "total_cost": 0.068991,
      "observations": [
        {
          "id": "time-23-58-13-697434_chatcmpl-D5fAzA80arqQ3LHJazXetFn4Occ9D",
          "traceId": "c74d1c76",
          "type": "GENERATION",
          "name": "c74d_psg_gen_attempt#1",
          "startTime": "2026-02-04T21:58:13.697000+00:00",
          "endTime": "2026-02-04T22:00:35.623000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.068991,
          "latency": 141926.0,
          "totalTokens": 8370,
          "costDetails": {
            "total": 0.068991
          },
          "completionTokens": 6870,
          "promptTokens": 1500,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T22:01:26.030Z",
      "createdAt": "2026-02-04T21:58:13.836Z",
      "environment": "default"
    },
    {
      "id": "2cea11ed",
      "timestamp": "2026-02-04T21:47:28.387000+00:00",
      "name": "2cea_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nObject Detection via a video file on Raspberry Pi 4B\n- Loads a TFLite SSD MobileNet v1 model using ai_edge_litert Interpreter\n- Reads a single video file from input_path\n- Performs inference per frame, draws bounding boxes and labels\n- Applies post-processing: confidence thresholding, coordinate scaling, clipping\n- Writes an output video with detections drawn\n- Calculates and overlays a proxy mAP (mean Average Precision) metric over time\n  (Proxy mAP is computed without ground truth by using NMS-suppressed predictions\n   as pseudo ground-truth per frame; this measures duplicate suppression quality)\nRequirements:\n- Only standard libraries (os, time, numpy) and cv2 for video processing are used.\n- Interpreter import is: from ai_edge_litert.interpreter import Interpreter\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Interpreter per guideline\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from configuration)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')  # Confidence Threshold\n\n# Utility: Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n\ndef load_labels(label_path):\n    \"\"\"\n    Load labels from a given label map file.\n    Returns:\n        labels (list[str]): list of label names indexed by class id.\n    \"\"\"\n    labels = []\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            name = line.strip()\n            if name != '':\n                labels.append(name)\n    return labels\n\n\ndef letterbox_resize(img, target_size):\n    \"\"\"\n    Resize image to target_size without preserving aspect ratio (direct resize).\n    For SSD models, direct resize is acceptable.\n    Args:\n        img (np.ndarray): BGR frame\n        target_size (tuple): (width, height)\n    Returns:\n        resized (np.ndarray): resized image\n    \"\"\"\n    target_w, target_h = target_size\n    resized = cv2.resize(img, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n    return resized\n\n\ndef preprocess_frame(frame_bgr, input_shape, floating_model):\n    \"\"\"\n    Preprocess BGR frame to match model input tensor requirements.\n    - Resize to (input_w,input_h)\n    - Convert BGR to RGB\n    - Add batch dimension\n    - Normalize if floating model: (x - 127.5) / 127.5\n    Args:\n        frame_bgr (np.ndarray): input frame in BGR\n        input_shape (tuple): model input tensor shape (1, H, W, C)\n        floating_model (bool): if True, normalize to [-1,1]\n    Returns:\n        input_data (np.ndarray): preprocessed tensor ready for set_tensor\n        resized_rgb (np.ndarray): resized RGB image (H,W,3) for potential debugging\n    \"\"\"\n    _, in_h, in_w, in_c = input_shape\n    if in_c != 3:\n        raise ValueError(\"Model input channel count is not 3; unsupported input format for this script.\")\n\n    resized_bgr = letterbox_resize(frame_bgr, (in_w, in_h))\n    resized_rgb = cv2.cvtColor(resized_bgr, cv2.COLOR_BGR2RGB)\n    input_data = resized_rgb\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = np.uint8(input_data)\n\n    input_data = np.expand_dims(input_data, axis=0)  # add batch dimension\n    return input_data, resized_rgb\n\n\ndef extract_detections_from_outputs(outputs):\n    \"\"\"\n    Robustly extract boxes, classes, scores, and num_detections from TFLite outputs.\n    The function attempts to infer the correct arrays based on shapes/dtypes.\n    Args:\n        outputs (list[np.ndarray]): raw output tensors as returned by interpreter.get_tensor()\n    Returns:\n        boxes (np.ndarray): [N,4] float (ymin,xmin,ymax,xmax) normalized 0..1\n        classes (np.ndarray): [N] int class ids\n        scores (np.ndarray): [N] float confidences 0..1\n        num (int): number of detections\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Collect candidates\n    boxes_cand = []\n    classes_cand = []\n    scores_cand = []\n    num_cand = []\n\n    for arr in outputs:\n        arr_np = np.array(arr)\n        if arr_np.ndim == 3 and arr_np.shape[0] == 1 and arr_np.shape[2] == 4:\n            boxes_cand.append(arr_np[0])\n        elif arr_np.ndim == 2 and arr_np.shape[0] == 1:\n            # Could be classes or scores\n            a0 = arr_np[0]\n            if a0.dtype.kind == 'f':\n                # Likely scores if values are within [0,1]\n                maxv = np.max(a0) if a0.size > 0 else 0.0\n                minv = np.min(a0) if a0.size > 0 else 0.0\n                if (minv >= -1e-3) and (maxv <= 1.0 + 1e-3):\n                    scores_cand.append(a0.astype(np.float32))\n                else:\n                    classes_cand.append(a0.astype(np.float32))\n            else:\n                classes_cand.append(a0.astype(np.int32))\n        elif arr_np.size == 1:\n            num_cand.append(int(np.squeeze(arr_np)))\n        else:\n            # Ignore unknown shapes\n            pass\n\n    boxes = boxes_cand[0] if boxes_cand else None\n    scores = scores_cand[0] if scores_cand else None\n\n    # Classes: prefer int dtype; if float, cast to int\n    if classes_cand:\n        if np.issubdtype(classes_cand[0].dtype, np.integer):\n            classes = classes_cand[0].astype(np.int32)\n        else:\n            classes = np.round(classes_cand[0]).astype(np.int32)\n\n    if num_cand:\n        num = num_cand[0]\n\n    # Fallback to typical SSD order if inference failed to map\n    if boxes is None or scores is None or classes is None or num is None:\n        # Attempt to interpret typical order: [boxes, classes, scores, num_detections]\n        try:\n            boxes = outputs[0][0]\n            classes = np.round(outputs[1][0]).astype(np.int32)\n            scores = outputs[2][0].astype(np.float32)\n            num = int(np.squeeze(outputs[3]))\n        except Exception:\n            raise RuntimeError(\"Unable to parse TFLite detection outputs. Check model outputs.\")\n\n    n = min(num, boxes.shape[0], classes.shape[0], scores.shape[0])\n    return boxes[:n], classes[:n], scores[:n], n\n\n\ndef clip_box_xyxy(box, W, H):\n    \"\"\"\n    Clip box to image boundaries.\n    Args:\n        box (tuple/list/np.ndarray): [x1,y1,x2,y2]\n        W (int): image width\n        H (int): image height\n    Returns:\n        (np.ndarray): clipped [x1,y1,x2,y2]\n    \"\"\"\n    x1, y1, x2, y2 = box\n    x1 = max(0, min(W - 1, x1))\n    y1 = max(0, min(H - 1, y1))\n    x2 = max(0, min(W - 1, x2))\n    y2 = max(0, min(H - 1, y2))\n    return np.array([x1, y1, x2, y2], dtype=np.float32)\n\n\ndef class_color(cid):\n    \"\"\"\n    Deterministic pseudo-random color for a class id.\n    Returns BGR tuple.\n    \"\"\"\n    # Use simple hashing with prime multipliers\n    r = int((37 * (cid + 1)) % 255)\n    g = int((17 * (cid + 1)) % 255)\n    b = int((29 * (cid + 1)) % 255)\n    # Avoid very dark colors\n    r = r + 60 if r < 60 else r\n    g = g + 60 if g < 60 else g\n    b = b + 60 if b < 60 else b\n    return (b, g, r)\n\n\ndef iou_xyxy(box, boxes):\n    \"\"\"\n    Compute IoU between a single box and an array of boxes.\n    Args:\n        box (np.ndarray shape [4]): [x1,y1,x2,y2]\n        boxes (np.ndarray shape [N,4]): [x1,y1,x2,y2]\n    Returns:\n        ious (np.ndarray shape [N]): IoU values\n    \"\"\"\n    x1 = np.maximum(box[0], boxes[:, 0])\n    y1 = np.maximum(box[1], boxes[:, 1])\n    x2 = np.minimum(box[2], boxes[:, 2])\n    y2 = np.minimum(box[3], boxes[:, 3])\n\n    inter_w = np.maximum(0, x2 - x1 + 1)\n    inter_h = np.maximum(0, y2 - y1 + 1)\n    inter = inter_w * inter_h\n\n    area_box = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n    area_boxes = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n\n    union = area_box + area_boxes - inter + 1e-6\n    ious = inter / union\n    return ious\n\n\ndef nms_xyxy(boxes, scores, iou_threshold=0.5):\n    \"\"\"\n    Non-Maximum Suppression for xyxy boxes.\n    Args:\n        boxes (np.ndarray [N,4])\n        scores (np.ndarray [N])\n        iou_threshold (float)\n    Returns:\n        keep_indices (list[int])\n    \"\"\"\n    if boxes.size == 0:\n        return []\n\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = int(order[0])\n        keep.append(i)\n\n        if order.size == 1:\n            break\n\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)\n\n        inds = np.where(iou <= iou_threshold)[0]\n        order = order[inds + 1]\n\n    return keep\n\n\ndef compute_proxy_map(preds_by_class, gts_by_class, iou_threshold=0.5):\n    \"\"\"\n    Compute a proxy mAP using predictions as both the input detections and pseudo ground-truth.\n    Pseudo ground-truth per frame/class is produced via NMS to remove duplicates.\n    Standard AP computation is then applied across all frames for each class, and averaged.\n    Args:\n        preds_by_class (dict[int, list[dict]]): class_id -> list of {frame_id:int, box:np.ndarray[4], score:float}\n        gts_by_class (dict[int, list[dict]]): class_id -> list of {frame_id:int, box:np.ndarray[4]}\n        iou_threshold (float): IoU threshold for matching\n    Returns:\n        mAP (float): mean Average Precision across all classes that have at least one pseudo GT\n    \"\"\"\n    aps = []\n\n    for cid in sorted(set(list(preds_by_class.keys()) + list(gts_by_class.keys()))):\n        preds = preds_by_class.get(cid, [])\n        gts = gts_by_class.get(cid, [])\n\n        if len(gts) == 0:\n            # No pseudo ground-truth for this class; skip from mAP\n            continue\n\n        # Prepare GT bookkeeping per frame\n        gt_by_frame = {}\n        for idx, gt in enumerate(gts):\n            fid = gt['frame_id']\n            if fid not in gt_by_frame:\n                gt_by_frame[fid] = []\n            gt_by_frame[fid].append({'box': gt['box'], 'matched': False})\n\n        # Sort predictions by score descending\n        preds_sorted = sorted(preds, key=lambda x: x['score'], reverse=True)\n\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n\n        for i, p in enumerate(preds_sorted):\n            fid = p['frame_id']\n            p_box = p['box']\n            matched = False\n\n            if fid in gt_by_frame and len(gt_by_frame[fid]) > 0:\n                # Compute IoU with all unmatched GT in the same frame\n                gt_boxes = np.array([g['box'] for g in gt_by_frame[fid]])\n                matched_flags = np.array([g['matched'] for g in gt_by_frame[fid]], dtype=bool)\n\n                if gt_boxes.size > 0:\n                    ious = iou_xyxy(p_box, gt_boxes)\n                    # Consider only unmatched\n                    ious[matched_flags] = -1.0\n                    j = int(np.argmax(ious))\n                    max_iou = ious[j]\n                    if max_iou >= iou_threshold and not gt_by_frame[fid][j]['matched']:\n                        matched = True\n                        gt_by_frame[fid][j]['matched'] = True\n\n            if matched:\n                tp[i] = 1.0\n            else:\n                fp[i] = 1.0\n\n        # Cumulative sums\n        tp_cum = np.cumsum(tp)\n        fp_cum = np.cumsum(fp)\n\n        rec = tp_cum / (len(gts) + 1e-12)\n        prec = tp_cum / np.maximum(tp_cum + fp_cum, 1e-12)\n\n        # Compute AP using precision envelope\n        mrec = np.concatenate(([0.0], rec, [1.0]))\n        mpre = np.concatenate(([0.0], prec, [0.0]))\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = max(mpre[i - 1], mpre[i])\n\n        # Integration points where recall changes\n        idxs = np.where(mrec[1:] != mrec[:-1])[0]\n        ap = 0.0\n        for i in idxs:\n            ap += (mrec[i + 1] - mrec[i]) * mpre[i + 1]\n\n        aps.append(ap)\n\n    if len(aps) == 0:\n        return 0.0\n\n    return float(np.mean(aps))\n\n\ndef main():\n    # 1.3 Load Labels\n    labels = load_labels(LABEL_PATH)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: open the given video file\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Could not open input video: {INPUT_PATH}\")\n\n    # Prepare output video writer with same size as input frames\n    in_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 25.0  # fallback FPS\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (in_w, in_h))\n    if not writer.isOpened():\n        raise RuntimeError(f\"Could not open output video for writing: {OUTPUT_PATH}\")\n\n    # Accumulators for proxy mAP\n    preds_by_class = {}  # cid -> list of {frame_id, box, score}\n    gts_by_class = {}    # cid -> list of {frame_id, box} (from NMS of predictions)\n    frame_index = 0\n\n    start_time = time.time()\n    last_time = start_time\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # End of video\n\n        H, W = frame_bgr.shape[:2]\n\n        # 2.2 Preprocess Data\n        input_data, _ = preprocess_frame(frame_bgr, input_shape, floating_model)\n\n        # 2.3 Quantization Handling already done in preprocess (normalize for floating)\n        # 2.4 Loop Control: implicit via while loop\n\n        # Phase 3: Inference\n        # 3.1 Set Input Tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        # 3.2 Run Inference\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling\n        # 4.1 Get Output Tensor(s)\n        raw_outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n        # 4.2 Interpret Results\n        boxes_norm, classes, scores, n = extract_detections_from_outputs(raw_outputs)\n\n        # Convert normalized boxes to image coordinates and collect detections\n        detections_for_frame = []  # for drawing (thresholded)\n        preds_for_frame_by_class = {}  # for proxy mAP (all predictions without threshold)\n\n        for i in range(n):\n            score = float(scores[i])\n            if score < 0.0:\n                continue  # invalid score\n\n            cls_id = int(classes[i])\n            ymin, xmin, ymax, xmax = boxes_norm[i].tolist()\n\n            # 4.3 Post-processing: thresholding, coordinate scaling, clipping\n            x1 = int(xmin * W)\n            y1 = int(ymin * H)\n            x2 = int(xmax * W)\n            y2 = int(ymax * H)\n\n            # Ensure proper ordering\n            if x2 < x1:\n                x1, x2 = x2, x1\n            if y2 < y1:\n                y1, y2 = y2, y1\n\n            box_xyxy = clip_box_xyxy([x1, y1, x2, y2], W, H)\n\n            # Store for drawing (apply confidence threshold)\n            if score >= CONF_THRESHOLD:\n                detections_for_frame.append({\n                    'box': box_xyxy,\n                    'score': score,\n                    'class_id': cls_id\n                })\n\n            # Store all predictions (no threshold) for proxy mAP\n            if cls_id not in preds_for_frame_by_class:\n                preds_for_frame_by_class[cls_id] = []\n            preds_for_frame_by_class[cls_id].append({'frame_id': frame_index, 'box': box_xyxy, 'score': score})\n\n        # Proxy mAP: produce pseudo-GT via NMS per class for the current frame\n        gts_for_frame_by_class = {}\n        for cid, plist in preds_for_frame_by_class.items():\n            if len(plist) == 0:\n                continue\n            boxes = np.array([p['box'] for p in plist], dtype=np.float32)\n            scs = np.array([p['score'] for p in plist], dtype=np.float32)\n            keep = nms_xyxy(boxes, scs, iou_threshold=0.5)\n            if len(keep) == 0:\n                continue\n            if cid not in gts_for_frame_by_class:\n                gts_for_frame_by_class[cid] = []\n            for idx in keep:\n                gts_for_frame_by_class[cid].append({'frame_id': frame_index, 'box': boxes[idx]})\n\n        # Append to accumulators\n        for cid, plist in preds_for_frame_by_class.items():\n            if cid not in preds_by_class:\n                preds_by_class[cid] = []\n            preds_by_class[cid].extend(plist)\n\n        for cid, glist in gts_for_frame_by_class.items():\n            if cid not in gts_by_class:\n                gts_by_class[cid] = []\n            gts_by_class[cid].extend(glist)\n\n        # Compute running proxy mAP (can be computationally acceptable since detections per frame are small)\n        running_map = compute_proxy_map(preds_by_class, gts_by_class, iou_threshold=0.5)\n\n        # 4.4 Handle Output: draw detections on frame and write to output video\n        # Draw bounding boxes and labels\n        for det in detections_for_frame:\n            x1, y1, x2, y2 = det['box'].astype(int)\n            score = det['score']\n            cid = det['class_id']\n            color = class_color(cid)\n            label_name = labels[cid] if 0 <= cid < len(labels) else f\"id_{cid}\"\n\n            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n            label_text = f\"{label_name}: {score:.2f}\"\n            # Put text background for readability\n            (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame_bgr, (x1, max(0, y1 - th - baseline - 4)), (x1 + tw + 4, y1), color, -1)\n            cv2.putText(frame_bgr, label_text, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n        # Overlay running mAP\n        map_text = f\"Proxy mAP@0.5: {running_map:.3f}\"\n        cv2.rectangle(frame_bgr, (5, 5), (5 + 210, 30), (0, 0, 0), -1)\n        cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2, cv2.LINE_AA)\n\n        writer.write(frame_bgr)\n\n        frame_index += 1\n\n        # 4.5 Loop Continuation: handled by while, will break at end-of-file\n\n    # Final proxy mAP after full video processed\n    final_map = compute_proxy_map(preds_by_class, gts_by_class, iou_threshold=0.5)\n    elapsed = time.time() - start_time\n\n    # Print summary\n    print(\"Processing complete.\")\n    print(f\"Input video: {INPUT_PATH}\")\n    print(f\"Output video: {OUTPUT_PATH}\")\n    print(f\"Frames processed: {frame_index}\")\n    print(f\"Proxy mAP@0.5 (no GT, NMS-based): {final_map:.4f}\")\n    print(f\"Total time: {elapsed:.2f}s  ({(frame_index / max(elapsed, 1e-6)):.2f} FPS)\")\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_54c5_psg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 168.257,
      "total_cost": 0.112335,
      "observations": [
        {
          "id": "time-23-47-28-742136_chatcmpl-D5f0bReS3ApREiNslEkPcZDo7i6je",
          "traceId": "2cea11ed",
          "type": "GENERATION",
          "name": "2cea_psg_gen_attempt#1",
          "startTime": "2026-02-04T21:47:28.742000+00:00",
          "endTime": "2026-02-04T21:50:16.999000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.112335,
          "latency": 168257.0,
          "totalTokens": 12546,
          "costDetails": {
            "total": 0.112335
          },
          "completionTokens": 11046,
          "promptTokens": 1500,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:58:08.604Z",
      "createdAt": "2026-02-04T21:47:28.909Z",
      "environment": "default"
    },
    {
      "id": "58c95614",
      "timestamp": "2026-02-04T21:43:15.452000+00:00",
      "name": "58c9_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.1 Import Interpreter exactly as specified\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from configuration)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')  # Convert from provided string to float\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (if needed)\ndef load_labels(label_path):\n    labels = []\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            lbl = line.strip()\n            if lbl:\n                labels.append(lbl)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input tensor info\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']\n# Expected shape: [1, height, width, channels]\nin_height, in_width = int(input_shape[1]), int(input_shape[2])\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Helper to determine output indices for boxes/classes/scores/num\ndef determine_output_indices(output_details):\n    # Attempt to use names first\n    boxes_idx = classes_idx = scores_idx = num_idx = None\n    for i, d in enumerate(output_details):\n        name = str(d.get('name', '')).lower()\n        shape = d.get('shape', [])\n        if 'box' in name:\n            boxes_idx = i\n        elif 'class' in name:\n            classes_idx = i\n        elif 'score' in name:\n            scores_idx = i\n        elif 'num' in name:\n            num_idx = i\n\n    # Fallback by shapes if necessary\n    if boxes_idx is None:\n        for i, d in enumerate(output_details):\n            shape = d.get('shape', [])\n            if len(shape) == 3 and shape[-1] == 4:\n                boxes_idx = i\n                break\n\n    if num_idx is None:\n        for i, d in enumerate(output_details):\n            shape = d.get('shape', [])\n            if np.prod(shape) == 1:\n                num_idx = i\n                break\n\n    # We will resolve classes/scores (both [1, num]) after first inference reading ranges\n    return boxes_idx, classes_idx, scores_idx, num_idx\n\nboxes_idx, classes_idx, scores_idx, num_idx = determine_output_indices(output_details)\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# 2.1 Acquire input data: Open video file\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Prepare VideoWriter with same resolution as input video\norig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0 or np.isnan(fps):\n    fps = 30.0  # Fallback if FPS not set in the file\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_width, orig_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n# Helper functions for Phase 2 preprocessing\ndef preprocess_frame(frame_bgr):\n    # Resize to model input size and convert BGR to RGB\n    resized = cv2.resize(frame_bgr, (in_width, in_height))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(rgb, axis=0)\n\n    # 2.3 Quantization handling\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # If model expects uint8, ensure dtype is uint8\n        input_data = np.asarray(input_data, dtype=input_dtype)\n    return input_data\n\n# =========================\n# Functions for Phases 3 and 4\n# =========================\n\ndef run_inference(input_data):\n    # Phase 3: Inference\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\ndef fetch_raw_outputs():\n    # 4.1 Get output tensors\n    outputs = []\n    for d in output_details:\n        outputs.append(interpreter.get_tensor(d['index']))\n    return outputs\n\ndef resolve_class_score_indices(outputs, classes_idx, scores_idx):\n    # Both classes and scores likely shaped [1, num]; determine by value ranges.\n    def is_scores(arr):\n        # Scores are typically in [0, 1]\n        mn = float(np.min(arr))\n        mx = float(np.max(arr))\n        return (mn >= 0.0) and (mx <= 1.0)\n\n    if classes_idx is None or scores_idx is None:\n        # Find two outputs with shape [1, num] (float)\n        candidates = []\n        for i, d in enumerate(output_details):\n            shape = d.get('shape', [])\n            if len(shape) == 2 and shape[0] == 1:\n                candidates.append(i)\n        # Remove already known indices like num or boxes\n        if num_idx in candidates:\n            candidates.remove(num_idx)\n        if boxes_idx in candidates:\n            candidates.remove(boxes_idx)\n\n        # Evaluate candidates after reading their values\n        if len(candidates) >= 2:\n            arr0 = outputs[candidates[0]]\n            arr1 = outputs[candidates[1]]\n            if is_scores(arr0):\n                scores_idx = candidates[0]\n                classes_idx = candidates[1]\n            elif is_scores(arr1):\n                scores_idx = candidates[1]\n                classes_idx = candidates[0]\n            else:\n                # Fallback: assume first is classes, second is scores\n                classes_idx = candidates[0]\n                scores_idx = candidates[1]\n        elif len(candidates) == 1:\n            # If only one candidate, try to decide\n            arr = outputs[candidates[0]]\n            if is_scores(arr):\n                scores_idx = candidates[0]\n            else:\n                classes_idx = candidates[0]\n        # else leave as is (unlikely)\n    return classes_idx, scores_idx\n\ndef scale_and_clip_boxes(boxes_norm, frame_w, frame_h):\n    # boxes_norm expected as [N, 4] with [ymin, xmin, ymax, xmax] in [0,1]\n    boxes_px = []\n    for (ymin, xmin, ymax, xmax) in boxes_norm:\n        x1 = int(max(0, min(frame_w - 1, xmin * frame_w)))\n        y1 = int(max(0, min(frame_h - 1, ymin * frame_h)))\n        x2 = int(max(0, min(frame_w - 1, xmax * frame_w)))\n        y2 = int(max(0, min(frame_h - 1, ymax * frame_h)))\n        # Ensure proper ordering\n        x1, x2 = sorted((x1, x2))\n        y1, y2 = sorted((y1, y2))\n        boxes_px.append((x1, y1, x2, y2))\n    return boxes_px\n\ndef iou(boxA, boxB):\n    # boxes are (x1, y1, x2, y2)\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0, xB - xA)\n    inter_h = max(0, yB - yA)\n    interArea = inter_w * inter_h\n    if interArea <= 0:\n        return 0.0\n    boxAArea = max(0, (boxA[2] - boxA[0])) * max(0, (boxA[3] - boxA[1]))\n    boxBArea = max(0, (boxB[2] - boxB[0])) * max(0, (boxB[3] - boxB[1]))\n    denom = float(boxAArea + boxBArea - interArea)\n    if denom <= 0:\n        return 0.0\n    return interArea / denom\n\ndef match_and_update_metrics(prev_dets, curr_dets, preds_by_class, gt_counts_by_class, iou_thresh=0.5):\n    \"\"\"\n    prev_dets: list of dicts with keys: class_id, score, box (x1,y1,x2,y2)\n    curr_dets: same format (current frame)\n    preds_by_class: dict[class_id] -> list of (score, is_tp)\n    gt_counts_by_class: dict[class_id] -> int (accumulated pseudo ground-truth instances)\n    This matches current detections to previous frame detections of the same class.\n    \"\"\"\n    # Organize by class\n    prev_by_cls = {}\n    curr_by_cls = {}\n    for d in prev_dets:\n        prev_by_cls.setdefault(d['class_id'], []).append(d)\n    for d in curr_dets:\n        curr_by_cls.setdefault(d['class_id'], []).append(d)\n\n    all_classes = set(prev_by_cls.keys()).union(set(curr_by_cls.keys()))\n\n    for cls in all_classes:\n        prev_list = prev_by_cls.get(cls, [])\n        curr_list = curr_by_cls.get(cls, [])\n        gt_counts_by_class[cls] = gt_counts_by_class.get(cls, 0) + len(prev_list)\n\n        if len(curr_list) == 0:\n            continue\n\n        # Build IoU matrix between prev (GT) and curr (preds)\n        if len(prev_list) == 0:\n            # No GT, all current are FPs\n            for d in curr_list:\n                preds_by_class.setdefault(cls, []).append((d['score'], 0))\n            continue\n\n        iou_matrix = np.zeros((len(prev_list), len(curr_list)), dtype=np.float32)\n        for i, gt in enumerate(prev_list):\n            for j, pr in enumerate(curr_list):\n                iou_matrix[i, j] = iou(gt['box'], pr['box'])\n\n        # Greedy matching on IoU\n        matched_gt = set()\n        matched_pr = set()\n        # Flatten matrix with indices sorted by IoU descending\n        pairs = [(i, j, iou_matrix[i, j]) for i in range(iou_matrix.shape[0]) for j in range(iou_matrix.shape[1])]\n        pairs.sort(key=lambda x: x[2], reverse=True)\n\n        for i, j, v in pairs:\n            if v < iou_thresh:\n                break\n            if i in matched_gt or j in matched_pr:\n                continue\n            matched_gt.add(i)\n            matched_pr.add(j)\n\n        # Assign TP/FP for predictions\n        for j, pr in enumerate(curr_list):\n            is_tp = 1 if j in matched_pr else 0\n            preds_by_class.setdefault(cls, []).append((pr['score'], is_tp))\n\ndef compute_ap(precision, recall):\n    \"\"\"\n    VOC-style AP computation with precision envelope.\n    precision, recall are 1D numpy arrays sorted by descending score.\n    \"\"\"\n    # Append sentinel values at both ends\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([0.0], precision, [0.0]))\n\n    # Precision envelope\n    for i in range(len(mpre) - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n\n    # Sum over recall steps where recall changes\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = 0.0\n    for i in idx:\n        ap += (mrec[i + 1] - mrec[i]) * mpre[i + 1]\n    return ap\n\ndef compute_map(preds_by_class, gt_counts_by_class):\n    \"\"\"\n    Computes mAP over classes present in gt_counts_by_class with gt > 0.\n    preds_by_class: dict[class_id] -> list of (score, is_tp)\n    \"\"\"\n    aps = []\n    for cls, preds in preds_by_class.items():\n        gt = gt_counts_by_class.get(cls, 0)\n        if gt <= 0:\n            continue\n        # Sort predictions by score descending\n        preds_sorted = sorted(preds, key=lambda x: -x[0])\n        tps = np.array([int(p[1] == 1) for p in preds_sorted], dtype=np.float32)\n        fps = 1.0 - tps\n        cum_tps = np.cumsum(tps)\n        cum_fps = np.cumsum(fps)\n        # Avoid division by zero\n        denom = cum_tps + cum_fps\n        denom[denom == 0] = 1e-12\n        precision = cum_tps / denom\n        recall = cum_tps / float(gt)\n        ap = compute_ap(precision, recall)\n        aps.append(ap)\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\n# =========================\n# Phase 2.4 Loop control and processing\n# =========================\n\nprev_detections = []  # detections from previous frame for temporal mAP proxy\npreds_by_class = {}   # class_id -> list of (score, is_tp)\ngt_counts_by_class = {}  # class_id -> gt count\n\nframe_index = 0\nstart_time = time.time()\n\nwhile True:\n    # 2.1 Acquire next frame\n    ret, frame = cap.read()\n    if not ret:\n        break  # End of video\n\n    frame_h, frame_w = frame.shape[0], frame.shape[1]\n\n    # 2.2 Preprocess frame into input tensor\n    input_data = preprocess_frame(frame)\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    run_inference(input_data)\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling\n    # =========================\n\n    # 4.1 Get raw outputs\n    outputs = fetch_raw_outputs()\n\n    # Resolve indices for classes/scores at first iteration if needed\n    classes_idx, scores_idx = resolve_class_score_indices(outputs, classes_idx, scores_idx)\n\n    # Extract outputs\n    boxes = outputs[boxes_idx]\n    # boxes shape expected [1, num, 4]\n    boxes = np.squeeze(boxes, axis=0)\n\n    # num detections\n    if num_idx is not None:\n        num_raw = outputs[num_idx]\n        num = int(np.squeeze(num_raw).astype(np.int32))\n    else:\n        # Fallback: use length of scores or boxes\n        num = boxes.shape[0]\n\n    # Classes and scores\n    if classes_idx is not None:\n        classes = outputs[classes_idx]\n        classes = np.squeeze(classes, axis=0).astype(np.int32)\n    else:\n        classes = np.zeros((num,), dtype=np.int32)\n\n    if scores_idx is not None:\n        scores = outputs[scores_idx]\n        scores = np.squeeze(scores, axis=0).astype(np.float32)\n    else:\n        # If no scores available, set to 1.0 for all\n        scores = np.ones((num,), dtype=np.float32)\n\n    # Cap arrays to num\n    if boxes.shape[0] > num:\n        boxes = boxes[:num]\n    if classes.shape[0] > num:\n        classes = classes[:num]\n    if scores.shape[0] > num:\n        scores = scores[:num]\n\n    # 4.2 Interpret Results: thresholding, label mapping\n    # 4.3 Post-processing: confidence filtering, coordinate scaling, clipping\n    mask = scores >= CONF_THRESHOLD\n    filtered_boxes_norm = boxes[mask]\n    filtered_scores = scores[mask]\n    filtered_classes = classes[mask].astype(int)\n\n    # Scale normalized boxes to original frame size\n    filtered_boxes_px = scale_and_clip_boxes(filtered_boxes_norm, frame_w, frame_h)\n\n    # Build current detections list for temporal matching\n    current_detections = []\n    for b, s, c in zip(filtered_boxes_px, filtered_scores, filtered_classes):\n        current_detections.append({'class_id': int(c), 'score': float(s), 'box': b})\n\n    # Update temporal proxy metrics (self-consistency across frames)\n    if frame_index > 0:\n        match_and_update_metrics(prev_detections, current_detections, preds_by_class, gt_counts_by_class, iou_thresh=0.5)\n\n    # Compute running mAP\n    running_map = compute_map(preds_by_class, gt_counts_by_class)\n\n    # 4.4 Handle Output: draw detections and running mAP, then write frame\n    # Draw detections\n    for det in current_detections:\n        x1, y1, x2, y2 = det['box']\n        cls_id = det['class_id']\n        score = det['score']\n        color = (0, 255, 0)  # Green boxes\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness=2)\n\n        # Prepare label string\n        if 0 <= cls_id < len(labels):\n            label_name = labels[cls_id]\n        else:\n            label_name = f\"id_{cls_id}\"\n        label_text = f\"{label_name}: {score:.2f}\"\n        # Draw label background for readability\n        ((tw, th), _) = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        tx1, ty1 = x1, max(0, y1 - th - 4)\n        tx2, ty2 = x1 + tw + 4, y1\n        cv2.rectangle(frame, (tx1, ty1), (tx2, ty2), color, thickness=-1)\n        cv2.putText(frame, label_text, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Overlay running mAP (temporal proxy)\n    map_text = f\"mAP (temporal proxy): {running_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (30, 30, 230), 2, cv2.LINE_AA)\n\n    # Write frame to output\n    out_writer.write(frame)\n\n    # 4.5 Loop continuation\n    prev_detections = current_detections\n    frame_index += 1\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nout_writer.release()\n\n# Final report\nfinal_map = compute_map(preds_by_class, gt_counts_by_class)\nelapsed = time.time() - start_time\nprint(f\"Processing completed.\")\nprint(f\"Frames processed: {frame_index}\")\nprint(f\"Elapsed time: {elapsed:.2f} sec, Avg FPS: {frame_index / elapsed if elapsed > 0 else 0:.2f}\")\nprint(f\"Final mAP (temporal proxy): {final_map:.4f}\")\nprint(f\"Output saved to: {OUTPUT_PATH}\")"
      },
      "session_id": "gpt-5-2025-08-07_54c5_psg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 190.821,
      "total_cost": 0.090461,
      "observations": [
        {
          "id": "time-23-43-15-864634_chatcmpl-D5ewWlYpGLmSrKG2dehUmxxeayBQH",
          "traceId": "58c95614",
          "type": "GENERATION",
          "name": "58c9_psg_gen_attempt#1",
          "startTime": "2026-02-04T21:43:15.864000+00:00",
          "endTime": "2026-02-04T21:46:26.685000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.090461,
          "latency": 190821.0,
          "totalTokens": 10517,
          "costDetails": {
            "total": 0.090461
          },
          "completionTokens": 9017,
          "promptTokens": 1500,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:47:22.734Z",
      "createdAt": "2026-02-04T21:43:15.973Z",
      "environment": "default"
    },
    {
      "id": "1d7ddd9c",
      "timestamp": "2026-02-04T21:39:25.612000+00:00",
      "name": "1d7d_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection using a TFLite SSD MobileNet v1 model on a single input video file.\nIt writes an output video with bounding boxes and labels drawn for detected objects and overlays a\nper-frame \"mAP (vs previous frame)\" value computed by comparing current detections against detections\nfrom the previous frame (as pseudo ground truth). This provides an internal consistency metric when\nground truth annotations are unavailable.\n\nPhases implemented per Programming Guideline:\n- Phase 1: Setup (imports, paths, load labels, load interpreter, get model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video reading, resizing, normalization/quantization)\n- Phase 3: Inference (set tensors, invoke)\n- Phase 4: Output Interpretation & Handling Loop (parse detections, thresholding, scaling, clipping, draw)\n- Phase 5: Cleanup (release video resources)\n\nConfiguration Parameters:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'\n- input_path: 'data/object_detection/sheeps.mp4'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4'\n- confidence_threshold: 0.5\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1.1: Imports (Interpreter import must be literal per guideline)\ntry:\n    from ai_edge_litert.interpreter import Interpreter\nexcept Exception as e:\n    print(\"ERROR: Failed to import Interpreter from ai_edge_litert.interpreter.\")\n    print(\"Details:\", str(e))\n    raise SystemExit(1)\n\n\n# ----------------------------- Utility Functions -----------------------------\n\ndef load_labels(label_file_path):\n    \"\"\"\n    Phase 1.3: Load Labels.\n    Reads the label file line-by-line and returns a list of label strings.\n    Empty lines are ignored.\n    \"\"\"\n    labels = []\n    try:\n        with open(label_file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name != \"\":\n                    labels.append(name)\n    except FileNotFoundError:\n        print(f\"WARNING: Label file not found at '{label_file_path}'. Proceeding without labels.\")\n    return labels\n\n\ndef preprocess_frame(frame_bgr, input_shape, input_dtype, input_quant):\n    \"\"\"\n    Phase 2.2 and 2.3: Preprocess Data and Quantization Handling.\n    - Resize to model input size.\n    - Convert BGR to RGB.\n    - Expand dims to [1, H, W, 3].\n    - If floating model: normalize: (x - 127.5) / 127.5\n    - If int8 quantized: apply scale/zero_point.\n    - If uint8: pass through.\n    \"\"\"\n    _, in_h, in_w, in_c = input_shape  # Typically [1, H, W, 3]\n    assert in_c == 3, \"Expected 3-channel input.\"\n\n    # Resize\n    resized = cv2.resize(frame_bgr, (in_w, in_h), interpolation=cv2.INTER_NEAREST)\n    # BGR to RGB\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n    # Expand dims\n    input_data = np.expand_dims(rgb, axis=0)\n\n    # Cast/normalize/quantize according to dtype\n    floating_model = (input_dtype == np.float32)\n\n    if floating_model:\n        # Normalize to [-1, 1]\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # Handle quantized types\n        if input_dtype == np.uint8:\n            input_data = input_data.astype(np.uint8)\n        elif input_dtype == np.int8:\n            # Apply quantization parameters if available\n            scale, zero_point = input_quant\n            if scale is None or scale == 0:\n                # Fallback: just cast\n                input_data = input_data.astype(np.int8)\n            else:\n                # Quantize from [0,255] uint8-like to int8 using scale and zp\n                input_data = input_data.astype(np.float32)\n                input_data = input_data / 255.0  # normalize to [0,1] before quant\n                input_data = input_data / scale + zero_point\n                input_data = np.round(input_data).astype(np.int8)\n        else:\n            # Unexpected dtype; do a safe cast\n            input_data = input_data.astype(input_dtype)\n\n    return input_data\n\n\ndef dequantize_if_needed(tensor, tensor_detail):\n    \"\"\"\n    Dequantize output tensor if it has quantization parameters and is not float32.\n    \"\"\"\n    if tensor_detail.get('dtype', None) == np.float32:\n        return tensor\n    # Try to get quantization parameters\n    quant = tensor_detail.get('quantization', (0.0, 0))\n    scale, zero_point = (quant if isinstance(quant, (tuple, list)) and len(quant) == 2 else (0.0, 0))\n    if scale is not None and scale != 0:\n        # Dequantize to float32\n        return (tensor.astype(np.float32) - float(zero_point)) * float(scale)\n    # If no valid quantization info, cast to float32\n    return tensor.astype(np.float32)\n\n\ndef clip_bbox(xmin, ymin, xmax, ymax, width, height):\n    \"\"\"\n    Phase 4.3: Bounding box clipping to frame boundaries.\n    \"\"\"\n    xmin = max(0, min(int(xmin), width - 1))\n    ymin = max(0, min(int(ymin), height - 1))\n    xmax = max(0, min(int(xmax), width - 1))\n    ymax = max(0, min(int(ymax), height - 1))\n    # Ensure proper ordering\n    if xmax < xmin:\n        xmin, xmax = xmax, xmin\n    if ymax < ymin:\n        ymin, ymax = ymax, ymin\n    return xmin, ymin, xmax, ymax\n\n\ndef iou(box_a, box_b):\n    \"\"\"\n    Compute IoU between two boxes given as [x1, y1, x2, y2].\n    \"\"\"\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n\n    inter_w = max(0, inter_x2 - inter_x1 + 1)\n    inter_h = max(0, inter_y2 - inter_y1 + 1)\n    inter_area = inter_w * inter_h\n\n    area_a = max(0, ax2 - ax1 + 1) * max(0, ay2 - ay1 + 1)\n    area_b = max(0, bx2 - bx1 + 1) * max(0, by2 - by1 + 1)\n\n    union = area_a + area_b - inter_area\n    if union <= 0:\n        return 0.0\n    return inter_area / union\n\n\ndef compute_ap_for_class(preds, gts, iou_thresh=0.5):\n    \"\"\"\n    Compute Average Precision (AP) for a single class.\n    preds: list of tuples (score, bbox) where bbox is [x1, y1, x2, y2]\n    gts: list of bboxes [x1, y1, x2, y2]\n    Returns AP as float in [0,1].\n    \"\"\"\n    num_gts = len(gts)\n    if num_gts == 0:\n        # Undefined AP if no GT; caller should skip this class from mAP\n        return None\n\n    if len(preds) == 0:\n        return 0.0\n\n    # Sort predictions by descending score\n    preds_sorted = sorted(preds, key=lambda x: x[0], reverse=True)\n    gt_used = np.zeros((num_gts,), dtype=bool)\n\n    tp = np.zeros((len(preds_sorted),), dtype=np.float32)\n    fp = np.zeros((len(preds_sorted),), dtype=np.float32)\n\n    # Greedy matching\n    for i, (score, pb) in enumerate(preds_sorted):\n        best_iou = 0.0\n        best_j = -1\n        for j, gb in enumerate(gts):\n            if gt_used[j]:\n                continue\n            iou_val = iou(pb, gb)\n            if iou_val > best_iou:\n                best_iou = iou_val\n                best_j = j\n        if best_iou >= iou_thresh and best_j >= 0:\n            tp[i] = 1.0\n            gt_used[best_j] = True\n        else:\n            fp[i] = 1.0\n\n    # Precision-Recall curve\n    cum_tp = np.cumsum(tp)\n    cum_fp = np.cumsum(fp)\n    recalls = cum_tp / max(num_gts, 1)\n    precisions = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n\n    # Compute AP as area under precision envelope\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n    for i in range(len(mpre) - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n\n    # Sum over recall steps where it changes\n    ap = 0.0\n    for i in range(1, len(mrec)):\n        if mrec[i] != mrec[i - 1]:\n            ap += (mrec[i] - mrec[i - 1]) * mpre[i]\n    return float(ap)\n\n\ndef compute_map_against_previous(current_dets, prev_dets, iou_thresh=0.5):\n    \"\"\"\n    Compute mAP for current frame detections against previous frame detections (pseudo-GT).\n    - current_dets: list of dicts {class_id:int, score:float, bbox:[x1,y1,x2,y2]}\n    - prev_dets: list of dicts, same structure\n    Returns:\n    - mAP (float) if computable, else None when no GT exists for any class.\n    \"\"\"\n    # Organize detections by class\n    preds_by_class = {}\n    gts_by_class = {}\n    for d in current_dets:\n        preds_by_class.setdefault(d['class_id'], []).append((d['score'], d['bbox']))\n    for g in prev_dets:\n        gts_by_class.setdefault(g['class_id'], []).append(g['bbox'])\n\n    aps = []\n    all_classes = set(list(preds_by_class.keys()) + list(gts_by_class.keys()))\n    for cid in all_classes:\n        preds = preds_by_class.get(cid, [])\n        gts = gts_by_class.get(cid, [])\n        ap = compute_ap_for_class(preds, gts, iou_thresh=iou_thresh)\n        if ap is not None:\n            aps.append(ap)\n\n    if len(aps) == 0:\n        return None\n    return float(np.mean(aps))\n\n\ndef draw_detections(frame, detections, labels):\n    \"\"\"\n    Draw bounding boxes and labels onto the frame.\n    \"\"\"\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        cid = det['class_id']\n        score = det['score']\n        label_name = None\n        if labels and 0 <= cid < len(labels):\n            label_name = labels[cid]\n        else:\n            label_name = f\"id:{cid}\"\n\n        color = (0, 255, 0)  # Green box\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{label_name}: {score:.2f}\"\n        # Text background\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - 6), (x1 + tw + 2, y1), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n\n# ----------------------------- Main Application ------------------------------\n\ndef main():\n    # Phase 1.2: Paths/Parameters from Configuration\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = 0.5\n\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n    # Phase 1.3: Load Labels\n    labels = load_labels(label_path)\n\n    # Phase 1.4: Load Interpreter\n    if not os.path.exists(model_path):\n        print(f\"ERROR: Model file not found at '{model_path}'.\")\n        raise SystemExit(1)\n\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Phase 1.5: Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    if len(input_details) != 1:\n        print(\"WARNING: Model expects more than one input tensor; using the first.\")\n\n    in_detail = input_details[0]\n    input_index = in_detail['index']\n    input_shape = in_detail['shape']\n    input_dtype = in_detail['dtype']\n    input_quant = in_detail.get('quantization', (None, None))\n\n    # Identify output tensors: boxes, classes, scores, num\n    # Phase 4.1 will use these indices to retrieve outputs\n    out_indices = {'boxes': None, 'classes': None, 'scores': None, 'num': None}\n    for od in output_details:\n        od_shape = od['shape']\n        if len(od_shape) == 3 and od_shape[-1] == 4:\n            out_indices['boxes'] = od['index']\n        elif len(od_shape) == 2 and od_shape[-1] >= 1:\n            # Could be classes or scores; need dtype heuristic\n            if od.get('dtype') in (np.float32, np.float16):\n                out_indices['scores'] = od['index']\n            else:\n                out_indices['classes'] = od['index']\n        elif len(od_shape) == 1 and od_shape[0] == 1:\n            out_indices['num'] = od['index']\n\n    # Fallback if not identified by heuristic (handle typical SSD order)\n    if any(v is None for v in out_indices.values()):\n        # Attempt standard order by assuming 4 outputs\n        if len(output_details) >= 4:\n            out_indices['boxes'] = output_details[0]['index'] if out_indices['boxes'] is None else out_indices['boxes']\n            out_indices['classes'] = output_details[1]['index'] if out_indices['classes'] is None else out_indices['classes']\n            out_indices['scores'] = output_details[2]['index'] if out_indices['scores'] is None else out_indices['scores']\n            out_indices['num'] = output_details[3]['index'] if out_indices['num'] is None else out_indices['num']\n\n    # Phase 2.1: Acquire Input Data (video file)\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"ERROR: Unable to open input video: '{input_path}'.\")\n        raise SystemExit(1)\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 30.0  # reasonable default if unavailable\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        print(f\"ERROR: Unable to open output video writer: '{output_path}'.\")\n        cap.release()\n        raise SystemExit(1)\n\n    # Tracking previous frame detections for pseudo-GT mAP computation\n    prev_detections = []\n    running_map_sum = 0.0\n    running_map_count = 0\n    frame_count = 0\n\n    # Phase 2.4: Loop Control - process entire video\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # Phase 2.2 & 2.3: Preprocess\n        input_data = preprocess_frame(frame, input_shape, input_dtype, input_quant)\n\n        # Phase 3.1: Set Input Tensor\n        interpreter.set_tensor(input_index, input_data)\n\n        # Phase 3.2: Run Inference\n        start_infer = time.time()\n        interpreter.invoke()\n        infer_time_ms = (time.time() - start_infer) * 1000.0\n\n        # Phase 4.1: Get Output Tensors\n        # Retrieve and dequantize if necessary\n        boxes_raw = interpreter.get_tensor(out_indices['boxes'])\n        classes_raw = interpreter.get_tensor(out_indices['classes'])\n        scores_raw = interpreter.get_tensor(out_indices['scores'])\n        num_raw = interpreter.get_tensor(out_indices['num'])\n\n        # Dequantize outputs if needed\n        # Identify output details dict by index for dequantization\n        out_detail_by_index = {od['index']: od for od in output_details}\n        boxes = dequantize_if_needed(boxes_raw, out_detail_by_index[out_indices['boxes']])\n        classes = dequantize_if_needed(classes_raw, out_detail_by_index[out_indices['classes']])\n        scores = dequantize_if_needed(scores_raw, out_detail_by_index[out_indices['scores']])\n        num_det = int(np.squeeze(num_raw).astype(np.int32))\n\n        boxes = np.squeeze(boxes)\n        classes = np.squeeze(classes).astype(np.int32)\n        scores = np.squeeze(scores).astype(np.float32)\n\n        # Phase 4.2: Interpret Results - extract, map labels, prepare detections\n        detections = []\n        for i in range(num_det):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n            # SSD boxes are y_min, x_min, y_max, x_max normalized [0,1]\n            y_min, x_min, y_max, x_max = boxes[i]\n            # Phase 4.3: Post-processing - scale to pixel coords and clip\n            x1 = int(x_min * width)\n            y1 = int(y_min * height)\n            x2 = int(x_max * width)\n            y2 = int(y_max * height)\n            x1, y1, x2, y2 = clip_bbox(x1, y1, x2, y2, width, height)\n\n            cid = int(classes[i])\n            detections.append({\n                'class_id': cid,\n                'score': score,\n                'bbox': [x1, y1, x2, y2]\n            })\n\n        # Draw detections (labels if available)\n        draw_detections(frame, detections, labels)\n\n        # Compute per-frame mAP against previous frame detections (pseudo-GT)\n        frame_map = compute_map_against_previous(detections, prev_detections, iou_thresh=0.5)\n        if frame_map is not None:\n            running_map_sum += frame_map\n            running_map_count += 1\n\n        # Overlay metrics text (mAP and inference time)\n        if frame_map is not None:\n            map_text = f\"mAP (vs prev): {frame_map*100.0:.2f}%\"\n        else:\n            map_text = \"mAP (vs prev): N/A\"\n        inf_text = f\"Inference: {infer_time_ms:.1f} ms\"\n\n        cv2.putText(frame, map_text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50, 200, 50), 2, cv2.LINE_AA)\n        cv2.putText(frame, inf_text, (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50, 200, 50), 2, cv2.LINE_AA)\n\n        # Phase 4.4: Handle Output - write frame to output video\n        writer.write(frame)\n\n        # Phase 4.5: Loop continuation - update previous detections\n        prev_detections = detections\n\n    # Phase 5.1: Cleanup\n    cap.release()\n    writer.release()\n\n    # Final summary\n    if running_map_count > 0:\n        overall_map = running_map_sum / running_map_count\n        print(f\"Processed {frame_count} frames.\")\n        print(f\"Average per-frame mAP (vs previous frame): {overall_map*100.0:.2f}% over {running_map_count} evaluable frames.\")\n    else:\n        print(f\"Processed {frame_count} frames.\")\n        print(\"Average per-frame mAP could not be computed (no evaluable frames with previous detections).\")\n\n    print(f\"Output video written to: {output_path}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_54c5_psg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 173.86,
      "total_cost": 0.087191,
      "observations": [
        {
          "id": "time-23-39-25-845993_chatcmpl-D5esownDeMC1tG88jNKT6zYncUG0f",
          "traceId": "1d7ddd9c",
          "type": "GENERATION",
          "name": "1d7d_psg_gen_attempt#1",
          "startTime": "2026-02-04T21:39:25.845000+00:00",
          "endTime": "2026-02-04T21:42:19.705000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.087191,
          "latency": 173860.0,
          "totalTokens": 10190,
          "costDetails": {
            "total": 0.087191
          },
          "completionTokens": 8690,
          "promptTokens": 1500,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:43:10.404Z",
      "createdAt": "2026-02-04T21:39:26.135Z",
      "environment": "default"
    },
    {
      "id": "c977579e",
      "timestamp": "2026-02-04T21:36:36.783000+00:00",
      "name": "c977_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nObject Detection via a video file on Raspberry Pi 4B using a TFLite SSD model.\n\nPhases implemented per guideline:\n- Phase 1: Setup (imports, parameters, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video file reading, resizing, dtype handling)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results (labels mapping, boxes/classes/scores extraction)\n  - 4.3 Post-processing (thresholding, scaling, clipping)\n  - 4.4 Handle Output (draw rectangles and labels, write output video, overlay proxy mAP)\n- Phase 5: Cleanup\n\nNote on mAP:\n- Since no ground-truth annotations are provided, a true mAP cannot be computed.\n- This script computes a practical proxy metric: per-class AP_proxy = mean confidence of detections for that class,\n  and mAP_proxy = mean(AP_proxy) across classes that had at least one detection. It is reported on-console and overlaid on the video.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Interpreter from ai_edge_litert\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef load_labels(label_path):\n    labels = []\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            name = line.strip()\n            if name != '':\n                labels.append(name)\n    return labels\n\ndef ensure_dir_for_file(file_path):\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\ndef iou(boxA, boxB):\n    # boxes in (x1, y1, x2, y2) format\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interW = max(0, xB - xA)\n    interH = max(0, yB - yA)\n    interArea = interW * interH\n    if interArea <= 0:\n        return 0.0\n    boxAArea = max(0, (boxA[2] - boxA[0])) * max(0, (boxA[3] - boxA[1]))\n    boxBArea = max(0, (boxB[2] - boxB[0])) * max(0, (boxB[3] - boxB[1]))\n    denom = boxAArea + boxBArea - interArea\n    if denom <= 0:\n        return 0.0\n    return interArea / denom\n\ndef clip_box_xyxy(box, w, h):\n    x1, y1, x2, y2 = box\n    x1 = max(0, min(int(round(x1)), w - 1))\n    y1 = max(0, min(int(round(y1)), h - 1))\n    x2 = max(0, min(int(round(x2)), w - 1))\n    y2 = max(0, min(int(round(y2)), h - 1))\n    return [x1, y1, x2, y2]\n\ndef make_color_palette(num_classes):\n    # Fixed palette for reproducibility; HSV-based\n    colors = []\n    for i in range(num_classes):\n        hue = int(179 * (i / max(1, num_classes)))\n        color = cv2.cvtColor(np.uint8([[[hue, 255, 255]]]), cv2.COLOR_HSV2BGR)[0,0,:].tolist()\n        colors.append((int(color[0]), int(color[1]), int(color[2])))\n    if num_classes == 0:\n        colors.append((0, 255, 0))\n    return colors\n\ndef main():\n    # 1.2 Paths/Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = 0.5  # as provided\n\n    # 1.3 Load Labels (conditional/relevant)\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Assume single input tensor\n    input_index = input_details[0]['index']\n    in_dtype = input_details[0]['dtype']\n    in_shape = input_details[0]['shape']  # e.g., [1, height, width, 3]\n    # Some interpreters return tensor index mapping; capture outputs generically\n    output_indices = [od['index'] for od in output_details]\n\n    # Define input size from model\n    if len(in_shape) == 4:\n        batch, in_h, in_w, in_c = in_shape\n    else:\n        raise RuntimeError(\"Unexpected input tensor shape: {}\".format(in_shape))\n\n    # Floating model?\n    floating_model = (in_dtype == np.float32)\n\n    # Prepare color palette for labels\n    colors = make_color_palette(len(labels))\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: open the single video file specified by input_path\n    if not os.path.exists(input_path):\n        raise FileNotFoundError(\"Input video not found: {}\".format(input_path))\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(\"Failed to open input video: {}\".format(input_path))\n\n    # Prepare output writer\n    ensure_dir_for_file(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 25.0  # fallback\n    out_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    out_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (out_w, out_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(\"Failed to open output video for writing: {}\".format(output_path))\n\n    # Storage for proxy mAP computation: per-class list of confidences\n    class_confidences = {}  # class_id -> list of conf scores\n\n    frame_count = 0\n    t_start = time.time()\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # 2.2 Preprocess Data\n        # Convert BGR to RGB for typical TFLite models\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        # Resize to model input size\n        resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n\n        # 2.3 Quantization Handling\n        if floating_model:\n            # Normalize to [-1, 1] as per guideline\n            input_data = (np.float32(resized) - 127.5) / 127.5\n            input_data = np.expand_dims(input_data, axis=0).astype(np.float32)\n        else:\n            # uint8 quantized models expect [0,255]\n            input_data = np.expand_dims(resized, axis=0).astype(in_dtype)\n\n        # Phase 3: Inference\n        interpreter.set_tensor(input_index, input_data)\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling Loop\n        # 4.1 Get Output Tensors\n        # SSD MobileNet typically returns: boxes, classes, scores, num_detections\n        # We'll identify them by shapes/dtypes\n        raw_outputs = [interpreter.get_tensor(idx) for idx in output_indices]\n\n        boxes = None\n        classes_out = None\n        scores = None\n        num = None\n\n        # Heuristic mapping\n        for arr in raw_outputs:\n            arr_np = np.array(arr)\n            if arr_np.ndim == 3 and arr_np.shape[-1] == 4:\n                boxes = arr_np  # shape [1, N, 4]\n            elif arr_np.ndim == 2 and arr_np.shape[0] == 1 and arr_np.shape[1] > 1 and arr_np.dtype in [np.float32, np.float64]:\n                # could be scores or classes depending on dtype\n                # scores are float, classes often float in TFLite SSD\n                # We'll check value ranges later\n                # We'll assign after collecting candidates\n                pass\n            elif arr_np.ndim == 2 and arr_np.shape[0] == 1 and arr_np.shape[1] == 1:\n                num = int(np.squeeze(arr_np).astype(np.int32))\n\n        # Extract classes and scores by inspecting remaining arrays\n        cand = [np.array(a) for a in raw_outputs]\n        # Remove boxes and num from candidates to avoid confusion\n        cand_filt = []\n        for a in cand:\n            if boxes is not None and a.shape == boxes.shape and a.ndim == 3:\n                continue\n            if num is not None and a.size == 1:\n                continue\n            cand_filt.append(a)\n\n        # Among candidates, one is classes (float or int), one is scores (float)\n        classes_arr = None\n        scores_arr = None\n        for a in cand_filt:\n            a_squeezed = np.squeeze(a)\n            if a_squeezed.ndim == 1:\n                # If all values are small integers or near-integers, it's likely classes\n                if np.all(np.abs(a_squeezed - np.round(a_squeezed)) < 1e-3):\n                    classes_arr = a\n                else:\n                    scores_arr = a\n            elif a_squeezed.ndim == 2 and a_squeezed.shape[0] == 1:\n                # Typical shape [1, N]\n                vals = a_squeezed[0]\n                if np.all(np.abs(vals - np.round(vals)) < 1e-3):\n                    classes_arr = a\n                else:\n                    scores_arr = a\n\n        # Fallback if mapping ambiguous: pick by dtype\n        if classes_arr is None or scores_arr is None:\n            for a in cand_filt:\n                if a.dtype in (np.uint8, np.int32, np.int64):\n                    classes_arr = a\n                elif a.dtype in (np.float32, np.float64):\n                    scores_arr = a\n\n        # Squeeze to 1D arrays\n        if boxes is None:\n            raise RuntimeError(\"Could not find detection boxes in model outputs.\")\n        boxes = np.squeeze(boxes)  # [N,4] y_min, x_min, y_max, x_max in normalized coords\n\n        if classes_arr is None or scores_arr is None:\n            raise RuntimeError(\"Could not map classes/scores from model outputs.\")\n\n        classes_out = np.squeeze(classes_arr)  # [N]\n        scores = np.squeeze(scores_arr)        # [N]\n        # Some models return float class indices; cast to int\n        classes_out = classes_out.astype(np.int32)\n\n        # 4.2 Interpret Results: map indices to names, prepare detections\n        # 4.3 Post-processing: confidence thresholding and coordinate scaling/clipping\n        detections = []\n        H, W = frame_bgr.shape[:2]\n        for i in range(boxes.shape[0]):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n            cls_id = int(classes_out[i])\n            # Safety check for labels range\n            label = labels[cls_id] if 0 <= cls_id < len(labels) else f\"id_{cls_id}\"\n            y_min, x_min, y_max, x_max = boxes[i].tolist()\n            # Scale to absolute pixel coordinates\n            x1 = x_min * W\n            y1 = y_min * H\n            x2 = x_max * W\n            y2 = y_max * H\n            x1, y1, x2, y2 = clip_box_xyxy([x1, y1, x2, y2], W, H)\n            detections.append({\n                'bbox': [x1, y1, x2, y2],\n                'score': score,\n                'class_id': cls_id,\n                'label': label\n            })\n\n        # Accumulate confidences for proxy mAP\n        for det in detections:\n            cid = det['class_id']\n            class_confidences.setdefault(cid, []).append(det['score'])\n\n        # Draw detections\n        for det in detections:\n            x1, y1, x2, y2 = det['bbox']\n            cls_id = det['class_id']\n            color = colors[cls_id % len(colors)] if len(colors) > 0 else (0, 255, 0)\n            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n            label_text = \"{}: {:.2f}\".format(det['label'], det['score'])\n            # Background label box\n            (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame_bgr, (x1, y1 - th - baseline), (x1 + tw, y1), color, thickness=-1)\n            cv2.putText(frame_bgr, label_text, (x1, y1 - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n        # Compute running proxy mAP\n        ap_vals = []\n        for cid, conf_list in class_confidences.items():\n            if len(conf_list) > 0:\n                ap_vals.append(float(np.mean(conf_list)))\n        mAP_proxy = float(np.mean(ap_vals)) if len(ap_vals) > 0 else 0.0\n\n        # Overlay mAP proxy and FPS\n        elapsed = time.time() - t_start\n        fps_infer = frame_count / elapsed if elapsed > 0 else 0.0\n        overlay_text = \"mAP(proxy): {:.3f} | FPS: {:.1f}\".format(mAP_proxy, fps_infer)\n        cv2.putText(frame_bgr, overlay_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (20, 220, 20), 2, cv2.LINE_AA)\n\n        # 4.4 Handle Output: write the annotated frame to output video\n        writer.write(frame_bgr)\n\n        # 4.5 Loop Continuation: continue until video ends (handled by while loop)\n\n    # After processing all frames, finalize mAP proxy and report\n    final_ap_per_class = {}\n    for cid, conf_list in class_confidences.items():\n        if len(conf_list) > 0:\n            final_ap_per_class[cid] = float(np.mean(conf_list))\n    final_mAP_proxy = float(np.mean(list(final_ap_per_class.values()))) if len(final_ap_per_class) > 0 else 0.0\n\n    print(\"==== Inference Summary ====\")\n    print(\"Processed frames:\", frame_count)\n    print(\"Classes detected:\", len(final_ap_per_class))\n    # Print top classes by proxy AP\n    if len(final_ap_per_class) > 0:\n        sorted_items = sorted(final_ap_per_class.items(), key=lambda kv: kv[1], reverse=True)\n        for cid, ap in sorted_items[:10]:\n            cname = labels[cid] if 0 <= cid < len(labels) else f\"id_{cid}\"\n            print(f\"Class: {cname:20s} AP_proxy: {ap:.4f}\")\n    print(\"mAP(proxy): {:.4f}\".format(final_mAP_proxy))\n    print(\"Output video written to:\", output_path)\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_54c5_psg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 111.834,
      "total_cost": 0.059581,
      "observations": [
        {
          "id": "time-23-36-37-136594_chatcmpl-D5eq5768YM8NHy5XqZwNTBXOTLhjg",
          "traceId": "c977579e",
          "type": "GENERATION",
          "name": "c977_psg_gen_attempt#1",
          "startTime": "2026-02-04T21:36:37.136000+00:00",
          "endTime": "2026-02-04T21:38:28.970000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.059581,
          "latency": 111834.0,
          "totalTokens": 7429,
          "costDetails": {
            "total": 0.059581
          },
          "completionTokens": 5929,
          "promptTokens": 1500,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:39:19.459Z",
      "createdAt": "2026-02-04T21:36:37.305Z",
      "environment": "default"
    },
    {
      "id": "cb55e848",
      "timestamp": "2026-02-04T21:32:34.797000+00:00",
      "name": "cb55_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TFLite SSD model.\nIt annotates detected objects with bounding boxes and labels, and writes the annotated\nvideo to the specified output path. Additionally, it computes and overlays a running\nmAP (mean Average Precision) estimate based on self-consistency (per-class NMS clusters\nare treated as pseudo ground truths and duplicates are false positives).\n\nPhases implemented as per guideline:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (Read frames from a video file)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling (detection processing, thresholding, scaling, clipping, drawing, mAP)\n- Phase 5: Cleanup\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Import Interpreter exactly as specified\nfrom ai_edge_litert.interpreter import Interpreter\n\n\ndef load_labels(label_file_path):\n    \"\"\"\n    Load labels from a text file. Each line corresponds to a label.\n    Empty lines and comment lines are ignored.\n    \"\"\"\n    labels = []\n    try:\n        with open(label_file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                s = line.strip()\n                if not s:\n                    continue\n                if s.startswith('#'):\n                    continue\n                labels.append(s)\n    except Exception as ex:\n        print(f\"[WARN] Could not read label file '{label_file_path}': {ex}\")\n        labels = []\n    return labels\n\n\ndef preprocess_frame_bgr_to_model_input(frame_bgr, input_shape, floating_model):\n    \"\"\"\n    Convert a BGR frame to the model's expected input tensor.\n    - input_shape: [1, height, width, channels]\n    - floating_model: True if model input dtype is float32\n    Returns: input_data numpy array with shape matching input_shape\n    \"\"\"\n    in_h, in_w = int(input_shape[1]), int(input_shape[2])\n    # Convert BGR -> RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # Resize to model input size\n    resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n    # Add batch dimension\n    input_data = np.expand_dims(resized, axis=0)\n    if floating_model:\n        # Normalize to [-1, 1] as per many MobileNet-based models\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # Keep uint8 [0,255]\n        input_data = np.uint8(input_data)\n    return input_data\n\n\ndef clip_box(xmin, ymin, xmax, ymax, w, h):\n    \"\"\"\n    Clip the bounding box coordinates to valid image bounds.\n    \"\"\"\n    xmin = max(0, min(xmin, w - 1))\n    xmax = max(0, min(xmax, w - 1))\n    ymin = max(0, min(ymin, h - 1))\n    ymax = max(0, min(ymax, h - 1))\n    return xmin, ymin, xmax, ymax\n\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"\n    Compute IoU between two boxes in [xmin, ymin, xmax, ymax] format.\n    \"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0.0, xB - xA)\n    inter_h = max(0.0, yB - yA)\n    inter_area = inter_w * inter_h\n    if inter_area <= 0.0:\n        return 0.0\n    areaA = max(0.0, (boxA[2] - boxA[0])) * max(0.0, (boxA[3] - boxA[1]))\n    areaB = max(0.0, (boxB[2] - boxB[0])) * max(0.0, (boxB[3] - boxB[1]))\n    denom = areaA + areaB - inter_area\n    if denom <= 0.0:\n        return 0.0\n    return inter_area / denom\n\n\ndef per_class_pseudo_gt_and_tp_flags(dets_for_class, iou_thresh=0.5):\n    \"\"\"\n    Given detections for a class for a single frame (list of (score, [xmin,ymin,xmax,ymax])),\n    perform a greedy NMS-like clustering:\n    - The highest-confidence box in a cluster is treated as a true positive (TP).\n    - Any other box in that cluster (IoU >= iou_thresh to the cluster representative) is a false positive (FP).\n    Returns:\n    - det_records: list of (score, is_tp) for each detection in dets_for_class,\n                   ordered by descending score (consistent for AP calculation).\n    - num_clusters: number of clusters (pseudo ground truths) in this frame for this class.\n    \"\"\"\n    if not dets_for_class:\n        return [], 0\n\n    # Sort by descending score\n    sorted_indices = sorted(range(len(dets_for_class)), key=lambda i: dets_for_class[i][0], reverse=True)\n    assigned = [False] * len(dets_for_class)\n    det_records = []\n    num_clusters = 0\n\n    for idx in sorted_indices:\n        if assigned[idx]:\n            continue\n        # Start a new cluster with this detection as representative (TP)\n        num_clusters += 1\n        assigned[idx] = True\n        det_records.append((dets_for_class[idx][0], True))  # TP\n\n        # Suppress duplicates (FPs) for the cluster\n        rep_box = dets_for_class[idx][1]\n        for jdx in sorted_indices:\n            if assigned[jdx]:\n                continue\n            iou_val = iou_xyxy(rep_box, dets_for_class[jdx][1])\n            if iou_val >= iou_thresh:\n                assigned[jdx] = True\n                det_records.append((dets_for_class[jdx][0], False))  # FP\n\n    # det_records is in the order of cluster creation and suppression (descending score across clusters)\n    # Already suitable for global AP aggregation when concatenated across frames.\n    return det_records, num_clusters\n\n\ndef compute_ap_from_records(det_records, total_gt):\n    \"\"\"\n    Compute Average Precision (AP) given detection records for a class across frames\n    and total pseudo ground truth count (sum of clusters) for that class.\n\n    det_records: list of (score, is_tp) across frames\n    total_gt: int, number of pseudo-gt instances for the class\n\n    Uses the precision envelope integral method.\n    \"\"\"\n    if total_gt <= 0 or len(det_records) == 0:\n        return 0.0\n\n    # Sort by confidence descending across all records\n    det_records_sorted = sorted(det_records, key=lambda x: x[0], reverse=True)\n    tps = []\n    fps = []\n    for _, is_tp in det_records_sorted:\n        tps.append(1 if is_tp else 0)\n        fps.append(0 if is_tp else 1)\n\n    tps = np.array(tps, dtype=np.float32)\n    fps = np.array(fps, dtype=np.float32)\n\n    cum_tps = np.cumsum(tps)\n    cum_fps = np.cumsum(fps)\n\n    recalls = cum_tps / float(total_gt)\n    precisions = cum_tps / np.maximum(cum_tps + cum_fps, 1e-12)\n\n    # Precision envelope\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n    for i in range(len(mpre) - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n\n    # Integration points where recall changes\n    i = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return float(ap)\n\n\ndef compute_running_map(per_class_records, per_class_total_gt):\n    \"\"\"\n    Compute mean AP across classes with non-zero pseudo ground truths.\n    per_class_records: dict[class_id] -> list of (score, is_tp)\n    per_class_total_gt: dict[class_id] -> int\n    Returns mAP float in [0,1].\n    \"\"\"\n    aps = []\n    for cid, total_gt in per_class_total_gt.items():\n        if total_gt <= 0:\n            continue\n        records = per_class_records.get(cid, [])\n        ap = compute_ap_from_records(records, total_gt)\n        aps.append(ap)\n    if not aps:\n        return 0.0\n    return float(np.mean(aps))\n\n\ndef main():\n    # 1.2 Paths/Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')  # Provided as string; convert to float\n\n    # 1.3 Load Labels (if provided and relevant)\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Determine input tensor properties\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']  # Expected [1, h, w, c]\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n\n    # Attempt to map output tensors\n    # Many SSD TFLite models produce 4 outputs: boxes, classes, scores, num_detections\n    # We'll assume the common order; if not, we try to infer by shapes.\n    boxes_idx = None\n    classes_idx = None\n    scores_idx = None\n    num_idx = None\n\n    if len(output_details) >= 4:\n        # Try common order\n        boxes_idx = output_details[0]['index']\n        classes_idx = output_details[1]['index']\n        scores_idx = output_details[2]['index']\n        num_idx = output_details[3]['index']\n        # Basic sanity checks; if not matching expected shapes, attempt inference by shape\n        try:\n            boxes_shape = output_details[0]['shape']\n            classes_shape = output_details[1]['shape']\n            scores_shape = output_details[2]['shape']\n            num_shape = output_details[3]['shape']\n            if not (len(boxes_shape) == 3 and boxes_shape[-1] == 4):\n                raise ValueError(\"Boxes shape not as expected\")\n            if not (len(num_shape) == 1 and num_shape[0] == 1):\n                raise ValueError(\"Num detections shape not as expected\")\n        except Exception:\n            # Infer by shapes\n            boxes_idx = classes_idx = scores_idx = num_idx = None\n            for od in output_details:\n                shape = od['shape']\n                if len(shape) == 3 and shape[-1] == 4:\n                    boxes_idx = od['index']\n                elif len(shape) == 1 and shape[0] == 1:\n                    num_idx = od['index']\n                else:\n                    # classes and scores typically [1, N]\n                    # Prefer to assign scores to float tensors and classes to non-float or also float\n                    if scores_idx is None and od['dtype'] == np.float32:\n                        scores_idx = od['index']\n                    elif classes_idx is None:\n                        classes_idx = od['index']\n            # If still ambiguous, default to common order\n            if None in (boxes_idx, classes_idx, scores_idx, num_idx):\n                boxes_idx = output_details[0]['index']\n                classes_idx = output_details[1]['index']\n                scores_idx = output_details[2]['index']\n                num_idx = output_details[3]['index']\n    else:\n        raise RuntimeError(\"Unexpected number of output tensors from model.\")\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: Read a single video file from the given input_path\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise FileNotFoundError(f\"Could not open input video: {input_path}\")\n\n    # Prepare output writer\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 30.0  # Fallback if video doesn't report FPS\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Could not open video writer for: {output_path}\")\n\n    # Data structures for running mAP computation\n    per_class_records = {}     # class_id -> list of (score, is_tp)\n    per_class_total_gt = {}    # class_id -> int\n    frame_count = 0\n    inference_times = []\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break  # End of video\n            frame_count += 1\n\n            # 2.2 Preprocess Data\n            input_data = preprocess_frame_bgr_to_model_input(frame, input_shape, floating_model)\n\n            # 2.3 Quantization Handling for floating model already handled; uint8 kept as is.\n\n            # Phase 3: Inference\n            # 3.1 Set Input Tensor\n            interpreter.set_tensor(input_index, input_data)\n            # 3.2 Run Inference\n            t0 = time.time()\n            interpreter.invoke()\n            t1 = time.time()\n            inference_times.append((t1 - t0) * 1000.0)  # ms\n\n            # Phase 4: Output Interpretation & Handling\n            # 4.1 Get Output Tensors\n            boxes = interpreter.get_tensor(boxes_idx)     # Expected shape [1, N, 4]\n            classes = interpreter.get_tensor(classes_idx) # Expected shape [1, N]\n            scores = interpreter.get_tensor(scores_idx)   # Expected shape [1, N]\n            num_det = interpreter.get_tensor(num_idx)     # Expected shape [1]\n\n            # Squeeze batch dimension\n            boxes = np.squeeze(boxes, axis=0)\n            classes = np.squeeze(classes, axis=0)\n            scores = np.squeeze(scores, axis=0)\n            # num_det might be float; cast to int\n            try:\n                num = int(np.squeeze(num_det).astype(np.int32))\n            except Exception:\n                num = boxes.shape[0]\n\n            # 4.2 Interpret Results: process detections\n            detections = []\n            # Iterate only over detected count (num)\n            for i in range(min(num, boxes.shape[0], classes.shape[0], scores.shape[0])):\n                score = float(scores[i])\n                if score < confidence_threshold:\n                    continue\n                # TFLite SSD boxes format: [ymin, xmin, ymax, xmax], normalized [0,1]\n                y_min, x_min, y_max, x_max = boxes[i].tolist()\n                # Scale to pixel coordinates\n                xmin = int(x_min * width)\n                ymin = int(y_min * height)\n                xmax = int(x_max * width)\n                ymax = int(y_max * height)\n                # 4.3 Post-processing: clip to valid ranges\n                xmin, ymin, xmax, ymax = clip_box(xmin, ymin, xmax, ymax, width, height)\n\n                cls_id = int(classes[i])  # Usually 0-based for COCO SSD\n                label = labels[cls_id] if (labels and 0 <= cls_id < len(labels)) else f\"class_{cls_id}\"\n                detections.append({\n                    'class_id': cls_id,\n                    'label': label,\n                    'score': score,\n                    'box': [xmin, ymin, xmax, ymax],\n                })\n\n            # Prepare per-class detections for pseudo-GT and TP/FP records\n            dets_by_class = {}\n            for det in detections:\n                cid = det['class_id']\n                dets_by_class.setdefault(cid, []).append((det['score'], det['box']))\n\n            # Update running mAP statistics (self-consistency estimate)\n            for cid, det_list in dets_by_class.items():\n                records, num_clusters = per_class_pseudo_gt_and_tp_flags(det_list, iou_thresh=0.5)\n                # Append records\n                if cid not in per_class_records:\n                    per_class_records[cid] = []\n                per_class_records[cid].extend(records)\n                # Update total pseudo ground truths\n                per_class_total_gt[cid] = per_class_total_gt.get(cid, 0) + num_clusters\n\n            running_map = compute_running_map(per_class_records, per_class_total_gt)\n\n            # 4.4 Handle Output: draw boxes and labels; overlay running mAP, write to file\n            for det in detections:\n                xmin, ymin, xmax, ymax = det['box']\n                label = det['label']\n                score = det['score']\n                # Draw rectangle\n                cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n                # Label text\n                text = f\"{label}: {score:.2f}\"\n                # Background for text for readability\n                (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n                cv2.rectangle(frame, (xmin, max(0, ymin - th - baseline)), (xmin + tw + 2, ymin), (0, 255, 0), -1)\n                cv2.putText(frame, text, (xmin + 1, max(0, ymin - baseline - 1)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n            # Overlay running mAP and FPS/inference time\n            avg_inf_ms = float(np.mean(inference_times)) if inference_times else 0.0\n            map_text = f\"mAP(est): {running_map*100:.2f}%   Inference: {avg_inf_ms:.1f} ms\"\n            cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50, 170, 250), 2, cv2.LINE_AA)\n\n            writer.write(frame)\n\n        # Phase 4.5 Loop end; done after video ends\n\n    finally:\n        # Phase 5: Cleanup\n        cap.release()\n        writer.release()\n\n    # Print summary\n    final_map = compute_running_map(per_class_records, per_class_total_gt)\n    print(\"Processing complete.\")\n    print(f\"Frames processed: {frame_count}\")\n    if inference_times:\n        print(f\"Average inference time: {np.mean(inference_times):.2f} ms\")\n    print(f\"Final mAP (self-consistency estimate): {final_map*100:.2f}%\")\n    print(f\"Output saved to: {output_path}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_54c5_psg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 181.497,
      "total_cost": 0.092421,
      "observations": [
        {
          "id": "time-23-32-35-325816_chatcmpl-D5emB23tQDeYrXluRK46hJURb92Dm",
          "traceId": "cb55e848",
          "type": "GENERATION",
          "name": "cb55_psg_gen_attempt#1",
          "startTime": "2026-02-04T21:32:35.325000+00:00",
          "endTime": "2026-02-04T21:35:36.822000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.092421,
          "latency": 181497.0,
          "totalTokens": 10713,
          "costDetails": {
            "total": 0.092421
          },
          "completionTokens": 9213,
          "promptTokens": 1500,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:36:30.631Z",
      "createdAt": "2026-02-04T21:32:35.359Z",
      "environment": "default"
    },
    {
      "id": "61b64920",
      "timestamp": "2026-02-04T21:28:22.925000+00:00",
      "name": "61b6_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TFLite SSD MobileNet v1 model\nand writes an annotated output video. It also computes and overlays a proxy mAP estimate based on\ntemporal consistency of detections across consecutive frames (no ground-truth is provided).\n\nPhases implemented according to the Programming Guidelines:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video file reading and preprocessing)\n- Phase 3: Inference (TFLite inference invocation)\n- Phase 4: Output Interpretation & Handling Loop (post-processing, drawing, proxy mAP, writing video)\n- Phase 5: Cleanup (release resources)\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Import Interpreter literally as required\nfrom ai_edge_litert.interpreter import Interpreter  # Per guideline: must import exactly this path\n\n\ndef load_labels(label_path):\n    \"\"\"Load labels from a file into a list.\"\"\"\n    labels = []\n    try:\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name != '':\n                    labels.append(name)\n    except Exception as e:\n        print(f\"[WARN] Failed to load labels from {label_path}: {e}\")\n    return labels\n\n\ndef ensure_dir_for_file(file_path):\n    \"\"\"Create directories needed for the given file path.\"\"\"\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n\n\ndef to_rgb_resized(frame_bgr, target_width, target_height):\n    \"\"\"Resize and convert BGR frame to RGB.\"\"\"\n    resized = cv2.resize(frame_bgr, (target_width, target_height), interpolation=cv2.INTER_LINEAR)\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    return rgb\n\n\ndef map_class_to_label(class_id_raw, labels):\n    \"\"\"\n    Map raw class id from TFLite output to label name.\n    Handles 0-based or 1-based class indexing differences.\n    \"\"\"\n    if not labels:\n        return str(int(class_id_raw)), int(class_id_raw)\n\n    cid = int(class_id_raw)\n    # Heuristic:\n    # - If cid in range [0, len(labels)-1], assume 0-based.\n    # - Else if cid-1 in range, assume 1-based and convert.\n    if 0 <= cid < len(labels):\n        name = labels[cid]\n        return name, cid\n    elif 1 <= cid <= len(labels):\n        name = labels[cid - 1]\n        return name, cid - 1\n    else:\n        # Clamp into range if out of bounds\n        cid_clamped = max(0, min(len(labels) - 1, cid))\n        return labels[cid_clamped], cid_clamped\n\n\ndef find_output_indices(output_details):\n    \"\"\"\n    Identify indices for boxes, classes, scores, and num_detections among output tensors.\n    Returns a dict with keys: 'boxes', 'classes', 'scores', 'num'\n    \"\"\"\n    idx = {'boxes': None, 'classes': None, 'scores': None, 'num': None}\n\n    # First pass using shapes and names if available\n    for i, od in enumerate(output_details):\n        shape = od.get('shape', None)\n        name = od.get('name', '').lower() if isinstance(od.get('name', ''), str) else ''\n        if shape is not None:\n            if len(shape) == 3 and shape[-1] == 4:\n                idx['boxes'] = i\n                continue\n            if len(shape) == 1 and (shape[0] == 1 or shape[0] == 4 or shape[0] == 100):\n                # Could be num_detections in some variants (usually [1])\n                if 'num' in name:\n                    idx['num'] = i\n                    continue\n            if len(shape) in (2, 3) and shape[-1] != 4:\n                # candidates for classes/scores\n                if 'class' in name:\n                    idx['classes'] = i\n                elif 'score' in name or 'confidence' in name:\n                    idx['scores'] = i\n\n    # If any still None, try heuristic by reading dtype/shape patterns later.\n    return idx\n\n\ndef get_detection_tensors(interpreter, output_details, idx_map):\n    \"\"\"\n    Retrieve detection outputs: boxes, classes, scores, num_detections.\n    If indices were ambiguous, attempt to infer based on value ranges.\n    \"\"\"\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    boxes = outputs[idx_map['boxes']] if idx_map['boxes'] is not None else None\n    classes = outputs[idx_map['classes']] if idx_map['classes'] is not None else None\n    scores = outputs[idx_map['scores']] if idx_map['scores'] is not None else None\n    num = outputs[idx_map['num']] if idx_map['num'] is not None else None\n\n    # Try to infer missing components\n    if boxes is None or classes is None or scores is None:\n        # Identify boxes by last dim == 4\n        if boxes is None:\n            for i, arr in enumerate(outputs):\n                if arr.ndim == 3 and arr.shape[-1] == 4:\n                    boxes = arr\n                    idx_map['boxes'] = i\n                    break\n        # Identify classes vs scores by value ranges:\n        # - scores are floats close to [0,1]\n        # - classes are ints/floats representing ids > 1\n        if classes is None or scores is None:\n            cand = []\n            for i, arr in enumerate(outputs):\n                if i == idx_map.get('boxes', -1):\n                    continue\n                if arr.ndim in (2, 3):\n                    cand.append((i, arr))\n            # Analyze candidates\n            score_idx_guess = None\n            class_idx_guess = None\n            for i, arr in cand:\n                v = arr.flatten()\n                if v.size == 0:\n                    continue\n                vsmall = v[:min(100, v.size)]\n                if np.all(vsmall >= -1.0) and np.all(vsmall <= 1.0):\n                    # Likely scores (some models output [0,1])\n                    score_idx_guess = i\n                else:\n                    # Likely classes\n                    class_idx_guess = i\n            if scores is None and score_idx_guess is not None:\n                scores = outputs[score_idx_guess]\n                idx_map['scores'] = score_idx_guess\n            if classes is None and class_idx_guess is not None:\n                classes = outputs[class_idx_guess]\n                idx_map['classes'] = class_idx_guess\n\n    # num detections\n    if num is None:\n        # Some models don't output num_detections; infer from scores shape\n        if scores is not None:\n            n = scores.shape[1] if scores.ndim >= 2 else scores.shape[0]\n            num = np.array([n], dtype=np.float32)\n        else:\n            num = np.array([0], dtype=np.float32)\n\n    return boxes, classes, scores, num\n\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"Compute IoU for two boxes in (x1, y1, x2, y2) format.\"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = xB - xA\n    inter_h = yB - yA\n    if inter_w <= 0 or inter_h <= 0:\n        return 0.0\n    inter = inter_w * inter_h\n    areaA = max(0, boxA[2] - boxA[0]) * max(0, boxA[3] - boxA[1])\n    areaB = max(0, boxB[2] - boxB[0]) * max(0, boxB[3] - boxB[1])\n    union = areaA + areaB - inter\n    if union <= 0:\n        return 0.0\n    return inter / union\n\n\ndef color_for_class(cid):\n    \"\"\"Deterministic color for a given class id.\"\"\"\n    # Simple hash-based color\n    r = (37 * (cid + 1) % 255)\n    g = (17 * (cid + 1) % 255)\n    b = (97 * (cid + 1) % 255)\n    return int(b), int(g), int(r)\n\n\ndef draw_detections(frame, detections, labels, map_proxy_value, fps_value=None):\n    \"\"\"Draw detections and proxy mAP on the frame.\"\"\"\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        cls = det['class_id']\n        score = det['score']\n        label_text = det['label']\n        color = color_for_class(cls)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{label_text}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(0, y1 - 5)\n        cv2.rectangle(frame, (x1, y_text - th - baseline), (x1 + tw, y_text + baseline), color, -1)\n        cv2.putText(frame, text, (x1, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    overlay_text = f\"Proxy mAP: {map_proxy_value:.3f}\"\n    cv2.putText(frame, overlay_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 220, 10), 2, cv2.LINE_AA)\n    if fps_value is not None:\n        cv2.putText(frame, f\"FPS: {fps_value:.1f}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 220, 220), 2, cv2.LINE_AA)\n\n\ndef compute_proxy_map(total_counts, consistent_counts):\n    \"\"\"\n    Compute a proxy mAP using temporal consistency as a surrogate for precision.\n    For each class: precision_proxy = consistent_detections / total_detections (if total > 0)\n    mAP_proxy = mean over classes with total > 0\n    \"\"\"\n    precisions = []\n    for cls, total in total_counts.items():\n        if total > 0:\n            consistent = consistent_counts.get(cls, 0)\n            precisions.append(consistent / float(total))\n    if len(precisions) == 0:\n        return 0.0\n    return float(np.mean(precisions))\n\n\ndef main():\n    # 1.2 Paths/Parameters from CONFIGURATION PARAMETERS\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')  # Provided as string; convert to float\n\n    # Create output directory\n    ensure_dir_for_file(output_path)\n\n    # 1.3 Load Labels\n    labels = load_labels(label_path)\n    if labels:\n        print(f\"[INFO] Loaded {len(labels)} labels.\")\n    else:\n        print(\"[WARN] No labels loaded; will use class IDs as labels.\")\n\n    # 1.4 Load Interpreter\n    print(f\"[INFO] Loading TFLite model from: {model_path}\")\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    if not input_details:\n        raise RuntimeError(\"No input details found in the model.\")\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n    input_h, input_w = int(input_shape[1]), int(input_shape[2])\n    floating_model = (input_dtype == np.float32)\n    print(f\"[INFO] Model input shape: {input_shape}, dtype: {input_dtype}, floating_model: {floating_model}\")\n\n    # Determine output indices for detection tensors\n    out_idx_map = find_output_indices(output_details)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data\n    if not os.path.exists(input_path):\n        print(f\"[ERROR] Input video not found: {input_path}\")\n        return\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"[ERROR] Failed to open video: {input_path}\")\n        return\n\n    # Prepare VideoWriter to write output annotated video\n    src_fps = cap.get(cv2.CAP_PROP_FPS)\n    if not src_fps or np.isnan(src_fps) or src_fps <= 0:\n        src_fps = 30.0  # fallback\n    src_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    src_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, src_fps, (src_w, src_h))\n    if not writer.isOpened():\n        print(f\"[ERROR] Failed to open VideoWriter for: {output_path}\")\n        cap.release()\n        return\n    print(f\"[INFO] Processing video: {input_path} ({src_w}x{src_h} @ {src_fps:.2f} FPS)\")\n    print(f\"[INFO] Writing annotated video to: {output_path}\")\n\n    # Stats for proxy mAP\n    total_counts = {}       # class_id -> total detections\n    consistent_counts = {}  # class_id -> detections matched temporally\n    prev_detections = []    # detections from previous frame: list of dicts with bbox, class_id\n\n    frame_index = 0\n    t0 = time.time()\n    last_time = t0\n    smoothed_fps = None\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret or frame_bgr is None:\n            break\n        frame_index += 1\n\n        # 2.2 Preprocess Data\n        rgb_resized = to_rgb_resized(frame_bgr, input_w, input_h)\n        input_data = np.expand_dims(rgb_resized, axis=0)\n\n        # 2.3 Quantization Handling / Normalization for floating model\n        if floating_model:\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n        else:\n            # For quantized uint8 models, use uint8 input (0..255)\n            input_data = np.uint8(input_data)\n\n        # Phase 3: Inference\n        # 3.1 Set Input Tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        # 3.2 Run Inference\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling Loop\n        # 4.1 Get Output Tensor(s)\n        boxes, classes, scores, num = get_detection_tensors(interpreter, output_details, out_idx_map)\n\n        # 4.2 Interpret Results\n        # Parse detections for this frame\n        detections = []\n        n_det = int(num[0]) if np.ndim(num) > 0 else int(num)\n        # Ensure shapes\n        if boxes is None or classes is None or scores is None:\n            n_det = 0\n\n        # 4.3 Post-processing: thresholding, scaling, clipping\n        for i in range(n_det):\n            score = float(scores[0, i]) if scores.ndim == 2 else float(scores[i])\n            if score < confidence_threshold:\n                continue\n\n            # Handle class id mapping and label lookup\n            raw_class_id = classes[0, i] if classes.ndim == 2 else classes[i]\n            label_text, class_id = map_class_to_label(raw_class_id, labels)\n\n            # Boxes are [ymin, xmin, ymax, xmax] normalized\n            b = boxes[0, i] if boxes.ndim == 3 else boxes[i]\n            ymin, xmin, ymax, xmax = float(b[0]), float(b[1]), float(b[2]), float(b[3])\n\n            # Scale to original frame size\n            x1 = int(max(0, min(src_w - 1, xmin * src_w)))\n            y1 = int(max(0, min(src_h - 1, ymin * src_h)))\n            x2 = int(max(0, min(src_w - 1, xmax * src_w)))\n            y2 = int(max(0, min(src_h - 1, ymax * src_h)))\n\n            # Ensure proper box ordering after scaling/clipping\n            x1, x2 = (x1, x2) if x1 <= x2 else (x2, x1)\n            y1, y2 = (y1, y2) if y1 <= y2 else (y2, y1)\n\n            detections.append({\n                'bbox': (x1, y1, x2, y2),\n                'class_id': class_id,\n                'label': label_text,\n                'score': score\n            })\n\n        # Temporal consistency-based proxy mAP updates\n        # Match current detections to previous detections (same class, IoU >= 0.5)\n        used_prev = set()\n        for det in detections:\n            cid = det['class_id']\n            total_counts[cid] = total_counts.get(cid, 0) + 1\n\n            best_iou = 0.0\n            best_j = -1\n            for j, pdet in enumerate(prev_detections):\n                if j in used_prev:\n                    continue\n                if pdet['class_id'] != cid:\n                    continue\n                iou = iou_xyxy(det['bbox'], pdet['bbox'])\n                if iou > best_iou:\n                    best_iou = iou\n                    best_j = j\n            if best_j >= 0 and best_iou >= 0.5:\n                consistent_counts[cid] = consistent_counts.get(cid, 0) + 1\n                used_prev.add(best_j)\n\n        map_proxy = compute_proxy_map(total_counts, consistent_counts)\n\n        # Estimate FPS\n        now = time.time()\n        dt = now - last_time\n        last_time = now\n        inst_fps = (1.0 / dt) if dt > 0 else src_fps\n        if smoothed_fps is None:\n            smoothed_fps = inst_fps\n        else:\n            smoothed_fps = 0.9 * smoothed_fps + 0.1 * inst_fps\n\n        # 4.4 Handle Output: draw and write frame\n        draw_detections(frame_bgr, detections, labels, map_proxy, fps_value=smoothed_fps)\n        writer.write(frame_bgr)\n\n        # 4.5 Loop Continuation: update previous detections\n        prev_detections = detections\n\n        # Optional: print progress every N frames\n        if frame_index % 30 == 0:\n            print(f\"[INFO] Processed {frame_index} frames; proxy mAP: {map_proxy:.3f}; FPS: {smoothed_fps:.1f}\")\n\n    t1 = time.time()\n    elapsed = t1 - t0\n    print(f\"[INFO] Finished processing. Frames: {frame_index}, Time: {elapsed:.2f}s, Avg FPS: {frame_index / elapsed if elapsed > 0 else 0:.1f}\")\n    final_map = compute_proxy_map(total_counts, consistent_counts)\n    print(f\"[INFO] Final proxy mAP (temporal-consistency): {final_map:.4f}\")\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n    print(f\"[INFO] Output saved to: {output_path}\")\n\n\nif __name__ == '__main__':\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_54c5_psg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 195.762,
      "total_cost": 0.092395,
      "observations": [
        {
          "id": "time-23-28-23-579511_chatcmpl-D5ei8KuKtnvDcOQyVx1aCtvEY0Wbr",
          "traceId": "61b64920",
          "type": "GENERATION",
          "name": "61b6_psg_gen_attempt#1",
          "startTime": "2026-02-04T21:28:23.579000+00:00",
          "endTime": "2026-02-04T21:31:39.341000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.092395,
          "latency": 195762.0,
          "totalTokens": 10552,
          "costDetails": {
            "total": 0.092395
          },
          "completionTokens": 9052,
          "promptTokens": 1500,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:32:29.234Z",
      "createdAt": "2026-02-04T21:28:23.491Z",
      "environment": "default"
    },
    {
      "id": "833bd5cb",
      "timestamp": "2026-02-04T21:24:58.082000+00:00",
      "name": "833b_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection using a TFLite SSD MobileNet model on a single input video.\nIt outputs a video with bounding boxes, labels, and a running mAP (proxy) overlay. The mAP proxy is\ncomputed as the mean of per-class average confidence scores accumulated over processed frames.\n\nPhases implemented according to the provided Programming Guidelines:\n- Phase 1: Setup (imports, paths, labels, interpreter, model I/O details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video read and preprocessing)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling (including 4.2 interpretation and 4.3 post-processing)\n- Phase 5: Cleanup\n\"\"\"\n\n# ---------------------------\n# Phase 1: Setup\n# ---------------------------\n\n# 1.1 Import interpreter literally as specified\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (provided configuration)\nmodel_path  = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path  = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path  = \"data/object_detection/sheeps.mp4\"\noutput_path  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n# 1.3 Load labels (conditional)\ndef load_labels(path):\n    labels = []\n    try:\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line != '':\n                    labels.append(line)\n    except Exception as e:\n        print(f\"Warning: Failed to read labels from {path}. Error: {e}\")\n    return labels\n\nlabels = load_labels(label_path)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Expecting a single input tensor\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # e.g., [1, height, width, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# ---------------------------\n# Helper Functions\n# ---------------------------\n\ndef preprocess_frame(frame, in_shape, floating):\n    \"\"\"\n    Resize and normalize frame to match model input requirements.\n    \"\"\"\n    # Model expects [1, h, w, 3]\n    _, in_h, in_w, in_c = in_shape\n    # Convert BGR (OpenCV) to RGB for typical TFLite SSD models\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n\n    if floating:\n        # Convert to float32 and normalize to [-1, 1] as per guideline\n        input_data = (np.float32(resized) - 127.5) / 127.5\n    else:\n        input_data = resized.astype(input_dtype)\n\n    # Add batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef get_tflite_detection_outputs(interpreter, output_details):\n    \"\"\"\n    Retrieve detection outputs from common SSD TFLite models.\n    Returns boxes, classes, scores, num_detections\n    boxes: [num, 4] in [ymin, xmin, ymax, xmax] normalized (0..1)\n    classes: [num] float class indices\n    scores: [num] float confidence\n    num_detections: int\n    \"\"\"\n    # Typical order: boxes, classes, scores, num_detections\n    # But we will detect by shape semantics\n    tensors = []\n    for od in output_details:\n        tensors.append(interpreter.get_tensor(od['index']))\n\n    # Flatten possible batch dimension\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n    for t in tensors:\n        arr = np.squeeze(t)\n        if arr.ndim == 2 and arr.shape[1] == 4:\n            boxes = arr\n        elif arr.ndim == 1 and arr.size > 10 and arr.dtype != np.int32 and np.issubdtype(arr.dtype, np.floating):\n            # Likely classes or scores; classes often floats; scores floats too. Distinguish by range.\n            # heuristic: scores in [0,1], classes typically >=0 and not bounded by 1.\n            if np.max(arr) <= 1.0 + 1e-6:\n                scores = arr\n            else:\n                classes = arr\n        elif arr.ndim == 0 or (arr.ndim == 1 and arr.size == 1):\n            num = int(np.round(float(arr)))\n\n    # Fallbacks in case shape extraction is slightly different\n    if boxes is None:\n        # try to find by shape in raw tensors\n        for t in tensors:\n            s = np.squeeze(t)\n            if s.ndim == 2 and s.shape[1] == 4:\n                boxes = s\n                break\n    if classes is None:\n        for t in tensors:\n            s = np.squeeze(t)\n            if s.ndim == 1 and np.max(s) > 1.0:\n                classes = s\n                break\n    if scores is None:\n        for t in tensors:\n            s = np.squeeze(t)\n            if s.ndim == 1 and np.max(s) <= 1.0 + 1e-6:\n                scores = s\n                break\n    if num is None:\n        # If num not provided, infer from boxes length\n        num = boxes.shape[0] if boxes is not None else 0\n\n    # Clip lengths to num\n    if boxes is not None:\n        boxes = boxes[:num]\n    if classes is not None:\n        classes = classes[:num]\n    if scores is not None:\n        scores = scores[:num]\n\n    return boxes, classes, scores, num\n\ndef clip_box(xmin, ymin, xmax, ymax, width, height):\n    xmin = max(0, min(xmin, width - 1))\n    xmax = max(0, min(xmax, width - 1))\n    ymin = max(0, min(ymin, height - 1))\n    ymax = max(0, min(ymax, height - 1))\n    return int(xmin), int(ymin), int(xmax), int(ymax)\n\ndef iou(box_a, box_b):\n    \"\"\"\n    Compute IoU between two boxes in (xmin, ymin, xmax, ymax)\n    \"\"\"\n    xa1, ya1, xa2, ya2 = box_a\n    xb1, yb1, xb2, yb2 = box_b\n    inter_x1 = max(xa1, xb1)\n    inter_y1 = max(ya1, yb1)\n    inter_x2 = min(xa2, xb2)\n    inter_y2 = min(ya2, yb2)\n    inter_w = max(0, inter_x2 - inter_x1 + 1)\n    inter_h = max(0, inter_y2 - inter_y1 + 1)\n    inter_area = inter_w * inter_h\n    area_a = max(0, xa2 - xa1 + 1) * max(0, ya2 - ya1 + 1)\n    area_b = max(0, xb2 - xb1 + 1) * max(0, yb2 - yb1 + 1)\n    union = area_a + area_b - inter_area\n    if union <= 0:\n        return 0.0\n    return inter_area / union\n\ndef nms_per_class(detections, iou_threshold=0.5):\n    \"\"\"\n    Apply Non-Maximum Suppression per class.\n    detections: list of dicts with keys: 'class_id', 'score', 'box' (xmin, ymin, xmax, ymax)\n    returns list of kept detections\n    \"\"\"\n    kept = []\n    # Group by class\n    by_class = {}\n    for det in detections:\n        by_class.setdefault(det['class_id'], []).append(det)\n\n    for cls, dets in by_class.items():\n        # Sort by score descending\n        dets_sorted = sorted(dets, key=lambda d: d['score'], reverse=True)\n        suppressed = [False] * len(dets_sorted)\n        for i in range(len(dets_sorted)):\n            if suppressed[i]:\n                continue\n            kept.append(dets_sorted[i])\n            box_i = dets_sorted[i]['box']\n            for j in range(i + 1, len(dets_sorted)):\n                if suppressed[j]:\n                    continue\n                box_j = dets_sorted[j]['box']\n                if iou(box_i, box_j) > iou_threshold:\n                    suppressed[j] = True\n    return kept\n\ndef class_id_to_name(class_id, labels_list):\n    \"\"\"\n    Map class id from TFLite SSD output to a human-readable label.\n    Many SSD models are 1-based indices. We'll attempt id-1 mapping.\n    \"\"\"\n    # Ensure integer\n    cid = int(class_id)\n    # Try 1-based indexing\n    idx_1based = cid - 1\n    if labels_list and 0 <= idx_1based < len(labels_list):\n        return labels_list[idx_1based]\n    # Fallback to 0-based\n    if labels_list and 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    return f\"id_{cid}\"\n\n# ---------------------------\n# Phase 2: Input Acquisition & Preprocessing Loop\n# ---------------------------\n\n# Acquire input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n# Video properties\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps <= 0 or np.isnan(fps):\n    fps = 25.0  # sensible fallback\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Prepare VideoWriter for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\nif not out_writer.isOpened():\n    cap.release()\n    raise RuntimeError(f\"Failed to open output video for writing: {output_path}\")\n\n# For mAP (proxy) computation: accumulate per-class scores\nper_class_scores = {}  # class_id -> list of scores\n\nframe_count = 0\ninference_times = []\n\n# ---------------------------\n# Processing Loop (single input video processed frame-by-frame)\n# ---------------------------\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frame_count += 1\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame(frame, input_shape, floating_model)\n\n    # 3.1 Set Input Tensor(s)\n    interpreter.set_tensor(input_index, input_data)\n\n    # 3.2 Run Inference\n    t0 = time.time()\n    interpreter.invoke()\n    t1 = time.time()\n    inference_times.append((t1 - t0) * 1000.0)  # ms\n\n    # ---------------------------\n    # Phase 4: Output Interpretation & Handling\n    # ---------------------------\n\n    # 4.1 Get Output Tensors\n    boxes, classes, scores, num = get_tflite_detection_outputs(interpreter, output_details)\n    if boxes is None or classes is None or scores is None:\n        # If model outputs are not as expected, skip frame\n        out_writer.write(frame)\n        continue\n\n    # 4.2 Interpret Results (assemble detections list)\n    detections = []\n    for i in range(num):\n        score = float(scores[i])\n        if score < confidence_threshold:\n            continue\n        cls_id = int(classes[i])\n        # boxes are normalized [ymin, xmin, ymax, xmax]\n        ymin, xmin, ymax, xmax = boxes[i]\n        # 4.3 Post-processing: scale to pixel coords and clip\n        x1 = int(xmin * width)\n        y1 = int(ymin * height)\n        x2 = int(xmax * width)\n        y2 = int(ymax * height)\n        x1, y1, x2, y2 = clip_box(x1, y1, x2, y2, width, height)\n        if x2 <= x1 or y2 <= y1:\n            continue  # invalid box after clipping\n        detections.append({\n            'class_id': cls_id,\n            'score': score,\n            'box': (x1, y1, x2, y2)\n        })\n\n    # 4.3 Post-processing: Non-Maximum Suppression per class\n    detections_nms = nms_per_class(detections, iou_threshold=0.5)\n\n    # Update mAP proxy accumulators: per-class mean confidence\n    for det in detections_nms:\n        cid = det['class_id']\n        per_class_scores.setdefault(cid, []).append(det['score'])\n\n    # Compute running mAP proxy: mean over classes of mean(scores)\n    if len(per_class_scores) > 0:\n        per_class_means = [np.mean(scores_list) for scores_list in per_class_scores.values() if len(scores_list) > 0]\n        running_map_proxy = float(np.mean(per_class_means)) if len(per_class_means) > 0 else 0.0\n    else:\n        running_map_proxy = 0.0\n\n    # 4.4 Handle Output: draw boxes and labels, overlay running mAP proxy\n    for det in detections_nms:\n        x1, y1, x2, y2 = det['box']\n        score = det['score']\n        cid = det['class_id']\n        label_name = class_id_to_name(cid, labels)\n        # Draw rectangle\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 200, 0), 2)\n        # Label text\n        text = f\"{label_name}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        # Background for text\n        cv2.rectangle(frame, (x1, y1 - th - baseline), (x1 + tw, y1), (0, 200, 0), thickness=-1)\n        cv2.putText(frame, text, (x1, y1 - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Overlay running mAP (proxy)\n    map_text = f\"mAP: {running_map_proxy:.3f}\"\n    cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (50, 50, 255), 2, cv2.LINE_AA)\n\n    # Optionally overlay FPS/inference time\n    if len(inference_times) > 0:\n        avg_inf_ms = np.mean(inference_times[-30:])  # moving average over last 30 frames\n        inf_text = f\"Inference: {avg_inf_ms:.1f} ms\"\n        cv2.putText(frame, inf_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 200, 50), 2, cv2.LINE_AA)\n\n    # Write frame to output video\n    out_writer.write(frame)\n\n# ---------------------------\n# Phase 5: Cleanup\n# ---------------------------\ncap.release()\nout_writer.release()\n\n# Final overall mAP (proxy)\nif len(per_class_scores) > 0:\n    per_class_means = [np.mean(scores_list) for scores_list in per_class_scores.values() if len(scores_list) > 0]\n    overall_map_proxy = float(np.mean(per_class_means)) if len(per_class_means) > 0 else 0.0\nelse:\n    overall_map_proxy = 0.0\n\nprint(\"Processing completed.\")\nprint(f\"Frames processed: {frame_count}\")\nif len(inference_times) > 0:\n    print(f\"Average inference time: {np.mean(inference_times):.2f} ms\")\nprint(f\"Estimated mAP (proxy based on mean per-class confidence): {overall_map_proxy:.4f}\")\nprint(f\"Output video saved to: {output_path}\")"
      },
      "session_id": "gpt-5-2025-08-07_54c5_psg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "latency": 147.629,
      "total_cost": 0.064335,
      "observations": [
        {
          "id": "time-23-24-58-384874_chatcmpl-D5eeo04mG2Q6SSW6RI16GkxTVI2W8",
          "traceId": "833bd5cb",
          "type": "GENERATION",
          "name": "833b_psg_gen_attempt#1",
          "startTime": "2026-02-04T21:24:58.384000+00:00",
          "endTime": "2026-02-04T21:27:26.013000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.064335,
          "latency": 147629.0,
          "totalTokens": 7746,
          "costDetails": {
            "total": 0.064335
          },
          "completionTokens": 6246,
          "promptTokens": 1500,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:28:16.803Z",
      "createdAt": "2026-02-04T21:24:58.603Z",
      "environment": "default"
    }
  ],
  "meta": {
    "total_items": 10
  }
}