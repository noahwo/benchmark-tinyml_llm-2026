{
  "data": [
    {
      "id": "edc559c1",
      "timestamp": "2026-02-01T23:53:28.718000+00:00",
      "name": "edc5_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Input size expected by the model\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Get video properties\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize frame to match the model's expected input size\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Loop over detections and draw bounding boxes if confidence > threshold\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n            box = boxes[0][i]\n\n            # Convert normalized coordinates to frame dimensions\n            ymin, xmin, ymax, xmax = box\n            left = int(xmin * frame_width)\n            top = int(ymin * frame_height)\n            right = int(xmax * frame_width)\n            bottom = int(ymax * frame_height)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {int(scores[0][i]*100)}%', (left, top - 8),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 86.755,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-53-28-718815_chatcmpl-7cf02c66-23bf-45a5-a601-b2a6686d9f35",
          "traceId": "edc559c1",
          "type": "GENERATION",
          "name": "edc5_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:53:28.718000+00:00",
          "endTime": "2026-02-01T23:53:57.076000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28358.0,
          "totalTokens": 874,
          "promptTokens": 152,
          "completionTokens": 722,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-53-57-257169_chatcmpl-44a8a156-8f6e-443e-ba37-13d3c555e132",
          "traceId": "edc559c1",
          "type": "GENERATION",
          "name": "edc5_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:53:57.257000+00:00",
          "endTime": "2026-02-01T23:54:27.114000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29857.0,
          "totalTokens": 915,
          "promptTokens": 152,
          "completionTokens": 763,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-54-27-271377_chatcmpl-9033792f-4dba-4397-94b2-c6f26acde078",
          "traceId": "edc559c1",
          "type": "GENERATION",
          "name": "edc5_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:54:27.271000+00:00",
          "endTime": "2026-02-01T23:54:55.473000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28202.0,
          "totalTokens": 872,
          "promptTokens": 152,
          "completionTokens": 720,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:55:42.923Z",
      "createdAt": "2026-02-01T23:53:29.239Z"
    },
    {
      "id": "1f13cc6d",
      "timestamp": "2026-02-01T23:52:00.351000+00:00",
      "name": "1f13_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess frame\n    input_data = cv2.resize(frame, (300, 300))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.uint8)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract output data from the results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Loop over all detections and draw box if confidence is above threshold\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            ymin = int(max(1, boxes[0][i][0] * frame_height))\n            xmin = int(max(1, boxes[0][i][1] * frame_width))\n            ymax = int(min(frame_height, boxes[0][i][2] * frame_height))\n            xmax = int(min(frame_width, boxes[0][i][3] * frame_width))\n\n            label = labels[int(classes[0][i]) - 1]\n            confidence = scores[0][i]\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {confidence:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame with boxes and labels to the output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 29.603,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-52-00-352349_chatcmpl-c7b6514b-417d-4d90-adc5-fdf7c1e726c9",
          "traceId": "1f13cc6d",
          "type": "GENERATION",
          "name": "1f13_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:52:00.352000+00:00",
          "endTime": "2026-02-01T23:52:29.955000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29603.0,
          "totalTokens": 906,
          "promptTokens": 152,
          "completionTokens": 754,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:53:17.992Z",
      "createdAt": "2026-02-01T23:52:00.874Z"
    },
    {
      "id": "bfde5f83",
      "timestamp": "2026-02-01T23:50:33.132000+00:00",
      "name": "bfde_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1:3]\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0).astype(np.uint8)\n\n    # Set tensor to point to the preprocessed image\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get all outputs from the network.\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Loop over all detections and draw boxes around detected objects\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            label = labels[int(classes[i])]\n            score = scores[i]\n\n            # Draw rectangle and label on the image\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (int(left), int(top) - 5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame with detection results to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 29.067,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-50-33-133136_chatcmpl-7de46734-7c21-4299-8aad-6c0a9b40d27b",
          "traceId": "bfde5f83",
          "type": "GENERATION",
          "name": "bfde_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:50:33.133000+00:00",
          "endTime": "2026-02-01T23:51:02.200000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29067.0,
          "totalTokens": 892,
          "promptTokens": 152,
          "completionTokens": 740,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:51:49.777Z",
      "createdAt": "2026-02-01T23:50:33.653Z"
    },
    {
      "id": "bd46f06d",
      "timestamp": "2026-02-01T23:49:04.857000+00:00",
      "name": "bd46_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess frame\n    input_data = cv2.resize(frame, (300, 300))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.uint8)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]  # Confidence of detected objects\n\n    # Draw detection results on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection results to the output file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 30.471,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-49-04-858807_chatcmpl-7fcf1c2e-d2c1-467a-b674-e642f06cd1d0",
          "traceId": "bd46f06d",
          "type": "GENERATION",
          "name": "bd46_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:49:04.858000+00:00",
          "endTime": "2026-02-01T23:49:35.329000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30471.0,
          "totalTokens": 905,
          "promptTokens": 152,
          "completionTokens": 753,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:50:22.556Z",
      "createdAt": "2026-02-01T23:49:05.382Z"
    },
    {
      "id": "998cf9b5",
      "timestamp": "2026-02-01T23:46:42.449000+00:00",
      "name": "998c_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Cannot open video {input_path}\")\n\n# Video writer setup\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Process video frame by frame\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess frame\n    input_shape = input_details[0]['shape']\n    input_height, input_width = input_shape[1], input_shape[2]\n    resized_frame = cv2.resize(frame, (input_width, input_height))\n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract output data from the interpreter\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Postprocess and draw bounding boxes\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            label = labels[int(classes[0][i])]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[0][i]:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 84.861,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-46-42-451166_chatcmpl-c40faec0-4195-4eb5-9f6d-2841ffe20b41",
          "traceId": "998cf9b5",
          "type": "GENERATION",
          "name": "998c_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:46:42.451000+00:00",
          "endTime": "2026-02-01T23:47:10.805000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28354.0,
          "totalTokens": 877,
          "promptTokens": 152,
          "completionTokens": 725,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-47-10-996208_chatcmpl-e86716a5-1fec-42b4-bb09-37a048df6580",
          "traceId": "998cf9b5",
          "type": "GENERATION",
          "name": "998c_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:47:10.996000+00:00",
          "endTime": "2026-02-01T23:47:40.553000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29557.0,
          "totalTokens": 909,
          "promptTokens": 152,
          "completionTokens": 757,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-47-40-899341_chatcmpl-3158b266-c280-4b1b-9d8c-d0a130b0da6d",
          "traceId": "998cf9b5",
          "type": "GENERATION",
          "name": "998c_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:47:40.899000+00:00",
          "endTime": "2026-02-01T23:48:07.312000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26413.0,
          "totalTokens": 828,
          "promptTokens": 152,
          "completionTokens": 676,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:48:54.647Z",
      "createdAt": "2026-02-01T23:46:42.974Z"
    },
    {
      "id": "3ac4fb62",
      "timestamp": "2026-02-01T23:44:48.040000+00:00",
      "name": "3ac4_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1], input_shape[2]\n    img_resized = cv2.resize(frame, (width, height))\n    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(img_rgb, axis=0)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = interpreter.get_tensor(output_details[3]['index'])\n\n    for i in range(int(num_detections)):\n        score = scores[0][i]\n        if score > confidence_threshold:\n            box = boxes[0][i]\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n\n            # Convert normalized coordinates to pixel values\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (xmin * width, xmax * width, ymin * height, ymax * height)\n            left, right, top, bottom = int(left), int(right), int(top), int(bottom)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame into the file 'output_path'\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 56.333,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-44-48-042246_chatcmpl-07b4696e-9cb5-496b-b1ac-2ee638d3154f",
          "traceId": "3ac4fb62",
          "type": "GENERATION",
          "name": "3ac4_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:44:48.042000+00:00",
          "endTime": "2026-02-01T23:45:14.365000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26323.0,
          "totalTokens": 824,
          "promptTokens": 152,
          "completionTokens": 672,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-45-14-575807_chatcmpl-a76287e2-afea-431b-bf66-a5431c196797",
          "traceId": "3ac4fb62",
          "type": "GENERATION",
          "name": "3ac4_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:45:14.575000+00:00",
          "endTime": "2026-02-01T23:45:44.375000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29800.0,
          "totalTokens": 917,
          "promptTokens": 152,
          "completionTokens": 765,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:46:31.726Z",
      "createdAt": "2026-02-01T23:44:48.564Z"
    },
    {
      "id": "728145f5",
      "timestamp": "2026-02-01T23:41:59.473000+00:00",
      "name": "7281_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(f\"Error opening video file: {input_path}\")\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    height, width, _ = input_shape\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            # Get bounding box coordinates\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            im_height, im_width, _ = frame.shape\n            (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                          ymin * im_height, ymax * im_height)\n\n            # Get class label and score\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n            score = scores[0][i]\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame with detection results to the output file\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202014421_psg_qwen2.5-coder:32b/tmp_20260202014421_psg_qwen2.5-coder:32b.py\", line 43, in <module>\n    height, width, _ = input_shape\nValueError: too many values to unpack (expected 3)\n"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 142.014,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-41-59-473631_chatcmpl-e286fc04-a54f-4f2e-8e34-0d7c4200aef5",
          "traceId": "728145f5",
          "type": "GENERATION",
          "name": "7281_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:41:59.473000+00:00",
          "endTime": "2026-02-01T23:42:27.172000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27699.0,
          "totalTokens": 856,
          "promptTokens": 152,
          "completionTokens": 704,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-42-27-554701_chatcmpl-4c5a3517-ebb3-4e62-834a-7e6d955d0757",
          "traceId": "728145f5",
          "type": "GENERATION",
          "name": "7281_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:42:27.554000+00:00",
          "endTime": "2026-02-01T23:42:54.375000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26821.0,
          "totalTokens": 838,
          "promptTokens": 152,
          "completionTokens": 686,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-42-54-538109_chatcmpl-076ed729-0ed1-415a-8d1f-34bec116c6d6",
          "traceId": "728145f5",
          "type": "GENERATION",
          "name": "7281_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:42:54.538000+00:00",
          "endTime": "2026-02-01T23:43:23.150000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28612.0,
          "totalTokens": 883,
          "promptTokens": 152,
          "completionTokens": 731,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-43-23-314357_chatcmpl-be37aed8-bb82-4e81-92df-77bed9431817",
          "traceId": "728145f5",
          "type": "GENERATION",
          "name": "7281_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:43:23.314000+00:00",
          "endTime": "2026-02-01T23:43:50.430000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27116.0,
          "totalTokens": 845,
          "promptTokens": 152,
          "completionTokens": 693,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-43-50-791763_chatcmpl-8a8568db-3111-4a41-b922-0a08ec0e0ee0",
          "traceId": "728145f5",
          "type": "GENERATION",
          "name": "7281_psg_gen_attempt#5",
          "startTime": "2026-02-01T23:43:50.791000+00:00",
          "endTime": "2026-02-01T23:44:21.314000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30523.0,
          "totalTokens": 934,
          "promptTokens": 152,
          "completionTokens": 782,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "b5461421-1165-4b62-8f01-00e4fb51e3b4",
          "traceId": "728145f5",
          "type": "SPAN",
          "name": "error_72_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-01T23:44:21.487000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202014421_psg_qwen2.5-coder:32b/tmp_20260202014421_psg_qwen2.5-coder:32b.py\", line 43, in <module>\n    height, width, _ = input_shape\nValueError: too many values to unpack (expected 3)\n",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:44:26.872Z",
      "createdAt": "2026-02-01T23:41:59.999Z"
    },
    {
      "id": "305fdf97",
      "timestamp": "2026-02-01T23:39:41.050000+00:00",
      "name": "305f_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess frame\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract output data from the results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i, score in enumerate(scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            \n            # Convert normalized coordinates to frame coordinates\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            \n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 80.389,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-39-41-051438_chatcmpl-c5f18453-a3a7-40d2-ae8f-060e8a1405e4",
          "traceId": "305fdf97",
          "type": "GENERATION",
          "name": "305f_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:39:41.051000+00:00",
          "endTime": "2026-02-01T23:40:07.978000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26927.0,
          "totalTokens": 841,
          "promptTokens": 152,
          "completionTokens": 689,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-40-08-150010_chatcmpl-278ba688-bb16-4f0f-ae55-bc7bffdfecd9",
          "traceId": "305fdf97",
          "type": "GENERATION",
          "name": "305f_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:40:08.150000+00:00",
          "endTime": "2026-02-01T23:40:34.770000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26620.0,
          "totalTokens": 836,
          "promptTokens": 152,
          "completionTokens": 684,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-40-34-933821_chatcmpl-d02a63c6-166e-4239-a19c-c71e2feb5bae",
          "traceId": "305fdf97",
          "type": "GENERATION",
          "name": "305f_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:40:34.933000+00:00",
          "endTime": "2026-02-01T23:41:01.440000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26507.0,
          "totalTokens": 833,
          "promptTokens": 152,
          "completionTokens": 681,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:41:38.772Z",
      "createdAt": "2026-02-01T23:39:41.575Z"
    },
    {
      "id": "c21e4075",
      "timestamp": "2026-02-01T23:36:51.858000+00:00",
      "name": "c21e_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load the TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load video\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.array(img_resized, dtype=np.float32)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Normalize the image\n    input_data = input_data / 255.0\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[1]['index'])\n    classes = interpreter.get_tensor(output_details[3]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Process detection results\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            box = boxes[0][i]\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n            score = scores[0][i]\n\n            # Convert box coordinates to frame dimensions\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (\n                int(xmin * width), int(xmax * width),\n                int(ymin * height), int(ymax * height)\n            )\n\n            # Draw bounding box and label on the frame\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (left, top - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202013915_psg_qwen2.5-coder:32b/tmp_20260202013915_psg_qwen2.5-coder:32b.py\", line 51, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 143.586,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-36-51-860030_chatcmpl-fd64fa62-1e0c-4e0e-b4b4-7353712822cb",
          "traceId": "c21e4075",
          "type": "GENERATION",
          "name": "c21e_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:36:51.860000+00:00",
          "endTime": "2026-02-01T23:37:21.686000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29826.0,
          "totalTokens": 908,
          "promptTokens": 152,
          "completionTokens": 756,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-37-21-867622_chatcmpl-c710037f-f735-4f6c-a8e6-86f762900c8b",
          "traceId": "c21e4075",
          "type": "GENERATION",
          "name": "c21e_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:37:21.867000+00:00",
          "endTime": "2026-02-01T23:37:50.690000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28823.0,
          "totalTokens": 890,
          "promptTokens": 152,
          "completionTokens": 738,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-37-50-852304_chatcmpl-5dca8db1-ad46-4e7c-8fa8-e843df06e6f6",
          "traceId": "c21e4075",
          "type": "GENERATION",
          "name": "c21e_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:37:50.852000+00:00",
          "endTime": "2026-02-01T23:38:17.657000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26805.0,
          "totalTokens": 840,
          "promptTokens": 152,
          "completionTokens": 688,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-38-17-837781_chatcmpl-6d6769e0-0168-4608-8f47-5ae04b8c779a",
          "traceId": "c21e4075",
          "type": "GENERATION",
          "name": "c21e_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:38:17.837000+00:00",
          "endTime": "2026-02-01T23:38:46.610000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28773.0,
          "totalTokens": 891,
          "promptTokens": 152,
          "completionTokens": 739,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-38-46-763642_chatcmpl-8b186405-266b-4fdb-8e46-8cc311b6582f",
          "traceId": "c21e4075",
          "type": "GENERATION",
          "name": "c21e_psg_gen_attempt#5",
          "startTime": "2026-02-01T23:38:46.763000+00:00",
          "endTime": "2026-02-01T23:39:15.291000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28528.0,
          "totalTokens": 884,
          "promptTokens": 152,
          "completionTokens": 732,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "7b440ebb-2951-4163-81ed-334f3476f42a",
          "traceId": "c21e4075",
          "type": "SPAN",
          "name": "error_c2_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-01T23:39:15.446000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202013915_psg_qwen2.5-coder:32b/tmp_20260202013915_psg_qwen2.5-coder:32b.py\", line 51, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:39:21.371Z",
      "createdAt": "2026-02-01T23:36:52.387Z"
    },
    {
      "id": "55185c68",
      "timestamp": "2026-02-01T23:33:59.376000+00:00",
      "name": "5518_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\n\n# Check if camera opened successfully\nif not cap.isOpened():\n    print(\"Error opening video stream or file\")\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence of detected objects\n\n    # Loop over all detections and draw boxes on the frame for each one that exceeds the minimum confidence\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            # Draw the bounding box of the object\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n\n            # Draw label and score\n            object_name = labels[int(classes[i])]\n            label = f'{object_name}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 115.033,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-33-59-377519_chatcmpl-09c5df4d-1e0c-4edc-9f45-16f0b122b081",
          "traceId": "55185c68",
          "type": "GENERATION",
          "name": "5518_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:33:59.377000+00:00",
          "endTime": "2026-02-01T23:34:28.516000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29139.0,
          "totalTokens": 897,
          "promptTokens": 152,
          "completionTokens": 745,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-34-28-681794_chatcmpl-9d874163-992f-4079-88d7-fd5da19bca21",
          "traceId": "55185c68",
          "type": "GENERATION",
          "name": "5518_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:34:28.681000+00:00",
          "endTime": "2026-02-01T23:34:56.504000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27823.0,
          "totalTokens": 863,
          "promptTokens": 152,
          "completionTokens": 711,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-34-56-680973_chatcmpl-a072db37-727f-4151-8e35-a894010c17dd",
          "traceId": "55185c68",
          "type": "GENERATION",
          "name": "5518_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:34:56.680000+00:00",
          "endTime": "2026-02-01T23:35:25.567000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28887.0,
          "totalTokens": 893,
          "promptTokens": 152,
          "completionTokens": 741,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-35-25-726852_chatcmpl-d46c4505-3ac0-4e68-afb4-6b9c7d667745",
          "traceId": "55185c68",
          "type": "GENERATION",
          "name": "5518_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:35:25.726000+00:00",
          "endTime": "2026-02-01T23:35:54.410000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28684.0,
          "totalTokens": 888,
          "promptTokens": 152,
          "completionTokens": 736,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:36:31.561Z",
      "createdAt": "2026-02-01T23:33:59.902Z"
    },
    {
      "id": "04b4b10a",
      "timestamp": "2026-02-01T23:31:07.107000+00:00",
      "name": "04b4_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\n\n# Check if camera opened successfully\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    img = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    img = np.expand_dims(img, axis=0)\n    img = np.float32(img) / 255.0\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], img)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get all output details\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Draw the bounding boxes and labels on the frame\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            label = labels[int(classes[0][i])]\n            score = scores[0][i]\n\n            # Convert normalized coordinates to frame coordinates\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame with bounding boxes to the output file\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202013332_psg_qwen2.5-coder:32b/tmp_20260202013332_psg_qwen2.5-coder:32b.py\", line 51, in <module>\n    interpreter.set_tensor(input_details[0]['index'], img)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 145.973,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-31-07-108806_chatcmpl-8db4aa5d-3e3e-42c0-8a62-1fa6c59181d7",
          "traceId": "04b4b10a",
          "type": "GENERATION",
          "name": "04b4_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:31:07.108000+00:00",
          "endTime": "2026-02-01T23:31:36.976000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29868.0,
          "totalTokens": 911,
          "promptTokens": 152,
          "completionTokens": 759,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-31-37-180848_chatcmpl-764da098-2574-4e7a-8173-3595f7bc15fa",
          "traceId": "04b4b10a",
          "type": "GENERATION",
          "name": "04b4_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:31:37.180000+00:00",
          "endTime": "2026-02-01T23:32:07.476000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30296.0,
          "totalTokens": 928,
          "promptTokens": 152,
          "completionTokens": 776,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-32-07-648390_chatcmpl-18020ae5-b42c-4e8a-af3f-f75ebb866efb",
          "traceId": "04b4b10a",
          "type": "GENERATION",
          "name": "04b4_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:32:07.648000+00:00",
          "endTime": "2026-02-01T23:32:35.060000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27412.0,
          "totalTokens": 855,
          "promptTokens": 152,
          "completionTokens": 703,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-32-35-227052_chatcmpl-0fbef057-9ded-4574-8537-e8f7b8921c6b",
          "traceId": "04b4b10a",
          "type": "GENERATION",
          "name": "04b4_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:32:35.227000+00:00",
          "endTime": "2026-02-01T23:33:04.408000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29181.0,
          "totalTokens": 901,
          "promptTokens": 152,
          "completionTokens": 749,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-33-04-570447_chatcmpl-eddfb13f-f156-4a35-868b-c48f42ee6df4",
          "traceId": "04b4b10a",
          "type": "GENERATION",
          "name": "04b4_psg_gen_attempt#5",
          "startTime": "2026-02-01T23:33:04.570000+00:00",
          "endTime": "2026-02-01T23:33:32.922000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28352.0,
          "totalTokens": 877,
          "promptTokens": 152,
          "completionTokens": 725,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "42cf5b60-1d8f-410e-b0dd-75566bfde95b",
          "traceId": "04b4b10a",
          "type": "SPAN",
          "name": "error_04_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-01T23:33:33.081000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202013332_psg_qwen2.5-coder:32b/tmp_20260202013332_psg_qwen2.5-coder:32b.py\", line 51, in <module>\n    interpreter.set_tensor(input_details[0]['index'], img)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:33:38.520Z",
      "createdAt": "2026-02-01T23:31:07.631Z"
    },
    {
      "id": "487afb89",
      "timestamp": "2026-02-01T23:29:40.671000+00:00",
      "name": "487a_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the TFLite interpreter\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load video\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n\n    # Set the tensor to point to the input data\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Post-process the detections\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            # Draw the bounding box and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 28.643,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-29-40-672504_chatcmpl-2ee2b3c0-940e-4e44-ad07-f80f957819f9",
          "traceId": "487afb89",
          "type": "GENERATION",
          "name": "487a_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:29:40.672000+00:00",
          "endTime": "2026-02-01T23:30:09.315000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28643.0,
          "totalTokens": 882,
          "promptTokens": 152,
          "completionTokens": 730,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:30:46.300Z",
      "createdAt": "2026-02-01T23:29:41.197Z"
    },
    {
      "id": "36ea838f",
      "timestamp": "2026-02-01T23:28:15.330000+00:00",
      "name": "36ea_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load the labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load the TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from file\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    raise Exception(\"Error opening video stream or file\")\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1], input_shape[2]\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            # Draw the bounding box on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n\n            # Get the label and draw it on the frame\n            label = labels[int(classes[i])]\n            label_text = f'{label}: {scores[i]:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 27.992,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-28-15-331979_chatcmpl-4c517d0b-06fd-48d6-b01a-6a7a5746c0fc",
          "traceId": "36ea838f",
          "type": "GENERATION",
          "name": "36ea_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:28:15.331000+00:00",
          "endTime": "2026-02-01T23:28:43.323000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27992.0,
          "totalTokens": 865,
          "promptTokens": 152,
          "completionTokens": 713,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:29:20.494Z",
      "createdAt": "2026-02-01T23:28:15.856Z"
    },
    {
      "id": "7df6f34d",
      "timestamp": "2026-02-01T23:25:23.083000+00:00",
      "name": "7df6_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from file\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object to save output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image to match input tensor\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(img_resized, axis=0)\n    input_data = input_data.astype(np.uint8)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[1]['index'])[0]\n    classes = interpreter.get_tensor(output_details[3]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels for detections above the confidence threshold\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = f\"{labels[class_id]}: {scores[i]:.2f}\"\n            y_min, x_min, y_max, x_max = box\n\n            # Convert normalized coordinates to image dimensions\n            im_height, im_width, _ = frame.shape\n            left = int(x_min * im_width)\n            right = int(x_max * im_width)\n            top = int(y_min * im_height)\n            bottom = int(y_max * im_height)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            cv2.putText(frame, label, (left, top - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with bounding boxes to output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202012748_psg_qwen2.5-coder:32b/tmp_20260202012748_psg_qwen2.5-coder:32b.py\", line 60, in <module>\n    class_id = int(classes[i])\nIndexError: invalid index to scalar variable.\n"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 146.022,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-25-23-085032_chatcmpl-0d46434d-b73a-46de-a040-e312b38f3845",
          "traceId": "7df6f34d",
          "type": "GENERATION",
          "name": "7df6_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:25:23.085000+00:00",
          "endTime": "2026-02-01T23:25:52.280000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29195.0,
          "totalTokens": 898,
          "promptTokens": 152,
          "completionTokens": 746,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-25-52-453231_chatcmpl-3245cf11-8612-4d67-a6b0-4d64bb49344a",
          "traceId": "7df6f34d",
          "type": "GENERATION",
          "name": "7df6_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:25:52.453000+00:00",
          "endTime": "2026-02-01T23:26:21.408000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28955.0,
          "totalTokens": 893,
          "promptTokens": 152,
          "completionTokens": 741,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-26-21-566240_chatcmpl-df12934c-5a01-4827-812b-bd2284f5d6af",
          "traceId": "7df6f34d",
          "type": "GENERATION",
          "name": "7df6_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:26:21.566000+00:00",
          "endTime": "2026-02-01T23:26:50.307000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28741.0,
          "totalTokens": 888,
          "promptTokens": 152,
          "completionTokens": 736,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-26-50-465466_chatcmpl-b2e641e5-734d-459b-9ac6-80dcf3abb8a4",
          "traceId": "7df6f34d",
          "type": "GENERATION",
          "name": "7df6_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:26:50.465000+00:00",
          "endTime": "2026-02-01T23:27:18.582000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28117.0,
          "totalTokens": 871,
          "promptTokens": 152,
          "completionTokens": 719,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-27-18-736054_chatcmpl-2fa776b3-df83-4bfb-9795-68addecd3232",
          "traceId": "7df6f34d",
          "type": "GENERATION",
          "name": "7df6_psg_gen_attempt#5",
          "startTime": "2026-02-01T23:27:18.736000+00:00",
          "endTime": "2026-02-01T23:27:48.752000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30016.0,
          "totalTokens": 920,
          "promptTokens": 152,
          "completionTokens": 768,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "c5fafc69-bdeb-44e0-bc12-ccf64218edbb",
          "traceId": "7df6f34d",
          "type": "SPAN",
          "name": "error_7d_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-01T23:27:49.107000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202012748_psg_qwen2.5-coder:32b/tmp_20260202012748_psg_qwen2.5-coder:32b.py\", line 60, in <module>\n    class_id = int(classes[i])\nIndexError: invalid index to scalar variable.\n",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:27:54.491Z",
      "createdAt": "2026-02-01T23:25:23.615Z"
    },
    {
      "id": "7cae8b79",
      "timestamp": "2026-02-01T23:22:01.544000+00:00",
      "name": "7cae_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\n\n# Check if camera opened successfully\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Define codec and create VideoWriter object\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Prepare input\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(image_resized, axis=0)\n    \n    # Normalize pixel values if required by the model\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    \n    # Draw bounding boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin = int(max(1, (boxes[i][0] * frame_height)))\n            xmin = int(max(1, (boxes[i][1] * frame_width)))\n            ymax = int(min(frame_height, (boxes[i][2] * frame_height)))\n            xmax = int(min(frame_width, (boxes[i][3] * frame_width)))\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            label_size, base_line = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            label_ymin = max(ymin, label_size[1] + 10)\n            cv2.rectangle(frame, (xmin, label_ymin - label_size[1] - 10), \\\n                          (xmin + label_size[0], label_ymin + base_line - 10), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (xmin, label_ymin-7),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Write the frame with detection results to the output video file\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 144.76,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-22-01-545532_chatcmpl-9f281002-26c9-43c0-bc35-968b760f7ca4",
          "traceId": "7cae8b79",
          "type": "GENERATION",
          "name": "7cae_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:22:01.545000+00:00",
          "endTime": "2026-02-01T23:22:30.651000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29106.0,
          "totalTokens": 892,
          "promptTokens": 152,
          "completionTokens": 740,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-22-30-841008_chatcmpl-e6308cc9-b17d-42cc-9bd0-73f03858d08d",
          "traceId": "7cae8b79",
          "type": "GENERATION",
          "name": "7cae_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:22:30.841000+00:00",
          "endTime": "2026-02-01T23:22:57.262000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26421.0,
          "totalTokens": 827,
          "promptTokens": 152,
          "completionTokens": 675,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-22-57-426491_chatcmpl-73e46b1b-b65a-45a9-a70d-4e756b63167c",
          "traceId": "7cae8b79",
          "type": "GENERATION",
          "name": "7cae_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:22:57.426000+00:00",
          "endTime": "2026-02-01T23:23:24.306000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26880.0,
          "totalTokens": 840,
          "promptTokens": 152,
          "completionTokens": 688,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-23-24-461562_chatcmpl-d6793ff0-c874-4aab-b9a8-4dd426c5cded",
          "traceId": "7cae8b79",
          "type": "GENERATION",
          "name": "7cae_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:23:24.461000+00:00",
          "endTime": "2026-02-01T23:23:53.227000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28766.0,
          "totalTokens": 887,
          "promptTokens": 152,
          "completionTokens": 735,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-23-53-588123_chatcmpl-aa08b368-050e-46bd-82ab-9ba8a09bc0ed",
          "traceId": "7cae8b79",
          "type": "GENERATION",
          "name": "7cae_psg_gen_attempt#5",
          "startTime": "2026-02-01T23:23:53.588000+00:00",
          "endTime": "2026-02-01T23:24:26.305000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32717.0,
          "totalTokens": 988,
          "promptTokens": 152,
          "completionTokens": 836,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:25:03.978Z",
      "createdAt": "2026-02-01T23:22:02.070Z"
    },
    {
      "id": "e54dde56",
      "timestamp": "2026-02-01T23:20:03.105000+00:00",
      "name": "e54d_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check if the input size is fixed\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\n# Read video from file\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties to write the output video with the same codec and fps\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize frame to match the input size of the model\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    # Normalize pixel values if required by the model\n    floating_model = (input_details[0]['dtype'] == np.float32)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = interpreter.get_tensor(output_details[3]['index'])\n\n    # Loop over all detections and draw box if confidence is above threshold\n    for i in range(int(num_detections)):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            p1 = (int(left), int(top))\n            p2 = (int(right), int(bottom))\n\n            # Draw rectangle and label\n            cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n            score = scores[0][i]\n            text = f'{label}: {score:.2f}'\n            cv2.putText(frame, text, p1, cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 59.779,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-20-03-106516_chatcmpl-783c8b6a-d9bc-4806-a4bb-1c736c60d972",
          "traceId": "e54dde56",
          "type": "GENERATION",
          "name": "e54d_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:20:03.106000+00:00",
          "endTime": "2026-02-01T23:20:31.501000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28395.0,
          "totalTokens": 875,
          "promptTokens": 152,
          "completionTokens": 723,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-20-31-876856_chatcmpl-83454926-955d-4469-9e1e-6bef92c603f1",
          "traceId": "e54dde56",
          "type": "GENERATION",
          "name": "e54d_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:20:31.876000+00:00",
          "endTime": "2026-02-01T23:21:02.885000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31009.0,
          "totalTokens": 944,
          "promptTokens": 152,
          "completionTokens": 792,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:21:40.841Z",
      "createdAt": "2026-02-01T23:20:03.631Z"
    },
    {
      "id": "a8a97f32",
      "timestamp": "2026-02-01T23:17:04.593000+00:00",
      "name": "a8a9_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check the type of the input tensor\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Initialize video capture\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Get video properties to set up video writer\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Initialize video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1:3]\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set tensor to image\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection results on frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Convert normalized coordinates to pixel values\n            ymin, xmin, ymax, xmax = box\n            left = int(xmin * frame_width)\n            top = int(ymin * frame_height)\n            right = int(xmax * frame_width)\n            bottom = int(ymax * frame_height)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (left, top - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 120.927,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-17-04-594538_chatcmpl-71a13f59-8977-4fad-9092-54bdf1ae7f77",
          "traceId": "a8a97f32",
          "type": "GENERATION",
          "name": "a8a9_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:17:04.594000+00:00",
          "endTime": "2026-02-01T23:17:34.358000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29764.0,
          "totalTokens": 910,
          "promptTokens": 152,
          "completionTokens": 758,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-17-34-549232_chatcmpl-40566ffe-9d38-4139-901c-a03711f17bac",
          "traceId": "a8a97f32",
          "type": "GENERATION",
          "name": "a8a9_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:17:34.549000+00:00",
          "endTime": "2026-02-01T23:18:03.158000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28609.0,
          "totalTokens": 884,
          "promptTokens": 152,
          "completionTokens": 732,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-18-03-328807_chatcmpl-2d46c2e7-d898-4eae-8e2a-cd0828e7150f",
          "traceId": "a8a97f32",
          "type": "GENERATION",
          "name": "a8a9_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:18:03.328000+00:00",
          "endTime": "2026-02-01T23:18:33.976000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30648.0,
          "totalTokens": 926,
          "promptTokens": 152,
          "completionTokens": 774,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-18-34-335861_chatcmpl-ca0e636e-0b12-43c6-b168-0edb62a12363",
          "traceId": "a8a97f32",
          "type": "GENERATION",
          "name": "a8a9_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:18:34.335000+00:00",
          "endTime": "2026-02-01T23:19:05.521000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31186.0,
          "totalTokens": 945,
          "promptTokens": 152,
          "completionTokens": 793,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:19:43.020Z",
      "createdAt": "2026-02-01T23:17:05.120Z"
    },
    {
      "id": "a0872453",
      "timestamp": "2026-02-01T23:14:14.063000+00:00",
      "name": "a087_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Error opening video file\")\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(img_rgb, axis=0)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Loop over all detections and draw on the frame\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            ymin = int(max(1, (boxes[0][i][0] * frame_height)))\n            xmin = int(max(1, (boxes[0][i][1] * frame_width)))\n            ymax = int(min(frame_height, (boxes[0][i][2] * frame_height)))\n            xmax = int(min(frame_width, (boxes[0][i][3] * frame_width)))\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n            label = labels[int(classes[0][i])]\n            score = scores[0][i]\n            display_text = f\"{label}: {score:.2f}\"\n            cv2.putText(frame, display_text, (xmin, ymin - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (10, 255, 0), 2)\n\n    # Write the frame with detection results to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 113.04,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-14-14-064622_chatcmpl-743815c4-e5dc-4bda-aa53-ee869410c76d",
          "traceId": "a0872453",
          "type": "GENERATION",
          "name": "a087_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:14:14.064000+00:00",
          "endTime": "2026-02-01T23:14:40.635000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26571.0,
          "totalTokens": 825,
          "promptTokens": 152,
          "completionTokens": 673,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-14-40-814428_chatcmpl-6994dfde-9173-4d53-8939-c22840e48882",
          "traceId": "a0872453",
          "type": "GENERATION",
          "name": "a087_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:14:40.814000+00:00",
          "endTime": "2026-02-01T23:15:07.734000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26920.0,
          "totalTokens": 841,
          "promptTokens": 152,
          "completionTokens": 689,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-15-07-911307_chatcmpl-fd502890-3ef7-493e-af4a-b9646a9d6191",
          "traceId": "a0872453",
          "type": "GENERATION",
          "name": "a087_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:15:07.911000+00:00",
          "endTime": "2026-02-01T23:15:38.829000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30918.0,
          "totalTokens": 940,
          "promptTokens": 152,
          "completionTokens": 788,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-15-39-049466_chatcmpl-322c2a98-a2da-41ca-83ac-720914ca2e21",
          "traceId": "a0872453",
          "type": "GENERATION",
          "name": "a087_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:15:39.049000+00:00",
          "endTime": "2026-02-01T23:16:07.104000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28055.0,
          "totalTokens": 871,
          "promptTokens": 152,
          "completionTokens": 719,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:16:44.274Z",
      "createdAt": "2026-02-01T23:14:14.591Z"
    },
    {
      "id": "2cb8a46e",
      "timestamp": "2026-02-01T23:11:23.876000+00:00",
      "name": "2cb8_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    input_data = np.expand_dims(frame, axis=0)\n    input_data = cv2.resize(input_data[0], (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = np.float32(input_data) / 255.0\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index']).astype(np.int)\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Draw detection results on the frame\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            class_id = classes[0][i]\n            box = boxes[0][i]\n            label = labels[class_id]\n            score = scores[0][i]\n\n            # Get coordinates\n            ymin, xmin, ymax, xmax = box\n\n            # Convert to pixel values\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            # Draw rectangle and label on frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), thickness=2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to the output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202011347_psg_qwen2.5-coder:32b/tmp_20260202011347_psg_qwen2.5-coder:32b.py\", line 49, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 144.15,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-11-23-877359_chatcmpl-17e56f52-8c91-472f-b00d-f720cf72e085",
          "traceId": "2cb8a46e",
          "type": "GENERATION",
          "name": "2cb8_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:11:23.877000+00:00",
          "endTime": "2026-02-01T23:11:52.203000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28326.0,
          "totalTokens": 876,
          "promptTokens": 152,
          "completionTokens": 724,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-11-52-366339_chatcmpl-dacbd8f4-f8f8-4838-abda-0b5a573529b4",
          "traceId": "2cb8a46e",
          "type": "GENERATION",
          "name": "2cb8_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:11:52.366000+00:00",
          "endTime": "2026-02-01T23:12:20.962000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28596.0,
          "totalTokens": 884,
          "promptTokens": 152,
          "completionTokens": 732,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-12-21-125724_chatcmpl-707f0d74-76ea-41c7-8851-f13eae107355",
          "traceId": "2cb8a46e",
          "type": "GENERATION",
          "name": "2cb8_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:12:21.125000+00:00",
          "endTime": "2026-02-01T23:12:50.553000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29428.0,
          "totalTokens": 908,
          "promptTokens": 152,
          "completionTokens": 756,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-12-50-711606_chatcmpl-c7342b1f-01f1-4028-8025-e1d6f1581034",
          "traceId": "2cb8a46e",
          "type": "GENERATION",
          "name": "2cb8_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:12:50.711000+00:00",
          "endTime": "2026-02-01T23:13:19.465000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28754.0,
          "totalTokens": 892,
          "promptTokens": 152,
          "completionTokens": 740,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-13-19-619371_chatcmpl-3ffc4405-b6bf-411f-bfca-11faeee5f040",
          "traceId": "2cb8a46e",
          "type": "GENERATION",
          "name": "2cb8_psg_gen_attempt#5",
          "startTime": "2026-02-01T23:13:19.619000+00:00",
          "endTime": "2026-02-01T23:13:47.866000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28247.0,
          "totalTokens": 880,
          "promptTokens": 152,
          "completionTokens": 728,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "7c14294b-ac28-46f1-a590-922bd7b00721",
          "traceId": "2cb8a46e",
          "type": "SPAN",
          "name": "error_2c_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-01T23:13:48.027000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202011347_psg_qwen2.5-coder:32b/tmp_20260202011347_psg_qwen2.5-coder:32b.py\", line 49, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tflite_runtime/interpreter.py\", line 720, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor \n",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:13:53.561Z",
      "createdAt": "2026-02-01T23:11:24.403Z"
    },
    {
      "id": "7523bd7d",
      "timestamp": "2026-02-01T23:08:36.265000+00:00",
      "name": "7523_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import tflite_runtime.interpreter as tflite\nimport numpy as np\nimport cv2\nfrom datetime import datetime\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check the type of the input tensor\nif input_details[0]['dtype'] == np.float32:\n    floating_model = True\n\nheight, width = input_details[0]['shape'][1:3]\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nout = cv2.VideoWriter(output_path, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and normalize image\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Perform the detection\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            score = float(scores[i])\n\n            # Convert normalized coordinates to image coordinates\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            # Draw bounding box and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection to the output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202011058_psg_qwen2.5-coder:32b/tmp_20260202011058_psg_qwen2.5-coder:32b.py\", line 45, in <module>\n    if floating_model:\nNameError: name 'floating_model' is not defined\n"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 141.979,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-08-36-266290_chatcmpl-7f4b67e5-aa03-4f4f-894c-589362500495",
          "traceId": "7523bd7d",
          "type": "GENERATION",
          "name": "7523_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:08:36.266000+00:00",
          "endTime": "2026-02-01T23:09:02.051000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25785.0,
          "totalTokens": 813,
          "promptTokens": 152,
          "completionTokens": 661,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-09-02-246786_chatcmpl-68e43fbe-5c00-4eac-89ba-4b54825b989a",
          "traceId": "7523bd7d",
          "type": "GENERATION",
          "name": "7523_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:09:02.246000+00:00",
          "endTime": "2026-02-01T23:09:30.079000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27833.0,
          "totalTokens": 863,
          "promptTokens": 152,
          "completionTokens": 711,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-09-30-234068_chatcmpl-45bd5013-b832-4a53-b141-af775a1df946",
          "traceId": "7523bd7d",
          "type": "GENERATION",
          "name": "7523_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:09:30.234000+00:00",
          "endTime": "2026-02-01T23:09:58.997000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28763.0,
          "totalTokens": 886,
          "promptTokens": 152,
          "completionTokens": 734,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-09-59-180251_chatcmpl-93432125-89d4-418b-bc8e-2ff3970c017d",
          "traceId": "7523bd7d",
          "type": "GENERATION",
          "name": "7523_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:09:59.180000+00:00",
          "endTime": "2026-02-01T23:10:29.757000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30577.0,
          "totalTokens": 934,
          "promptTokens": 152,
          "completionTokens": 782,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-10-29-955746_chatcmpl-16e0a2c2-6bc8-4f65-b455-827e93b9bd7b",
          "traceId": "7523bd7d",
          "type": "GENERATION",
          "name": "7523_psg_gen_attempt#5",
          "startTime": "2026-02-01T23:10:29.955000+00:00",
          "endTime": "2026-02-01T23:10:58.041000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28086.0,
          "totalTokens": 869,
          "promptTokens": 152,
          "completionTokens": 717,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "4f1169a5-f398-49f7-8db8-d6f636cf3bea",
          "traceId": "7523bd7d",
          "type": "SPAN",
          "name": "error_75_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-01T23:10:58.245000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260202011058_psg_qwen2.5-coder:32b/tmp_20260202011058_psg_qwen2.5-coder:32b.py\", line 45, in <module>\n    if floating_model:\nNameError: name 'floating_model' is not defined\n",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:11:03.853Z",
      "createdAt": "2026-02-01T23:08:36.788Z"
    },
    {
      "id": "5191c008",
      "timestamp": "2026-02-01T23:06:13.815000+00:00",
      "name": "5191_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    input_shape = input_details[0]['shape']\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_frame, axis=0)\n    input_data = input_data.astype(np.uint8)\n\n    # Set tensor to point to the preprocessed image\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection results on the frame\n    for i, score in enumerate(scores):\n        if score > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            xmin, ymin, xmax, ymax = box\n\n            # Convert normalized coordinates to pixel values\n            xmin = int(xmin * frame_width)\n            ymin = int(ymin * frame_height)\n            xmax = int(xmax * frame_width)\n            ymax = int(ymax * frame_height)\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 83.957,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-06-13-816673_chatcmpl-fdf093f7-5bc9-4e69-a9d4-119e193d298d",
          "traceId": "5191c008",
          "type": "GENERATION",
          "name": "5191_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:06:13.816000+00:00",
          "endTime": "2026-02-01T23:06:41.554000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27738.0,
          "totalTokens": 861,
          "promptTokens": 152,
          "completionTokens": 709,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-06-41-747185_chatcmpl-ad87ea95-8f7a-43c9-b298-c6121b0e3ffb",
          "traceId": "5191c008",
          "type": "GENERATION",
          "name": "5191_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:06:41.747000+00:00",
          "endTime": "2026-02-01T23:07:11.920000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30173.0,
          "totalTokens": 921,
          "promptTokens": 152,
          "completionTokens": 769,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-07-12-065972_chatcmpl-a05861f6-c1a9-400a-89e5-eaf96f6fb7b1",
          "traceId": "5191c008",
          "type": "GENERATION",
          "name": "5191_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:07:12.065000+00:00",
          "endTime": "2026-02-01T23:07:37.773000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25708.0,
          "totalTokens": 809,
          "promptTokens": 152,
          "completionTokens": 657,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:08:15.540Z",
      "createdAt": "2026-02-01T23:06:14.339Z"
    },
    {
      "id": "7fd116a0",
      "timestamp": "2026-02-01T23:03:53.345000+00:00",
      "name": "7fd1_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(\"Error opening video file\")\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1:3]\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0).astype(np.uint8)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index']).squeeze()\n    scores = interpreter.get_tensor(output_details[2]['index']).squeeze()\n\n    for box, cls, score in zip(boxes[0], classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            label = labels[int(cls)]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame to output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 83.232,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-03-53-346106_chatcmpl-865d76cf-aa21-4d9c-9818-360a3aabbf6f",
          "traceId": "7fd116a0",
          "type": "GENERATION",
          "name": "7fd1_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:03:53.346000+00:00",
          "endTime": "2026-02-01T23:04:19.415000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26069.0,
          "totalTokens": 813,
          "promptTokens": 152,
          "completionTokens": 661,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-04-19-780335_chatcmpl-6b88b7f1-4b2d-4d07-8b57-ab482b8ba28a",
          "traceId": "7fd116a0",
          "type": "GENERATION",
          "name": "7fd1_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:04:19.780000+00:00",
          "endTime": "2026-02-01T23:04:48.873000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29093.0,
          "totalTokens": 899,
          "promptTokens": 152,
          "completionTokens": 747,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-04-49-031466_chatcmpl-a223cf2f-98de-4e14-a5ff-e62a4fd7aa51",
          "traceId": "7fd116a0",
          "type": "GENERATION",
          "name": "7fd1_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:04:49.031000+00:00",
          "endTime": "2026-02-01T23:05:16.578000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27547.0,
          "totalTokens": 857,
          "promptTokens": 152,
          "completionTokens": 705,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:05:54.026Z",
      "createdAt": "2026-02-01T23:03:53.868Z"
    },
    {
      "id": "53fef6c9",
      "timestamp": "2026-02-01T23:00:58.937000+00:00",
      "name": "53fe_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check the type of the input tensor\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(\"Error opening video stream or file\")\n\n# Get video properties and initialize video writer\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Process video frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess frame\n    input_shape = input_details[0]['shape']\n    height, width, _ = frame.shape\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_frame, axis=0)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection results on frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 116.798,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-00-58-938022_chatcmpl-accbbc38-e3a4-4388-a92f-e2ef9a070644",
          "traceId": "53fef6c9",
          "type": "GENERATION",
          "name": "53fe_psg_gen_attempt#1",
          "startTime": "2026-02-01T23:00:58.938000+00:00",
          "endTime": "2026-02-01T23:01:29.352000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30414.0,
          "totalTokens": 927,
          "promptTokens": 152,
          "completionTokens": 775,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-01-29-546722_chatcmpl-3b7dd0fc-ee87-4b3b-b28c-7082ac5fe4eb",
          "traceId": "53fef6c9",
          "type": "GENERATION",
          "name": "53fe_psg_gen_attempt#2",
          "startTime": "2026-02-01T23:01:29.546000+00:00",
          "endTime": "2026-02-01T23:01:57.543000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27997.0,
          "totalTokens": 872,
          "promptTokens": 152,
          "completionTokens": 720,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-01-57-717005_chatcmpl-b5eea880-aa9f-43c0-93b3-c89075829eb1",
          "traceId": "53fef6c9",
          "type": "GENERATION",
          "name": "53fe_psg_gen_attempt#3",
          "startTime": "2026-02-01T23:01:57.717000+00:00",
          "endTime": "2026-02-01T23:02:25.958000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28241.0,
          "totalTokens": 877,
          "promptTokens": 152,
          "completionTokens": 725,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-02-26-129257_chatcmpl-6057fee2-2cb8-4168-91ee-bb21ccc81381",
          "traceId": "53fef6c9",
          "type": "GENERATION",
          "name": "53fe_psg_gen_attempt#4",
          "startTime": "2026-02-01T23:02:26.129000+00:00",
          "endTime": "2026-02-01T23:02:55.736000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29607.0,
          "totalTokens": 910,
          "promptTokens": 152,
          "completionTokens": 758,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:03:33.222Z",
      "createdAt": "2026-02-01T23:00:59.460Z"
    },
    {
      "id": "5f97f27c",
      "timestamp": "2026-02-01T22:59:29.496000+00:00",
      "name": "5f97_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load the labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load the TFLite model and allocate tensors\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check if the input size is fixed\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\n# Open the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Get video properties to save output video with the same settings\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize the frame to match input size\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Loop over all detections and draw boxes around detected objects\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            label = labels[int(classes[i]) - 1]  # Labels are 1-indexed\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything if job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 30.688,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-59-29-497819_chatcmpl-ea33920c-13fd-4e78-8122-fd35e8f7c6f4",
          "traceId": "5f97f27c",
          "type": "GENERATION",
          "name": "5f97_psg_gen_attempt#1",
          "startTime": "2026-02-01T22:59:29.497000+00:00",
          "endTime": "2026-02-01T23:00:00.185000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30688.0,
          "totalTokens": 934,
          "promptTokens": 152,
          "completionTokens": 782,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T23:00:38.189Z",
      "createdAt": "2026-02-01T22:59:30.021Z"
    },
    {
      "id": "3e620ac1",
      "timestamp": "2026-02-01T22:58:01.198000+00:00",
      "name": "3e62_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Initialize video capture\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(img_rgb, axis=0)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Get all outputs from the interpreter\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Draw detections\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n            score = float(scores[0][i])\n\n            # Convert normalized coordinates to frame coordinates\n            left = int(xmin * frame_width)\n            right = int(xmax * frame_width)\n            top = int(ymin * frame_height)\n            bottom = int(ymax * frame_height)\n\n            # Draw rectangle and label\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection results to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 30.079,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-58-01-199852_chatcmpl-e42a6711-110e-4afd-9865-0be26566545f",
          "traceId": "3e620ac1",
          "type": "GENERATION",
          "name": "3e62_psg_gen_attempt#1",
          "startTime": "2026-02-01T22:58:01.199000+00:00",
          "endTime": "2026-02-01T22:58:31.278000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30079.0,
          "totalTokens": 917,
          "promptTokens": 152,
          "completionTokens": 765,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T22:59:08.872Z",
      "createdAt": "2026-02-01T22:58:01.721Z"
    },
    {
      "id": "58e1c9f4",
      "timestamp": "2026-02-01T22:56:00.818000+00:00",
      "name": "58e1_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load the labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load the TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check if the input tensor is quantized\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Load video capture\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    # Normalize pixel values if using a floating model (note: only for float models)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Run the inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Loop over all detections and draw boxes around detected objects above the threshold\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = map(int, (boxes[i][0] * height, boxes[i][1] * width, \n                                                boxes[i][2] * height, boxes[i][3] * width))\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Draw the bounding box and label on the frame\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame with detection to output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 62.87,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-56-00-819428_chatcmpl-17a5e756-25b1-433a-a401-fd6626eecd23",
          "traceId": "58e1c9f4",
          "type": "GENERATION",
          "name": "58e1_psg_gen_attempt#1",
          "startTime": "2026-02-01T22:56:00.819000+00:00",
          "endTime": "2026-02-01T22:56:33.450000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32631.0,
          "totalTokens": 985,
          "promptTokens": 152,
          "completionTokens": 833,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-56-33-616024_chatcmpl-45765f42-3696-441b-bafa-b472a48a7155",
          "traceId": "58e1c9f4",
          "type": "GENERATION",
          "name": "58e1_psg_gen_attempt#2",
          "startTime": "2026-02-01T22:56:33.616000+00:00",
          "endTime": "2026-02-01T22:57:03.689000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30073.0,
          "totalTokens": 922,
          "promptTokens": 152,
          "completionTokens": 770,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T22:57:41.057Z",
      "createdAt": "2026-02-01T22:56:01.341Z"
    },
    {
      "id": "0808fece",
      "timestamp": "2026-02-01T22:54:35.651000+00:00",
      "name": "0808_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Process video frames\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = map(int, (boxes[i][0] * frame_height,\n                                                boxes[i][1] * frame_width,\n                                                boxes[i][2] * frame_height,\n                                                boxes[i][3] * frame_width))\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 28.076,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-54-35-651852_chatcmpl-44eb1e55-3b7c-4263-a438-33e496fcc5e1",
          "traceId": "0808fece",
          "type": "GENERATION",
          "name": "0808_psg_gen_attempt#1",
          "startTime": "2026-02-01T22:54:35.651000+00:00",
          "endTime": "2026-02-01T22:55:03.727000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28076.0,
          "totalTokens": 868,
          "promptTokens": 152,
          "completionTokens": 716,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T22:55:40.823Z",
      "createdAt": "2026-02-01T22:54:36.176Z"
    },
    {
      "id": "67751551",
      "timestamp": "2026-02-01T22:51:46.102000+00:00",
      "name": "6775_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties to write the output video with the same properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    input_data = np.expand_dims(frame, axis=0)\n    input_data = cv2.resize(input_data[0], (input_shape[1], input_shape[2]))\n    input_data = np.array(input_data, dtype=np.uint8)\n\n    # Run the model\n    interpreter.set_tensor(input_details[0]['index'], [input_data])\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(left), int(top) - 5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    # Write the frame to output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 111.182,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-51-46-103104_chatcmpl-098d81a9-b43f-410a-a485-ce1693d82b9a",
          "traceId": "67751551",
          "type": "GENERATION",
          "name": "6775_psg_gen_attempt#1",
          "startTime": "2026-02-01T22:51:46.103000+00:00",
          "endTime": "2026-02-01T22:52:14.711000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28608.0,
          "totalTokens": 871,
          "promptTokens": 152,
          "completionTokens": 719,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-52-15-127846_chatcmpl-cf21f6ae-e813-4e0f-b054-768cfa9d23a8",
          "traceId": "67751551",
          "type": "GENERATION",
          "name": "6775_psg_gen_attempt#2",
          "startTime": "2026-02-01T22:52:15.127000+00:00",
          "endTime": "2026-02-01T22:52:42.745000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27618.0,
          "totalTokens": 859,
          "promptTokens": 152,
          "completionTokens": 707,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-52-42-946018_chatcmpl-e1af50a2-17d3-43d6-8ee8-1fdbb85d2aa3",
          "traceId": "67751551",
          "type": "GENERATION",
          "name": "6775_psg_gen_attempt#3",
          "startTime": "2026-02-01T22:52:42.946000+00:00",
          "endTime": "2026-02-01T22:53:10.123000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27177.0,
          "totalTokens": 847,
          "promptTokens": 152,
          "completionTokens": 695,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-53-10-335426_chatcmpl-f8f6c9f2-9127-4823-bfaf-92bdeb999846",
          "traceId": "67751551",
          "type": "GENERATION",
          "name": "6775_psg_gen_attempt#4",
          "startTime": "2026-02-01T22:53:10.335000+00:00",
          "endTime": "2026-02-01T22:53:37.285000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26950.0,
          "totalTokens": 843,
          "promptTokens": 152,
          "completionTokens": 691,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T22:54:14.819Z",
      "createdAt": "2026-02-01T22:51:46.624Z"
    },
    {
      "id": "9b17d199",
      "timestamp": "2026-02-01T22:50:15.737000+00:00",
      "name": "9b17_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = tflite.Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check the type of the input tensor\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1], input_shape[2]\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    # Loop over all detections and draw box on the frame\n    for i in range(num_detections):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            label = labels[int(classes[0][i])]\n            score = scores[0][i]\n\n            # Convert normalized coordinates to actual frame coordinates\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            # Draw rectangle and text on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 32.244,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-50-15-738666_chatcmpl-0e0a0f89-29ba-41f4-940c-a0c60bc08851",
          "traceId": "9b17d199",
          "type": "GENERATION",
          "name": "9b17_psg_gen_attempt#1",
          "startTime": "2026-02-01T22:50:15.738000+00:00",
          "endTime": "2026-02-01T22:50:47.982000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32244.0,
          "totalTokens": 974,
          "promptTokens": 152,
          "completionTokens": 822,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T22:51:25.399Z",
      "createdAt": "2026-02-01T22:50:16.259Z"
    },
    {
      "id": "275746f9",
      "timestamp": "2026-02-01T22:47:57.243000+00:00",
      "name": "2757_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from file\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Process video\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Prepare input tensor\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(img_rgb, axis=0)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = interpreter.get_tensor(output_details[3]['index'])\n\n    # Draw bounding boxes on the frame\n    for i in range(int(num_detections)):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = map(int, (boxes[0][i] * np.array([frame_height, frame_width, frame_height, frame_width])))\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n            score = scores[0][i]\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (10, 255, 0), 2)\n\n    # Write the frame with bounding boxes to the output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_e906_psg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 80.204,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-47-57-245091_chatcmpl-655dccd2-8104-425a-bb9a-170eee521cfc",
          "traceId": "275746f9",
          "type": "GENERATION",
          "name": "2757_psg_gen_attempt#1",
          "startTime": "2026-02-01T22:47:57.245000+00:00",
          "endTime": "2026-02-01T22:49:17.449000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 80204.0,
          "totalTokens": 881,
          "promptTokens": 152,
          "completionTokens": 729,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-01T22:49:54.974Z",
      "createdAt": "2026-02-01T22:47:57.767Z"
    }
  ],
  "meta": {
    "total_items": 30
  }
}