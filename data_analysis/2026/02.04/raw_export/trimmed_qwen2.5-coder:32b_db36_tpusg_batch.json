{
  "data": [
    {
      "id": "6bb7d69b",
      "timestamp": "2026-02-03T10:47:16.650000+00:00",
      "name": "6bb7_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize interpreter\ninterpreter = tflite.Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit('ERROR: Video failed to open.')\n\n# Get video properties\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    img = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]), Image.ANTIALIAS)\n    input_data = np.expand_dims(img, axis=0).astype(input_details[0]['dtype'])\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Get results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels on the frame\n    for i, score in enumerate(scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width, ymin * height, ymax * height)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n\n            label = '%s: %d%%' % (labels[int(classes[i])], int(score * 100))\n            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            top = max(top, labelSize[1])\n            cv2.rectangle(frame, (int(left), int(top - round(1.5*labelSize[1]))),\n                          (int(left + round(1.5*labelSize[0])), int(top + baseLine)),\n                          (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(top - 4)), cv2.FONT_HERSHEY_SIMPLEX,\n                        0.7, (0, 0, 0), 2)\n\n    # Write the frame to the output file\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 138.088,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-47-16-652150_chatcmpl-e95dc54f-181d-479d-8c43-67cc405623f6",
          "traceId": "6bb7d69b",
          "type": "GENERATION",
          "name": "6bb7_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:47:16.652000+00:00",
          "endTime": "2026-02-03T10:47:43.803000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27151.0,
          "totalTokens": 909,
          "promptTokens": 180,
          "completionTokens": 729,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-47-51-713643_chatcmpl-3baf6d87-f0a0-406e-b955-3e9bfc25b36d",
          "traceId": "6bb7d69b",
          "type": "GENERATION",
          "name": "6bb7_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:47:51.713000+00:00",
          "endTime": "2026-02-03T10:48:21.275000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29562.0,
          "totalTokens": 967,
          "promptTokens": 180,
          "completionTokens": 787,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-48-29-053751_chatcmpl-9c34fe05-cdbf-49ae-b45f-66eadfc6b587",
          "traceId": "6bb7d69b",
          "type": "GENERATION",
          "name": "6bb7_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T10:48:29.053000+00:00",
          "endTime": "2026-02-03T10:48:56.656000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27603.0,
          "totalTokens": 914,
          "promptTokens": 180,
          "completionTokens": 734,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-49-04-655882_chatcmpl-b6e8d5ad-e08a-45da-b80a-69428ed8bc26",
          "traceId": "6bb7d69b",
          "type": "GENERATION",
          "name": "6bb7_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T10:49:04.655000+00:00",
          "endTime": "2026-02-03T10:49:34.740000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30085.0,
          "totalTokens": 988,
          "promptTokens": 180,
          "completionTokens": 808,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:50:36.367Z",
      "createdAt": "2026-02-03T10:47:17.180Z"
    },
    {
      "id": "d26c0def",
      "timestamp": "2026-02-03T10:43:26.264000+00:00",
      "name": "d26c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the TFLite interpreter with Edge TPU\ninterpreter = Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Get video properties to set up the writer\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_shape = input_details[0]['shape']\n    input_data = np.expand_dims(frame, axis=0)\n    input_data = cv2.resize(input_data, (input_shape[1], input_shape[2]))\n    input_data = np.uint8(input_data)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels on the frame\n    for i, score in enumerate(scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {score:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to the output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_5692abbd_1770115604.py\", line 17, in <module>\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\nNameError: name 'tflite' is not defined."
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 205.588,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-43-26-265915_chatcmpl-b4b22d94-6c8e-4ed5-b61e-f4f86e381476",
          "traceId": "d26c0def",
          "type": "GENERATION",
          "name": "d26c_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:43:26.265000+00:00",
          "endTime": "2026-02-03T10:43:49.651000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23386.0,
          "totalTokens": 809,
          "promptTokens": 180,
          "completionTokens": 629,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-43-58-258870_chatcmpl-ba74fba5-7ff0-43f0-a27f-e0d90819cdc6",
          "traceId": "d26c0def",
          "type": "GENERATION",
          "name": "d26c_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:43:58.258000+00:00",
          "endTime": "2026-02-03T10:45:02.594000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 64336.0,
          "totalTokens": 1889,
          "promptTokens": 180,
          "completionTokens": 1709,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-45-11-181621_chatcmpl-2e8a0abb-0da6-4b41-9852-de90b1c626f9",
          "traceId": "d26c0def",
          "type": "GENERATION",
          "name": "d26c_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T10:45:11.181000+00:00",
          "endTime": "2026-02-03T10:45:34.907000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23726.0,
          "totalTokens": 810,
          "promptTokens": 180,
          "completionTokens": 630,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-45-43-497929_chatcmpl-ac00d816-2d58-41c5-a535-be31b9369034",
          "traceId": "d26c0def",
          "type": "GENERATION",
          "name": "d26c_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T10:45:43.497000+00:00",
          "endTime": "2026-02-03T10:46:08.188000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24691.0,
          "totalTokens": 842,
          "promptTokens": 180,
          "completionTokens": 662,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-46-16-196195_chatcmpl-0599ea3a-337b-4a5a-b08f-c8315bf27dce",
          "traceId": "d26c0def",
          "type": "GENERATION",
          "name": "d26c_tpusg_gen_attempt#5",
          "startTime": "2026-02-03T10:46:16.196000+00:00",
          "endTime": "2026-02-03T10:46:44.034000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27838.0,
          "totalTokens": 931,
          "promptTokens": 180,
          "completionTokens": 751,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "fe708c10-ca80-43dc-8497-34ed3791d55f",
          "traceId": "d26c0def",
          "type": "SPAN",
          "name": "error_d2_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-03T10:46:51.853000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_5692abbd_1770115604.py\", line 17, in <module>\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\nNameError: name 'tflite' is not defined.",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:46:57.079Z",
      "createdAt": "2026-02-03T10:43:26.797Z"
    },
    {
      "id": "b14fd022",
      "timestamp": "2026-02-03T10:40:37.825000+00:00",
      "name": "b14f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nimport numpy as np\nfrom pycoral.adapters.common import input_size\nfrom pycoral.adapters.detect import get_objects\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Initialize the interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Read labels\nlabels = read_label_file(label_path)\n\n# Open input video\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties to save output video with the same properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert the frame to a PIL image and resize it\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_img = img.resize(input_size(interpreter), Image.ANTIALIAS)\n\n    # Run inference\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], np.array(resized_img).reshape(1, *resized_img.size, 3))\n    interpreter.invoke()\n\n    # Get detection results\n    objs = get_objects(interpreter, confidence_threshold)\n\n    # Draw bounding boxes and labels on the frame\n    for obj in objs:\n        bbox = obj.bbox\n        score = obj.score\n        label = labels.get(obj.id, 'Unknown')\n\n        # Scale bounding box coordinates to original image dimensions\n        xmin = int(bbox.xmin * frame_width)\n        xmax = int(bbox.xmax * frame_width)\n        ymin = int(bbox.ymin * frame_height)\n        ymax = int(bbox.ymax * frame_height)\n\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 86.756,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-40-37-826320_chatcmpl-d7095f99-3a17-4a71-8fd9-75ab56b63a72",
          "traceId": "b14fd022",
          "type": "GENERATION",
          "name": "b14f_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:40:37.826000+00:00",
          "endTime": "2026-02-03T10:40:59.426000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21600.0,
          "totalTokens": 761,
          "promptTokens": 180,
          "completionTokens": 581,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-41-07-593454_chatcmpl-7dcaf86e-f08a-46f8-809d-ba31bb9853b4",
          "traceId": "b14fd022",
          "type": "GENERATION",
          "name": "b14f_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:41:07.593000+00:00",
          "endTime": "2026-02-03T10:41:32.597000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25004.0,
          "totalTokens": 855,
          "promptTokens": 180,
          "completionTokens": 675,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-41-40-982227_chatcmpl-7b4a03eb-0068-40c0-b590-e37044a0e85f",
          "traceId": "b14fd022",
          "type": "GENERATION",
          "name": "b14f_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T10:41:40.982000+00:00",
          "endTime": "2026-02-03T10:42:04.582000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23600.0,
          "totalTokens": 819,
          "promptTokens": 180,
          "completionTokens": 639,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:43:06.055Z",
      "createdAt": "2026-02-03T10:40:38.360Z"
    },
    {
      "id": "0207b3b1",
      "timestamp": "2026-02-03T10:38:49.446000+00:00",
      "name": "0207_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the TFLite interpreter with Edge TPU support\ninterpreter = tflite.Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Get video properties\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    img = img.resize((input_shape[1], input_shape[2]), Image.ANTIALIAS)\n    input_data = np.expand_dims(img, axis=0)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Get results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detections on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame with detection to output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 26.306,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-38-49-448110_chatcmpl-2be691a8-9835-478b-be89-da0019c25be6",
          "traceId": "0207b3b1",
          "type": "GENERATION",
          "name": "0207_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:38:49.448000+00:00",
          "endTime": "2026-02-03T10:39:15.754000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26306.0,
          "totalTokens": 888,
          "promptTokens": 180,
          "completionTokens": 708,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:40:17.146Z",
      "createdAt": "2026-02-03T10:38:49.982Z"
    },
    {
      "id": "329001bd",
      "timestamp": "2026-02-03T10:35:49.839000+00:00",
      "name": "3290_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nfrom pycoral.adapters import common\nfrom pycoral.adapters.detect import get_objects\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Initialize the interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Load labels\nlabels = read_label_file(label_path)\n\n# Open input video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Error opening video stream or file\")\n\n# Get video properties to set for output video\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert the frame to a PIL image\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    \n    # Resize and prepare the input tensor\n    resized_img = img.resize(common.input_size(interpreter), Image.ANTIALIAS)\n    common.set_input(interpreter, resized_img)\n\n    # Run inference\n    interpreter.invoke()\n    \n    # Get detection results\n    objs = get_objects(interpreter, confidence_threshold)\n\n    # Draw detections on the frame\n    for obj in objs:\n        bbox = obj.bbox\n        class_id = int(obj.id)\n        label = labels.get(class_id, f'unknown {class_id}')\n        score = obj.score\n\n        # Calculate original bounding box coordinates\n        xmin = int(bbox.xmin * frame_width / resized_img.width)\n        ymin = int(bbox.ymin * frame_height / resized_img.height)\n        xmax = int(bbox.xmax * frame_width / resized_img.width)\n        ymax = int(bbox.ymax * frame_height / resized_img.height)\n\n        # Draw rectangle and label\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (xmin, ymin - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to output file\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 97.65,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-35-49-840509_chatcmpl-dae5bd1f-2ef9-436a-afc1-be8c3875407c",
          "traceId": "329001bd",
          "type": "GENERATION",
          "name": "3290_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:35:49.840000+00:00",
          "endTime": "2026-02-03T10:36:20.108000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30268.0,
          "totalTokens": 992,
          "promptTokens": 180,
          "completionTokens": 812,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-36-28-469839_chatcmpl-f32ea325-449b-4366-87ab-caa9d8f94555",
          "traceId": "329001bd",
          "type": "GENERATION",
          "name": "3290_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:36:28.469000+00:00",
          "endTime": "2026-02-03T10:36:55.187000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26718.0,
          "totalTokens": 900,
          "promptTokens": 180,
          "completionTokens": 720,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-37-02-801875_chatcmpl-c1cd5b11-d0fa-458c-b17a-d85ed7787b72",
          "traceId": "329001bd",
          "type": "GENERATION",
          "name": "3290_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T10:37:02.801000+00:00",
          "endTime": "2026-02-03T10:37:27.490000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24689.0,
          "totalTokens": 846,
          "promptTokens": 180,
          "completionTokens": 666,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:38:29.563Z",
      "createdAt": "2026-02-03T10:35:50.380Z"
    },
    {
      "id": "512e0dc1",
      "timestamp": "2026-02-03T10:33:08.438000+00:00",
      "name": "512e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nfrom pycoral.utils.edgetpu import make_interpreter\nfrom pycoral.adapters import common, detect\nimport numpy as np\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\n\n# Check if video opened successfully\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert the frame to a PIL image\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\n    # Resize and normalize image\n    resized_img = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]), Image.ANTIALIAS)\n    \n    # Set the tensor to point to the new frame data\n    common.set_input(interpreter, resized_img)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    objs = detect.get_objects(interpreter, score_threshold=confidence_threshold)\n\n    # Draw bounding boxes and labels on the original frame\n    for obj in objs:\n        bbox = obj.bbox\n        label = labels.get(obj.id, 'Unknown')\n        cv2.rectangle(frame, (int(bbox.xmin * img.width), int(bbox.ymin * img.height)),\n                      (int(bbox.xmax * img.width), int(bbox.ymax * img.height)), (0, 255, 0), 2)\n        cv2.putText(frame, f'{label}: {obj.score:.2f}', (int(bbox.xmin * img.width), int(bbox.ymin * img.height) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 80.538,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-33-08-440390_chatcmpl-626b98b7-bae6-48d3-8da4-674c2cdaa1c4",
          "traceId": "512e0dc1",
          "type": "GENERATION",
          "name": "512e_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:33:08.440000+00:00",
          "endTime": "2026-02-03T10:33:54.370000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 45930.0,
          "totalTokens": 1399,
          "promptTokens": 180,
          "completionTokens": 1219,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-34-03-013139_chatcmpl-ea818085-2aca-418d-bfc0-8a772bb927f3",
          "traceId": "512e0dc1",
          "type": "GENERATION",
          "name": "512e_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:34:03.013000+00:00",
          "endTime": "2026-02-03T10:34:28.978000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25965.0,
          "totalTokens": 877,
          "promptTokens": 180,
          "completionTokens": 697,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:35:30.222Z",
      "createdAt": "2026-02-03T10:33:08.986Z"
    },
    {
      "id": "1f4f9105",
      "timestamp": "2026-02-03T10:31:18.598000+00:00",
      "name": "1f4f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.adapters.common import input_size\nfrom pycoral.adapters.detect import get_objects\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nlabels = read_label_file(label_path)\n\n# Initialize the interpreter with Edge TPU support\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Determine the required image size for the model\n_, height, width, _ = input_details[0]['shape']\n\n# Open video file for reading\ncap = cv2.VideoCapture(input_path)\n\n# Check if video opened successfully\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Get video properties to set up the output video writer\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert the frame to RGB and resize it\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized_rgb_frame = Image.fromarray(cv2.resize(rgb_frame, (width, height)))\n\n    # Prepare input tensor\n    input_tensor = np.array(resized_rgb_frame).reshape((1, height, width, 3))\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Get detection results\n    objs = get_objects(interpreter, confidence_threshold)\n\n    # Draw bounding boxes and labels on the original frame\n    for obj in objs:\n        bbox = obj.bbox\n        x0, y0 = int(bbox.xmin * frame_width), int(bbox.ymin * frame_height)\n        x1, y1 = int(bbox.xmax * frame_width), int(bbox.ymax * frame_height)\n\n        # Draw bounding box\n        cv2.rectangle(frame, (x0, y0), (x1, y1), (0, 255, 0), 2)\n\n        # Get label and score\n        label = labels.get(obj.id, obj.id)\n        score = int(100 * obj.score)\n\n        # Draw label text\n        label_text = f'{label}: {score}%'\n        cv2.putText(frame, label_text, (x0, y0 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame with detections to output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 30.842,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-31-18-599180_chatcmpl-d508f956-ccd0-4efa-9ddc-c9fcc763683e",
          "traceId": "1f4f9105",
          "type": "GENERATION",
          "name": "1f4f_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:31:18.599000+00:00",
          "endTime": "2026-02-03T10:31:49.441000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30842.0,
          "totalTokens": 930,
          "promptTokens": 180,
          "completionTokens": 750,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:32:47.343Z",
      "createdAt": "2026-02-03T10:31:19.146Z"
    },
    {
      "id": "28fcb332",
      "timestamp": "2026-02-03T10:26:51.957000+00:00",
      "name": "28fc_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the interpreter with Edge TPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Open the video file for reading\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Video writer setup\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Process frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n    input_data = frame_rgb.reshape(input_shape)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detections on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Convert box coordinates to pixel values\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n\n            # Draw rectangle and label on frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), thickness=2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 187.799,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-26-51-958617_chatcmpl-b5716810-a0c2-47c7-9424-f02d1dec808f",
          "traceId": "28fcb332",
          "type": "GENERATION",
          "name": "28fc_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:26:51.958000+00:00",
          "endTime": "2026-02-03T10:27:23.622000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31664.0,
          "totalTokens": 964,
          "promptTokens": 180,
          "completionTokens": 784,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-27-32-133855_chatcmpl-bd41624e-2355-4489-bcdc-f5bb2c932746",
          "traceId": "28fcb332",
          "type": "GENERATION",
          "name": "28fc_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:27:32.133000+00:00",
          "endTime": "2026-02-03T10:28:01.281000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29148.0,
          "totalTokens": 903,
          "promptTokens": 180,
          "completionTokens": 723,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-28-09-089946_chatcmpl-e6b3ec08-db09-4a2f-918e-25f27e5ecb51",
          "traceId": "28fcb332",
          "type": "GENERATION",
          "name": "28fc_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T10:28:09.089000+00:00",
          "endTime": "2026-02-03T10:28:41.178000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32089.0,
          "totalTokens": 974,
          "promptTokens": 180,
          "completionTokens": 794,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-28-48-988040_chatcmpl-49d9edd6-abf6-4d83-b8af-2576bfb32238",
          "traceId": "28fcb332",
          "type": "GENERATION",
          "name": "28fc_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T10:28:48.988000+00:00",
          "endTime": "2026-02-03T10:29:19.286000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30298.0,
          "totalTokens": 930,
          "promptTokens": 180,
          "completionTokens": 750,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-29-27-959728_chatcmpl-ee88ad55-7282-4d90-8fcd-1282767d0243",
          "traceId": "28fcb332",
          "type": "GENERATION",
          "name": "28fc_tpusg_gen_attempt#5",
          "startTime": "2026-02-03T10:29:27.959000+00:00",
          "endTime": "2026-02-03T10:29:59.757000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31798.0,
          "totalTokens": 966,
          "promptTokens": 180,
          "completionTokens": 786,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:30:57.768Z",
      "createdAt": "2026-02-03T10:26:52.504Z"
    },
    {
      "id": "ec966d23",
      "timestamp": "2026-02-03T10:24:29.587000+00:00",
      "name": "ec96_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.adapters.common import input_size, set_input\nfrom pycoral.adapters.detect import get_objects\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Initialize the interpreter\ninterpreter = make_interpreter(model_path, device=':0')\ninterpreter.allocate_tensors()\nlabels = read_label_file(label_path)\ninference_size = input_size(interpreter)\n\n# Open the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit('ERROR: Unable to open video source.')\n\n# Get video properties\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef draw_objects(frame, objs, labels):\n    for obj in objs:\n        bbox = obj.bbox\n        label = f'{labels.get(obj.id, obj.id)}: {obj.score:.2f}'\n        if obj.score >= confidence_threshold:\n            cv2.rectangle(frame, (bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax), (0, 255, 0), 2)\n            cv2.putText(frame, label, (bbox.xmin, bbox.ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert frame to RGB\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized_img = Image.fromarray(cv2.resize(rgb_frame, inference_size))\n\n    # Run inference\n    set_input(interpreter, resized_img)\n    interpreter.invoke()\n    objs = get_objects(interpreter, confidence_threshold)\n\n    # Draw the results on the frame\n    draw_objects(frame, objs, labels)\n\n    # Write the frame to output video file\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 63.6,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-24-29-589039_chatcmpl-227298f1-36ef-4767-8eb9-40b32fb97263",
          "traceId": "ec966d23",
          "type": "GENERATION",
          "name": "ec96_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:24:29.589000+00:00",
          "endTime": "2026-02-03T10:24:58.943000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29354.0,
          "totalTokens": 907,
          "promptTokens": 180,
          "completionTokens": 727,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-25-07-345966_chatcmpl-a0407712-dba2-4c1d-b21b-c671cb774bae",
          "traceId": "ec966d23",
          "type": "GENERATION",
          "name": "ec96_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:25:07.345000+00:00",
          "endTime": "2026-02-03T10:25:33.189000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25844.0,
          "totalTokens": 822,
          "promptTokens": 180,
          "completionTokens": 642,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:26:31.813Z",
      "createdAt": "2026-02-03T10:24:30.134Z"
    },
    {
      "id": "32b7f03f",
      "timestamp": "2026-02-03T10:21:01.062000+00:00",
      "name": "32b7_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nfrom tflite_runtime.interpreter import Interpreter\nimport numpy as np\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize interpreter\ninterpreter = Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Cannot open video file: {input_path}\")\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    input_shape = input_details[0]['shape']\n    height, width, _ = input_shape\n    img_resized = cv2.resize(frame, (width, height))\n    img_resized = np.expand_dims(img_resized, axis=0)\n    img_resized = np.float32(img_resized) / 255.0\n\n    # Set input tensor and invoke interpreter\n    interpreter.set_tensor(input_details[0]['index'], img_resized)\n    interpreter.invoke()\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    # Draw bounding boxes and labels on the frame\n    for i in range(num_detections):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            label = labels[int(classes[0][i])]\n            score = scores[0][i]\n\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (int(left), int(top) - 5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame with detection to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_6241bec8_1770114235.py\", line 17, in <module>\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\nNameError: name 'tflite' is not defined."
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 182.596,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-21-01-064225_chatcmpl-d1bb0753-16b4-4630-af0d-f0d357fc28a1",
          "traceId": "32b7f03f",
          "type": "GENERATION",
          "name": "32b7_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:21:01.064000+00:00",
          "endTime": "2026-02-03T10:21:28.682000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27618.0,
          "totalTokens": 863,
          "promptTokens": 180,
          "completionTokens": 683,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-21-36-528211_chatcmpl-811d77b1-980d-4bc0-a2e0-03eec35dd443",
          "traceId": "32b7f03f",
          "type": "GENERATION",
          "name": "32b7_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:21:36.528000+00:00",
          "endTime": "2026-02-03T10:21:58.035000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21507.0,
          "totalTokens": 713,
          "promptTokens": 180,
          "completionTokens": 533,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-22-06-353306_chatcmpl-39868c7c-94cc-4060-a5c5-825a20e3d5b3",
          "traceId": "32b7f03f",
          "type": "GENERATION",
          "name": "32b7_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T10:22:06.353000+00:00",
          "endTime": "2026-02-03T10:22:37.951000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31598.0,
          "totalTokens": 962,
          "promptTokens": 180,
          "completionTokens": 782,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-22-45-728196_chatcmpl-a50ad577-790f-4a6f-a1c8-c89de42bde72",
          "traceId": "32b7f03f",
          "type": "GENERATION",
          "name": "32b7_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T10:22:45.728000+00:00",
          "endTime": "2026-02-03T10:23:16.829000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31101.0,
          "totalTokens": 949,
          "promptTokens": 180,
          "completionTokens": 769,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-23-24-565869_chatcmpl-01c69e5c-653d-4392-97f3-eb212d7a1c85",
          "traceId": "32b7f03f",
          "type": "GENERATION",
          "name": "32b7_tpusg_gen_attempt#5",
          "startTime": "2026-02-03T10:23:24.565000+00:00",
          "endTime": "2026-02-03T10:23:55.933000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31368.0,
          "totalTokens": 959,
          "promptTokens": 180,
          "completionTokens": 779,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "3ac63f55-62ec-488b-9b9e-3bab02aab094",
          "traceId": "32b7f03f",
          "type": "SPAN",
          "name": "error_32_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-03T10:24:03.660000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_6241bec8_1770114235.py\", line 17, in <module>\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\nNameError: name 'tflite' is not defined.",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:24:08.904Z",
      "createdAt": "2026-02-03T10:21:01.608Z"
    },
    {
      "id": "6f168498",
      "timestamp": "2026-02-03T10:17:56.426000+00:00",
      "name": "6f16_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nfrom pycoral.adapters import common\nfrom pycoral.adapters import detect\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Initialize the interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Load labels\nlabels = read_label_file(label_path)\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit('ERROR: Unable to open input video.')\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    # Read a new frame from the video\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert the frame to a PIL image and resize it to fit the model input size\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_img = img.resize((interpreter.get_input_details()[0]['shape'][1], interpreter.get_input_details()[0]['shape'][2]), Image.ANTIALIAS)\n\n    # Run inference\n    common.set_input(interpreter, resized_img)\n    interpreter.invoke()\n    objs = detect.get_objects(interpreter, score_threshold=confidence_threshold)\n\n    # Draw bounding boxes and labels on the frame\n    for obj in objs:\n        bbox = obj.bbox\n        label = labels.get(obj.id, 'Unknown')\n        score = obj.score\n\n        # Scale the bounding box to the original image size\n        xmin = int(bbox.xmin * frame_width)\n        ymin = int(bbox.ymin * frame_height)\n        xmax = int(bbox.xmax * frame_width)\n        ymax = int(bbox.ymax * frame_height)\n\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 102.093,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-17-56-428428_chatcmpl-aa4957ea-fac2-4fb3-9716-de67fab822fd",
          "traceId": "6f168498",
          "type": "GENERATION",
          "name": "6f16_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:17:56.428000+00:00",
          "endTime": "2026-02-03T10:18:23.143000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26715.0,
          "totalTokens": 833,
          "promptTokens": 180,
          "completionTokens": 653,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-18-30-991219_chatcmpl-739878ea-f324-477c-9c18-243fe29c37ce",
          "traceId": "6f168498",
          "type": "GENERATION",
          "name": "6f16_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:18:30.991000+00:00",
          "endTime": "2026-02-03T10:19:02.432000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31441.0,
          "totalTokens": 959,
          "promptTokens": 180,
          "completionTokens": 779,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-19-10-228416_chatcmpl-8272b413-8b00-4af9-878a-093cc9382d1e",
          "traceId": "6f168498",
          "type": "GENERATION",
          "name": "6f16_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T10:19:10.228000+00:00",
          "endTime": "2026-02-03T10:19:38.521000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28293.0,
          "totalTokens": 882,
          "promptTokens": 180,
          "completionTokens": 702,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:20:39.654Z",
      "createdAt": "2026-02-03T10:17:56.971Z"
    },
    {
      "id": "d450a5b5",
      "timestamp": "2026-02-03T10:15:23.777000+00:00",
      "name": "d450_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return {index: line.strip() for index, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize interpreter\ninterpreter = tflite.Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Write video\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    input_shape = input_details[0]['shape']\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    img = img.resize((input_shape[1], input_shape[2]), Image.ANTIALIAS)\n    input_data = np.expand_dims(img, axis=0)\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (xminn, xmaxx, yminn, ymaxx) = (xmin * width, xmax * width, ymin * height, ymax * height)\n            cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(xminn), int(yminn) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 70.93,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-15-23-777956_chatcmpl-83ac4ca7-e70a-45df-8a99-9c843e85ed20",
          "traceId": "d450a5b5",
          "type": "GENERATION",
          "name": "d450_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:15:23.777000+00:00",
          "endTime": "2026-02-03T10:15:56.621000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32844.0,
          "totalTokens": 992,
          "promptTokens": 180,
          "completionTokens": 812,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-16-04-337116_chatcmpl-e1240134-935c-48ad-8df0-d1ee4b066c1a",
          "traceId": "d450a5b5",
          "type": "GENERATION",
          "name": "d450_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:16:04.337000+00:00",
          "endTime": "2026-02-03T10:16:34.707000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30370.0,
          "totalTokens": 934,
          "promptTokens": 180,
          "completionTokens": 754,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:17:35.999Z",
      "createdAt": "2026-02-03T10:15:24.324Z"
    },
    {
      "id": "6e6e37a9",
      "timestamp": "2026-02-03T10:11:52.372000+00:00",
      "name": "6e6e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.adapters import common, detect\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nlabels = read_label_file(label_path)\n\n# Initialize interpreter and allocate tensors\ninterpreter = make_interpreter(model_path, device=':0')  # Use Edge TPU\ninterpreter.allocate_tensors()\n_, input_height, input_width, _ = interpreter.get_input_details()[0]['shape']\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit('Cannot open video')\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nout = cv2.VideoWriter(output_path, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert the frame to a PIL image and resize it\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).resize((input_width, input_height), Image.ANTIALIAS)\n\n    # Run inference\n    common.set_input(interpreter, img)\n    interpreter.invoke()\n    objs = detect.get_objects(interpreter, confidence_threshold)\n\n    # Draw bounding boxes and labels on the frame\n    for obj in objs:\n        bbox = obj.bbox\n        label = f'{labels.get(obj.id, obj.id)}: {obj.score:.2f}'\n        cv2.rectangle(frame, (bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax), (0, 255, 0), 2)\n        cv2.putText(frame, label, (bbox.xmin, bbox.ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 129.109,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-11-52-373484_chatcmpl-4fa47a03-2a62-493d-a3f0-a3f37f3b5dc2",
          "traceId": "6e6e37a9",
          "type": "GENERATION",
          "name": "6e6e_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:11:52.373000+00:00",
          "endTime": "2026-02-03T10:12:19.063000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26690.0,
          "totalTokens": 896,
          "promptTokens": 180,
          "completionTokens": 716,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-12-26-798815_chatcmpl-44bcc564-e9eb-41ba-ae85-06ea169f588a",
          "traceId": "6e6e37a9",
          "type": "GENERATION",
          "name": "6e6e_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:12:26.798000+00:00",
          "endTime": "2026-02-03T10:12:52.541000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25743.0,
          "totalTokens": 870,
          "promptTokens": 180,
          "completionTokens": 690,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-13-01-195131_chatcmpl-4daee37f-5417-4f64-934c-8e0392835d13",
          "traceId": "6e6e37a9",
          "type": "GENERATION",
          "name": "6e6e_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T10:13:01.195000+00:00",
          "endTime": "2026-02-03T10:13:31.419000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30224.0,
          "totalTokens": 992,
          "promptTokens": 180,
          "completionTokens": 812,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-13-39-198855_chatcmpl-db967769-e071-4d08-b378-3077e5f1d61a",
          "traceId": "6e6e37a9",
          "type": "GENERATION",
          "name": "6e6e_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T10:13:39.198000+00:00",
          "endTime": "2026-02-03T10:14:01.482000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22284.0,
          "totalTokens": 782,
          "promptTokens": 180,
          "completionTokens": 602,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:15:03.668Z",
      "createdAt": "2026-02-03T10:11:52.916Z"
    },
    {
      "id": "3f76c142",
      "timestamp": "2026-02-03T10:10:09.731000+00:00",
      "name": "3f76_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.adapters import common, detect\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the TFLite interpreter with Edge TPU\ninterpreter = tflite.Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load video\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_img = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.array(resized_img)\n    common.set_input(interpreter, input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get detection results\n    detections = detect.get_objects(interpreter, confidence_threshold)\n\n    # Draw bounding boxes and labels on the frame\n    for obj in detections:\n        bbox = obj.bbox\n        x0, y0 = int(bbox.xmin * width), int(bbox.ymin * height)\n        x1, y1 = int(bbox.xmax * width), int(bbox.ymax * height)\n\n        cv2.rectangle(frame, (x0, y0), (x1, y1), (0, 255, 0), 2)\n        label = f'{labels[obj.id]}: {obj.score:.2f}'\n        cv2.putText(frame, label, (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to the output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 25.232,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-10-09-732788_chatcmpl-36085dc8-c622-468f-9b38-97ab248ca034",
          "traceId": "3f76c142",
          "type": "GENERATION",
          "name": "3f76_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:10:09.732000+00:00",
          "endTime": "2026-02-03T10:10:34.964000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25232.0,
          "totalTokens": 859,
          "promptTokens": 180,
          "completionTokens": 679,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:11:32.000Z",
      "createdAt": "2026-02-03T10:10:10.275Z"
    },
    {
      "id": "3f6fba32",
      "timestamp": "2026-02-03T10:06:56.489000+00:00",
      "name": "3f6f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize interpreter with Edge TPU delegate\ninterpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit(\"Could not open video source\")\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(img_resized, axis=0)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detections on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            y_min, x_min, y_max, x_max = box\n            y_min *= height\n            x_min *= width\n            y_max *= height\n            x_max *= width\n\n            cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(x_min), int(y_min) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 115.645,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-06-56-490000_chatcmpl-47889248-5c32-4676-900f-a5bf235cd52c",
          "traceId": "3f6fba32",
          "type": "GENERATION",
          "name": "3f6f_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:06:56.490000+00:00",
          "endTime": "2026-02-03T10:07:44.759000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 48269.0,
          "totalTokens": 1464,
          "promptTokens": 180,
          "completionTokens": 1284,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-07-53-087159_chatcmpl-92a34abe-4592-49b8-80bf-1f40d93d0f15",
          "traceId": "3f6fba32",
          "type": "GENERATION",
          "name": "3f6f_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:07:53.087000+00:00",
          "endTime": "2026-02-03T10:08:18.493000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25406.0,
          "totalTokens": 862,
          "promptTokens": 180,
          "completionTokens": 682,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-08-26-369757_chatcmpl-36446feb-d2b8-4c49-875b-93ffbe74688f",
          "traceId": "3f6fba32",
          "type": "GENERATION",
          "name": "3f6f_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T10:08:26.369000+00:00",
          "endTime": "2026-02-03T10:08:52.135000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25766.0,
          "totalTokens": 876,
          "promptTokens": 180,
          "completionTokens": 696,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:09:50.251Z",
      "createdAt": "2026-02-03T10:06:57.032Z"
    },
    {
      "id": "e89b4023",
      "timestamp": "2026-02-03T10:02:46.883000+00:00",
      "name": "e89b_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with Edge TPU delegate\ninterpreter = tflite.Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer for saving the output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef draw_boxes(frame, boxes, class_ids, confidences):\n    for box, class_id, confidence in zip(boxes, class_ids, confidences):\n        if confidence >= confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            label = labels[int(class_id)]\n            color = (10, 255, 0)\n            cv2.rectangle(frame, (int(xmin * width), int(ymin * height)), (int(xmax * width), int(ymax * height)), color, 2)\n            cv2.putText(frame, f'{label}: {confidence:.2f}', (int(xmin * width), int(ymin * height) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = cv2.resize(frame, (300, 300))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.uint8)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    class_ids = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels on the frame\n    draw_boxes(frame, boxes, class_ids, scores)\n\n    # Write the frame with detection to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 171.892,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-02-46-884904_chatcmpl-7d552ad1-6fef-4c03-a642-12a6b5930eaa",
          "traceId": "e89b4023",
          "type": "GENERATION",
          "name": "e89b_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:02:46.884000+00:00",
          "endTime": "2026-02-03T10:03:21.434000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34550.0,
          "totalTokens": 1108,
          "promptTokens": 180,
          "completionTokens": 928,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-03-29-992726_chatcmpl-60b81fc1-5432-4963-8388-3fcc574164df",
          "traceId": "e89b4023",
          "type": "GENERATION",
          "name": "e89b_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:03:29.992000+00:00",
          "endTime": "2026-02-03T10:03:57.140000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27148.0,
          "totalTokens": 913,
          "promptTokens": 180,
          "completionTokens": 733,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-04-04-932409_chatcmpl-9c8d650b-a187-4a95-b3b9-f76b5a06426d",
          "traceId": "e89b4023",
          "type": "GENERATION",
          "name": "e89b_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T10:04:04.932000+00:00",
          "endTime": "2026-02-03T10:04:32.226000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27294.0,
          "totalTokens": 917,
          "promptTokens": 180,
          "completionTokens": 737,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-04-40-118776_chatcmpl-c1f6f0b8-4daf-480d-ba3c-e1433fd69dbd",
          "traceId": "e89b4023",
          "type": "GENERATION",
          "name": "e89b_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T10:04:40.118000+00:00",
          "endTime": "2026-02-03T10:05:02.619000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22501.0,
          "totalTokens": 788,
          "promptTokens": 180,
          "completionTokens": 608,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-05-11-081552_chatcmpl-e48e9057-88af-490d-9104-04b88adb585b",
          "traceId": "e89b4023",
          "type": "GENERATION",
          "name": "e89b_tpusg_gen_attempt#5",
          "startTime": "2026-02-03T10:05:11.081000+00:00",
          "endTime": "2026-02-03T10:05:38.776000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27695.0,
          "totalTokens": 928,
          "promptTokens": 180,
          "completionTokens": 748,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:06:36.661Z",
      "createdAt": "2026-02-03T10:02:47.425Z"
    },
    {
      "id": "46209b4b",
      "timestamp": "2026-02-03T10:00:22.623000+00:00",
      "name": "4620_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize interpreter\ninterpreter = tflite.Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess image\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    img = img.resize((input_details[0]['shape'][1], input_details[0]['shape'][2]), Image.ANTIALIAS)\n    input_data = np.array(img).reshape(input_details[0]['shape'])\n\n    # Set tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract output data from the interpreter\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * cap.get(3), xmax * cap.get(3),\n                                          ymin * cap.get(4), ymax * cap.get(4))\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f'{labels[int(classes[i])]}: {scores[i]:.2f}'\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 62.648,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-00-22-624480_chatcmpl-32718906-7806-4afa-90f7-d1a4fe715fbc",
          "traceId": "46209b4b",
          "type": "GENERATION",
          "name": "4620_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T10:00:22.624000+00:00",
          "endTime": "2026-02-03T10:00:49.802000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27178.0,
          "totalTokens": 914,
          "promptTokens": 180,
          "completionTokens": 734,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-00-57-574619_chatcmpl-a3f34357-df46-4c17-b59e-fb9f68a13309",
          "traceId": "46209b4b",
          "type": "GENERATION",
          "name": "4620_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T10:00:57.574000+00:00",
          "endTime": "2026-02-03T10:01:25.272000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27698.0,
          "totalTokens": 927,
          "promptTokens": 180,
          "completionTokens": 747,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:02:26.883Z",
      "createdAt": "2026-02-03T10:00:23.165Z"
    },
    {
      "id": "14485eae",
      "timestamp": "2026-02-03T09:56:38.019000+00:00",
      "name": "1448_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nimport numpy as np\nfrom pycoral.adapters import common, detect\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Initialize the interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Load labels\nlabels = read_label_file(label_path)\n\n# Open input video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit('ERROR: Unable to open input video')\n\n# Get video properties\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert frame to PIL Image and resize to model input size\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_img = img.resize(common.input_size(interpreter), Image.ANTIALIAS)\n\n    # Run inference\n    common.set_input(interpreter, resized_img)\n    interpreter.invoke()\n    objs = detect.get_objects(interpreter, confidence_threshold)\n\n    # Draw bounding boxes on the frame\n    for obj in objs:\n        bbox = obj.bbox\n        x0, y0 = int(bbox.xmin * width), int(bbox.ymin * height)\n        x1, y1 = int(bbox.xmax * width), int(bbox.ymax * height)\n        cv2.rectangle(frame, (x0, y0), (x1, y1), (0, 255, 0), 2)\n\n        # Add label and confidence\n        label = labels.get(obj.id, obj.id)\n        text = f'{label}: {obj.score:.2f}'\n        cv2.putText(frame, text, (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame to output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 142.224,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-56-38-021251_chatcmpl-9b45e00c-8e11-47ad-a682-755dd7a333d6",
          "traceId": "14485eae",
          "type": "GENERATION",
          "name": "1448_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:56:38.021000+00:00",
          "endTime": "2026-02-03T09:57:01.870000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23849.0,
          "totalTokens": 824,
          "promptTokens": 180,
          "completionTokens": 644,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-57-10-478688_chatcmpl-53c9df05-0bc3-4aa0-bfda-5159e68cd5c5",
          "traceId": "14485eae",
          "type": "GENERATION",
          "name": "1448_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:57:10.478000+00:00",
          "endTime": "2026-02-03T09:57:31.947000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21469.0,
          "totalTokens": 762,
          "promptTokens": 180,
          "completionTokens": 582,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-57-40-205330_chatcmpl-5e90489a-25d7-4417-9b9a-b96a434bcc94",
          "traceId": "14485eae",
          "type": "GENERATION",
          "name": "1448_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T09:57:40.205000+00:00",
          "endTime": "2026-02-03T09:58:27.442000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 47237.0,
          "totalTokens": 1428,
          "promptTokens": 180,
          "completionTokens": 1248,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-58-35-615663_chatcmpl-5232fcfa-0990-4df3-8ff2-5b50a17dbc21",
          "traceId": "14485eae",
          "type": "GENERATION",
          "name": "1448_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T09:58:35.615000+00:00",
          "endTime": "2026-02-03T09:59:00.245000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24630.0,
          "totalTokens": 835,
          "promptTokens": 180,
          "completionTokens": 655,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T10:00:01.758Z",
      "createdAt": "2026-02-03T09:56:38.561Z"
    },
    {
      "id": "d0293294",
      "timestamp": "2026-02-03T09:53:28.572000+00:00",
      "name": "d029_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nfrom PIL import Image\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom pycoral.adapters import common, detect\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\n# Initialize interpreter\ninterpreter = tflite.Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nlabels = load_labels(label_path)\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert frame to RGB and resize\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    input_tensor = np.expand_dims(rgb_frame, axis=0)\n\n    # Set tensor to point to the input data to be inferred\n    common.set_input(interpreter, input_tensor)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get detection results\n    detections = detect.get_objects(interpreter, score_threshold=confidence_threshold)\n\n    # Draw bounding boxes and labels on frame\n    for obj in detections:\n        bbox = obj.bbox\n        class_id = int(obj.id)\n        score = float(obj.score)\n\n        label = f'{labels[class_id]}: {score:.2f}'\n        color = (0, 255, 0)  # Green\n\n        cv2.rectangle(frame, (bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax), color, 2)\n        cv2.putText(frame, label, (bbox.xmin, bbox.ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n    # Write the frame with bounding boxes to the output video\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_95f6a033_1770112564.py\", line 52, in <module>\n    common.set_input(interpreter, input_tensor)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/pycoral/adapters/common.py\", line 75, in set_input\n    input_tensor(interpreter)[:, :] = data\nValueError: could not broadcast input array from shape (640,360,3) into shape (300,300,3)."
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 164.552,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-53-28-574249_chatcmpl-4172d831-3712-482a-b4d5-d3303f4fdd68",
          "traceId": "d0293294",
          "type": "GENERATION",
          "name": "d029_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:53:28.574000+00:00",
          "endTime": "2026-02-03T09:53:51.920000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23346.0,
          "totalTokens": 812,
          "promptTokens": 180,
          "completionTokens": 632,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-54-00-434668_chatcmpl-00f2ef62-5bd0-4433-8776-5d23dd48b3b1",
          "traceId": "d0293294",
          "type": "GENERATION",
          "name": "d029_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:54:00.434000+00:00",
          "endTime": "2026-02-03T09:54:26.810000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26376.0,
          "totalTokens": 895,
          "promptTokens": 180,
          "completionTokens": 715,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-54-35-348168_chatcmpl-63d7c5bf-e7fc-4a49-8f94-f6a93313885b",
          "traceId": "d0293294",
          "type": "GENERATION",
          "name": "d029_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T09:54:35.348000+00:00",
          "endTime": "2026-02-03T09:55:00.896000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25548.0,
          "totalTokens": 872,
          "promptTokens": 180,
          "completionTokens": 692,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-55-08-968120_chatcmpl-b54ed91c-f01d-49c4-a850-a53caaaa8d43",
          "traceId": "d0293294",
          "type": "GENERATION",
          "name": "d029_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T09:55:08.968000+00:00",
          "endTime": "2026-02-03T09:55:31.880000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22912.0,
          "totalTokens": 801,
          "promptTokens": 180,
          "completionTokens": 621,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-55-40-690084_chatcmpl-9c37dab1-080f-4fa5-8eeb-2f429e94d8a2",
          "traceId": "d0293294",
          "type": "GENERATION",
          "name": "d029_tpusg_gen_attempt#5",
          "startTime": "2026-02-03T09:55:40.690000+00:00",
          "endTime": "2026-02-03T09:56:04.773000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24083.0,
          "totalTokens": 833,
          "promptTokens": 180,
          "completionTokens": 653,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "4a88efec-5484-4b89-8cc9-eb1cb71d0860",
          "traceId": "d0293294",
          "type": "SPAN",
          "name": "error_d0_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-03T09:56:13.126000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_95f6a033_1770112564.py\", line 52, in <module>\n    common.set_input(interpreter, input_tensor)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/pycoral/adapters/common.py\", line 75, in set_input\n    input_tensor(interpreter)[:, :] = data\nValueError: could not broadcast input array from shape (640,360,3) into shape (300,300,3).",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:56:18.365Z",
      "createdAt": "2026-02-03T09:53:29.113Z"
    },
    {
      "id": "98936b75",
      "timestamp": "2026-02-03T09:49:41.987000+00:00",
      "name": "9893_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.adapters.common import input_size\nfrom pycoral.adapters.detect import get_objects\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Initialize the interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Read labels\nlabels = read_label_file(label_path)\n\n# Get input size\n_, height, width, _ = interpreter.get_input_details()[0]['shape']\n\n# Open video file for reading and writing\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nout = cv2.VideoWriter(output_path, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert the frame to a PIL image and resize it\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).resize((width, height), Image.ANTIALIAS)\n\n    # Prepare input tensor\n    scale = common.set_resized_input(interpreter, img.size, lambda size: img.resize(size, Image.ANTIALIAS))\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get detection results\n    objs = get_objects(interpreter, confidence_threshold, scale)\n\n    # Draw bounding boxes and labels on the frame\n    for obj in objs:\n        bbox = obj.bbox\n        label = f'{labels.get(obj.id, obj.id)}: {obj.score:.2f}'\n        cv2.rectangle(frame, (bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax), (0, 255, 0), 2)\n        cv2.putText(frame, label, (bbox.xmin, bbox.ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_cf55b8fc_1770112374.py\", line 41, in <module>\n    scale = common.set_resized_input(interpreter, img.size, lambda size: img.resize(size, Image.ANTIALIAS))\nNameError: name 'common' is not defined."
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 200.847,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-49-41-987708_chatcmpl-fca906b4-a174-40cd-b5eb-bd26821de169",
          "traceId": "98936b75",
          "type": "GENERATION",
          "name": "9893_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:49:41.987000+00:00",
          "endTime": "2026-02-03T09:50:09.386000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27399.0,
          "totalTokens": 918,
          "promptTokens": 180,
          "completionTokens": 738,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-50-17-293903_chatcmpl-f2a5de35-398d-4021-8761-21faf88d9475",
          "traceId": "98936b75",
          "type": "GENERATION",
          "name": "9893_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:50:17.293000+00:00",
          "endTime": "2026-02-03T09:50:48.618000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31325.0,
          "totalTokens": 1028,
          "promptTokens": 180,
          "completionTokens": 848,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-50-57-461393_chatcmpl-66a3cb33-4d8f-404b-932d-85a1e4a21bad",
          "traceId": "98936b75",
          "type": "GENERATION",
          "name": "9893_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T09:50:57.461000+00:00",
          "endTime": "2026-02-03T09:51:41.879000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 44418.0,
          "totalTokens": 1375,
          "promptTokens": 180,
          "completionTokens": 1195,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-51-50-451468_chatcmpl-2d5d292a-5e00-48c7-9cd1-ca8632c80281",
          "traceId": "98936b75",
          "type": "GENERATION",
          "name": "9893_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T09:51:50.451000+00:00",
          "endTime": "2026-02-03T09:52:22.814000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32363.0,
          "totalTokens": 1002,
          "promptTokens": 180,
          "completionTokens": 822,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-52-31-314706_chatcmpl-382ca589-ec8d-4bfe-9f56-becd9bd99d22",
          "traceId": "98936b75",
          "type": "GENERATION",
          "name": "9893_tpusg_gen_attempt#5",
          "startTime": "2026-02-03T09:52:31.314000+00:00",
          "endTime": "2026-02-03T09:52:54.036000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22722.0,
          "totalTokens": 786,
          "promptTokens": 180,
          "completionTokens": 606,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "0d8aa4ba-bdaf-4865-8b5f-083642aa4e78",
          "traceId": "98936b75",
          "type": "SPAN",
          "name": "error_98_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-03T09:53:02.834000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_cf55b8fc_1770112374.py\", line 41, in <module>\n    scale = common.set_resized_input(interpreter, img.size, lambda size: img.resize(size, Image.ANTIALIAS))\nNameError: name 'common' is not defined.",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:53:08.278Z",
      "createdAt": "2026-02-03T09:49:42.528Z"
    },
    {
      "id": "3e7fc381",
      "timestamp": "2026-02-03T09:45:43.454000+00:00",
      "name": "3e7f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.utils.edgetpu import make_interpreter\nfrom pycoral.adapters import common, detect\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Open input video file\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert frame to RGB and resize for model\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    input_tensor = Image.fromarray(rgb_frame).resize((300, 300), Image.ANTIALIAS)\n\n    # Set tensor to the interpreter and run inference\n    common.set_input(interpreter, input_tensor)\n    interpreter.invoke()\n\n    # Get detection results\n    detections = detect.get_objects(interpreter, confidence_threshold)\n\n    # Draw bounding boxes and labels on frame\n    for obj in detections:\n        bbox = obj.bbox\n        score = obj.score\n        label = labels[obj.id]\n\n        # Convert normalized coordinates to image coordinates\n        ymin, xmin, ymax, xmax = bbox\n        ymin, ymax = int(ymin * frame_height), int(ymax * frame_height)\n        xmin, xmax = int(xmin * frame_width), int(xmax * frame_width)\n\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n        label_text = f'{label}: {score:.2f}'\n        cv2.putText(frame, label_text, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 156.463,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-45-43-456063_chatcmpl-be4af65e-9fed-4950-926e-6dbe4dabeda2",
          "traceId": "3e7fc381",
          "type": "GENERATION",
          "name": "3e7f_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:45:43.456000+00:00",
          "endTime": "2026-02-03T09:46:05.557000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22101.0,
          "totalTokens": 778,
          "promptTokens": 180,
          "completionTokens": 598,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-46-14-390868_chatcmpl-8fd09d51-a003-46a2-a1f2-430623fefc53",
          "traceId": "3e7fc381",
          "type": "GENERATION",
          "name": "3e7f_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:46:14.390000+00:00",
          "endTime": "2026-02-03T09:46:38.388000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23998.0,
          "totalTokens": 829,
          "promptTokens": 180,
          "completionTokens": 649,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-46-46-241791_chatcmpl-d38ef2a7-4918-4057-a5a9-f425a9a42087",
          "traceId": "3e7fc381",
          "type": "GENERATION",
          "name": "3e7f_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T09:46:46.241000+00:00",
          "endTime": "2026-02-03T09:47:14.212000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27971.0,
          "totalTokens": 938,
          "promptTokens": 180,
          "completionTokens": 758,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-47-22-890195_chatcmpl-b12f5fb8-0953-461a-a81f-1910cd964020",
          "traceId": "3e7fc381",
          "type": "GENERATION",
          "name": "3e7f_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T09:47:22.890000+00:00",
          "endTime": "2026-02-03T09:47:46.763000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23873.0,
          "totalTokens": 826,
          "promptTokens": 180,
          "completionTokens": 646,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-47-54-539144_chatcmpl-c2cf8be9-445f-498d-be6c-9f56f6897cf9",
          "traceId": "3e7fc381",
          "type": "GENERATION",
          "name": "3e7f_tpusg_gen_attempt#5",
          "startTime": "2026-02-03T09:47:54.539000+00:00",
          "endTime": "2026-02-03T09:48:19.919000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25380.0,
          "totalTokens": 866,
          "promptTokens": 180,
          "completionTokens": 686,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:49:21.272Z",
      "createdAt": "2026-02-03T09:45:43.995Z"
    },
    {
      "id": "b265fb91",
      "timestamp": "2026-02-03T09:43:59.071000+00:00",
      "name": "b265_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.adapters import common\nfrom pycoral.adapters import detect\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Initialize the interpreter\ninterpreter = make_interpreter(model_path, device=':0')\ninterpreter.allocate_tensors()\n\n# Read labels\nlabels = read_label_file(label_path)\n\n# Open input video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert the frame to a PIL image and preprocess it\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_img = img.resize(common.input_size(interpreter), Image.ANTIALIAS)\n    common.set_input(interpreter, resized_img)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get detection results\n    detections = detect.get_objects(interpreter, confidence_threshold)\n\n    # Draw bounding boxes on the frame\n    for obj in detections:\n        bbox = obj.bbox\n        x0, y0, x1, y1 = int(bbox.xmin * frame_width), int(bbox.ymin * frame_height), int(bbox.xmax * frame_width), int(bbox.ymax * frame_height)\n        cv2.rectangle(frame, (x0, y0), (x1, y1), (0, 255, 0), 2)\n        label = f'{labels.get(obj.id, obj.id)}: {obj.score:.2f}'\n        cv2.putText(frame, label, (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 22.886,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-43-59-073368_chatcmpl-07a80c55-2467-4b1b-b119-0cd7180bcf1b",
          "traceId": "b265fb91",
          "type": "GENERATION",
          "name": "b265_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:43:59.073000+00:00",
          "endTime": "2026-02-03T09:44:21.959000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22886.0,
          "totalTokens": 797,
          "promptTokens": 180,
          "completionTokens": 617,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:45:23.214Z",
      "createdAt": "2026-02-03T09:43:59.611Z"
    },
    {
      "id": "e2d38972",
      "timestamp": "2026-02-03T09:40:24.373000+00:00",
      "name": "e2d3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {i: line.strip() for i, line in enumerate(f.readlines())}\n\n# Initialize the interpreter with Edge TPU delegate\ninterpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Video capture\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    image_resized = Image.fromarray(frame).resize((input_shape[1], input_shape[2]))\n    image_np = np.array(image_resized)\n    input_data = np.expand_dims(image_np, axis=0)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            score = scores[i]\n\n            # Convert normalized coordinates to pixel values\n            im_height, im_width, _ = frame.shape\n            (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                          ymin * im_height, ymax * im_height)\n\n            # Draw the bounding box and label on the image\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 137.083,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-40-24-375489_chatcmpl-1e46c3ea-a37c-4cba-851e-7b6f230ed57b",
          "traceId": "e2d38972",
          "type": "GENERATION",
          "name": "e2d3_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:40:24.375000+00:00",
          "endTime": "2026-02-03T09:41:05.587000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 41212.0,
          "totalTokens": 1198,
          "promptTokens": 180,
          "completionTokens": 1018,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-41-14-221420_chatcmpl-cc931ad9-1147-48ce-8c8d-129b8e099e5d",
          "traceId": "e2d38972",
          "type": "GENERATION",
          "name": "e2d3_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:41:14.221000+00:00",
          "endTime": "2026-02-03T09:42:00.934000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 46713.0,
          "totalTokens": 1333,
          "promptTokens": 180,
          "completionTokens": 1153,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-42-09-535271_chatcmpl-b2b67389-9e88-4319-b961-1f4f38a91a34",
          "traceId": "e2d38972",
          "type": "GENERATION",
          "name": "e2d3_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T09:42:09.535000+00:00",
          "endTime": "2026-02-03T09:42:41.458000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31923.0,
          "totalTokens": 975,
          "promptTokens": 180,
          "completionTokens": 795,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:43:38.644Z",
      "createdAt": "2026-02-03T09:40:24.911Z"
    },
    {
      "id": "90f55eed",
      "timestamp": "2026-02-03T09:37:55.959000+00:00",
      "name": "90f5_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nfrom pycoral.adapters import common\nfrom pycoral.adapters import detect\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load the TFLite model and labels\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\nlabels = read_label_file(label_path)\n\n# Open the input video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit('ERROR: Unable to open video source.')\n\n# Get video properties to initialize output video writer\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert the frame to a format suitable for the model\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n    resized_img = img.resize((300, 300), Image.ANTIALIAS)\n\n    # Run inference\n    common.set_input(interpreter, resized_img)\n    interpreter.invoke()\n    objs = detect.get_objects(interpreter, confidence_threshold)\n\n    # Draw detection results on the frame\n    for obj in objs:\n        bbox = obj.bbox\n        label = labels.get(obj.id, obj.id)\n        score = int(obj.score * 100)\n        display_str = f'{label}: {score}%'\n        \n        # Scale bounding box coordinates to original image size\n        ymin, xmin, ymax, xmax = bbox\n        ymin, xmin, ymax, xmax = (ymin * frame_height, xmin * frame_width,\n                                  ymax * frame_height, xmax * frame_width)\n        \n        cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), (0, 255, 0), 2)\n        cv2.putText(frame, display_str, (int(xmin), int(ymin) - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame with detection results to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 66.185,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-37-55-961506_chatcmpl-b7196c17-6de2-4374-b18c-624efd05f52a",
          "traceId": "90f55eed",
          "type": "GENERATION",
          "name": "90f5_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:37:55.961000+00:00",
          "endTime": "2026-02-03T09:38:26.243000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30282.0,
          "totalTokens": 929,
          "promptTokens": 180,
          "completionTokens": 749,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-38-34-716708_chatcmpl-d0ecde24-1f7b-404f-b3b2-71e0c573c6b6",
          "traceId": "90f55eed",
          "type": "GENERATION",
          "name": "90f5_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:38:34.716000+00:00",
          "endTime": "2026-02-03T09:39:02.146000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27430.0,
          "totalTokens": 860,
          "promptTokens": 180,
          "completionTokens": 680,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:40:03.673Z",
      "createdAt": "2026-02-03T09:37:56.496Z"
    },
    {
      "id": "b380bc76",
      "timestamp": "2026-02-03T09:34:19.126000+00:00",
      "name": "b380_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nimport numpy as np\nfrom PIL import Image\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the TFLite interpreter with Edge TPU support\ninterpreter = Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\ninterpreter.allocate_tensors()\n\n# Get model details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Open the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    img = Image.fromarray(frame)\n    img = img.resize((width, height))\n    input_data = np.array(img)\n\n    # Add a fourth dimension to the image (1, height, width, channels)\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Normalize pixel values if using a floating model\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Render detections\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * cap.get(3), xmax * cap.get(3),\n                                          ymin * cap.get(4), ymax * cap.get(4))\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\n    # Write the frame with detections to the output file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_b7c17a3a_1770111442.py\", line 18, in <module>\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\nNameError: name 'tflite' is not defined."
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 190.924,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-34-19-129218_chatcmpl-18211569-e052-4b3a-bfb1-c83c749e8729",
          "traceId": "b380bc76",
          "type": "GENERATION",
          "name": "b380_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:34:19.129000+00:00",
          "endTime": "2026-02-03T09:34:48.819000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29690.0,
          "totalTokens": 902,
          "promptTokens": 180,
          "completionTokens": 722,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-34-57-497636_chatcmpl-ac919ac8-c4cb-49d0-8170-c984e2f4f16f",
          "traceId": "b380bc76",
          "type": "GENERATION",
          "name": "b380_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:34:57.497000+00:00",
          "endTime": "2026-02-03T09:35:23.114000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25617.0,
          "totalTokens": 816,
          "promptTokens": 180,
          "completionTokens": 636,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-35-31-381532_chatcmpl-a5584765-2c78-4037-ba32-43e7b11bbe39",
          "traceId": "b380bc76",
          "type": "GENERATION",
          "name": "b380_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T09:35:31.381000+00:00",
          "endTime": "2026-02-03T09:35:57.809000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26428.0,
          "totalTokens": 833,
          "promptTokens": 180,
          "completionTokens": 653,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-36-06-164360_chatcmpl-e9116231-9349-403e-a0d3-40bcee5900b8",
          "traceId": "b380bc76",
          "type": "GENERATION",
          "name": "b380_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T09:36:06.164000+00:00",
          "endTime": "2026-02-03T09:36:41.790000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 35626.0,
          "totalTokens": 1054,
          "promptTokens": 180,
          "completionTokens": 874,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-36-49-594691_chatcmpl-cfd93b52-b257-430a-8209-94cb3dce4697",
          "traceId": "b380bc76",
          "type": "GENERATION",
          "name": "b380_tpusg_gen_attempt#5",
          "startTime": "2026-02-03T09:36:49.594000+00:00",
          "endTime": "2026-02-03T09:37:22.292000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32698.0,
          "totalTokens": 985,
          "promptTokens": 180,
          "completionTokens": 805,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "509569f7-09c9-46a5-b253-5e3ead2e5b2b",
          "traceId": "b380bc76",
          "type": "SPAN",
          "name": "error_b3_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-03T09:37:30.053000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_b7c17a3a_1770111442.py\", line 18, in <module>\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\nNameError: name 'tflite' is not defined.",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:37:35.456Z",
      "createdAt": "2026-02-03T09:34:19.660Z"
    },
    {
      "id": "145c162d",
      "timestamp": "2026-02-03T09:31:11.734000+00:00",
      "name": "145c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nimport numpy as np\nimport tflite_runtime.interpreter as tflite\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf8') as f:\n        return {index: line.strip() for index, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize TFLite interpreter with Edge TPU delegate\ninterpreter = tflite.Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check the type of the input tensor\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1], input_shape[2]\n    img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (width, height))\n    input_data = np.expand_dims(img, axis=0)\n\n    # Normalize pixel values if using a floating model (note: normalization is not always necessary and depends on the model)\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Postprocess the results\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            p1 = (int(left), int(top))\n            p2 = (int(right), int(bottom))\n\n            # Draw bounding box\n            cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n\n            # Draw label and confidence score\n            class_id = int(classes[i])\n            label = f'{labels.get(class_id, \"Unknown\")}: {int(scores[i]*100)}%'\n            position = (int(left), int(top) - 10)\n            cv2.putText(frame, label, position, cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n                        (0, 255, 0), 2)\n\n    # Write the frame with detection to the output file\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 109.158,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-31-11-735227_chatcmpl-cc0129dc-7971-4f40-abce-ac74b78e49a6",
          "traceId": "145c162d",
          "type": "GENERATION",
          "name": "145c_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:31:11.735000+00:00",
          "endTime": "2026-02-03T09:31:38.677000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26942.0,
          "totalTokens": 834,
          "promptTokens": 180,
          "completionTokens": 654,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-31-47-058229_chatcmpl-cf0e81cd-78a7-4ff5-a909-e5ba7b6e139a",
          "traceId": "145c162d",
          "type": "GENERATION",
          "name": "145c_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:31:47.058000+00:00",
          "endTime": "2026-02-03T09:32:16.313000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29255.0,
          "totalTokens": 901,
          "promptTokens": 180,
          "completionTokens": 721,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-32-23-990054_chatcmpl-3f796cf6-9749-4537-a0f0-cc28b0d125d4",
          "traceId": "145c162d",
          "type": "GENERATION",
          "name": "145c_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T09:32:23.990000+00:00",
          "endTime": "2026-02-03T09:33:00.893000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 36903.0,
          "totalTokens": 1093,
          "promptTokens": 180,
          "completionTokens": 913,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:33:58.986Z",
      "createdAt": "2026-02-03T09:31:12.265Z"
    },
    {
      "id": "a967c3c5",
      "timestamp": "2026-02-03T09:28:26.109000+00:00",
      "name": "a967_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nfrom PIL import Image\nfrom pycoral.utils.edgetpu import make_interpreter\nfrom pycoral.adapters import common, detect\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize the interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert the frame to RGB and resize it for the model\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    img = Image.fromarray(rgb_frame)\n    resized_img = img.resize((common.input_size(interpreter)[0], common.input_size(interpreter)[1]), Image.ANTIALIAS)\n\n    # Run inference\n    common.set_input(interpreter, resized_img)\n    interpreter.invoke()\n    detections = detect.get_objects(interpreter, confidence_threshold)\n\n    # Draw bounding boxes and labels on the original frame\n    for obj in detections:\n        bbox = obj.bbox\n        x0, y0 = int(bbox.xmin * width), int(bbox.ymin * height)\n        x1, y1 = int(bbox.xmax * width), int(bbox.ymax * height)\n        label = f'{labels.get(obj.id, \"unknown\")}: {obj.score:.2f}'\n        \n        cv2.rectangle(frame, (x0, y0), (x1, y1), (0, 255, 0), 2)\n        cv2.putText(frame, label, (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 84.15,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-28-26-111152_chatcmpl-d9021446-bff3-40f5-bdc5-1b6c92d7a9d8",
          "traceId": "a967c3c5",
          "type": "GENERATION",
          "name": "a967_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:28:26.111000+00:00",
          "endTime": "2026-02-03T09:29:14.868000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 48757.0,
          "totalTokens": 1379,
          "promptTokens": 180,
          "completionTokens": 1199,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-29-23-425912_chatcmpl-aeabe1f6-6303-41ae-9bb6-8f91a0e0cee3",
          "traceId": "a967c3c5",
          "type": "GENERATION",
          "name": "a967_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:29:23.425000+00:00",
          "endTime": "2026-02-03T09:29:50.261000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26836.0,
          "totalTokens": 846,
          "promptTokens": 180,
          "completionTokens": 666,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:30:51.920Z",
      "createdAt": "2026-02-03T09:28:26.636Z"
    },
    {
      "id": "1a697c0c",
      "timestamp": "2026-02-03T09:26:37.662000+00:00",
      "name": "1a69_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import cv2\nfrom PIL import Image\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {index: line.strip() for index, line in enumerate(f.readlines())}\n\n# Initialize interpreter with Edge TPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from file\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties to set up the writer\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Process frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert frame to RGB and resize\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    input_shape = input_details[0]['shape']\n    resized_frame = Image.fromarray(rgb_frame).resize((input_shape[1], input_shape[2]), Image.ANTIALIAS)\n\n    # Prepare input tensor\n    input_tensor = np.array(resized_frame).reshape(input_shape)\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detections on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (10, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (xminn, yminn - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 26.711,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-26-37-663824_chatcmpl-06220500-63bb-41f5-bf84-94511f9536b8",
          "traceId": "1a697c0c",
          "type": "GENERATION",
          "name": "1a69_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:26:37.663000+00:00",
          "endTime": "2026-02-03T09:27:04.374000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26711.0,
          "totalTokens": 897,
          "promptTokens": 180,
          "completionTokens": 717,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:28:05.830Z",
      "createdAt": "2026-02-03T09:26:38.185Z"
    },
    {
      "id": "c838b56b",
      "timestamp": "2026-02-03T09:23:24.023000+00:00",
      "name": "c838_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import cv2\nfrom PIL import Image\nfrom pycoral.adapters.common import input_size\nfrom pycoral.adapters.detect import get_objects\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Initialize the interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Load labels\nlabels = read_label_file(label_path)\n\n# Get input size\n_, height, width, _ = interpreter.get_input_details()[0]['shape']\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and convert image to RGB\n    img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).resize((width, height), Image.ANTIALIAS)\n\n    # Run inference\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], img)\n    interpreter.invoke()\n\n    # Get detection results\n    objs = get_objects(interpreter, confidence_threshold)\n\n    # Draw bounding boxes and labels on the frame\n    for obj in objs:\n        bbox = obj.bbox\n        label = '{}: {:.2f}'.format(labels.get(obj.id, obj.id), obj.score)\n        cv2.rectangle(frame, (bbox.xmin, bbox.ymin), (bbox.xmax, bbox.ymax), (0, 255, 0), 2)\n        cv2.putText(frame, label, (bbox.xmin, bbox.ymin - 10 if bbox.ymin > 30 else bbox.ymin + 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_e2d6239f_1770110763.py\", line 43, in <module>\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], img)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Dimension mismatch. Got 3 but expected 4 for input 175.."
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 167.686,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-23-24-024607_chatcmpl-96264efc-512e-49f9-9b7c-cb4cfb821752",
          "traceId": "c838b56b",
          "type": "GENERATION",
          "name": "c838_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:23:24.024000+00:00",
          "endTime": "2026-02-03T09:23:55.263000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31239.0,
          "totalTokens": 1024,
          "promptTokens": 180,
          "completionTokens": 844,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-24-03-125873_chatcmpl-6c942dfc-74e0-4c07-82df-1fd05c6490d0",
          "traceId": "c838b56b",
          "type": "GENERATION",
          "name": "c838_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:24:03.125000+00:00",
          "endTime": "2026-02-03T09:24:24.286000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21161.0,
          "totalTokens": 754,
          "promptTokens": 180,
          "completionTokens": 574,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-24-32-878238_chatcmpl-9b3e8f03-5a19-423d-9d47-533ab464e879",
          "traceId": "c838b56b",
          "type": "GENERATION",
          "name": "c838_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T09:24:32.878000+00:00",
          "endTime": "2026-02-03T09:24:58.583000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25705.0,
          "totalTokens": 877,
          "promptTokens": 180,
          "completionTokens": 697,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-25-07-524383_chatcmpl-2a63d14f-3829-41bb-afc2-1def4c776e2a",
          "traceId": "c838b56b",
          "type": "GENERATION",
          "name": "c838_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T09:25:07.524000+00:00",
          "endTime": "2026-02-03T09:25:34.575000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27051.0,
          "totalTokens": 913,
          "promptTokens": 180,
          "completionTokens": 733,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-25-42-328524_chatcmpl-8ede61cf-0591-49c3-94f2-869f49a62fed",
          "traceId": "c838b56b",
          "type": "GENERATION",
          "name": "c838_tpusg_gen_attempt#5",
          "startTime": "2026-02-03T09:25:42.328000+00:00",
          "endTime": "2026-02-03T09:26:03.091000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20763.0,
          "totalTokens": 742,
          "promptTokens": 180,
          "completionTokens": 562,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "f5a5615e-8b14-48c6-b3a0-990771b0ae76",
          "traceId": "c838b56b",
          "type": "SPAN",
          "name": "error_c8_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-03T09:26:11.710000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_e2d6239f_1770110763.py\", line 43, in <module>\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], img)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Dimension mismatch. Got 3 but expected 4 for input 175..",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "promptTokens": 0,
          "completionTokens": 0,
          "costDetails": {},
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:26:17.246Z",
      "createdAt": "2026-02-03T09:23:24.541Z"
    },
    {
      "id": "7c6dfa6e",
      "timestamp": "2026-02-03T09:18:43.345000+00:00",
      "name": "7c6d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom pycoral.adapters.common import input_size\nfrom pycoral.adapters.detect import get_objects\nfrom pycoral.utils.dataset import read_label_file\nfrom pycoral.utils.edgetpu import make_interpreter\n\n# Configuration parameters\nmodel_path = '/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite'\nlabel_path = '/home/mendel/tinyml_autopilot/models/labelmap.txt'\ninput_path = '/home/mendel/tinyml_autopilot/data/sheeps.mp4'\noutput_path = '/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4'\nconfidence_threshold = 0.5\n\n# Initialize the interpreter\ninterpreter = make_interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Load labels\nlabels = read_label_file(label_path)\n\n# Get input size\nwidth, height = input_size(interpreter)\n\n# Open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit('ERROR: Unable to open video source.')\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and prepare the image\n    im = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)).resize((width, height), Image.ANTIALIAS)\n    scale = (frame.shape[1] / width, frame.shape[0] / height)\n\n    # Run inference\n    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], np.array(im).reshape(1, width, height, 3))\n    interpreter.invoke()\n\n    # Get detected objects\n    objs = get_objects(interpreter, confidence_threshold)\n    \n    # Draw results on the frame\n    for obj in objs:\n        bbox = obj.bbox\n        bbox_scaled = (int(bbox.xmin * scale[0]), int(bbox.ymin * scale[1]),\n                       int(bbox.width * scale[0]), int(bbox.height * scale[1]))\n        \n        cv2.rectangle(frame, (bbox_scaled[0], bbox_scaled[1]), \n                      (bbox_scaled[0] + bbox_scaled[2], bbox_scaled[1] + bbox_scaled[3]), \n                      (0, 255, 0), 2)\n        \n        label = f'{labels.get(obj.id, obj.id)}: {obj.score:.2f}'\n        cv2.putText(frame, label, (bbox_scaled[0], bbox_scaled[1] - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    # Write the frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_db36_tpusg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 198.987,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-11-18-43-347596_chatcmpl-9dd0d6ff-3cbb-4605-ab1a-562d2ab9afc0",
          "traceId": "7c6dfa6e",
          "type": "GENERATION",
          "name": "7c6d_tpusg_gen_attempt#1",
          "startTime": "2026-02-03T09:18:43.347000+00:00",
          "endTime": "2026-02-03T09:19:52.797000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 69450.0,
          "totalTokens": 961,
          "promptTokens": 180,
          "completionTokens": 781,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-20-00-547443_chatcmpl-bf347ea0-f1dc-44a1-9bfe-c2a53485d6c9",
          "traceId": "7c6dfa6e",
          "type": "GENERATION",
          "name": "7c6d_tpusg_gen_attempt#2",
          "startTime": "2026-02-03T09:20:00.547000+00:00",
          "endTime": "2026-02-03T09:20:33.676000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 33129.0,
          "totalTokens": 1075,
          "promptTokens": 180,
          "completionTokens": 895,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-20-41-640227_chatcmpl-86cfcbc5-26af-495d-a475-3ab3c8edc95d",
          "traceId": "7c6dfa6e",
          "type": "GENERATION",
          "name": "7c6d_tpusg_gen_attempt#3",
          "startTime": "2026-02-03T09:20:41.640000+00:00",
          "endTime": "2026-02-03T09:21:29.869000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 48229.0,
          "totalTokens": 1477,
          "promptTokens": 180,
          "completionTokens": 1297,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-11-21-38-460024_chatcmpl-b4d7fb5f-e4df-4581-b11d-dd14ef08bd25",
          "traceId": "7c6dfa6e",
          "type": "GENERATION",
          "name": "7c6d_tpusg_gen_attempt#4",
          "startTime": "2026-02-03T09:21:38.460000+00:00",
          "endTime": "2026-02-03T09:22:02.334000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23874.0,
          "totalTokens": 827,
          "promptTokens": 180,
          "completionTokens": 647,
          "costDetails": {},
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "environment": "default",
      "updatedAt": "2026-02-03T09:23:04.149Z",
      "createdAt": "2026-02-03T09:18:43.866Z"
    }
  ],
  "meta": {
    "total_items": 30
  }
}