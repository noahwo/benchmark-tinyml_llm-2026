{
  "data": [
    {
      "id": "5b8423b0",
      "timestamp": "2026-02-06T23:35:58.999000+00:00",
      "name": "5b84_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open input video\")\n\n# Prepare to write the output video\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nwidth  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess_image(frame):\n    # Ensure the frame is in BGR format and convert it to RGB\n    if len(frame.shape) == 3 and frame.shape[2] == 4:\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGRA2BGR)\n    elif len(frame.shape) != 3 or frame.shape[2] != 3:\n        raise ValueError(\"Invalid image format\")\n    input_data = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data.astype(np.uint8)\n\ndef draw_detections(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (left, right, top, bottom) = (int(xmin * w), int(xmax * w),\n                                          int(ymin * h), int(ymax * h))\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (left, top - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    \n    # Set tensor to point to the preprocessed image data.\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    \n    # Run inference\n    start_time = time.time()\n    interpreter.invoke()\n    end_time = time.time()\n    print(f\"Inference time: {end_time - start_time:.2f} seconds\")\n    \n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    draw_detections(frame, boxes[0], classes[0], scores[0])\n    \n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 155.557,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-35-59-000923_chatcmpl-aa592962-3878-429f-986f-21a24190a0e5",
          "traceId": "5b8423b0",
          "type": "GENERATION",
          "name": "5b84_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:35:59+00:00",
          "endTime": "2026-02-06T23:36:21.146000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22146.0,
          "costDetails": {},
          "totalTokens": 1313,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 839,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-36-28-725391_chatcmpl-1581b7dc-83cc-4706-8423-b8fc2e38e90a",
          "traceId": "5b8423b0",
          "type": "GENERATION",
          "name": "5b84_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:36:28.725000+00:00",
          "endTime": "2026-02-06T23:36:50.712000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21987.0,
          "costDetails": {},
          "totalTokens": 1527,
          "environment": "default",
          "promptTokens": 706,
          "completionTokens": 821,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-36-58-427129_chatcmpl-9a3f5cc6-9c80-4117-8453-f134eb3a991e",
          "traceId": "5b8423b0",
          "type": "GENERATION",
          "name": "5b84_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:36:58.427000+00:00",
          "endTime": "2026-02-06T23:37:23.554000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25127.0,
          "costDetails": {},
          "totalTokens": 1680,
          "environment": "default",
          "promptTokens": 738,
          "completionTokens": 942,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-37-31-770151_chatcmpl-e8f16314-a67e-4513-9dc6-b1fdae6e5df5",
          "traceId": "5b8423b0",
          "type": "GENERATION",
          "name": "5b84_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T23:37:31.770000+00:00",
          "endTime": "2026-02-06T23:38:00.090000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28320.0,
          "costDetails": {},
          "totalTokens": 1728,
          "environment": "default",
          "promptTokens": 660,
          "completionTokens": 1068,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-38-08-321565_chatcmpl-bc780b08-8674-4e62-872f-7f4b668b98db",
          "traceId": "5b8423b0",
          "type": "GENERATION",
          "name": "5b84_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T23:38:08.321000+00:00",
          "endTime": "2026-02-06T23:38:34.557000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26236.0,
          "costDetails": {},
          "totalTokens": 1718,
          "environment": "default",
          "promptTokens": 737,
          "completionTokens": 981,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T23:35:59.522Z",
      "updatedAt": "2026-02-06T23:39:32.326Z",
      "environment": "default"
    },
    {
      "id": "2d84efb4",
      "timestamp": "2026-02-06T23:33:09.205000+00:00",
      "name": "2d84_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Preprocessing and inference\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and normalize image\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1], input_shape[2]\n    resized_frame = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(resized_frame, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is uint8\n\n    # Set tensor to model and run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    start_time = time.time()\n    interpreter.invoke()\n    inference_time = time.time() - start_time\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label_text = f'{label}: {scores[i]:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 8),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n    print(f'Inference time: {inference_time:.3f}s')\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 91.345,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-33-09-206465_chatcmpl-ae306c45-2c30-40c4-b4c6-908e43032e41",
          "traceId": "2d84efb4",
          "type": "GENERATION",
          "name": "2d84_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:33:09.206000+00:00",
          "endTime": "2026-02-06T23:33:32.365000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23159.0,
          "costDetails": {},
          "totalTokens": 1352,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 878,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-33-39-996168_chatcmpl-348abbd1-285c-4a70-a477-ca3336688be6",
          "traceId": "2d84efb4",
          "type": "GENERATION",
          "name": "2d84_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:33:39.996000+00:00",
          "endTime": "2026-02-06T23:34:09.343000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29347.0,
          "costDetails": {},
          "totalTokens": 1703,
          "environment": "default",
          "promptTokens": 599,
          "completionTokens": 1104,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-34-17-712177_chatcmpl-39c9ad1c-fa0b-465c-9f88-c55b3a901c2a",
          "traceId": "2d84efb4",
          "type": "GENERATION",
          "name": "2d84_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:34:17.712000+00:00",
          "endTime": "2026-02-06T23:34:40.551000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22839.0,
          "costDetails": {},
          "totalTokens": 1521,
          "environment": "default",
          "promptTokens": 659,
          "completionTokens": 862,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T23:33:09.728Z",
      "updatedAt": "2026-02-06T23:35:38.642Z",
      "environment": "default"
    },
    {
      "id": "8ac6bfd4",
      "timestamp": "2026-02-06T23:30:20.543000+00:00",
      "name": "8ac6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file from the given path\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Get video properties to save the result video with the same settings\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize image to match input tensor shape\n    input_shape = input_details[0]['shape']\n    resized_image = cv2.resize(image, (input_shape[1], input_shape[2]))\n    \n    # Normalize the image if required by the model\n    mean = np.array([127.5])\n    std = np.array([127.5])\n    normalized_image = (resized_image.astype(np.float32) - mean) / std\n    \n    # Convert to UINT8 as expected by the model\n    input_data = normalized_image.astype(np.uint8)\n    \n    # Add batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef draw_detections(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (left, right, top, bottom) = (xmin * w, xmax * w,\n                                          ymin * h, ymax * h)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n\n            # Draw label\n            object_name = labels[int(cls)]\n            label = f'{object_name}: {score:.2f}'\n            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            label_ymin = max(top - labelSize[1], 1)\n            cv2.rectangle(frame, (int(left), int(label_ymin - labelSize[1])),\n                          (int(left + labelSize[0]), int(label_ymin + baseLine)), (0, 255, 0), cv2.FILLED)\n            cv2.putText(frame, label, (int(left), int(label_ymin)),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n# Process each frame in the video\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    \n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    \n    # Run inference\n    interpreter.invoke()\n    \n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    \n    # Draw detections on the frame\n    draw_detections(frame, boxes, classes, scores)\n    \n    # Write the processed frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 89.938,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-30-20-544415_chatcmpl-1b303d64-3a21-49c5-93f1-c7a7b96ae8e4",
          "traceId": "8ac6bfd4",
          "type": "GENERATION",
          "name": "8ac6_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:30:20.544000+00:00",
          "endTime": "2026-02-06T23:30:44.649000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24105.0,
          "costDetails": {},
          "totalTokens": 1387,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 913,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-30-52-244311_chatcmpl-5c39af3f-039e-40b2-b9f0-6d9ad23022e9",
          "traceId": "8ac6bfd4",
          "type": "GENERATION",
          "name": "8ac6_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:30:52.244000+00:00",
          "endTime": "2026-02-06T23:31:14.530000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22286.0,
          "costDetails": {},
          "totalTokens": 1438,
          "environment": "default",
          "promptTokens": 600,
          "completionTokens": 838,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-31-22-847972_chatcmpl-3b36b23d-9cc9-40ec-9093-9d1294830cf3",
          "traceId": "8ac6bfd4",
          "type": "GENERATION",
          "name": "8ac6_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:31:22.847000+00:00",
          "endTime": "2026-02-06T23:31:50.482000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27635.0,
          "costDetails": {},
          "totalTokens": 1703,
          "environment": "default",
          "promptTokens": 661,
          "completionTokens": 1042,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T23:30:21.065Z",
      "updatedAt": "2026-02-06T23:32:48.304Z",
      "environment": "default"
    },
    {
      "id": "2d3a1765",
      "timestamp": "2026-02-06T23:26:57.625000+00:00",
      "name": "2d3a_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    return {int(line.split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines if line.strip() and len(line.split(maxsplit=1)) > 1}\n\ndef set_input_tensor(interpreter, image):\n    tensor_index = interpreter.get_input_details()[0]['index']\n    input_tensor = interpreter.tensor(tensor_index)()[0]\n    input_tensor[:, :] = np.expand_dims(image, axis=0)\n\ndef get_output_tensor(interpreter, index):\n    output_details = interpreter.get_output_details()[index]\n    tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n    return tensor\n\ndef detect_objects(interpreter, image, threshold):\n    set_input_tensor(interpreter, image)\n    interpreter.invoke()\n    \n    boxes = get_output_tensor(interpreter, 0)\n    classes = get_output_tensor(interpreter, 1)\n    scores = get_output_tensor(interpreter, 2)\n    count = int(get_output_tensor(interpreter, 3))\n    \n    results = []\n    for i in range(count):\n        if scores[i] >= threshold:\n            result = {\n                'bounding_box': boxes[i],\n                'class_id': classes[i],\n                'score': scores[i]\n            }\n            results.append(result)\n    return results\n\ndef main():\n    # Load the TFLite model and allocate tensors.\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    interpreter.allocate_tensors()\n    \n    # Load labels\n    labels = load_labels(label_path)\n    \n    # Read the input video\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video at {input_path}\")\n        return\n    \n    # Get video properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    \n    # Define the codec and create VideoWriter object\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Preprocess the image\n        input_image = cv2.resize(frame, (300, 300))  # Assuming model expects 300x300 images\n        input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n        \n        # Perform inference\n        results = detect_objects(interpreter, input_image, confidence_threshold)\n        \n        # Draw bounding boxes and labels on the frame\n        for result in results:\n            ymin, xmin, ymax, xmax = result['bounding_box']\n            xmin = int(xmin * width)\n            xmax = int(xmax * width)\n            ymin = int(ymin * height)\n            ymax = int(ymax * height)\n            \n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            label = f\"{labels[int(result['class_id'])]}: {result['score']:.2f}\"\n            cv2.putText(frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n        \n        # Write the frame to the output video\n        out.write(frame)\n    \n    # Release everything when job is finished\n    cap.release()\n    out.release()\n    print(f\"Processed video saved at {output_path}\")\n\nif __name__ == '__main__':\n    main()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_ea0e7223_1770420586.py\", line 105, in <module>\n    main()\n  File \"script_ea0e7223_1770420586.py\", line 55, in main\n    labels = load_labels(label_path)\n  File \"script_ea0e7223_1770420586.py\", line 17, in load_labels\n    return {int(line.split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines if line.strip() and len(line.split(maxsplit=1)) > 1}\n  File \"script_ea0e7223_1770420586.py\", line 17, in <dictcomp>\n    return {int(line.split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines if line.strip() and len(line.split(maxsplit=1)) > 1}\nValueError: invalid literal for int() with base 10: 'traffic'."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 176.732,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-26-57-627117_chatcmpl-6913d198-dc78-4e7c-90bf-d93bbe40afef",
          "traceId": "2d3a1765",
          "type": "GENERATION",
          "name": "2d3a_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:26:57.627000+00:00",
          "endTime": "2026-02-06T23:27:26.718000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29091.0,
          "costDetails": {},
          "totalTokens": 1576,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 1102,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-27-34-305779_chatcmpl-0c59d995-8463-4849-94eb-eb7065d0a25a",
          "traceId": "2d3a1765",
          "type": "GENERATION",
          "name": "2d3a_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:27:34.305000+00:00",
          "endTime": "2026-02-06T23:28:00.310000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26005.0,
          "costDetails": {},
          "totalTokens": 1681,
          "environment": "default",
          "promptTokens": 709,
          "completionTokens": 972,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-28-08-146314_chatcmpl-a9b8bef6-26e0-48e1-b413-ea028f0dc034",
          "traceId": "2d3a1765",
          "type": "GENERATION",
          "name": "2d3a_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:28:08.146000+00:00",
          "endTime": "2026-02-06T23:28:34.463000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26317.0,
          "costDetails": {},
          "totalTokens": 1706,
          "environment": "default",
          "promptTokens": 715,
          "completionTokens": 991,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-28-42-105040_chatcmpl-e38c7f85-3613-461a-95c1-c551c3cd84aa",
          "traceId": "2d3a1765",
          "type": "GENERATION",
          "name": "2d3a_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T23:28:42.105000+00:00",
          "endTime": "2026-02-06T23:29:09.649000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27544.0,
          "costDetails": {},
          "totalTokens": 1775,
          "environment": "default",
          "promptTokens": 743,
          "completionTokens": 1032,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-29-17-372112_chatcmpl-d8f6ed47-1a1e-4e90-b66b-90e8681d7dbe",
          "traceId": "2d3a1765",
          "type": "GENERATION",
          "name": "2d3a_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T23:29:17.372000+00:00",
          "endTime": "2026-02-06T23:29:46.685000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29313.0,
          "costDetails": {},
          "totalTokens": 1867,
          "environment": "default",
          "promptTokens": 778,
          "completionTokens": 1089,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "79b02163-d66d-4126-a5b9-88ed4c2ce029",
          "traceId": "2d3a1765",
          "type": "SPAN",
          "name": "error_2d_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T23:29:54.359000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_ea0e7223_1770420586.py\", line 105, in <module>\n    main()\n  File \"script_ea0e7223_1770420586.py\", line 55, in main\n    labels = load_labels(label_path)\n  File \"script_ea0e7223_1770420586.py\", line 17, in load_labels\n    return {int(line.split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines if line.strip() and len(line.split(maxsplit=1)) > 1}\n  File \"script_ea0e7223_1770420586.py\", line 17, in <dictcomp>\n    return {int(line.split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines if line.strip() and len(line.split(maxsplit=1)) > 1}\nValueError: invalid literal for int() with base 10: 'traffic'.",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T23:26:58.153Z",
      "updatedAt": "2026-02-06T23:29:59.966Z",
      "environment": "default"
    },
    {
      "id": "4db29dd1",
      "timestamp": "2026-02-06T23:23:44.887000+00:00",
      "name": "4db2_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    labels = {}\n    for row_number, content in enumerate(lines):\n        pair = re.split(r'[:\\s]+', content.strip(), maxsplit=1)\n        if len(pair) == 2:\n            index, label = pair\n            labels[int(index)] = label\n    return labels\n\n# Load TFLite model and allocate tensors.\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details.\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\nfloating_model = (input_details[0]['dtype'] == np.float32)\n\n# Load labels\nlabels = load_labels(label_path)\n\n# Read video from the given path\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Cannot open video file {input_path}\")\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nout = cv2.VideoWriter(output_path, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n\ndef preprocess(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (width, height))\n    input_data = np.expand_dims(frame_resized, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n\n    return input_data\n\n# Inference and output handling\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    preprocessed_frame = preprocess(frame)\n\n    # Perform inference.\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index'])[0])\n\n    # Draw detection boxes with labels\n    for i in range(num_detections):\n        if scores[i] >= confidence_threshold:\n            ymin = int(max(1, (boxes[i][0] * frame.shape[0])))\n            xmin = int(max(1, (boxes[i][1] * frame.shape[1])))\n            ymax = int(min(frame.shape[0], (boxes[i][2] * frame.shape[0])))\n            xmax = int(min(frame.shape[1], (boxes[i][3] * frame.shape[1])))\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n            label = '%s: %.2f' % (labels[int(classes[i])], scores[i])\n            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            label_ymin = max(ymin, labelSize[1] + 10)\n            cv2.rectangle(frame, (xmin, label_ymin - labelSize[1] - 10), (xmin + labelSize[0], label_ymin + baseLine - 10), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything if job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_b9a0c451_1770420383.py\", line 43, in <module>\n    labels = load_labels(label_path)\n  File \"script_b9a0c451_1770420383.py\", line 20, in load_labels\n    pair = re.split(r'[:\\s]+', content.strip(), maxsplit=1)\nNameError: name 're' is not defined."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 166.639,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-23-44-889225_chatcmpl-bf176ff9-d4a7-481c-b952-56d349165ee9",
          "traceId": "4db29dd1",
          "type": "GENERATION",
          "name": "4db2_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:23:44.889000+00:00",
          "endTime": "2026-02-06T23:24:09.691000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24802.0,
          "costDetails": {},
          "totalTokens": 1426,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 952,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-24-17-544273_chatcmpl-b1714d73-38d3-4f49-9d5e-9c9ea26a6541",
          "traceId": "4db29dd1",
          "type": "GENERATION",
          "name": "4db2_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:24:17.544000+00:00",
          "endTime": "2026-02-06T23:24:39.919000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22375.0,
          "costDetails": {},
          "totalTokens": 1462,
          "environment": "default",
          "promptTokens": 625,
          "completionTokens": 837,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-24-47-517778_chatcmpl-39a4b7b9-2c2c-4ea4-9e36-863790403ba6",
          "traceId": "4db29dd1",
          "type": "GENERATION",
          "name": "4db2_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:24:47.517000+00:00",
          "endTime": "2026-02-06T23:25:14.835000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27318.0,
          "costDetails": {},
          "totalTokens": 1695,
          "environment": "default",
          "promptTokens": 670,
          "completionTokens": 1025,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-25-22-771134_chatcmpl-f4b42748-29db-4dbe-9cca-d4249ddd513d",
          "traceId": "4db29dd1",
          "type": "GENERATION",
          "name": "4db2_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T23:25:22.771000+00:00",
          "endTime": "2026-02-06T23:25:46.883000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24112.0,
          "costDetails": {},
          "totalTokens": 1536,
          "environment": "default",
          "promptTokens": 625,
          "completionTokens": 911,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-25-54-447405_chatcmpl-708d4bc4-b62a-468a-88c3-d18d311c1778",
          "traceId": "4db29dd1",
          "type": "GENERATION",
          "name": "4db2_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T23:25:54.447000+00:00",
          "endTime": "2026-02-06T23:26:23.796000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29349.0,
          "costDetails": {},
          "totalTokens": 1743,
          "environment": "default",
          "promptTokens": 635,
          "completionTokens": 1108,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "a2b7b6bf-a3e6-4574-b669-69d342848de3",
          "traceId": "4db29dd1",
          "type": "SPAN",
          "name": "error_4d_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T23:26:31.528000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_b9a0c451_1770420383.py\", line 43, in <module>\n    labels = load_labels(label_path)\n  File \"script_b9a0c451_1770420383.py\", line 20, in load_labels\n    pair = re.split(r'[:\\s]+', content.strip(), maxsplit=1)\nNameError: name 're' is not defined.",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T23:23:45.410Z",
      "updatedAt": "2026-02-06T23:26:37.109Z",
      "environment": "default"
    },
    {
      "id": "4c005bf7",
      "timestamp": "2026-02-06T23:22:01.240000+00:00",
      "name": "4c00_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter with EdgeTPU\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess_image(image):\n    # Resize image to input tensor size\n    input_shape = input_details[0]['shape']\n    resized_image = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_image, axis=0)\n    return input_data\n\ndef postprocess_output(output_data):\n    # Extract bounding boxes and scores\n    detection_boxes = output_data[0][0]\n    detection_scores = output_data[1][0]\n    num_detections = int(output_data[3][0])\n    detections = []\n    for i in range(num_detections):\n        score = float(detection_scores[i])\n        if score > confidence_threshold:\n            bbox = detection_boxes[i]\n            label_id = int(output_data[2][0][i])\n            label = labels[label_id]\n            detections.append((bbox, score, label))\n    return detections\n\n# mAP calculation placeholder (not fully implemented here)\ndef calculate_mAP(gt_boxes, pred_boxes):\n    # Placeholder function for demonstration\n    return 0.0\n\nframe_count = 0\nall_detections = []\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_image(frame)\n    \n    # Set input tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Get output tensors\n    output_data = [interpreter.get_tensor(output_detail['index']) for output_detail in output_details]\n    \n    # Postprocess the output data\n    detections = postprocess_output(output_data)\n    all_detections.append(detections)\n    \n    # Draw detection boxes and labels on the frame\n    for (bbox, score, label) in detections:\n        ymin, xmin, ymax, xmax = bbox\n        h, w, _ = frame.shape\n        left = int(xmin * w)\n        top = int(ymin * h)\n        right = int(xmax * w)\n        bottom = int(ymax * h)\n        \n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n        label_text = f\"{label}: {score:.2f}\"\n        cv2.putText(frame, label_text, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    # Write the frame to output video\n    out.write(frame)\n    frame_count += 1\n\ncap.release()\nout.release()\n\n# Placeholder for mAP calculation\nmAP = calculate_mAP([], all_detections)  # Provide ground truth boxes as needed\nprint(f\"Mean Average Precision (mAP): {mAP:.2f}\")"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 25.646,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-22-01-241710_chatcmpl-f18192aa-5c45-47e2-b3ad-9b191b38925f",
          "traceId": "4c005bf7",
          "type": "GENERATION",
          "name": "4c00_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:22:01.241000+00:00",
          "endTime": "2026-02-06T23:22:26.887000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25646.0,
          "costDetails": {},
          "totalTokens": 1446,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 972,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T23:22:01.765Z",
      "updatedAt": "2026-02-06T23:23:24.987Z",
      "environment": "default"
    },
    {
      "id": "b1b68eb6",
      "timestamp": "2026-02-06T23:18:48.477000+00:00",
      "name": "b1b6_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        lines = f.readlines()\n    return {int(line.split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines if len(line.strip().split(maxsplit=1)) > 1}\n\ndef preprocess_image(image):\n    input_details = interpreter.get_input_details()\n    _, height, width, _ = input_details[0]['shape']\n    image_resized = cv2.resize(image, (width, height))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n    return input_data\n\ndef detect_objects(interpreter, input_data):\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    output_details = interpreter.get_output_details()\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = interpreter.get_tensor(output_details[3]['index'])[0]\n    \n    results = []\n    for i in range(int(num_detections)):\n        if scores[i] >= confidence_threshold:\n            results.append({\n                'box': boxes[i],\n                'class_id': int(classes[i]),\n                'score': scores[i]\n            })\n    return results\n\ndef draw_boxes(image, results, labels):\n    height, width, _ = image.shape\n    for result in results:\n        ymin, xmin, ymax, xmax = result['box']\n        label = labels[result['class_id']]\n        score = result['score']\n\n        # Calculate coordinates on the original image\n        left, right, top, bottom = int(xmin * width), int(xmax * width), int(ymin * height), int(ymax * height)\n        \n        # Draw box and label\n        cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n        cv2.putText(image, f'{label}: {score:.2f}', (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\ndef main():\n    # Load TFLite model and allocate tensors\n    delegate = load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate])\n    interpreter.allocate_tensors()\n    \n    # Get input and output details\n    global input_details\n    input_details = interpreter.get_input_details()\n    \n    # Load labels\n    labels = load_labels(label_path)\n    \n    # Read video from the given path\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise IOError(\"Cannot open video file\")\n    \n    # Define codec and create VideoWriter object\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n    \n    # Process video frames\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        input_data = preprocess_image(frame)\n        results = detect_objects(interpreter, input_data)\n        draw_boxes(frame, results, labels)\n        \n        # Write the frame into the file 'output.avi'\n        out.write(frame)\n    \n    # Release everything when job is finished\n    cap.release()\n    out.release()\n\nif __name__ == '__main__':\n    main()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_bbb94064_1770420087.py\", line 103, in <module>\n    main()\n  File \"script_bbb94064_1770420087.py\", line 71, in main\n    labels = load_labels(label_path)\n  File \"script_bbb94064_1770420087.py\", line 17, in load_labels\n    return {int(line.split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines if len(line.strip().split(maxsplit=1)) > 1}\n  File \"script_bbb94064_1770420087.py\", line 17, in <dictcomp>\n    return {int(line.split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines if len(line.strip().split(maxsplit=1)) > 1}\nValueError: invalid literal for int() with base 10: 'traffic'."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 166.48,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-18-48-479239_chatcmpl-b6ed247c-a749-4de7-9c62-7b94c5d67846",
          "traceId": "b1b68eb6",
          "type": "GENERATION",
          "name": "b1b6_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:18:48.479000+00:00",
          "endTime": "2026-02-06T23:19:14.304000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25825.0,
          "costDetails": {},
          "totalTokens": 1447,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 973,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-19-21-875045_chatcmpl-d15561cb-71d8-4a43-94eb-48074e6c2b69",
          "traceId": "b1b68eb6",
          "type": "GENERATION",
          "name": "b1b6_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:19:21.875000+00:00",
          "endTime": "2026-02-06T23:19:47.429000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25554.0,
          "costDetails": {},
          "totalTokens": 1657,
          "environment": "default",
          "promptTokens": 700,
          "completionTokens": 957,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-19-55-079698_chatcmpl-722f44b0-6068-467b-8cae-b2ba89a1602c",
          "traceId": "b1b68eb6",
          "type": "GENERATION",
          "name": "b1b6_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:19:55.079000+00:00",
          "endTime": "2026-02-06T23:20:21.208000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26129.0,
          "costDetails": {},
          "totalTokens": 1720,
          "environment": "default",
          "promptTokens": 738,
          "completionTokens": 982,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-20-28-745820_chatcmpl-5436ce1b-f688-4182-b813-5d4bd52bc61d",
          "traceId": "b1b68eb6",
          "type": "GENERATION",
          "name": "b1b6_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T23:20:28.745000+00:00",
          "endTime": "2026-02-06T23:20:52.807000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24062.0,
          "costDetails": {},
          "totalTokens": 1680,
          "environment": "default",
          "promptTokens": 779,
          "completionTokens": 901,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-21-00-345118_chatcmpl-2ba21da5-4d5d-467f-bd1c-b6a414dce061",
          "traceId": "b1b68eb6",
          "type": "GENERATION",
          "name": "b1b6_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T23:21:00.345000+00:00",
          "endTime": "2026-02-06T23:21:27.356000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27011.0,
          "costDetails": {},
          "totalTokens": 1792,
          "environment": "default",
          "promptTokens": 782,
          "completionTokens": 1010,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "fba221a6-9083-4a1e-885b-24cb5e134513",
          "traceId": "b1b68eb6",
          "type": "SPAN",
          "name": "error_b1_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T23:21:34.959000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_bbb94064_1770420087.py\", line 103, in <module>\n    main()\n  File \"script_bbb94064_1770420087.py\", line 71, in main\n    labels = load_labels(label_path)\n  File \"script_bbb94064_1770420087.py\", line 17, in load_labels\n    return {int(line.split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines if len(line.strip().split(maxsplit=1)) > 1}\n  File \"script_bbb94064_1770420087.py\", line 17, in <dictcomp>\n    return {int(line.split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines if len(line.strip().split(maxsplit=1)) > 1}\nValueError: invalid literal for int() with base 10: 'traffic'.",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T23:18:49.002Z",
      "updatedAt": "2026-02-06T23:21:40.201Z",
      "environment": "default"
    },
    {
      "id": "4b7378b1",
      "timestamp": "2026-02-06T23:15:29.621000+00:00",
      "name": "4b73_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(label_path):\n    labels = {}\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f.readlines():\n            parts = line.strip().split(': ')\n            if len(parts) == 2:\n                labels[int(parts[0])] = parts[1]\n    return labels\n\ndef preprocess_image(image, input_size):\n    image = cv2.resize(image, (input_size, input_size))\n    image = np.expand_dims(image, axis=0)\n    image = image / 255.0\n    return image.astype(np.float32)\n\ndef main():\n    # Load TFLite model and allocate tensors\n    delegate = load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate])\n    interpreter.allocate_tensors()\n\n    # Get input and output details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_shape = input_details[0]['shape']\n    input_height, input_width = input_shape[1], input_shape[2]\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Open video file for reading and writing\n    cap = cv2.VideoCapture(input_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess the image\n        input_data = preprocess_image(frame, input_height)\n\n        # Set the tensor to point to the input data to be inferred\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        # Run inference\n        interpreter.invoke()\n\n        # Extract results\n        boxes = interpreter.get_tensor(output_details[0]['index'])\n        classes = interpreter.get_tensor(output_details[1]['index'])\n        scores = interpreter.get_tensor(output_details[2]['index'])\n\n        # Draw detection boxes and labels\n        for i in range(len(scores)):\n            if scores[i] > confidence_threshold:\n                box = boxes[i]\n                class_id = int(classes[i])\n                score = scores[i]\n                label = labels.get(class_id, 'unknown')\n\n                y_min, x_min, y_max, x_max = box\n                h, w, _ = frame.shape\n\n                # Convert to pixel coordinates\n                top_left = (int(x_min * w), int(y_min * h))\n                bottom_right = (int(x_max * w), int(y_max * h))\n\n                cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n                label_text = f\"{label}: {score:.2f}\"\n                cv2.putText(frame, label_text, (top_left[0], top_left[1] - 10),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n        # Write the frame to the output video\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\nif __name__ == '__main__':\n    main()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_92304e2f_1770419894.py\", line 96, in <module>\n    main()\n  File \"script_92304e2f_1770419894.py\", line 59, in main\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor ."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 172.799,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-15-29-623177_chatcmpl-38da2cb4-1062-40e3-85a5-352ff2add4fe",
          "traceId": "4b7378b1",
          "type": "GENERATION",
          "name": "4b73_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:15:29.623000+00:00",
          "endTime": "2026-02-06T23:15:54.786000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25163.0,
          "costDetails": {},
          "totalTokens": 1428,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 954,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-16-02-374180_chatcmpl-edd490c4-ab12-47d0-9280-55dcbcba8f4b",
          "traceId": "4b7378b1",
          "type": "GENERATION",
          "name": "4b73_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:16:02.374000+00:00",
          "endTime": "2026-02-06T23:16:25.893000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23519.0,
          "costDetails": {},
          "totalTokens": 1485,
          "environment": "default",
          "promptTokens": 600,
          "completionTokens": 885,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-16-33-496023_chatcmpl-496ce235-3ed8-4456-8b05-a1a38bbefeb1",
          "traceId": "4b7378b1",
          "type": "GENERATION",
          "name": "4b73_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:16:33.496000+00:00",
          "endTime": "2026-02-06T23:17:03.556000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 30060.0,
          "costDetails": {},
          "totalTokens": 1834,
          "environment": "default",
          "promptTokens": 705,
          "completionTokens": 1129,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-17-11-327446_chatcmpl-3e5b0b66-d3f0-406b-bdaa-503bca507c38",
          "traceId": "4b7378b1",
          "type": "GENERATION",
          "name": "4b73_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T23:17:11.327000+00:00",
          "endTime": "2026-02-06T23:17:39.955000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28628.0,
          "costDetails": {},
          "totalTokens": 1791,
          "environment": "default",
          "promptTokens": 716,
          "completionTokens": 1075,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-17-47-629218_chatcmpl-7d055f09-83ff-4f93-ab52-323a2c17dbcc",
          "traceId": "4b7378b1",
          "type": "GENERATION",
          "name": "4b73_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T23:17:47.629000+00:00",
          "endTime": "2026-02-06T23:18:14.359000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26730.0,
          "costDetails": {},
          "totalTokens": 1680,
          "environment": "default",
          "promptTokens": 672,
          "completionTokens": 1008,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "0b94f026-427c-4cb6-bea2-31d040dd21ae",
          "traceId": "4b7378b1",
          "type": "SPAN",
          "name": "error_4b_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T23:18:22.422000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_92304e2f_1770419894.py\", line 96, in <module>\n    main()\n  File \"script_92304e2f_1770419894.py\", line 59, in main\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor .",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T23:15:30.145Z",
      "updatedAt": "2026-02-06T23:18:27.875Z",
      "environment": "default"
    },
    {
      "id": "b4461741",
      "timestamp": "2026-02-06T23:13:12.572000+00:00",
      "name": "b446_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration Parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Could not open video file {input_path}\")\n\n# Video writer setup for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Preprocessing function\ndef preprocess_image(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n    return input_data\n\n# Function to draw bounding boxes and labels\ndef draw_boxes(frame, boxes, classes, scores):\n    for i in range(len(boxes)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            score = scores[i]\n            label = f\"{labels[class_id]}: {score:.2f}\"\n            top_left = (int(box[0] * width), int(box[1] * height))\n            bottom_right = (int(box[2] * width), int(box[3] * height))\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            cv2.putText(frame, label, top_left, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n# Main loop for video processing\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n    interpreter.invoke()\n\n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    draw_boxes(frame, boxes, classes, scores)\n    out.write(frame)\n\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 57.84,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-13-12-572761_chatcmpl-8f0f06f2-cf3b-46cb-a7ba-3735a9023dbe",
          "traceId": "b4461741",
          "type": "GENERATION",
          "name": "b446_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:13:12.572000+00:00",
          "endTime": "2026-02-06T23:13:37.600000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25028.0,
          "costDetails": {},
          "totalTokens": 1421,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 947,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-13-45-507065_chatcmpl-37920b01-0a3e-44f3-8d05-89b8c9af6c8d",
          "traceId": "b4461741",
          "type": "GENERATION",
          "name": "b446_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:13:45.507000+00:00",
          "endTime": "2026-02-06T23:14:10.412000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24905.0,
          "costDetails": {},
          "totalTokens": 1523,
          "environment": "default",
          "promptTokens": 589,
          "completionTokens": 934,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T23:13:13.092Z",
      "updatedAt": "2026-02-06T23:15:08.322Z",
      "environment": "default"
    },
    {
      "id": "ccaec43c",
      "timestamp": "2026-02-06T23:10:27.935000+00:00",
      "name": "ccae_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Preprocess function\ndef preprocess_image(image):\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(image_resized, axis=0)\n    input_data = np.uint8(input_data)  # Convert to UINT8\n    return input_data\n\n# Main inference loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_image(frame)\n    \n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n            \n            top, left, bottom, right = box\n            height, width, _ = frame.shape\n            \n            y_min = int(max(1, (top * height)))\n            x_min = int(max(1, (left * width)))\n            y_max = int(min(height, (bottom * height)))\n            x_max = int(min(width, (right * width)))\n\n            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n            label_text = f'{label}: {int(score*100)}%'\n            cv2.putText(frame, label_text, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f\"Processed video saved to {output_path}\")"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.27,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-10-27-937813_chatcmpl-8a1ecddf-d16a-42ff-afde-23f0f1d3d50c",
          "traceId": "ccaec43c",
          "type": "GENERATION",
          "name": "ccae_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:10:27.937000+00:00",
          "endTime": "2026-02-06T23:10:52.802000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24865.0,
          "costDetails": {},
          "totalTokens": 1416,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 942,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-11-00-436671_chatcmpl-72e0a1a0-b5bd-4a47-8077-40566770a680",
          "traceId": "ccaec43c",
          "type": "GENERATION",
          "name": "ccae_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:11:00.436000+00:00",
          "endTime": "2026-02-06T23:11:24.699000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24263.0,
          "costDetails": {},
          "totalTokens": 1511,
          "environment": "default",
          "promptTokens": 601,
          "completionTokens": 910,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-11-32-764264_chatcmpl-59e204d8-2741-4438-b174-f62145aea236",
          "traceId": "ccaec43c",
          "type": "GENERATION",
          "name": "ccae_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:11:32.764000+00:00",
          "endTime": "2026-02-06T23:11:55.207000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22443.0,
          "costDetails": {},
          "totalTokens": 1503,
          "environment": "default",
          "promptTokens": 660,
          "completionTokens": 843,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T23:10:28.457Z",
      "updatedAt": "2026-02-06T23:12:53.121Z",
      "environment": "default"
    },
    {
      "id": "8d151dc3",
      "timestamp": "2026-02-06T23:07:18.928000+00:00",
      "name": "8d15_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_input(frame):\n    # Resize frame to match input tensor shape\n    resized_frame = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    # Normalize pixel values if necessary (e.g., dividing by 255)\n    normalized_frame = resized_frame.astype(np.float32) / 255.0\n    # Add batch dimension\n    input_tensor = np.expand_dims(normalized_frame, axis=0)\n    return input_tensor\n\ndef postprocess_output(interpreter, frame):\n    # Retrieve detection results from the interpreter's output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index']).astype(int)\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = interpreter.get_tensor(output_details[3]['index'])\n\n    for i in range(int(num_detections)):\n        if scores[i] > confidence_threshold:\n            # Get bounding box coordinates and class label\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            label = labels[int(classes[i])]\n            \n            # Draw rectangle and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_tensor = preprocess_input(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n    interpreter.invoke()\n\n    postprocess_output(interpreter, frame)\n\n    # Write the processed frame to output video file\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_14050693_1770419394.py\", line 72, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor ."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 163.779,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-07-18-929942_chatcmpl-8a01ff46-fc7b-4fc6-9d9d-3eaf662c0e8d",
          "traceId": "8d151dc3",
          "type": "GENERATION",
          "name": "8d15_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:07:18.929000+00:00",
          "endTime": "2026-02-06T23:07:44.286000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25357.0,
          "costDetails": {},
          "totalTokens": 1433,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 959,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-07-51-817966_chatcmpl-f352a883-250c-4df6-ae0b-46486192866e",
          "traceId": "8d151dc3",
          "type": "GENERATION",
          "name": "8d15_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:07:51.817000+00:00",
          "endTime": "2026-02-06T23:08:13.584000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21767.0,
          "costDetails": {},
          "totalTokens": 1420,
          "environment": "default",
          "promptTokens": 605,
          "completionTokens": 815,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-08-21-224392_chatcmpl-5a85355f-803b-4a1a-bbdd-2efd1d450c7e",
          "traceId": "8d151dc3",
          "type": "GENERATION",
          "name": "8d15_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:08:21.224000+00:00",
          "endTime": "2026-02-06T23:08:44.994000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23770.0,
          "costDetails": {},
          "totalTokens": 1502,
          "environment": "default",
          "promptTokens": 604,
          "completionTokens": 898,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-08-52-987177_chatcmpl-6c34b336-1c00-463d-9a66-73d39cd4d14a",
          "traceId": "8d151dc3",
          "type": "GENERATION",
          "name": "8d15_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T23:08:52.987000+00:00",
          "endTime": "2026-02-06T23:09:20.403000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27416.0,
          "costDetails": {},
          "totalTokens": 1690,
          "environment": "default",
          "promptTokens": 658,
          "completionTokens": 1032,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-09-28-612919_chatcmpl-8725ef89-4a1c-4576-a497-8b77c7853e42",
          "traceId": "8d151dc3",
          "type": "GENERATION",
          "name": "8d15_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T23:09:28.612000+00:00",
          "endTime": "2026-02-06T23:09:54.678000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26066.0,
          "costDetails": {},
          "totalTokens": 1620,
          "environment": "default",
          "promptTokens": 638,
          "completionTokens": 982,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "ca5309ba-f549-4ed7-b2b1-6e1787fe8d6f",
          "traceId": "8d151dc3",
          "type": "SPAN",
          "name": "error_8d_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T23:10:02.708000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_14050693_1770419394.py\", line 72, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor .",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T23:07:19.450Z",
      "updatedAt": "2026-02-06T23:10:08.204Z",
      "environment": "default"
    },
    {
      "id": "228081ba",
      "timestamp": "2026-02-06T23:04:16.324000+00:00",
      "name": "2280_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    labels = {}\n    for line in lines:\n        parts = line.strip().split(' ', maxsplit=1)\n        if len(parts) == 2:\n            index, label = parts\n            labels[int(index)] = label\n    return labels\n\ndef preprocess_image(image):\n    image_resized = cv2.resize(image, (300, 300))\n    input_tensor = np.expand_dims(image_resized, axis=0)\n    return input_tensor\n\ndef draw_boxes(frame, boxes, classes, scores, labels):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = tuple(boxes[i])\n            h, w, _ = frame.shape\n            (left, right, top, bottom) = (xmin * w, xmax * w,\n                                          ymin * h, ymax * h)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\ndef main():\n    # Load labels\n    labels = load_labels(label_path)\n    \n    # Initialize TFLite interpreter with EdgeTPU delegate\n    interpreter = Interpreter(model_path=model_path,\n                              experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\n    interpreter.allocate_tensors()\n    \n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Read video from input path\n    cap = cv2.VideoCapture(input_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    \n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess image\n        input_tensor = preprocess_image(frame)\n        interpreter.set_tensor(input_details[0]['index'], input_tensor)\n\n        # Run inference\n        interpreter.invoke()\n\n        # Extract results\n        boxes = np.squeeze(interpreter.get_tensor(output_details[0]['index']))\n        classes = np.squeeze(interpreter.get_tensor(output_details[1]['index']))\n        scores = np.squeeze(interpreter.get_tensor(output_details[2]['index']))\n\n        # Draw bounding boxes and labels on the frame\n        draw_boxes(frame, boxes, classes, scores, labels)\n\n        # Write processed frame to output video file\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_e8b0e8de_1770419205.py\", line 91, in <module>\n    main()\n  File \"script_e8b0e8de_1770419205.py\", line 45, in main\n    labels = load_labels(label_path)\n  File \"script_e8b0e8de_1770419205.py\", line 22, in load_labels\n    labels[int(index)] = label\nValueError: invalid literal for int() with base 10: 'traffic'."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 156.477,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-04-16-329531_chatcmpl-3802c095-80fd-4173-858f-8c6db8e65081",
          "traceId": "228081ba",
          "type": "GENERATION",
          "name": "2280_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:04:16.329000+00:00",
          "endTime": "2026-02-06T23:04:41.203000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24874.0,
          "costDetails": {},
          "totalTokens": 1414,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 940,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-04-48-794211_chatcmpl-13cfb99e-a71c-4c65-b855-a46ee94a18bd",
          "traceId": "228081ba",
          "type": "GENERATION",
          "name": "2280_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:04:48.794000+00:00",
          "endTime": "2026-02-06T23:05:11.139000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22345.0,
          "costDetails": {},
          "totalTokens": 1441,
          "environment": "default",
          "promptTokens": 601,
          "completionTokens": 840,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-05-19-607053_chatcmpl-13ed9092-d583-4815-b4b1-a3ff45f222e1",
          "traceId": "228081ba",
          "type": "GENERATION",
          "name": "2280_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:05:19.607000+00:00",
          "endTime": "2026-02-06T23:05:42.866000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23259.0,
          "costDetails": {},
          "totalTokens": 1523,
          "environment": "default",
          "promptTokens": 645,
          "completionTokens": 878,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-05-50-389394_chatcmpl-26a37984-bc9b-40a6-9a1d-a50f447c1710",
          "traceId": "228081ba",
          "type": "GENERATION",
          "name": "2280_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T23:05:50.389000+00:00",
          "endTime": "2026-02-06T23:06:14.145000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23756.0,
          "costDetails": {},
          "totalTokens": 1608,
          "environment": "default",
          "promptTokens": 716,
          "completionTokens": 892,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-06-22-013434_chatcmpl-500bf0a1-56b0-4d15-88e9-6907f86f9cda",
          "traceId": "228081ba",
          "type": "GENERATION",
          "name": "2280_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T23:06:22.013000+00:00",
          "endTime": "2026-02-06T23:06:45.305000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23292.0,
          "costDetails": {},
          "totalTokens": 1595,
          "environment": "default",
          "promptTokens": 721,
          "completionTokens": 874,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "34bc19f2-6ceb-4387-b99f-375502cf969b",
          "traceId": "228081ba",
          "type": "SPAN",
          "name": "error_22_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T23:06:52.806000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_e8b0e8de_1770419205.py\", line 91, in <module>\n    main()\n  File \"script_e8b0e8de_1770419205.py\", line 45, in main\n    labels = load_labels(label_path)\n  File \"script_e8b0e8de_1770419205.py\", line 22, in load_labels\n    labels[int(index)] = label\nValueError: invalid literal for int() with base 10: 'traffic'.",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T23:04:16.855Z",
      "updatedAt": "2026-02-06T23:06:58.095Z",
      "environment": "default"
    },
    {
      "id": "628c079e",
      "timestamp": "2026-02-06T23:01:06.629000+00:00",
      "name": "628c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {index: line.strip() for index, line in enumerate(f.readlines())}\n\n# Setup TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Check if the model expects a single image or batch of images\nheight, width = input_details[0]['shape'][1:3]\n\n# Read the input video\ncap = cv2.VideoCapture(input_path)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = np.float32(input_data) / 255.0\n\n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Postprocessing: Extracting results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    # Drawing detection boxes and labels\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame.shape[1], xmax * frame.shape[1],\n                                          ymin * frame.shape[0], ymax * frame.shape[0])\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything if job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_b4a35ec3_1770419019.py\", line 46, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor ."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 161.168,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-01-01-06-631622_chatcmpl-30415647-ae71-4b4d-9929-50c4dc92f66e",
          "traceId": "628c079e",
          "type": "GENERATION",
          "name": "628c_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T23:01:06.631000+00:00",
          "endTime": "2026-02-06T23:01:34.665000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28034.0,
          "costDetails": {},
          "totalTokens": 1534,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 1060,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-01-42-182536_chatcmpl-c477a8d7-8e36-46a3-acaf-5a8dd16de02a",
          "traceId": "628c079e",
          "type": "GENERATION",
          "name": "628c_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T23:01:42.182000+00:00",
          "endTime": "2026-02-06T23:02:05.230000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23048.0,
          "costDetails": {},
          "totalTokens": 1465,
          "environment": "default",
          "promptTokens": 601,
          "completionTokens": 864,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-02-14-007502_chatcmpl-cee795e7-aab4-4a28-8bc8-4a9c68fa974e",
          "traceId": "628c079e",
          "type": "GENERATION",
          "name": "628c_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T23:02:14.007000+00:00",
          "endTime": "2026-02-06T23:02:38.675000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24668.0,
          "costDetails": {},
          "totalTokens": 1571,
          "environment": "default",
          "promptTokens": 643,
          "completionTokens": 928,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-02-47-006704_chatcmpl-434e53eb-6e60-47b8-880c-1ca4e90161ab",
          "traceId": "628c079e",
          "type": "GENERATION",
          "name": "628c_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T23:02:47.006000+00:00",
          "endTime": "2026-02-06T23:03:09.932000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22926.0,
          "costDetails": {},
          "totalTokens": 1493,
          "environment": "default",
          "promptTokens": 630,
          "completionTokens": 863,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-01-03-18-075430_chatcmpl-7e847f29-e755-46e0-8b7f-6c04b6d24035",
          "traceId": "628c079e",
          "type": "GENERATION",
          "name": "628c_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T23:03:18.075000+00:00",
          "endTime": "2026-02-06T23:03:39.525000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21450.0,
          "costDetails": {},
          "totalTokens": 1399,
          "environment": "default",
          "promptTokens": 587,
          "completionTokens": 812,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "84d2aa85-8698-42ad-b4e3-7bca37521859",
          "traceId": "628c079e",
          "type": "SPAN",
          "name": "error_62_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T23:03:47.799000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_b4a35ec3_1770419019.py\", line 46, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor .",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T23:01:07.154Z",
      "updatedAt": "2026-02-06T23:03:53.399Z",
      "environment": "default"
    },
    {
      "id": "39e76dbc",
      "timestamp": "2026-02-06T22:58:20.718000+00:00",
      "name": "39e7_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return {index: line.strip() for index, line in enumerate(f)}\n\nlabels = load_labels(label_path)\n\n# Initialize TFLite interpreter with EdgeTPU\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocess the image\ndef preprocess_image(frame):\n    frame_resized = cv2.resize(frame, (300, 300))\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(frame_rgb, axis=0).astype(np.uint8)  # Ensure the data type is uint8\n    return input_data\n\n# Read video from the given path\ncap = cv2.VideoCapture(input_path)\n\n# Prepare to write the output video\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Inference and output handling\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    \n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    \n    # Draw detection boxes and labels\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n\n            object_name = labels[int(classes[i])]\n            label_text = f\"{object_name}: {scores[i]:.2f}\"\n            cv2.putText(frame, label_text, (int(left), int(top) - 8),\n                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything if job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Video with detections has been saved to:\", output_path)"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 88.024,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-58-20-719507_chatcmpl-50e49801-a5ad-4230-ad6c-584a67c98f75",
          "traceId": "39e76dbc",
          "type": "GENERATION",
          "name": "39e7_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:58:20.719000+00:00",
          "endTime": "2026-02-06T22:58:47.245000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26526.0,
          "costDetails": {},
          "totalTokens": 1478,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 1004,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-58-54-905391_chatcmpl-cba99f50-f54b-4b5a-88c2-50bb4654915a",
          "traceId": "39e76dbc",
          "type": "GENERATION",
          "name": "39e7_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:58:54.905000+00:00",
          "endTime": "2026-02-06T22:59:18.348000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23443.0,
          "costDetails": {},
          "totalTokens": 1488,
          "environment": "default",
          "promptTokens": 608,
          "completionTokens": 880,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-59-26-243608_chatcmpl-263c819b-ed65-408a-b46b-a48a076fa26c",
          "traceId": "39e76dbc",
          "type": "GENERATION",
          "name": "39e7_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:59:26.243000+00:00",
          "endTime": "2026-02-06T22:59:48.743000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22500.0,
          "costDetails": {},
          "totalTokens": 1509,
          "environment": "default",
          "promptTokens": 662,
          "completionTokens": 847,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:58:21.243Z",
      "updatedAt": "2026-02-06T23:00:46.497Z",
      "environment": "default"
    },
    {
      "id": "21b35ec3",
      "timestamp": "2026-02-06T22:56:05.716000+00:00",
      "name": "21b3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Get video properties to save output video with same codec, frame size, etc.\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    input_shape = input_details[0]['shape']\n    resize_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resize_img, axis=0)\n    return input_data\n\ndef draw_boxes(frame, boxes, class_ids, scores):\n    for box, class_id, score in zip(boxes, class_ids, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label_text = f\"{labels[int(class_id)]}: {score:.2f}\"\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Process frames\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    \n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    \n    # Run inference\n    interpreter.invoke()\n    \n    # Get all output details\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    class_ids = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    \n    draw_boxes(frame, boxes, class_ids, scores)\n    \n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 57.4,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-56-05-717851_chatcmpl-1120c346-74e7-4aa4-95d4-dcab207aee9f",
          "traceId": "21b35ec3",
          "type": "GENERATION",
          "name": "21b3_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:56:05.717000+00:00",
          "endTime": "2026-02-06T22:56:28.480000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22763.0,
          "costDetails": {},
          "totalTokens": 1336,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 862,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-56-36-007383_chatcmpl-a8709b7f-793d-4d6b-9a9f-aea80672f375",
          "traceId": "21b35ec3",
          "type": "GENERATION",
          "name": "21b3_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:56:36.007000+00:00",
          "endTime": "2026-02-06T22:57:03.117000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27110.0,
          "costDetails": {},
          "totalTokens": 1619,
          "environment": "default",
          "promptTokens": 600,
          "completionTokens": 1019,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:56:06.240Z",
      "updatedAt": "2026-02-06T22:58:01.081Z",
      "environment": "default"
    },
    {
      "id": "a7ec8bf2",
      "timestamp": "2026-02-06T22:52:36.843000+00:00",
      "name": "a7ec_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {i: line.strip() for i, line in enumerate(f.readlines())}\n\n# Initialize the TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n# Function to preprocess the image\ndef preprocess_image(frame):\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(frame_rgb, axis=0).astype(np.uint8)  # Ensure the data type is uint8\n    return input_data\n\n# Function to draw bounding boxes and labels\ndef draw_detections(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Draw rectangle\n            top_left = (int(box[1] * width), int(box[0] * height))\n            bottom_right = (int(box[3] * width), int(box[2] * height))\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n\n            # Draw label\n            label_text = f\"{label}: {score:.2f}\"\n            label_size, _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            label_origin = (top_left[0], top_left[1] - 10 if top_left[1] > 20 else top_left[1] + 10)\n            cv2.rectangle(frame, (label_origin[0] - 5, label_origin[1] + 5), \n                          (label_origin[0] + label_size[0] + 5, label_origin[1] - label_size[1] - 5), \n                          (0, 255, 0), cv2.FILLED)\n            cv2.putText(frame, label_text, label_origin, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n\n# Main loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_tensor = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n    interpreter.invoke()\n\n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    draw_detections(frame, boxes[0], classes[0], scores[0])\n\n    out.write(frame)\n\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 130.59,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-52-36-844909_chatcmpl-deb5953b-aba5-4685-9a5b-c957b1a6067d",
          "traceId": "a7ec8bf2",
          "type": "GENERATION",
          "name": "a7ec_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:52:36.844000+00:00",
          "endTime": "2026-02-06T22:53:05.186000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28342.0,
          "costDetails": {},
          "totalTokens": 1526,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 1052,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-53-13-218736_chatcmpl-a57853b8-4c99-4374-a9fd-7bd274e1a1af",
          "traceId": "a7ec8bf2",
          "type": "GENERATION",
          "name": "a7ec_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:53:13.218000+00:00",
          "endTime": "2026-02-06T22:53:40.100000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26882.0,
          "costDetails": {},
          "totalTokens": 1661,
          "environment": "default",
          "promptTokens": 660,
          "completionTokens": 1001,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-53-48-097726_chatcmpl-c041af66-5f5c-458f-8452-0c8a519e4cd0",
          "traceId": "a7ec8bf2",
          "type": "GENERATION",
          "name": "a7ec_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:53:48.097000+00:00",
          "endTime": "2026-02-06T22:54:13.453000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25356.0,
          "costDetails": {},
          "totalTokens": 1551,
          "environment": "default",
          "promptTokens": 591,
          "completionTokens": 960,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-54-21-747801_chatcmpl-e6c6da39-40e6-49e5-86a9-b87d96964871",
          "traceId": "a7ec8bf2",
          "type": "GENERATION",
          "name": "a7ec_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T22:54:21.747000+00:00",
          "endTime": "2026-02-06T22:54:47.434000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25687.0,
          "costDetails": {},
          "totalTokens": 1629,
          "environment": "default",
          "promptTokens": 660,
          "completionTokens": 969,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:52:37.365Z",
      "updatedAt": "2026-02-06T22:55:45.074Z",
      "environment": "default"
    },
    {
      "id": "8ab25e7c",
      "timestamp": "2026-02-06T22:50:49.290000+00:00",
      "name": "8ab2_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\nfrom tflite_runtime.interpreter import load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup: Load the TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\ndef preprocess_image(image):\n    # Resize image to input size\n    input_shape = input_details[0]['shape']\n    resize_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    # Normalize pixel values if necessary\n    resize_img = np.expand_dims(resize_img, axis=0)\n    return resize_img\n\ndef postprocess_detection(results, frame):\n    boxes = results['detection_boxes'][0]\n    classes = results['detection_classes'][0].astype(int) + 1\n    scores = results['detection_scores'][0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            # Get bounding box coordinates and label\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[classes[i]]\n            score = scores[i]\n\n            # Draw rectangle and text on frame\n            h, w, _ = frame.shape\n            (left, right, top, bottom) = (xmin * w, xmax * w,\n                                          ymin * h, ymax * h)\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the input data\n    input_data = preprocess_image(frame)\n    \n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    start_time = time.time()\n    interpreter.invoke()\n    end_time = time.time()\n\n    # Post-process the output\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = interpreter.get_tensor(output_details[3]['index'])\n\n    results = {\n        'detection_boxes': boxes,\n        'detection_classes': classes,\n        'detection_scores': scores,\n        'num_detections': num_detections\n    }\n\n    postprocess_detection(results, frame)\n\n    # Write the processed frame to output video file\n    out.write(frame)\n    \n    # Print inference time (optional)\n    print(f'Inference Time: {end_time - start_time:.3f} seconds')\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 27.474,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-50-49-291707_chatcmpl-e8dc5b09-efd9-40d0-b529-25ca5ecb7af6",
          "traceId": "8ab25e7c",
          "type": "GENERATION",
          "name": "8ab2_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:50:49.291000+00:00",
          "endTime": "2026-02-06T22:51:16.765000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27474.0,
          "costDetails": {},
          "totalTokens": 1453,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 979,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:50:49.815Z",
      "updatedAt": "2026-02-06T22:52:14.578Z",
      "environment": "default"
    },
    {
      "id": "a5dc1257",
      "timestamp": "2026-02-06T22:47:30.579000+00:00",
      "name": "a5dc_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return {index: line.strip() for index, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize the TFLite interpreter and allocate tensors\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize to the expected input size of the model\n    image_resized = cv2.resize(image, (300, 300))\n    return image_resized[np.newaxis, :, :, :] / 255.0\n\n# Inference and output handling\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(int)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes with labels\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            score = scores[i]\n\n            im_height, im_width, _ = frame.shape\n\n            (xminn, xmaxx, yminn, ymaxx) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n            cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (0, 255, 0), 2)\n\n            display_str = '{}: {}%'.format(label, int(score * 100))\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            labelSize = cv2.getTextSize(display_str, font, 1.2, 2)\n            labelXMin = int(xminn)\n            labelYMin = int(max(yminn - labelSize[1], 10))\n\n            cv2.rectangle(frame, (labelXMin, labelYMin - labelSize[1] - 10), \n                          (labelXMin + labelSize[0][0] + 10, labelYMin + 10),\n                          (0, 255, 0), -1)\n            cv2.putText(frame, display_str, (int(xminn) + 5, int(yminn) - 7),\n                        font, 1.2, (0, 0, 0), 2)\n\n    # Write the frame with the boxes\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_eafe9982_1770418215.py\", line 60, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT64 but expected type UINT8 for input 175, name: normalized_input_image_tensor ."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 172.85,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-47-30-581006_chatcmpl-1cb6428c-5f7f-4005-8b04-2097471bf8ac",
          "traceId": "a5dc1257",
          "type": "GENERATION",
          "name": "a5dc_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:47:30.581000+00:00",
          "endTime": "2026-02-06T22:48:02.283000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31702.0,
          "costDetails": {},
          "totalTokens": 1412,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 938,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-48-10-127245_chatcmpl-44a1145c-5a2b-42c4-8e31-769b3e1562a4",
          "traceId": "a5dc1257",
          "type": "GENERATION",
          "name": "a5dc_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:48:10.127000+00:00",
          "endTime": "2026-02-06T22:48:37.046000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26919.0,
          "costDetails": {},
          "totalTokens": 1592,
          "environment": "default",
          "promptTokens": 705,
          "completionTokens": 887,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-48-44-848785_chatcmpl-d9411683-c519-4f7f-af3d-685c4bf19836",
          "traceId": "a5dc1257",
          "type": "GENERATION",
          "name": "a5dc_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:48:44.848000+00:00",
          "endTime": "2026-02-06T22:49:08.452000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23604.0,
          "costDetails": {},
          "totalTokens": 1570,
          "environment": "default",
          "promptTokens": 710,
          "completionTokens": 860,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-49-16-589326_chatcmpl-9548ea4d-f09a-4a93-9ddb-f45409b0cf20",
          "traceId": "a5dc1257",
          "type": "GENERATION",
          "name": "a5dc_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T22:49:16.589000+00:00",
          "endTime": "2026-02-06T22:49:41.443000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24854.0,
          "costDetails": {},
          "totalTokens": 1554,
          "environment": "default",
          "promptTokens": 660,
          "completionTokens": 894,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-49-49-556178_chatcmpl-9ccae5a5-d065-4389-8b85-023eb9c873d4",
          "traceId": "a5dc1257",
          "type": "GENERATION",
          "name": "a5dc_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T22:49:49.556000+00:00",
          "endTime": "2026-02-06T22:50:15.399000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25843.0,
          "costDetails": {},
          "totalTokens": 1617,
          "environment": "default",
          "promptTokens": 655,
          "completionTokens": 962,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "3c19c448-5d84-44d3-906c-d5db87a87534",
          "traceId": "a5dc1257",
          "type": "SPAN",
          "name": "error_a5_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T22:50:23.431000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_eafe9982_1770418215.py\", line 60, in <module>\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT64 but expected type UINT8 for input 175, name: normalized_input_image_tensor .",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T22:47:31.101Z",
      "updatedAt": "2026-02-06T22:50:28.891Z",
      "environment": "default"
    },
    {
      "id": "2d4dc766",
      "timestamp": "2026-02-06T22:44:03.392000+00:00",
      "name": "2d4d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(' ', maxsplit=1)[1] for line in lines if len(line.strip()) > 0 and not line.startswith('#')}\n\ndef main():\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize interpreter with EdgeTPU delegate\n    delegate = load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate])\n    interpreter.allocate_tensors()\n\n    # Get input and output details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Read the video file\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"Error: Could not open video.\")\n        return\n\n    # Prepare to write the output video\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess the input image\n        input_shape = input_details[0]['shape']\n        input_tensor = np.expand_dims(frame, 0)\n        resized_frame = cv2.resize(input_tensor, (input_shape[1], input_shape[2]))\n        normalized_frame = resized_frame / 255.0\n\n        # Run inference\n        interpreter.set_tensor(input_details[0]['index'], normalized_frame.astype(np.float32))\n        interpreter.invoke()\n\n        # Extract results\n        boxes = interpreter.get_tensor(output_details[0]['index'])\n        classes = interpreter.get_tensor(output_details[1]['index'])\n        scores = interpreter.get_tensor(output_details[2]['index'])\n\n        for i in range(len(scores[0])):\n            if scores[0][i] >= confidence_threshold:\n                # Draw detection box and label\n                ymin, xmin, ymax, xmax = boxes[0][i]\n                (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                              ymin * frame_height, ymax * frame_height)\n                cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n                label = f\"{labels[int(classes[0][i])]}: {scores[0][i]:.2f}\"\n                cv2.putText(frame, label, (int(left), int(top) - 10),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n        # Write the frame to the output video\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_a312d6bd_1770418016.py\", line 83, in <module>\n    main()\n  File \"script_a312d6bd_1770418016.py\", line 21, in main\n    labels = load_labels(label_path)\n  File \"script_a312d6bd_1770418016.py\", line 17, in load_labels\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(' ', maxsplit=1)[1] for line in lines if len(line.strip()) > 0 and not line.startswith('#')}\n  File \"script_a312d6bd_1770418016.py\", line 17, in <dictcomp>\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(' ', maxsplit=1)[1] for line in lines if len(line.strip()) > 0 and not line.startswith('#')}\nIndexError: list index out of range."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 180.335,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-44-03-393517_chatcmpl-674ffcce-d9c4-40b4-8def-16a35f63ddc6",
          "traceId": "2d4dc766",
          "type": "GENERATION",
          "name": "2d4d_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:44:03.393000+00:00",
          "endTime": "2026-02-06T22:44:35.116000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 31723.0,
          "costDetails": {},
          "totalTokens": 1676,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 1202,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-44-42-800361_chatcmpl-b7f1de0d-65d0-435a-99ec-9a17a3f27025",
          "traceId": "2d4dc766",
          "type": "GENERATION",
          "name": "2d4d_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:44:42.800000+00:00",
          "endTime": "2026-02-06T22:45:09.979000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27179.0,
          "costDetails": {},
          "totalTokens": 1722,
          "environment": "default",
          "promptTokens": 707,
          "completionTokens": 1015,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-45-17-616622_chatcmpl-0e053369-d532-44b0-949f-0b1aad3b4d78",
          "traceId": "2d4dc766",
          "type": "GENERATION",
          "name": "2d4d_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:45:17.616000+00:00",
          "endTime": "2026-02-06T22:45:47.098000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29482.0,
          "costDetails": {},
          "totalTokens": 1822,
          "environment": "default",
          "promptTokens": 715,
          "completionTokens": 1107,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-45-54-762218_chatcmpl-f7998cf6-34fd-4823-800f-546935d22e85",
          "traceId": "2d4dc766",
          "type": "GENERATION",
          "name": "2d4d_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T22:45:54.762000+00:00",
          "endTime": "2026-02-06T22:46:22.670000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27908.0,
          "costDetails": {},
          "totalTokens": 1826,
          "environment": "default",
          "promptTokens": 782,
          "completionTokens": 1044,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-46-30-288549_chatcmpl-548192ef-82c2-4fbf-bbca-8690ea9bbd68",
          "traceId": "2d4dc766",
          "type": "GENERATION",
          "name": "2d4d_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T22:46:30.288000+00:00",
          "endTime": "2026-02-06T22:46:56.210000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25922.0,
          "costDetails": {},
          "totalTokens": 1715,
          "environment": "default",
          "promptTokens": 780,
          "completionTokens": 935,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "b584f892-4c33-4a6c-a38f-6a73d9f68bd6",
          "traceId": "2d4dc766",
          "type": "SPAN",
          "name": "error_2d_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T22:47:03.728000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_a312d6bd_1770418016.py\", line 83, in <module>\n    main()\n  File \"script_a312d6bd_1770418016.py\", line 21, in main\n    labels = load_labels(label_path)\n  File \"script_a312d6bd_1770418016.py\", line 17, in load_labels\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(' ', maxsplit=1)[1] for line in lines if len(line.strip()) > 0 and not line.startswith('#')}\n  File \"script_a312d6bd_1770418016.py\", line 17, in <dictcomp>\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(' ', maxsplit=1)[1] for line in lines if len(line.strip()) > 0 and not line.startswith('#')}\nIndexError: list index out of range.",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T22:44:03.913Z",
      "updatedAt": "2026-02-06T22:47:09.135Z",
      "environment": "default"
    },
    {
      "id": "7f7f404e",
      "timestamp": "2026-02-06T22:41:49.568000+00:00",
      "name": "7f7f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter with EdgeTPU\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Could not open video file.\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Setup output video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize and normalize image to uint8 as required by the model\n    input_shape = input_details[0]['shape']\n    resized_image = cv2.resize(image, (input_shape[1], input_shape[2]))\n    normalized_image = np.array(resized_image, dtype=np.uint8)\n    return normalized_image\n\ndef draw_detections(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (int(xmin * w), int(xmax * w), int(ymin * h), int(ymax * h))\n            \n            cv2.rectangle(frame, (xminn, yminn), (xmaxx, ymaxx), (0, 255, 0), 2)\n            label = f'{labels[int(cls)]}: {score:.2f}'\n            cv2.putText(frame, label, (xminn, yminn - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension\n\n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence of detected objects\n\n    # Draw detections on the frame\n    draw_detections(frame, boxes, classes, scores)\n\n    # Write processed frame to output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 55.104,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-41-49-570579_chatcmpl-09475aac-8a10-4123-88c2-6fe7150e836a",
          "traceId": "7f7f404e",
          "type": "GENERATION",
          "name": "7f7f_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:41:49.570000+00:00",
          "endTime": "2026-02-06T22:42:13.229000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23659.0,
          "costDetails": {},
          "totalTokens": 1372,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 898,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-42-21-260113_chatcmpl-482c1153-2374-445d-a305-57780a1e19cd",
          "traceId": "7f7f404e",
          "type": "GENERATION",
          "name": "7f7f_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:42:21.260000+00:00",
          "endTime": "2026-02-06T22:42:44.674000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23414.0,
          "costDetails": {},
          "totalTokens": 1534,
          "environment": "default",
          "promptTokens": 661,
          "completionTokens": 873,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:41:50.087Z",
      "updatedAt": "2026-02-06T22:43:42.357Z",
      "environment": "default"
    },
    {
      "id": "6283b01e",
      "timestamp": "2026-02-06T22:39:24.918000+00:00",
      "name": "6283_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n\ndef preprocess(frame):\n    # Convert frame to RGB and resize\n    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized_frame = cv2.resize(rgb_frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    return np.expand_dims(resized_frame, axis=0).astype(np.uint8)\n\ndef draw_detection_boxes(image, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            y_min = int(max(1, box[0] * image.shape[0]))\n            x_min = int(max(1, box[1] * image.shape[1]))\n            y_max = int(min(image.shape[0], box[2] * image.shape[0]))\n            x_max = int(min(image.shape[1], box[3] * image.shape[1]))\n\n            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n            label = f'{labels[int(cls)]}: {score:.2f}'\n            cv2.putText(image, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess frame\n    input_data = preprocess(frame)\n    \n    # Perform inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes and labels on the frame\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write processed frame to output video file\n    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Video processing completed.\")"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 55.592,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-39-24-919273_chatcmpl-a83f0a8b-6954-4e0a-b2af-05e60139d3fd",
          "traceId": "6283b01e",
          "type": "GENERATION",
          "name": "6283_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:39:24.919000+00:00",
          "endTime": "2026-02-06T22:39:49.759000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24840.0,
          "costDetails": {},
          "totalTokens": 1415,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 941,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-39-57-426056_chatcmpl-b5f699ee-8231-4cb9-a3c3-a0d5f8afe2bb",
          "traceId": "6283b01e",
          "type": "GENERATION",
          "name": "6283_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:39:57.426000+00:00",
          "endTime": "2026-02-06T22:40:20.511000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23085.0,
          "costDetails": {},
          "totalTokens": 1476,
          "environment": "default",
          "promptTokens": 607,
          "completionTokens": 869,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:39:25.436Z",
      "updatedAt": "2026-02-06T22:41:18.718Z",
      "environment": "default"
    },
    {
      "id": "ee92957c",
      "timestamp": "2026-02-06T22:36:36.112000+00:00",
      "name": "ee92_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {i: line.strip() for i, line in enumerate(f.readlines())}\n\n# Initialize TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video from the given path\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Preprocessing and inference loop\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Resize and normalize the image to required input shape\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1:3]\n    img_resized = cv2.resize(frame, (width, height))\n    img_normalized = np.expand_dims(img_resized, axis=0).astype(np.uint8)\n\n    # Set tensor to the model and run inference\n    interpreter.set_tensor(input_details[0]['index'], img_normalized)\n    interpreter.invoke()\n\n    # Extract results from output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(np.int64)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[classes[i]]\n            score = scores[i]\n            \n            # Convert normalized coordinates to pixel values\n            (xminn, xmaxx, yminn, ymaxx) = (xmin * width, xmax * width, ymin * height, ymax * height)\n            cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(xminn), int(yminn) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 89.357,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-36-36-113780_chatcmpl-79e426e8-056f-4460-aef4-36958abd0718",
          "traceId": "ee92957c",
          "type": "GENERATION",
          "name": "ee92_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:36:36.113000+00:00",
          "endTime": "2026-02-06T22:37:01.503000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25390.0,
          "costDetails": {},
          "totalTokens": 1322,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 848,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-37-09-437451_chatcmpl-f7b0f9ca-fd05-4b38-8abf-0e4a93d9d46c",
          "traceId": "ee92957c",
          "type": "GENERATION",
          "name": "ee92_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:37:09.437000+00:00",
          "endTime": "2026-02-06T22:37:33.481000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24044.0,
          "costDetails": {},
          "totalTokens": 1459,
          "environment": "default",
          "promptTokens": 591,
          "completionTokens": 868,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-37-41-562593_chatcmpl-919b319f-8854-42e7-b2af-0a5b0f68aa53",
          "traceId": "ee92957c",
          "type": "GENERATION",
          "name": "ee92_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:37:41.562000+00:00",
          "endTime": "2026-02-06T22:38:05.470000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23908.0,
          "costDetails": {},
          "totalTokens": 1549,
          "environment": "default",
          "promptTokens": 661,
          "completionTokens": 888,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:36:36.628Z",
      "updatedAt": "2026-02-06T22:39:03.915Z",
      "environment": "default"
    },
    {
      "id": "90e39d52",
      "timestamp": "2026-02-06T22:34:14.850000+00:00",
      "name": "90e3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from the given path\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 30.0, (int(cap.get(3)), int(cap.get(4))))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_shape = input_details[0]['shape']\n    img_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.array(img_resized, dtype=np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n\n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    start_time = time.time()\n    interpreter.invoke()\n    end_time = time.time()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes and labels\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            score = scores[i]\n\n            im_height, im_width, _ = frame.shape\n            (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                          ymin * im_height, ymax * im_height)\n\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with boxes and labels to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\n\nprint(\"Video processing complete.\")"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 62.432,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-34-14-851487_chatcmpl-d4bb5bb3-d626-4122-9f03-df789197b784",
          "traceId": "90e39d52",
          "type": "GENERATION",
          "name": "90e3_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:34:14.851000+00:00",
          "endTime": "2026-02-06T22:34:49.047000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 34196.0,
          "costDetails": {},
          "totalTokens": 1632,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 1158,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-34-56-617594_chatcmpl-b5b40cf4-0074-4d42-87d5-8c7edf66ac2b",
          "traceId": "90e39d52",
          "type": "GENERATION",
          "name": "90e3_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:34:56.617000+00:00",
          "endTime": "2026-02-06T22:35:17.283000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20666.0,
          "costDetails": {},
          "totalTokens": 1358,
          "environment": "default",
          "promptTokens": 598,
          "completionTokens": 760,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:34:15.367Z",
      "updatedAt": "2026-02-06T22:36:15.169Z",
      "environment": "default"
    },
    {
      "id": "639ea3a1",
      "timestamp": "2026-02-06T22:31:30.125000+00:00",
      "name": "639e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return {index: line.strip() for index, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize TFLite interpreter and allocate tensors\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Cannot open video file: {input_path}\")\n\n# Prepare output video writer\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Function to preprocess the input image\ndef preprocess_image(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(frame_resized, axis=0)\n    return input_data.astype(np.uint8)  # Corrected to UINT8\n\n# Function to draw bounding boxes and labels on the frame\ndef draw_detections(frame, results):\n    for obj in results:\n        if obj['score'] > confidence_threshold:\n            box = obj['bounding_box']\n            label = f\"{labels[obj['class_id']]}: {obj['score']:.2f}\"\n            cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n            cv2.putText(frame, label, (int(box[0]), int(box[1]) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    class_ids = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    results = [{'bounding_box': box, 'class_id': int(class_id), 'score': score} for box, class_id, score in zip(boxes, class_ids, scores)]\n    \n    draw_detections(frame, results)\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 87.312,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-31-30-127331_chatcmpl-3ac0db76-bbdf-4ace-ac05-e0b8276f7bb9",
          "traceId": "639ea3a1",
          "type": "GENERATION",
          "name": "639e_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:31:30.127000+00:00",
          "endTime": "2026-02-06T22:31:53.944000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23817.0,
          "costDetails": {},
          "totalTokens": 1377,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 903,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-32-01-459998_chatcmpl-6c4ece68-92d4-4e6d-bd3e-bd1069152b0e",
          "traceId": "639ea3a1",
          "type": "GENERATION",
          "name": "639e_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:32:01.459000+00:00",
          "endTime": "2026-02-06T22:32:23.857000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22398.0,
          "costDetails": {},
          "totalTokens": 1429,
          "environment": "default",
          "promptTokens": 585,
          "completionTokens": 844,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-32-32-060954_chatcmpl-2f82a303-1629-4321-a077-652064ee6508",
          "traceId": "639ea3a1",
          "type": "GENERATION",
          "name": "639e_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:32:32.060000+00:00",
          "endTime": "2026-02-06T22:32:57.439000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25379.0,
          "costDetails": {},
          "totalTokens": 1620,
          "environment": "default",
          "promptTokens": 663,
          "completionTokens": 957,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:31:30.645Z",
      "updatedAt": "2026-02-06T22:33:54.446Z",
      "environment": "default"
    },
    {
      "id": "e06856ca",
      "timestamp": "2026-02-06T22:28:19.967000+00:00",
      "name": "e068_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(None, 1)[-1] for line in lines if line and not line.startswith('#')}\n\ndef set_input_tensor(interpreter, image):\n    tensor_index = interpreter.get_input_details()[0]['index']\n    input_tensor = interpreter.tensor(tensor_index)()[0]\n    input_tensor[:, :] = np.expand_dims(image, axis=0)\n\ndef get_output_tensor(interpreter, index):\n    output_details = interpreter.get_output_details()[index]\n    tensor = np.squeeze(interpreter.get_tensor(output_details['index']))\n    return tensor\n\n# Load TFLite model and allocate tensors\ninterpreter = Interpreter(model_path=model_path,\n                          experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Load labels\nlabels = load_labels(label_path)\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Get video properties to save output video with the same settings\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing: Resize and normalize the image\n    input_size = interpreter.get_input_details()[0]['shape'][1:3]\n    resized_frame = cv2.resize(frame, (input_size[1], input_size[0]))\n    normalized_frame = np.array(resized_frame, dtype=np.float32) / 255.0\n\n    # Set the tensor to point to the input data to be inferred\n    set_input_tensor(interpreter, normalized_frame)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract output data from the interpreter\n    boxes = get_output_tensor(interpreter, 0)\n    classes = get_output_tensor(interpreter, 1)\n    scores = get_output_tensor(interpreter, 2)\n    count = int(get_output_tensor(interpreter, 3))\n\n    # Draw detection boxes with labels and confidence scores on the frame\n    for i in range(count):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            score = scores[i]\n\n            # Convert normalized coordinates to pixel values\n            top_left = (int(xmin * frame_width), int(ymin * frame_height))\n            bottom_right = (int(xmax * frame_width), int(ymax * frame_height))\n\n            # Draw rectangle and label on the frame\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (top_left[0], top_left[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detections to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_4e347197_1770417056.py\", line 35, in <module>\n    labels = load_labels(label_path)\n  File \"script_4e347197_1770417056.py\", line 17, in load_labels\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(None, 1)[-1] for line in lines if line and not line.startswith('#')}\n  File \"script_4e347197_1770417056.py\", line 17, in <dictcomp>\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(None, 1)[-1] for line in lines if line and not line.startswith('#')}\nValueError: invalid literal for int() with base 10: 'person\\n'."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 164.015,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-28-19-968093_chatcmpl-e6dbf6f9-a559-45e3-a21b-28fe88180151",
          "traceId": "e06856ca",
          "type": "GENERATION",
          "name": "e068_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:28:19.968000+00:00",
          "endTime": "2026-02-06T22:28:46.285000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26317.0,
          "costDetails": {},
          "totalTokens": 1469,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 995,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-28-53-812054_chatcmpl-45cbf1b3-75b0-431b-b903-9df472ebb62d",
          "traceId": "e06856ca",
          "type": "GENERATION",
          "name": "e068_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:28:53.812000+00:00",
          "endTime": "2026-02-06T22:29:18.266000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24454.0,
          "costDetails": {},
          "totalTokens": 1633,
          "environment": "default",
          "promptTokens": 722,
          "completionTokens": 911,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-29-25-873533_chatcmpl-1b402062-3055-4326-b220-8611ea584a79",
          "traceId": "e06856ca",
          "type": "GENERATION",
          "name": "e068_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:29:25.873000+00:00",
          "endTime": "2026-02-06T22:29:48.267000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22394.0,
          "costDetails": {},
          "totalTokens": 1562,
          "environment": "default",
          "promptTokens": 723,
          "completionTokens": 839,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-29-56-109985_chatcmpl-d3763dfb-686d-49e5-9ea3-61f21b33dd69",
          "traceId": "e06856ca",
          "type": "GENERATION",
          "name": "e068_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T22:29:56.109000+00:00",
          "endTime": "2026-02-06T22:30:20.150000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24041.0,
          "costDetails": {},
          "totalTokens": 1638,
          "environment": "default",
          "promptTokens": 736,
          "completionTokens": 902,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-30-27-789262_chatcmpl-fe4b2364-fa2e-4c90-93ef-203ee76f89d3",
          "traceId": "e06856ca",
          "type": "GENERATION",
          "name": "e068_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T22:30:27.789000+00:00",
          "endTime": "2026-02-06T22:30:56.347000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28558.0,
          "costDetails": {},
          "totalTokens": 1796,
          "environment": "default",
          "promptTokens": 724,
          "completionTokens": 1072,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "ab876208-7d35-48d3-be74-b7df2ad2cf2b",
          "traceId": "e06856ca",
          "type": "SPAN",
          "name": "error_e0_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T22:31:03.983000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_4e347197_1770417056.py\", line 35, in <module>\n    labels = load_labels(label_path)\n  File \"script_4e347197_1770417056.py\", line 17, in load_labels\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(None, 1)[-1] for line in lines if line and not line.startswith('#')}\n  File \"script_4e347197_1770417056.py\", line 17, in <dictcomp>\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(None, 1)[-1] for line in lines if line and not line.startswith('#')}\nValueError: invalid literal for int() with base 10: 'person\\n'.",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T22:28:20.488Z",
      "updatedAt": "2026-02-06T22:31:09.239Z",
      "environment": "default"
    },
    {
      "id": "da6d0a2e",
      "timestamp": "2026-02-06T22:25:05.888000+00:00",
      "name": "da6d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        labels = {}\n        for line in f.readlines():\n            if line.strip():\n                parts = line.split(maxsplit=1)\n                key = parts[0].strip()\n                value = parts[1].strip() if len(parts) > 1 else None\n                labels[key] = value\n    return labels\n\ndef main():\n    # Load the TFLite model and allocate tensors.\n    delegate = load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate])\n    interpreter.allocate_tensors()\n\n    # Get input and output details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Read the video file\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise IOError(\"Cannot open video\")\n\n    # Get video properties and prepare to write the output video\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess the image\n        input_shape = input_details[0]['shape']\n        input_data = np.expand_dims(frame, axis=0)\n        input_data = cv2.resize(input_data[0], (input_shape[2], input_shape[1]))\n        input_data = np.float32(input_data) / 255.0\n        input_tensor = np.expand_dims(input_data, axis=0)\n\n        # Set the tensor to point to the input data to be inferred\n        interpreter.set_tensor(input_details[0]['index'], input_tensor)\n\n        # Run the inference\n        start_time = time.time()\n        interpreter.invoke()\n\n        # Get all output details\n        boxes = interpreter.get_tensor(output_details[0]['index'])\n        classes = interpreter.get_tensor(output_details[1]['index'])\n        scores = interpreter.get_tensor(output_details[2]['index'])\n        count = int(interpreter.get_tensor(output_details[3]['index']))\n\n        # Draw detection result on the frame\n        for i in range(count):\n            if scores[i] >= confidence_threshold:\n                ymin, xmin, ymax, xmax = boxes[i]\n                label = labels[str(int(classes[i]))]\n                score = scores[i]\n\n                im_height, im_width, _ = frame.shape\n\n                (xminn, xmaxx, yminn, ymaxx) = (xmin * im_width, xmax * im_width, ymin * im_height, ymax * im_height)\n                cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (0, 255, 0), 2)\n\n                label_text = f'{label}: {score:.2f}'\n                label_size, _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n                cv2.rectangle(frame, (int(xminn), int(yminn) - label_size[1] - 10),\n                              (int(xminn) + label_size[0], int(yminn) - 10), (0, 255, 0), -1)\n                cv2.putText(frame, label_text, (int(xminn), int(yminn) - 10),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n\n        # Write the frame to the output video\n        out.write(frame)\n\n    # Release resources\n    cap.release()\n    out.release()\n\nif __name__ == '__main__':\n    main()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_9445ee6f_1770416865.py\", line 102, in <module>\n    main()\n  File \"script_9445ee6f_1770416865.py\", line 63, in main\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor ."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 167.882,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-25-05-890472_chatcmpl-47f470cc-69fd-4ad1-897d-5a026c43d17b",
          "traceId": "da6d0a2e",
          "type": "GENERATION",
          "name": "da6d_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:25:05.890000+00:00",
          "endTime": "2026-02-06T22:25:32.892000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27002.0,
          "costDetails": {},
          "totalTokens": 1495,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 1021,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-25-40-451420_chatcmpl-a9470e84-3050-4e53-ab90-8e89b642542a",
          "traceId": "da6d0a2e",
          "type": "GENERATION",
          "name": "da6d_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:25:40.451000+00:00",
          "endTime": "2026-02-06T22:26:02.714000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22263.0,
          "costDetails": {},
          "totalTokens": 1533,
          "environment": "default",
          "promptTokens": 706,
          "completionTokens": 827,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-26-10-391425_chatcmpl-f0116e7b-d485-4967-a56b-ee520f49e856",
          "traceId": "da6d0a2e",
          "type": "GENERATION",
          "name": "da6d_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:26:10.391000+00:00",
          "endTime": "2026-02-06T22:26:37.081000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26690.0,
          "costDetails": {},
          "totalTokens": 1736,
          "environment": "default",
          "promptTokens": 735,
          "completionTokens": 1001,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-26-44-656219_chatcmpl-53bb59b4-220a-4c13-a1c7-da5fc6c34cea",
          "traceId": "da6d0a2e",
          "type": "GENERATION",
          "name": "da6d_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T22:26:44.656000+00:00",
          "endTime": "2026-02-06T22:27:09.479000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24823.0,
          "costDetails": {},
          "totalTokens": 1696,
          "environment": "default",
          "promptTokens": 768,
          "completionTokens": 928,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-27-17-296460_chatcmpl-6bfde7a4-35b4-4cb1-8eb1-1ace518893fe",
          "traceId": "da6d0a2e",
          "type": "GENERATION",
          "name": "da6d_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T22:27:17.296000+00:00",
          "endTime": "2026-02-06T22:27:45.787000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28491.0,
          "costDetails": {},
          "totalTokens": 1741,
          "environment": "default",
          "promptTokens": 668,
          "completionTokens": 1073,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "2f603b1c-21bc-407a-91c2-4180db585c53",
          "traceId": "da6d0a2e",
          "type": "SPAN",
          "name": "error_da_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T22:27:53.772000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_9445ee6f_1770416865.py\", line 102, in <module>\n    main()\n  File \"script_9445ee6f_1770416865.py\", line 63, in main\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n  File \"/home/mendel/tinyml_autopilot/tinyml-env/lib/python3.7/site-packages/tflite_runtime/interpreter.py\", line 572, in set_tensor\n    self._interpreter.SetTensor(tensor_index, value)\nValueError: Cannot set tensor: Got value of type FLOAT32 but expected type UINT8 for input 175, name: normalized_input_image_tensor .",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T22:25:06.409Z",
      "updatedAt": "2026-02-06T22:27:59.130Z",
      "environment": "default"
    },
    {
      "id": "4f6518af",
      "timestamp": "2026-02-06T22:22:52.235000+00:00",
      "name": "4f65_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(label_path):\n    with open(label_path, 'r') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Setup TFLite interpreter with EdgeTPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Preprocess the image\ndef preprocess_image(frame):\n    height, width = frame.shape[:2]\n    img_resized = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(img_resized, axis=0)\n    return input_data\n\n# Function to draw bounding boxes and labels\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes, classes, scores):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            height, width = frame.shape[:2]\n            (xminn, xmaxx, ymine, ymaxe) = (int(xmin * width), int(xmax * width),\n                                            int(ymin * height), int(ymax * height))\n            cv2.rectangle(frame, (xminn, ymine), (xmaxx, ymaxe), (10, 255, 0), 2)\n            label = f'{labels[int(cls)]}: {score:.2f}'\n            cv2.putText(frame, label, (xminn, ymine - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main processing loop\ncap = cv2.VideoCapture(input_path)\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    draw_boxes(frame, boxes, classes, scores)\n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 55.558,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-22-52-236631_chatcmpl-fed31cf2-7821-4957-8112-bd877415ee8a",
          "traceId": "4f6518af",
          "type": "GENERATION",
          "name": "4f65_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:22:52.236000+00:00",
          "endTime": "2026-02-06T22:23:18.442000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26206.0,
          "costDetails": {},
          "totalTokens": 1460,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 986,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-23-26-325780_chatcmpl-63d62ae0-3a59-44f1-8fd8-d118bb31844b",
          "traceId": "4f6518af",
          "type": "GENERATION",
          "name": "4f65_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:23:26.325000+00:00",
          "endTime": "2026-02-06T22:23:47.794000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21469.0,
          "costDetails": {},
          "totalTokens": 1432,
          "environment": "default",
          "promptTokens": 629,
          "completionTokens": 803,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:22:52.759Z",
      "updatedAt": "2026-02-06T22:24:45.998Z",
      "environment": "default"
    },
    {
      "id": "fa8e2abd",
      "timestamp": "2026-02-06T22:19:58.395000+00:00",
      "name": "fa8e_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load the TFLite model and allocate tensors\ninterpreter = Interpreter(model_path=model_path, experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')])\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open input video\")\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(frame):\n    # Convert to grayscale if necessary, resize, and normalize\n    frame_resized = cv2.resize(frame, (300, 300))\n    frame_normalized = np.array(frame_resized).astype(np.uint8)\n    return frame_normalized\n\ndef draw_detection_boxes(image, boxes, classes, scores):\n    for i in range(len(boxes)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            score = scores[i]\n\n            # Convert box coordinates to image dimensions\n            top = int(box[0] * frame_height)\n            left = int(box[1] * frame_width)\n            bottom = int(box[2] * frame_height)\n            right = int(box[3] * frame_width)\n\n            label_text = f\"{labels[class_id]}: {score:.2f}\"\n            cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n            cv2.putText(image, label_text, (left, top - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    processed_frame = preprocess_image(frame)\n    input_data = np.expand_dims(processed_frame, axis=0)  # Add batch dimension\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run the inference (invoke)\n    interpreter.invoke()\n\n    # Extract results from the output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # Bounding box coordinates of detected objects\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]  # Class index of detected objects\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]   # Confidence of detected objects\n\n    # Draw detection boxes on the frame\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write the frame with bounding boxes to the output video file\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 96.366,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-19-58-397412_chatcmpl-57590739-2086-476c-8382-aae6dbb2c774",
          "traceId": "fa8e2abd",
          "type": "GENERATION",
          "name": "fa8e_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:19:58.397000+00:00",
          "endTime": "2026-02-06T22:20:22.743000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24346.0,
          "costDetails": {},
          "totalTokens": 1389,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 915,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-20-30-463468_chatcmpl-50048ed1-b6f8-467c-a519-1b384cda9a5b",
          "traceId": "fa8e2abd",
          "type": "GENERATION",
          "name": "fa8e_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:20:30.463000+00:00",
          "endTime": "2026-02-06T22:20:59.320000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28857.0,
          "costDetails": {},
          "totalTokens": 1685,
          "environment": "default",
          "promptTokens": 601,
          "completionTokens": 1084,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-21-07-735681_chatcmpl-c301f9b9-0879-4972-ac6c-fc5999aed35e",
          "traceId": "fa8e2abd",
          "type": "GENERATION",
          "name": "fa8e_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:21:07.735000+00:00",
          "endTime": "2026-02-06T22:21:34.763000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27028.0,
          "costDetails": {},
          "totalTokens": 1679,
          "environment": "default",
          "promptTokens": 661,
          "completionTokens": 1018,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:19:58.920Z",
      "updatedAt": "2026-02-06T22:22:32.703Z",
      "environment": "default"
    },
    {
      "id": "3bf06d55",
      "timestamp": "2026-02-06T22:16:47.629000+00:00",
      "name": "3bf0_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(label_path):\n    \"\"\"Load labels from a text file.\"\"\"\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    labels = {}\n    for line in lines:\n        pair = line.strip().split(maxsplit=1)\n        if len(pair) == 2:\n            labels[int(pair[0])] = pair[1].strip()\n        else:\n            labels[len(labels)] = pair[0]\n    return labels\n\ndef preprocess_image(frame, input_size):\n    \"\"\"Preprocess the image to match the model's expected input.\"\"\"\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (input_size, input_size))\n    input_data = np.expand_dims(frame_resized, axis=0).astype(np.uint8)\n    return input_data\n\ndef draw_detection_boxes(frame, boxes, classes, scores, labels):\n    \"\"\"Draw detection boxes and labels on the frame.\"\"\"\n    height, width, _ = frame.shape\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            left = int(xmin * width)\n            right = int(xmax * width)\n            top = int(ymin * height)\n            bottom = int(ymax * height)\n\n            cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n            label_text = f'{labels[int(cls)]}: {score:.2f}'\n            cv2.putText(frame, label_text, (left, top - 8 if top > 30 else top + 16),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\ndef main():\n    # Load labels\n    labels = load_labels(label_path)\n    \n    # Initialize TFLite interpreter with EdgeTPU delegate\n    delegate = load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate])\n    interpreter.allocate_tensors()\n    \n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    input_shape = input_details[0]['shape']\n    input_size = input_shape[1]\n    \n    # Read the input video\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file {input_path}\")\n        return\n    \n    # Define codec and create VideoWriter object\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    out = cv2.VideoWriter(output_path, fourcc, fps, (int(cap.get(3)), int(cap.get(4))))\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Preprocess the image\n        input_data = preprocess_image(frame, input_size)\n        \n        # Run inference\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        interpreter.invoke()\n        \n        # Extract results\n        boxes = interpreter.get_tensor(output_details[0]['index'])\n        classes = interpreter.get_tensor(output_details[1]['index'])\n        scores = interpreter.get_tensor(output_details[2]['index'])\n        num_detections = interpreter.get_tensor(output_details[3]['index'])\n        \n        # Draw detection boxes and labels on the frame\n        draw_detection_boxes(frame, boxes, classes, scores, labels)\n        \n        # Write the frame with detections to the output video file\n        out.write(frame)\n    \n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_8ea4bad5_1770416364.py\", line 103, in <module>\n    main()\n  File \"script_8ea4bad5_1770416364.py\", line 52, in main\n    labels = load_labels(label_path)\n  File \"script_8ea4bad5_1770416364.py\", line 22, in load_labels\n    labels[int(pair[0])] = pair[1].strip()\nValueError: invalid literal for int() with base 10: 'traffic'."
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 164.152,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-16-47-630488_chatcmpl-92a0c633-e6b3-4bc5-a3e3-8a69b87bb876",
          "traceId": "3bf06d55",
          "type": "GENERATION",
          "name": "3bf0_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:16:47.630000+00:00",
          "endTime": "2026-02-06T22:17:09.347000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21717.0,
          "costDetails": {},
          "totalTokens": 1289,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 815,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-17-16-909214_chatcmpl-da424cbe-d9d9-457a-86ca-0d9b5b7f3153",
          "traceId": "3bf06d55",
          "type": "GENERATION",
          "name": "3bf0_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:17:16.909000+00:00",
          "endTime": "2026-02-06T22:17:41.741000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24832.0,
          "costDetails": {},
          "totalTokens": 1541,
          "environment": "default",
          "promptTokens": 608,
          "completionTokens": 933,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-17-49-305961_chatcmpl-f11b8731-60e5-4c4e-ac6c-52c880f71308",
          "traceId": "3bf06d55",
          "type": "GENERATION",
          "name": "3bf0_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:17:49.305000+00:00",
          "endTime": "2026-02-06T22:18:13.504000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24199.0,
          "costDetails": {},
          "totalTokens": 1618,
          "environment": "default",
          "promptTokens": 709,
          "completionTokens": 909,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-18-21-208125_chatcmpl-3e11f8f7-bab9-43af-822d-167672cf18b0",
          "traceId": "3bf06d55",
          "type": "GENERATION",
          "name": "3bf0_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T22:18:21.208000+00:00",
          "endTime": "2026-02-06T22:18:49.999000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28791.0,
          "costDetails": {},
          "totalTokens": 1755,
          "environment": "default",
          "promptTokens": 671,
          "completionTokens": 1084,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-18-57-593354_chatcmpl-8b9ef002-b569-4768-ae9f-c1798ff512a6",
          "traceId": "3bf06d55",
          "type": "GENERATION",
          "name": "3bf0_tpusg_gen_attempt#5",
          "startTime": "2026-02-06T22:18:57.593000+00:00",
          "endTime": "2026-02-06T22:19:24.172000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26579.0,
          "costDetails": {},
          "totalTokens": 1671,
          "environment": "default",
          "promptTokens": 669,
          "completionTokens": 1002,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "8b202300-20f8-4000-a05b-fc2adefbc67e",
          "traceId": "3bf06d55",
          "type": "SPAN",
          "name": "error_3b_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-06T22:19:31.782000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_8ea4bad5_1770416364.py\", line 103, in <module>\n    main()\n  File \"script_8ea4bad5_1770416364.py\", line 52, in main\n    labels = load_labels(label_path)\n  File \"script_8ea4bad5_1770416364.py\", line 22, in load_labels\n    labels[int(pair[0])] = pair[1].strip()\nValueError: invalid literal for int() with base 10: 'traffic'.",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T22:16:48.157Z",
      "updatedAt": "2026-02-06T22:19:37.348Z",
      "environment": "default"
    },
    {
      "id": "c93b6256",
      "timestamp": "2026-02-06T22:13:17.652000+00:00",
      "name": "c93b_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r', encoding='utf8') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize interpreter with EdgeTPU delegate\ninterpreter = Interpreter(\n    model_path=model_path,\n    experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(\"Error opening video stream or file\")\n\n# Get video properties to save processed video\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Resize to input size required by model\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(image, (input_shape[1], input_shape[2]))\n    \n    # Convert to uint8 if needed and add batch dimension\n    input_data = np.expand_dims(image_resized.astype(np.uint8), axis=0)\n    \n    return input_data\n\ndef postprocess_output(output_details):\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    \n    return boxes, classes, scores, num_detections\n\n# Main inference loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess image\n    input_data = preprocess_image(frame)\n    \n    # Set tensor and invoke interpreter\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Post-process output\n    boxes, classes, scores, num_detections = postprocess_output(output_details)\n    \n    # Draw detections on frame\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            score = scores[i]\n            \n            label = labels.get(class_id, 'Unknown')\n            display_text = f'{label}: {int(score * 100)}%'\n            \n            ymin, xmin, ymax, xmax = box\n            im_height, im_width, _ = frame.shape\n            \n            (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                          ymin * im_height, ymax * im_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, display_text, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    # Write the frame into the file 'output_path'\n    out.write(frame)\n\n# Release everything if job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Processed video saved to\", output_path)"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_tpusg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "qwen2.5-coder:32b",
        "tpu_sketch_generator"
      ],
      "latency": 131.175,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-13-17-654161_chatcmpl-8b6c9e4b-fb8b-4e0b-9464-d7e0e29277fe",
          "traceId": "c93b6256",
          "type": "GENERATION",
          "name": "c93b_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T22:13:17.654000+00:00",
          "endTime": "2026-02-06T22:13:46.388000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 28734.0,
          "costDetails": {},
          "totalTokens": 1562,
          "environment": "default",
          "promptTokens": 474,
          "completionTokens": 1088,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-13-53-926181_chatcmpl-6a9ad376-bb14-4b6e-a13a-e636e7266228",
          "traceId": "c93b6256",
          "type": "GENERATION",
          "name": "c93b_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T22:13:53.926000+00:00",
          "endTime": "2026-02-06T22:14:21.368000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27442.0,
          "costDetails": {},
          "totalTokens": 1619,
          "environment": "default",
          "promptTokens": 586,
          "completionTokens": 1033,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-14-29-697587_chatcmpl-e9c1e104-4f69-48c0-8e21-a78bb2cc6903",
          "traceId": "c93b6256",
          "type": "GENERATION",
          "name": "c93b_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T22:14:29.697000+00:00",
          "endTime": "2026-02-06T22:14:55.237000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25540.0,
          "costDetails": {},
          "totalTokens": 1574,
          "environment": "default",
          "promptTokens": 608,
          "completionTokens": 966,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-15-03-222218_chatcmpl-1581022c-78f3-40e2-9d86-0c9d1bfeb94e",
          "traceId": "c93b6256",
          "type": "GENERATION",
          "name": "c93b_tpusg_gen_attempt#4",
          "startTime": "2026-02-06T22:15:03.222000+00:00",
          "endTime": "2026-02-06T22:15:28.829000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25607.0,
          "costDetails": {},
          "totalTokens": 1626,
          "environment": "default",
          "promptTokens": 660,
          "completionTokens": 966,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:13:18.183Z",
      "updatedAt": "2026-02-06T22:16:26.437Z",
      "environment": "default"
    }
  ],
  "meta": {
    "total_items": 30
  }
}