{
  "data": [
    {
      "id": "6246c9b1",
      "timestamp": "2026-02-06T22:11:10.108000+00:00",
      "name": "6246_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f.readlines() if len(line.strip()) > 0 and not line.startswith('#')}\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 20, (frame_width, frame_height))\n\ndef preprocess_image(frame):\n    # Convert to RGB and resize\n    input_data = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    input_data = cv2.resize(input_data, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n# Function to compute mAP\ndef compute_mAP():\n    # Placeholder for actual mAP computation logic\n    # This is a complex task and requires ground truth annotations and predictions\n    # For simplicity, we will not implement this here.\n    return 0.0\n\nframe_count = 0\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    preprocessed_frame = preprocess_image(frame)\n    \n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n    \n    # Run inference\n    interpreter.invoke()\n    \n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    \n    # Draw detection boxes and labels\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            class_id = int(classes[i])\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = box\n            \n            # Convert to pixel values\n            im_height, im_width, _ = frame.shape\n            (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                          ymin * im_height, ymax * im_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[class_id]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n    \n    # Write the frame to output video\n    out.write(frame)\n    frame_count += 1\n\n# Release resources\ncap.release()\nout.release()\n\n# Compute mAP (placeholder function used here)\nmAP = compute_mAP()\nprint(f\"Mean Average Precision (mAP): {mAP:.4f}\")",
        "last_error": "Max retries reached with failure. Last error from execution: Traceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207001311_psg_qwen2.5-coder:32b/tmp_20260207001311_psg_qwen2.5-coder:32b.py\", line 16, in <module>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f.readlines() if len(line.strip()) > 0 and not line.startswith('#')}\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207001311_psg_qwen2.5-coder:32b/tmp_20260207001311_psg_qwen2.5-coder:32b.py\", line 16, in <dictcomp>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f.readlines() if len(line.strip()) > 0 and not line.startswith('#')}\nValueError: invalid literal for int() with base 10: 'person\\n'\n"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 121.225,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-11-10-110678_chatcmpl-8340facc-5f76-499d-acea-a50dec16d588",
          "traceId": "6246c9b1",
          "type": "GENERATION",
          "name": "6246_psg_gen_attempt#1",
          "startTime": "2026-02-06T22:11:10.110000+00:00",
          "endTime": "2026-02-06T22:11:32.142000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22032.0,
          "costDetails": {},
          "totalTokens": 1269,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 833,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-11-32-299312_chatcmpl-586ecf83-0b34-49b2-b33d-98bcdb851d37",
          "traceId": "6246c9b1",
          "type": "GENERATION",
          "name": "6246_psg_gen_attempt#2",
          "startTime": "2026-02-06T22:11:32.299000+00:00",
          "endTime": "2026-02-06T22:11:57.178000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24879.0,
          "costDetails": {},
          "totalTokens": 1672,
          "environment": "default",
          "promptTokens": 746,
          "completionTokens": 926,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-11-57-320499_chatcmpl-81f525ee-3eeb-43f5-92ef-839ab56f6914",
          "traceId": "6246c9b1",
          "type": "GENERATION",
          "name": "6246_psg_gen_attempt#3",
          "startTime": "2026-02-06T22:11:57.320000+00:00",
          "endTime": "2026-02-06T22:12:21.687000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24367.0,
          "costDetails": {},
          "totalTokens": 1682,
          "environment": "default",
          "promptTokens": 771,
          "completionTokens": 911,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-12-21-837144_chatcmpl-f2085921-fda6-4c64-a7df-b8e8464f911c",
          "traceId": "6246c9b1",
          "type": "GENERATION",
          "name": "6246_psg_gen_attempt#4",
          "startTime": "2026-02-06T22:12:21.837000+00:00",
          "endTime": "2026-02-06T22:12:46.738000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24901.0,
          "costDetails": {},
          "totalTokens": 1706,
          "environment": "default",
          "promptTokens": 774,
          "completionTokens": 932,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-12-46-874482_chatcmpl-1ba2bb0e-52a6-4f05-8fdd-c16b5d5ed758",
          "traceId": "6246c9b1",
          "type": "GENERATION",
          "name": "6246_psg_gen_attempt#5",
          "startTime": "2026-02-06T22:12:46.874000+00:00",
          "endTime": "2026-02-06T22:13:11.149000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24275.0,
          "costDetails": {},
          "totalTokens": 1681,
          "environment": "default",
          "promptTokens": 774,
          "completionTokens": 907,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "179a4129-13d4-41fa-b5e5-905bf5747c71",
          "traceId": "6246c9b1",
          "type": "SPAN",
          "name": "error_62_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-06T22:13:11.335000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: Traceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207001311_psg_qwen2.5-coder:32b/tmp_20260207001311_psg_qwen2.5-coder:32b.py\", line 16, in <module>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f.readlines() if len(line.strip()) > 0 and not line.startswith('#')}\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207001311_psg_qwen2.5-coder:32b/tmp_20260207001311_psg_qwen2.5-coder:32b.py\", line 16, in <dictcomp>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f.readlines() if len(line.strip()) > 0 and not line.startswith('#')}\nValueError: invalid literal for int() with base 10: 'person\\n'\n",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T22:11:10.655Z",
      "updatedAt": "2026-02-06T22:13:16.835Z",
      "environment": "default"
    },
    {
      "id": "1af1a59d",
      "timestamp": "2026-02-06T22:08:55.834000+00:00",
      "name": "1af1_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load the TFLite model and allocate tensors\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Read the input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Cannot open video file {input_path}\")\n\n# Prepare video writer for output\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the input data\n    input_shape = input_details[0]['shape']\n    input_data = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.uint8)  # Convert to UINT8\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes with labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_id = int(classes[i])\n            label = labels[class_id]\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = box\n\n            # Scale the bounding box coordinates to match the original image size\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label_text = f\"{label}: {scores[i]:.2f}\"\n            cv2.putText(frame, label_text, (int(left), int(top) - 8),\n                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n    # Write the frame with detection to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 67.981,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-08-55-835827_chatcmpl-bf5fb796-5817-4412-8399-a668dbb124c3",
          "traceId": "1af1a59d",
          "type": "GENERATION",
          "name": "1af1_psg_gen_attempt#1",
          "startTime": "2026-02-06T22:08:55.835000+00:00",
          "endTime": "2026-02-06T22:09:15.339000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19504.0,
          "costDetails": {},
          "totalTokens": 1188,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 752,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-09-15-548598_chatcmpl-fb73fc16-9e3d-44a0-9996-7bf1ba36cbda",
          "traceId": "1af1a59d",
          "type": "GENERATION",
          "name": "1af1_psg_gen_attempt#2",
          "startTime": "2026-02-06T22:09:15.548000+00:00",
          "endTime": "2026-02-06T22:09:38.884000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23336.0,
          "costDetails": {},
          "totalTokens": 1491,
          "environment": "default",
          "promptTokens": 615,
          "completionTokens": 876,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-09-39-058079_chatcmpl-1cc64af9-b5dd-4a2e-949f-34be5cf56b3b",
          "traceId": "1af1a59d",
          "type": "GENERATION",
          "name": "1af1_psg_gen_attempt#3",
          "startTime": "2026-02-06T22:09:39.058000+00:00",
          "endTime": "2026-02-06T22:10:03.816000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24758.0,
          "costDetails": {},
          "totalTokens": 1617,
          "environment": "default",
          "promptTokens": 684,
          "completionTokens": 933,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:08:56.373Z",
      "updatedAt": "2026-02-06T22:10:46.563Z",
      "environment": "default"
    },
    {
      "id": "bb65e48f",
      "timestamp": "2026-02-06T22:08:02.199000+00:00",
      "name": "bb65_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(label_path):\n    with open(label_path, 'r') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\n# Initialize interpreter and allocate tensors\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\nlabels = load_labels(label_path)\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nfps = cap.get(cv2.CAP_PROP_FPS)\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image_resized = cv2.resize(image_rgb, (width, height))\n    input_data = np.expand_dims(image_resized, axis=0)\n    return input_data\n\n# Main loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the image\n    input_tensor = preprocess_image(frame)\n    \n    # Perform inference\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n    interpreter.invoke()\n    \n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    \n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    # Write the frame into the file 'output_path'\n    out.write(frame)\n\n# Release everything if job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f\"Processed video saved to {output_path}\")"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 21.442,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-08-02-201225_chatcmpl-92981744-f644-4491-b281-f2dada550b92",
          "traceId": "bb65e48f",
          "type": "GENERATION",
          "name": "bb65_psg_gen_attempt#1",
          "startTime": "2026-02-06T22:08:02.201000+00:00",
          "endTime": "2026-02-06T22:08:23.643000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21442.0,
          "costDetails": {},
          "totalTokens": 1243,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 807,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:08:02.740Z",
      "updatedAt": "2026-02-06T22:08:34.882Z",
      "environment": "default"
    },
    {
      "id": "327287c0",
      "timestamp": "2026-02-06T22:05:42.569000+00:00",
      "name": "3272_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(label_path):\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n        labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in lines if not line.startswith('#')}\n    return labels\n\nlabels = load_labels(label_path)\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    img = np.expand_dims(resized_img, axis=0)\n    return img.astype(np.uint8)\n\n# Process video frames\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    preprocessed_frame = preprocess_image(frame)\n    \n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Draw detection boxes and labels\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = map(int, (boxes[i][0] * frame_height, boxes[i][1] * frame_width,\n                                               boxes[i][2] * frame_height, boxes[i][3] * frame_width))\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything if job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f\"Processed video saved to {output_path}\")",
        "last_error": "Max retries reached with failure. Last error from execution: Traceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000736_psg_qwen2.5-coder:32b/tmp_20260207000736_psg_qwen2.5-coder:32b.py\", line 21, in <module>\n    labels = load_labels(label_path)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000736_psg_qwen2.5-coder:32b/tmp_20260207000736_psg_qwen2.5-coder:32b.py\", line 18, in load_labels\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in lines if not line.startswith('#')}\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000736_psg_qwen2.5-coder:32b/tmp_20260207000736_psg_qwen2.5-coder:32b.py\", line 18, in <dictcomp>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in lines if not line.startswith('#')}\nValueError: invalid literal for int() with base 10: 'person\\n'\n"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 114.434,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-05-42-570225_chatcmpl-fe2002dc-5900-4ea0-90b1-5c64ba5ca62a",
          "traceId": "327287c0",
          "type": "GENERATION",
          "name": "3272_psg_gen_attempt#1",
          "startTime": "2026-02-06T22:05:42.570000+00:00",
          "endTime": "2026-02-06T22:06:06.990000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24420.0,
          "costDetails": {},
          "totalTokens": 1362,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 926,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-06-07-496360_chatcmpl-a2d9175d-37f7-42d1-858c-18a9e1e9cedc",
          "traceId": "327287c0",
          "type": "GENERATION",
          "name": "3272_psg_gen_attempt#2",
          "startTime": "2026-02-06T22:06:07.496000+00:00",
          "endTime": "2026-02-06T22:06:27.887000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20391.0,
          "costDetails": {},
          "totalTokens": 1450,
          "environment": "default",
          "promptTokens": 690,
          "completionTokens": 760,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-06-28-022388_chatcmpl-b9b14dfc-8168-429e-8cd8-67c9fac6b724",
          "traceId": "327287c0",
          "type": "GENERATION",
          "name": "3272_psg_gen_attempt#3",
          "startTime": "2026-02-06T22:06:28.022000+00:00",
          "endTime": "2026-02-06T22:06:48.492000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20470.0,
          "costDetails": {},
          "totalTokens": 1512,
          "environment": "default",
          "promptTokens": 746,
          "completionTokens": 766,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-06-48-634515_chatcmpl-301ce527-1164-45c5-a3f0-d27265ab68b6",
          "traceId": "327287c0",
          "type": "GENERATION",
          "name": "3272_psg_gen_attempt#4",
          "startTime": "2026-02-06T22:06:48.634000+00:00",
          "endTime": "2026-02-06T22:07:14.618000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25984.0,
          "costDetails": {},
          "totalTokens": 1730,
          "environment": "default",
          "promptTokens": 754,
          "completionTokens": 976,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-07-14-757037_chatcmpl-3f8b7b02-8479-4ff6-9248-a23047f3b0b2",
          "traceId": "327287c0",
          "type": "GENERATION",
          "name": "3272_psg_gen_attempt#5",
          "startTime": "2026-02-06T22:07:14.757000+00:00",
          "endTime": "2026-02-06T22:07:36.863000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22106.0,
          "costDetails": {},
          "totalTokens": 1581,
          "environment": "default",
          "promptTokens": 754,
          "completionTokens": 827,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "ce443648-d951-4e44-8441-9cb9e57b8cfd",
          "traceId": "327287c0",
          "type": "SPAN",
          "name": "error_32_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-06T22:07:37.004000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: Traceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000736_psg_qwen2.5-coder:32b/tmp_20260207000736_psg_qwen2.5-coder:32b.py\", line 21, in <module>\n    labels = load_labels(label_path)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000736_psg_qwen2.5-coder:32b/tmp_20260207000736_psg_qwen2.5-coder:32b.py\", line 18, in load_labels\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in lines if not line.startswith('#')}\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000736_psg_qwen2.5-coder:32b/tmp_20260207000736_psg_qwen2.5-coder:32b.py\", line 18, in <dictcomp>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in lines if not line.startswith('#')}\nValueError: invalid literal for int() with base 10: 'person\\n'\n",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T22:05:43.120Z",
      "updatedAt": "2026-02-06T22:07:42.300Z",
      "environment": "default"
    },
    {
      "id": "3c75cc4f",
      "timestamp": "2026-02-06T22:04:27.057000+00:00",
      "name": "3c75_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the input video file\ncap = cv2.VideoCapture(input_path)\n\n# Prepare to write the output video file\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), 10, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing: Convert to grayscale and resize\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(frame_resized, axis=0).astype(np.uint8)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(np.int64)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes with labels and confidence scores on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = map(int, (boxes[i] * np.array([frame_height, frame_width, frame_height, frame_width])))\n            label = labels[classes[i]]\n            score = float(scores[i])\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 41.0,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-04-27-059308_chatcmpl-c7456b5b-87cd-4be0-a9ba-ffb947b08f85",
          "traceId": "3c75cc4f",
          "type": "GENERATION",
          "name": "3c75_psg_gen_attempt#1",
          "startTime": "2026-02-06T22:04:27.059000+00:00",
          "endTime": "2026-02-06T22:04:48.217000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21158.0,
          "costDetails": {},
          "totalTokens": 1233,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 797,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-04-48-460806_chatcmpl-39747eb4-80c7-4b38-9253-638c6aeaa638",
          "traceId": "3c75cc4f",
          "type": "GENERATION",
          "name": "3c75_psg_gen_attempt#2",
          "startTime": "2026-02-06T22:04:48.460000+00:00",
          "endTime": "2026-02-06T22:05:08.059000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19599.0,
          "costDetails": {},
          "totalTokens": 1415,
          "environment": "default",
          "promptTokens": 685,
          "completionTokens": 730,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:04:27.605Z",
      "updatedAt": "2026-02-06T22:05:22.241Z",
      "environment": "default"
    },
    {
      "id": "6c85a0fa",
      "timestamp": "2026-02-06T22:03:03.184000+00:00",
      "name": "6c85_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load the TFLite model and allocate tensors\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Read the input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Get video properties to save processed video\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    # Convert to grayscale if necessary, then resize and normalize\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(image, (input_shape[1], input_shape[2]))\n    image_normalized = np.array(image_resized / 255.0 * 255, dtype=np.uint8)\n    return image_normalized\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n            \n            # Convert normalized coordinates to frame coordinates\n            y_min, x_min, y_max, x_max = box\n            height, width, _ = frame.shape\n            y_min, x_min, y_max, x_max = int(y_min * height), int(x_min * width), int(y_max * height), int(x_max * width)\n            \n            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n            label_text = f\"{label}: {score:.2f}\"\n            cv2.putText(frame, label_text, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Process each frame in the video\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the image\n    processed_image = preprocess_image(frame)\n    \n    # Set input tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], np.expand_dims(processed_image, axis=0))\n    interpreter.invoke()\n    \n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    \n    # Draw detection boxes on the frame\n    draw_detection_boxes(frame, boxes[0], classes[0], scores[0])\n    \n    # Write the processed frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Video processing complete. Output saved to\", output_path)"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 51.16,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-03-03-186207_chatcmpl-d44ab17a-e59d-4843-b9de-5acd356e3367",
          "traceId": "6c85a0fa",
          "type": "GENERATION",
          "name": "6c85_psg_gen_attempt#1",
          "startTime": "2026-02-06T22:03:03.186000+00:00",
          "endTime": "2026-02-06T22:03:27.302000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24116.0,
          "costDetails": {},
          "totalTokens": 1351,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 915,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-03-27-543000_chatcmpl-0977b375-af29-490c-b6ff-73c8a37ba119",
          "traceId": "6c85a0fa",
          "type": "GENERATION",
          "name": "6c85_psg_gen_attempt#2",
          "startTime": "2026-02-06T22:03:27.543000+00:00",
          "endTime": "2026-02-06T22:03:54.346000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26803.0,
          "costDetails": {},
          "totalTokens": 1688,
          "environment": "default",
          "promptTokens": 685,
          "completionTokens": 1003,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:03:03.736Z",
      "updatedAt": "2026-02-06T22:04:05.848Z",
      "environment": "default"
    },
    {
      "id": "172a6edb",
      "timestamp": "2026-02-06T22:01:47.561000+00:00",
      "name": "172a_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {i: line.strip() for i, line in enumerate(f.readlines())}\n\n# Initialize the TFLite interpreter and allocate tensors\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Video writer setup for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(frame):\n    # Resize and convert to uint8\n    input_shape = input_details[0]['shape']\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    preprocessed_frame = np.expand_dims(resized_frame, axis=0)\n    preprocessed_frame = preprocessed_frame.astype(np.uint8)  # Convert to uint8\n    return preprocessed_frame\n\ndef postprocess_output(detection_boxes, detection_classes, detection_scores):\n    height, width, _ = frame.shape\n    results = []\n    for box, score, class_id in zip(detection_boxes[0], detection_scores[0], detection_classes[0]):\n        if score > confidence_threshold:\n            ymin, xmin, ymax, xmax = box\n            left, right, top, bottom = int(xmin * width), int(xmax * width), int(ymin * height), int(ymax * height)\n            results.append((left, top, right - left, bottom - top, labels[int(class_id)], score))\n    return results\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    preprocessed_frame = preprocess_image(frame)\n\n    # Set tensor to interpreter and run inference\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n    interpreter.invoke()\n\n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Postprocess the results\n    detections = postprocess_output(boxes, classes, scores)\n\n    # Draw detection boxes and labels on the frame\n    for (x, y, w, h, label, score) in detections:\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n        cv2.putText(frame, f'{label}: {score:.2f}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\n    # Write the frame with detections to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 43.361,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-01-47-562626_chatcmpl-bf9a373a-c3d4-4b41-b888-f335da9de24e",
          "traceId": "172a6edb",
          "type": "GENERATION",
          "name": "172a_psg_gen_attempt#1",
          "startTime": "2026-02-06T22:01:47.562000+00:00",
          "endTime": "2026-02-06T22:02:08.214000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20652.0,
          "costDetails": {},
          "totalTokens": 1216,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 780,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-02-08-718675_chatcmpl-64b69e7a-1493-4e88-86c7-faca6ade2613",
          "traceId": "172a6edb",
          "type": "GENERATION",
          "name": "172a_psg_gen_attempt#2",
          "startTime": "2026-02-06T22:02:08.718000+00:00",
          "endTime": "2026-02-06T22:02:30.923000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22205.0,
          "costDetails": {},
          "totalTokens": 1515,
          "environment": "default",
          "promptTokens": 686,
          "completionTokens": 829,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T22:01:48.147Z",
      "updatedAt": "2026-02-06T22:02:42.345Z",
      "environment": "default"
    },
    {
      "id": "cecfe249",
      "timestamp": "2026-02-06T21:59:22.862000+00:00",
      "name": "cecf_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(label_path):\n    with open(label_path, 'r') as f:\n        labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f if len(line.split(' ', 1)) > 1}\n    return labels\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\nlabels = load_labels(label_path)\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Get video properties to save output video with the same properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Function to preprocess image\ndef preprocess_image(frame):\n    input_data = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data.astype(np.float32) / 255.0\n\n# Function to draw bounding boxes and labels\ndef draw_boxes(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin = int(max(1, (box[0] * frame_height)))\n            xmin = int(max(1, (box[1] * frame_width)))\n            ymax = int(min(frame_height, (box[2] * frame_height)))\n            xmax = int(min(frame_width, (box[3] * frame_width)))\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n# Main loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    \n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    draw_boxes(frame, boxes, classes, scores)\n\n    out.write(frame)\n\ncap.release()\nout.release()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000120_psg_qwen2.5-coder:32b/tmp_20260207000120_psg_qwen2.5-coder:32b.py\", line 31, in <module>\n    labels = load_labels(label_path)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000120_psg_qwen2.5-coder:32b/tmp_20260207000120_psg_qwen2.5-coder:32b.py\", line 17, in load_labels\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f if len(line.split(' ', 1)) > 1}\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000120_psg_qwen2.5-coder:32b/tmp_20260207000120_psg_qwen2.5-coder:32b.py\", line 17, in <dictcomp>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f if len(line.split(' ', 1)) > 1}\nValueError: invalid literal for int() with base 10: 'traffic'\n"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 117.618,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-00-00-11-165431_chatcmpl-b9e287ed-0051-4b40-abcf-845be16303fd",
          "traceId": "cecfe249",
          "type": "GENERATION",
          "name": "cecf_psg_gen_attempt#3",
          "startTime": "2026-02-06T22:00:11.165000+00:00",
          "endTime": "2026-02-06T22:00:36.723000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25558.0,
          "costDetails": {},
          "totalTokens": 1588,
          "environment": "default",
          "promptTokens": 620,
          "completionTokens": 968,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-00-36-901553_chatcmpl-a0665c91-a9d1-4f3c-b20d-9e7113562c87",
          "traceId": "cecfe249",
          "type": "GENERATION",
          "name": "cecf_psg_gen_attempt#4",
          "startTime": "2026-02-06T22:00:36.901000+00:00",
          "endTime": "2026-02-06T22:00:58.170000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21269.0,
          "costDetails": {},
          "totalTokens": 1422,
          "environment": "default",
          "promptTokens": 615,
          "completionTokens": 807,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-00-00-58-343758_chatcmpl-234db095-b674-4adc-9409-f60295a5f0ed",
          "traceId": "cecfe249",
          "type": "GENERATION",
          "name": "cecf_psg_gen_attempt#5",
          "startTime": "2026-02-06T22:00:58.343000+00:00",
          "endTime": "2026-02-06T22:01:20.312000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21969.0,
          "costDetails": {},
          "totalTokens": 1569,
          "environment": "default",
          "promptTokens": 746,
          "completionTokens": 823,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-59-22-863527_chatcmpl-823e1ac2-9d18-4ede-8b70-0f2842dd66da",
          "traceId": "cecfe249",
          "type": "GENERATION",
          "name": "cecf_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:59:22.863000+00:00",
          "endTime": "2026-02-06T21:59:47.053000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24190.0,
          "costDetails": {},
          "totalTokens": 1349,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 913,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-59-47-233676_chatcmpl-d2ba34bf-984a-4894-8cbe-2ce1fdab013d",
          "traceId": "cecfe249",
          "type": "GENERATION",
          "name": "cecf_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:59:47.233000+00:00",
          "endTime": "2026-02-06T22:00:10.763000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23530.0,
          "costDetails": {},
          "totalTokens": 1564,
          "environment": "default",
          "promptTokens": 685,
          "completionTokens": 879,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "adf7b832-196b-4177-ad41-7d6124df4d35",
          "traceId": "cecfe249",
          "type": "SPAN",
          "name": "error_ce_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-06T22:01:20.481000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000120_psg_qwen2.5-coder:32b/tmp_20260207000120_psg_qwen2.5-coder:32b.py\", line 31, in <module>\n    labels = load_labels(label_path)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000120_psg_qwen2.5-coder:32b/tmp_20260207000120_psg_qwen2.5-coder:32b.py\", line 17, in load_labels\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f if len(line.split(' ', 1)) > 1}\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260207000120_psg_qwen2.5-coder:32b/tmp_20260207000120_psg_qwen2.5-coder:32b.py\", line 17, in <dictcomp>\n    labels = {int(line.split(' ', 1)[0]): line.split(' ', 1)[1].strip() for line in f if len(line.split(' ', 1)) > 1}\nValueError: invalid literal for int() with base 10: 'traffic'\n",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T21:59:23.411Z",
      "updatedAt": "2026-02-06T22:01:26.282Z",
      "environment": "default"
    },
    {
      "id": "182f27b9",
      "timestamp": "2026-02-06T21:58:07.368000+00:00",
      "name": "182f_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {i: line.strip() for i, line in enumerate(f)}\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\n\n# Prepare video writer for output\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Function to preprocess the frame\ndef preprocess_frame(frame):\n    input_shape = input_details[0]['shape']\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    normalized_frame = np.expand_dims(resized_frame.astype(np.uint8), axis=0)\n    return normalized_frame\n\n# Inference and output handling\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    preprocessed_frame = preprocess_frame(frame)\n    \n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n    \n    # Run inference\n    interpreter.invoke()\n    \n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index'])[0])\n    \n    # Draw detection boxes and labels\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\n# Release everything when job is finished\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 43.426,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-58-07-370692_chatcmpl-31089e11-bd8a-483f-9c10-06ad3dd8e652",
          "traceId": "182f27b9",
          "type": "GENERATION",
          "name": "182f_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:58:07.370000+00:00",
          "endTime": "2026-02-06T21:58:30.629000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23259.0,
          "costDetails": {},
          "totalTokens": 1316,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 880,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-58-30-816812_chatcmpl-cbb9bbb3-c383-4870-96cf-2a3aaee36254",
          "traceId": "182f27b9",
          "type": "GENERATION",
          "name": "182f_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:58:30.816000+00:00",
          "endTime": "2026-02-06T21:58:50.796000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19980.0,
          "costDetails": {},
          "totalTokens": 1432,
          "environment": "default",
          "promptTokens": 686,
          "completionTokens": 746,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:58:07.918Z",
      "updatedAt": "2026-02-06T21:59:02.109Z",
      "environment": "default"
    },
    {
      "id": "8190449d",
      "timestamp": "2026-02-06T21:56:45.953000+00:00",
      "name": "8190_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\n\n# Get video properties\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess_frame(frame):\n    # Resize frame to the required input size\n    input_shape = input_details[0]['shape']\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    \n    # Convert to uint8 if necessary\n    if input_details[0]['dtype'] == np.uint8:\n        mean = 127.5\n        std = 127.5\n        resized_frame = (resized_frame - mean) / std\n        resized_frame = resized_frame.astype(np.uint8)\n    \n    # Add a batch dimension\n    input_data = np.expand_dims(resized_frame, axis=0)\n\n    return input_data\n\ndef postprocess_output(output_details, frame):\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    # Draw bounding boxes and labels on the frame\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            label = labels[int(classes[i])]\n            \n            # Convert normalized coordinates to image coordinates\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)),\n                          (0, 255, 0), 2)\n            label_text = f'{label}: {int(scores[i] * 100)}%'\n            cv2.putText(frame, label_text, (int(left), int(top) - 8),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n    \n    return frame\n\n# Main loop to process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    # Preprocess the frame\n    preprocessed_frame = preprocess_frame(frame)\n    \n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n    interpreter.invoke()\n    \n    # Postprocess output and draw on the frame\n    processed_frame = postprocess_output(output_details, frame)\n    \n    # Write the processed frame to the output video file\n    out.write(processed_frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 49.007,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-56-45-955018_chatcmpl-abcce45e-007d-4865-baf1-88878cd2cabe",
          "traceId": "8190449d",
          "type": "GENERATION",
          "name": "8190_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:56:45.955000+00:00",
          "endTime": "2026-02-06T21:57:07.437000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21482.0,
          "costDetails": {},
          "totalTokens": 1249,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 813,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-57-07-679631_chatcmpl-672bda1d-168f-41b3-a691-a368720b14fc",
          "traceId": "8190449d",
          "type": "GENERATION",
          "name": "8190_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:57:07.679000+00:00",
          "endTime": "2026-02-06T21:57:34.962000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27283.0,
          "costDetails": {},
          "totalTokens": 1703,
          "environment": "default",
          "promptTokens": 686,
          "completionTokens": 1017,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:56:46.502Z",
      "updatedAt": "2026-02-06T21:57:46.229Z",
      "environment": "default"
    },
    {
      "id": "d17248e2",
      "timestamp": "2026-02-06T21:55:14.154000+00:00",
      "name": "d172_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter and allocate tensors\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\n\n# Check if video opened successfully\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Get video properties to save processed video with the same settings\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Function to preprocess the image\ndef preprocess_image(image):\n    # Convert image from BGR to RGB\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    # Resize image to expected dimensions\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image_rgb, (input_shape[1], input_shape[2]))\n    # Normalize pixel values and convert to uint8\n    normalized_img = np.array(resized_img / 255.0 * 255, dtype=np.uint8)\n    # Add a batch dimension\n    input_tensor = np.expand_dims(normalized_img, axis=0)\n    return input_tensor\n\n# Function to draw bounding boxes and labels on the image\ndef draw_detections(image, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            # Scale the coordinates to the original size of the frame\n            ymin = int(max(1, (box[0] * frame_height)))\n            xmin = int(max(1, (box[1] * frame_width)))\n            ymax = int(min(frame_height, (box[2] * frame_height)))\n            xmax = int(min(frame_width, (box[3] * frame_width)))\n\n            # Draw the bounding box\n            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n\n            # Get label and score text\n            label = labels[int(cls)]\n            score_text = f'{label}: {score:.2f}'\n\n            # Draw the label background\n            font = cv2.FONT_HERSHEY_SIMPLEX\n            font_scale = 0.5\n            thickness = 1\n            (text_width, text_height), _ = cv2.getTextSize(score_text, font, font_scale, thickness)\n            cv2.rectangle(image, (xmin, ymin - text_height - 4), (xmin + text_width, ymin), (0, 255, 0), -1)\n\n            # Draw the label\n            cv2.putText(image, score_text, (xmin, ymin - 3), font, font_scale, (0, 0, 0), thickness, cv2.LINE_AA)\n    return image\n\n# Process video frame by frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_tensor = preprocess_image(frame)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results from output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Draw detections on the frame\n    output_frame = draw_detections(frame, boxes, classes, scores)\n\n    # Write the processed frame to the output video file\n    out.write(output_frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 57.604,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-55-14-157648_chatcmpl-e57d5cb3-294c-420c-a856-2adb644c4c42",
          "traceId": "d17248e2",
          "type": "GENERATION",
          "name": "d172_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:55:14.157000+00:00",
          "endTime": "2026-02-06T21:55:39.018000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24861.0,
          "costDetails": {},
          "totalTokens": 1377,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 941,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-55-39-272217_chatcmpl-12f863bb-33eb-48a8-9514-4001b6899e1c",
          "traceId": "d17248e2",
          "type": "GENERATION",
          "name": "d172_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:55:39.272000+00:00",
          "endTime": "2026-02-06T21:56:11.761000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 32489.0,
          "costDetails": {},
          "totalTokens": 1900,
          "environment": "default",
          "promptTokens": 685,
          "completionTokens": 1215,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:55:14.706Z",
      "updatedAt": "2026-02-06T21:56:24.926Z",
      "environment": "default"
    },
    {
      "id": "2b960686",
      "timestamp": "2026-02-06T21:53:56.038000+00:00",
      "name": "2b96_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the input video\ncap = cv2.VideoCapture(input_path)\n\n# Check if camera opened successfully\nif not cap.isOpened():\n    print(\"Error: Could not open video.\")\n    exit()\n\n# Get video properties\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    input_shape = input_details[0]['shape']\n    resized_img = cv2.resize(image, (input_shape[1], input_shape[2]))\n    # Convert to uint8 as expected by the model\n    input_data = np.expand_dims(resized_img, axis=0)\n    return input_data\n\ndef draw_detections(frame, boxes, classes, scores):\n    for i in range(len(boxes)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            score = scores[i]\n\n            # Get coordinates\n            y_min, x_min, y_max, x_max = box\n\n            # Convert normalized coordinates to image coordinates\n            h, w, _ = frame.shape\n            top_left = (int(x_min * w), int(y_min * h))\n            bottom_right = (int(x_max * w), int(y_max * h))\n\n            # Draw rectangle and label\n            cv2.rectangle(frame, top_left, bottom_right, (0, 255, 0), 2)\n            label = f\"{labels[class_id]}: {score:.2f}\"\n            cv2.putText(frame, label, (top_left[0], top_left[1] - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Main loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    \n    # Set tensor to point to the preprocessed input data\n    interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n    \n    # Run inference\n    interpreter.invoke()\n    \n    # Extract output data\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    \n    # Draw detections on the frame\n    draw_detections(frame, boxes, classes, scores)\n    \n    # Write the frame with detection to the output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Video processing completed and saved to\", output_path)"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 44.828,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-53-56-039574_chatcmpl-5d448719-84f1-4c6c-b668-1e3dde389dc0",
          "traceId": "2b960686",
          "type": "GENERATION",
          "name": "2b96_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:53:56.039000+00:00",
          "endTime": "2026-02-06T21:54:14.811000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18772.0,
          "costDetails": {},
          "totalTokens": 1146,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 710,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-54-14-980189_chatcmpl-a4dfa22f-874c-4966-9276-7ddfa0f20cea",
          "traceId": "2b960686",
          "type": "GENERATION",
          "name": "2b96_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:54:14.980000+00:00",
          "endTime": "2026-02-06T21:54:40.867000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25887.0,
          "costDetails": {},
          "totalTokens": 1657,
          "environment": "default",
          "promptTokens": 690,
          "completionTokens": 967,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:53:56.586Z",
      "updatedAt": "2026-02-06T21:54:52.280Z",
      "environment": "default"
    },
    {
      "id": "f9e5ea30",
      "timestamp": "2026-02-06T21:52:34.432000+00:00",
      "name": "f9e5_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = {i: line.strip() for i, line in enumerate(f.readlines())}\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Error opening video stream or file\")\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, 20.0, (frame_width, frame_height))\n\ndef preprocess_frame(frame):\n    input_shape = input_details[0]['shape']\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_frame, axis=0)\n    # Convert to UINT8\n    input_data = np.uint8(input_data)\n    return input_data\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_frame(frame)\n    \n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n    \n    for i in range(num_detections):\n        if scores[0][i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            label = labels[int(classes[0][i])]\n            \n            # Convert normalized coordinates to frame dimensions\n            (imH, imW) = frame.shape[:2]\n            (xminn, xmaxx, yminn, ymaxx) = (xmin * imW, xmax * imW, ymin * imH, ymax * imH)\n            \n            cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (10, 255, 0), 2)\n            label_text = f\"{label}: {scores[0][i]:.2f}\"\n            cv2.putText(frame, label_text, (int(xminn), int(yminn) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (10, 255, 0), 2)\n    \n    out.write(frame)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 50.008,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-52-34-433971_chatcmpl-9342447b-d6dd-44fe-8812-a9265af9df23",
          "traceId": "f9e5ea30",
          "type": "GENERATION",
          "name": "f9e5_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:52:34.433000+00:00",
          "endTime": "2026-02-06T21:53:01.431000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26998.0,
          "costDetails": {},
          "totalTokens": 1473,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 1037,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-53-01-643760_chatcmpl-5085f933-6f41-4a80-9195-82892cbb59b2",
          "traceId": "f9e5ea30",
          "type": "GENERATION",
          "name": "f9e5_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:53:01.643000+00:00",
          "endTime": "2026-02-06T21:53:24.441000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22798.0,
          "costDetails": {},
          "totalTokens": 1536,
          "environment": "default",
          "promptTokens": 685,
          "completionTokens": 851,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:52:34.980Z",
      "updatedAt": "2026-02-06T21:53:35.686Z",
      "environment": "default"
    },
    {
      "id": "d32900ab",
      "timestamp": "2026-02-06T21:51:37.148000+00:00",
      "name": "d329_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise Exception(\"Error opening video file\")\n\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(image):\n    image_resized = cv2.resize(image, (300, 300))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n    return input_data\n\n# Function to compute mAP\ndef compute_mAP(true_boxes, pred_boxes):\n    # Placeholder for mAP calculation logic\n    # In a real scenario, this would be more complex and require ground truth data\n    return 0.0\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    \n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{label}: {scores[i]:.2f}\", (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame into the file 'output.avi'\n    out.write(frame)\n\ncap.release()\nout.release()\n\n# Placeholder for computing mAP\ntrue_boxes = []  # This should be populated with ground truth data in real scenario\npred_boxes = []  # This should be populated with predicted boxes from detection\n\nmAP = compute_mAP(true_boxes, pred_boxes)\nprint(f\"Computed mAP: {mAP:.4f}\")"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 23.573,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-51-37-150472_chatcmpl-c3e33df0-f6e9-4f90-a799-e1e99cc5fc93",
          "traceId": "d32900ab",
          "type": "GENERATION",
          "name": "d329_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:51:37.150000+00:00",
          "endTime": "2026-02-06T21:52:00.723000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23573.0,
          "costDetails": {},
          "totalTokens": 1327,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 891,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:51:37.695Z",
      "updatedAt": "2026-02-06T21:52:13.375Z",
      "environment": "default"
    },
    {
      "id": "0cf5b584",
      "timestamp": "2026-02-06T21:49:30.924000+00:00",
      "name": "0cf5_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare video writer\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Process each frame\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n\n    # Convert to UINT8 if required\n    scale, zero_point = input_details[0]['quantization']\n    if scale != 0:\n        input_data = (input_data / scale + zero_point).astype(np.uint8)\n\n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    # Draw detection results on the frame\n    for i in range(num_detections):\n        if scores[0][i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[0][i]\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n            score = float(scores[0][i])\n\n            # Scale the bounding box coordinates\n            height, width, _ = frame.shape\n            xmin, xmax, ymin, ymax = int(xmin * width), int(xmax * width), int(ymin * height), int(ymax * height)\n\n            # Draw rectangle and label text\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 63.47,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-49-30-926838_chatcmpl-9139651b-a464-4f52-8b55-32915eb37413",
          "traceId": "0cf5b584",
          "type": "GENERATION",
          "name": "0cf5_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:49:30.926000+00:00",
          "endTime": "2026-02-06T21:49:52.326000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21400.0,
          "costDetails": {},
          "totalTokens": 1239,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 803,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-49-52-534877_chatcmpl-37e36a83-3df1-4a95-b335-8c7e5067f50a",
          "traceId": "0cf5b584",
          "type": "GENERATION",
          "name": "0cf5_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:49:52.534000+00:00",
          "endTime": "2026-02-06T21:50:12.705000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20171.0,
          "costDetails": {},
          "totalTokens": 1370,
          "environment": "default",
          "promptTokens": 615,
          "completionTokens": 755,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-50-12-987533_chatcmpl-2812c74c-b54e-42de-b42b-e35ef41f2327",
          "traceId": "0cf5b584",
          "type": "GENERATION",
          "name": "0cf5_psg_gen_attempt#3",
          "startTime": "2026-02-06T21:50:12.987000+00:00",
          "endTime": "2026-02-06T21:50:34.396000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21409.0,
          "costDetails": {},
          "totalTokens": 1495,
          "environment": "default",
          "promptTokens": 689,
          "completionTokens": 806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:49:31.472Z",
      "updatedAt": "2026-02-06T21:51:15.969Z",
      "environment": "default"
    },
    {
      "id": "fe69f2fe",
      "timestamp": "2026-02-06T21:47:03.708000+00:00",
      "name": "fe69_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(label_path):\n    with open(label_path, 'r', encoding='utf-8') as f:\n        lines = f.readlines()\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(' ', maxsplit=1)[1] for line in lines if ' ' in line}\n\ndef load_video(input_path):\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise IOError(f\"Cannot open video file {input_path}\")\n    return cap\n\ndef save_video(output_path, width, height, fps, frames):\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    for frame in frames:\n        out.write(frame)\n    out.release()\n\ndef preprocess_image(image, input_details):\n    h, w, _ = input_details[0]['shape']\n    image_resized = cv2.resize(image, (w, h))\n    image_np = np.array(image_resized).astype(np.float32)\n    image_np = (image_np - 127.5) / 127.5\n    return image_np\n\ndef main():\n    # Load TFLite model and allocate tensors.\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Get input and output details.\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Load labels.\n    labels = load_labels(label_path)\n\n    # Read the video file.\n    cap = load_video(input_path)\n    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps    = cap.get(cv2.CAP_PROP_FPS)\n\n    frames = []\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess the image.\n        input_data = preprocess_image(frame, input_details)\n        input_data = np.expand_dims(input_data, axis=0)  # Add batch dimension.\n\n        # Run inference.\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        interpreter.invoke()\n\n        # Extract results.\n        boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n        classes = interpreter.get_tensor(output_details[1]['index'])[0]\n        scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n        # Draw bounding boxes and labels on the frame.\n        for i in range(len(scores)):\n            if scores[i] > confidence_threshold:\n                ymin, xmin, ymax, xmax = boxes[i]\n                (left, right, top, bottom) = (xmin * width, xmax * width,\n                                              ymin * height, ymax * height)\n                p1 = (int(left), int(top))\n                p2 = (int(right), int(bottom))\n\n                cv2.rectangle(frame, p1, p2, (0, 255, 0), 2)\n\n                label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n                cv2.putText(frame, label, (p1[0], p1[1] - 10),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\n        frames.append(frame)\n\n    cap.release()\n\n    # Save the processed video.\n    save_video(output_path, width, height, fps, frames)\n\nif __name__ == \"__main__\":\n    main()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206234903_psg_qwen2.5-coder:32b/tmp_20260206234903_psg_qwen2.5-coder:32b.py\", line 100, in <module>\n    main()\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206234903_psg_qwen2.5-coder:32b/tmp_20260206234903_psg_qwen2.5-coder:32b.py\", line 49, in main\n    labels = load_labels(label_path)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206234903_psg_qwen2.5-coder:32b/tmp_20260206234903_psg_qwen2.5-coder:32b.py\", line 17, in load_labels\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(' ', maxsplit=1)[1] for line in lines if ' ' in line}\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206234903_psg_qwen2.5-coder:32b/tmp_20260206234903_psg_qwen2.5-coder:32b.py\", line 17, in <dictcomp>\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(' ', maxsplit=1)[1] for line in lines if ' ' in line}\nValueError: invalid literal for int() with base 10: 'traffic'\n"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 120.331,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-47-03-710295_chatcmpl-ce3d8b95-f4f4-4e2a-8b17-e0eabe9ed4c9",
          "traceId": "fe69f2fe",
          "type": "GENERATION",
          "name": "fe69_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:47:03.710000+00:00",
          "endTime": "2026-02-06T21:47:24.329000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20619.0,
          "costDetails": {},
          "totalTokens": 1214,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 778,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-47-24-517122_chatcmpl-cf48f4a8-d133-4ce5-bc51-3d03f81e5c99",
          "traceId": "fe69f2fe",
          "type": "GENERATION",
          "name": "fe69_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:47:24.517000+00:00",
          "endTime": "2026-02-06T21:47:48.302000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23785.0,
          "costDetails": {},
          "totalTokens": 1721,
          "environment": "default",
          "promptTokens": 843,
          "completionTokens": 878,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-47-48-460765_chatcmpl-9f0f74a8-d514-4060-9f6b-f4722cdba477",
          "traceId": "fe69f2fe",
          "type": "GENERATION",
          "name": "fe69_psg_gen_attempt#3",
          "startTime": "2026-02-06T21:47:48.460000+00:00",
          "endTime": "2026-02-06T21:48:12.394000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23934.0,
          "costDetails": {},
          "totalTokens": 1740,
          "environment": "default",
          "promptTokens": 851,
          "completionTokens": 889,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-48-12-635507_chatcmpl-17a9405a-6add-44b3-83f6-faf46e489dd8",
          "traceId": "fe69f2fe",
          "type": "GENERATION",
          "name": "fe69_psg_gen_attempt#4",
          "startTime": "2026-02-06T21:48:12.635000+00:00",
          "endTime": "2026-02-06T21:48:38.553000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25918.0,
          "costDetails": {},
          "totalTokens": 1813,
          "environment": "default",
          "promptTokens": 849,
          "completionTokens": 964,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-48-38-850772_chatcmpl-5d9ad17f-000c-4236-a9ea-16b9cd50598d",
          "traceId": "fe69f2fe",
          "type": "GENERATION",
          "name": "fe69_psg_gen_attempt#5",
          "startTime": "2026-02-06T21:48:38.850000+00:00",
          "endTime": "2026-02-06T21:49:03.863000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25013.0,
          "costDetails": {},
          "totalTokens": 1798,
          "environment": "default",
          "promptTokens": 870,
          "completionTokens": 928,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "7f021888-e7fa-4750-8796-9235baeedddf",
          "traceId": "fe69f2fe",
          "type": "SPAN",
          "name": "error_fe_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-06T21:49:04.041000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206234903_psg_qwen2.5-coder:32b/tmp_20260206234903_psg_qwen2.5-coder:32b.py\", line 100, in <module>\n    main()\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206234903_psg_qwen2.5-coder:32b/tmp_20260206234903_psg_qwen2.5-coder:32b.py\", line 49, in main\n    labels = load_labels(label_path)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206234903_psg_qwen2.5-coder:32b/tmp_20260206234903_psg_qwen2.5-coder:32b.py\", line 17, in load_labels\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(' ', maxsplit=1)[1] for line in lines if ' ' in line}\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206234903_psg_qwen2.5-coder:32b/tmp_20260206234903_psg_qwen2.5-coder:32b.py\", line 17, in <dictcomp>\n    return {int(line.split(' ', maxsplit=1)[0]): line.strip().split(' ', maxsplit=1)[1] for line in lines if ' ' in line}\nValueError: invalid literal for int() with base 10: 'traffic'\n",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T21:47:04.255Z",
      "updatedAt": "2026-02-06T21:49:09.493Z",
      "environment": "default"
    },
    {
      "id": "7dd7b09d",
      "timestamp": "2026-02-06T21:44:47.835000+00:00",
      "name": "7dd7_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Cannot open video file {input_path}\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Process each frame\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the input data\n    input_shape = input_details[0]['shape']\n    input_data = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the input is UINT8\n\n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes and labels\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = map(int, (boxes[i] * np.array([frame_height, frame_width, frame_height, frame_width])))\n            label = labels[int(classes[i])]\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{label}: {scores[i]:.2f}\", (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 70.03,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-44-47-836046_chatcmpl-82ad3466-25b1-49d4-bb6d-e5991040a42a",
          "traceId": "7dd7b09d",
          "type": "GENERATION",
          "name": "7dd7_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:44:47.836000+00:00",
          "endTime": "2026-02-06T21:45:11.507000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23671.0,
          "costDetails": {},
          "totalTokens": 1326,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 890,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-45-11-777499_chatcmpl-42c6087b-ea83-4885-96c4-d5eeb88bb42c",
          "traceId": "7dd7b09d",
          "type": "GENERATION",
          "name": "7dd7_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:45:11.777000+00:00",
          "endTime": "2026-02-06T21:45:35.158000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23381.0,
          "costDetails": {},
          "totalTokens": 1596,
          "environment": "default",
          "promptTokens": 725,
          "completionTokens": 871,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-45-35-343216_chatcmpl-1887da5c-5ce1-426f-8008-8fa5d7a4a609",
          "traceId": "7dd7b09d",
          "type": "GENERATION",
          "name": "7dd7_psg_gen_attempt#3",
          "startTime": "2026-02-06T21:45:35.343000+00:00",
          "endTime": "2026-02-06T21:45:57.866000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22523.0,
          "costDetails": {},
          "totalTokens": 1533,
          "environment": "default",
          "promptTokens": 684,
          "completionTokens": 849,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:44:48.380Z",
      "updatedAt": "2026-02-06T21:46:42.131Z",
      "environment": "default"
    },
    {
      "id": "4b0778d9",
      "timestamp": "2026-02-06T21:43:30.294000+00:00",
      "name": "4b07_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Preprocess and infer\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing the input data\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0)\n    \n    # Normalize the image to UINT8 if required\n    if input_details[0]['dtype'] == np.uint8:\n        scale, zero_point = input_details[0]['quantization']\n        input_data = (input_data / scale + zero_point).astype(np.uint8)\n\n    # Set the tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n    # Draw detection boxes with labels\n    for i in range(num_detections):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            \n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n    # Write the frame with detections to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 43.448,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-43-30-295826_chatcmpl-3425483e-d033-4bf6-84da-2eadeb7c6d8c",
          "traceId": "4b0778d9",
          "type": "GENERATION",
          "name": "4b07_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:43:30.295000+00:00",
          "endTime": "2026-02-06T21:43:51.423000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21128.0,
          "costDetails": {},
          "totalTokens": 1233,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 797,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-43-51-648319_chatcmpl-db750866-424c-410c-9d21-f75610c30d06",
          "traceId": "4b0778d9",
          "type": "GENERATION",
          "name": "4b07_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:43:51.648000+00:00",
          "endTime": "2026-02-06T21:44:13.743000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22095.0,
          "costDetails": {},
          "totalTokens": 1510,
          "environment": "default",
          "promptTokens": 685,
          "completionTokens": 825,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:43:30.837Z",
      "updatedAt": "2026-02-06T21:44:27.021Z",
      "environment": "default"
    },
    {
      "id": "bd52f96d",
      "timestamp": "2026-02-06T21:41:40.513000+00:00",
      "name": "bd52_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(frame):\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(image_resized, axis=0)\n    return input_data\n\n# Function to draw bounding boxes and labels\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            score = scores[i]\n\n            # Scale the bounding box coordinates\n            h, w, _ = frame.shape\n            ymin = int(max(1, (box[0] * h)))\n            xmin = int(max(1, (box[1] * w)))\n            ymax = int(min(h, (box[2] * h)))\n            xmax = int(min(w, (box[3] * w)))\n\n            # Draw the bounding box\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n\n            # Draw label and score\n            label = labels[class_id]\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n# Main loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes and labels on the frame\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write the frame with detections to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Video processing complete. Output saved to\", output_path)"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 50.489,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-41-40-514548_chatcmpl-8289e912-75cf-4bc1-ab37-ae4b19337c31",
          "traceId": "bd52f96d",
          "type": "GENERATION",
          "name": "bd52_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:41:40.514000+00:00",
          "endTime": "2026-02-06T21:42:07.926000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27412.0,
          "costDetails": {},
          "totalTokens": 1477,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 1041,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-42-08-319436_chatcmpl-b6a0d1f1-712c-4bc2-9473-19bae9f4ebec",
          "traceId": "bd52f96d",
          "type": "GENERATION",
          "name": "bd52_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:42:08.319000+00:00",
          "endTime": "2026-02-06T21:42:31.003000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22684.0,
          "costDetails": {},
          "totalTokens": 1458,
          "environment": "default",
          "promptTokens": 603,
          "completionTokens": 855,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:41:41.055Z",
      "updatedAt": "2026-02-06T21:43:09.226Z",
      "environment": "default"
    },
    {
      "id": "95546568",
      "timestamp": "2026-02-06T21:40:22.745000+00:00",
      "name": "9554_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter and allocate tensors\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Could not open video file {input_path}\")\n\n# Get video properties to initialize the video writer\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Initialize video writer for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing\n    input_data = cv2.resize(frame, (300, 300))\n    input_data = np.expand_dims(input_data, axis=0)\n    input_data = input_data.astype(np.uint8)  # Ensure the data type is uint8\n\n    # Set tensor and run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Draw detection boxes with labels\n    for i in range(len(scores[0])):\n        if scores[0][i] > confidence_threshold:\n            box = boxes[0][i]\n            class_id = int(classes[0][i])\n            label = labels[class_id]\n            score = float(scores[0][i])\n\n            # Scale the bounding box coordinates\n            ymin, xmin, ymax, xmax = box\n            h, w, _ = frame.shape\n            (left, right, top, bottom) = (xmin * w, xmax * w, ymin * h, ymax * h)\n\n            # Draw rectangle and label\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{label}: {score:.2f}\", (int(left), int(top) - 10), \n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection results to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 45.342,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-40-22-746483_chatcmpl-721002b9-9bfe-4996-aedc-4f70e7384795",
          "traceId": "95546568",
          "type": "GENERATION",
          "name": "9554_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:40:22.746000+00:00",
          "endTime": "2026-02-06T21:40:47.144000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24398.0,
          "costDetails": {},
          "totalTokens": 1354,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 918,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-40-47-407446_chatcmpl-b9b72b14-f418-44ce-9d34-ad4b5ef762f9",
          "traceId": "95546568",
          "type": "GENERATION",
          "name": "9554_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:40:47.407000+00:00",
          "endTime": "2026-02-06T21:41:08.088000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20681.0,
          "costDetails": {},
          "totalTokens": 1458,
          "environment": "default",
          "promptTokens": 685,
          "completionTokens": 773,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:40:23.286Z",
      "updatedAt": "2026-02-06T21:41:19.415Z",
      "environment": "default"
    },
    {
      "id": "df904940",
      "timestamp": "2026-02-06T21:38:32.104000+00:00",
      "name": "df90_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Could not open video file: {input_path}\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess_image(frame):\n    input_shape = input_details[0]['shape']\n    input_data = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef postprocess_detections(output_data, frame):\n    boxes = output_data['detection_boxes'][0]\n    classes = output_data['detection_classes'][0].astype(int)\n    scores = output_data['detection_scores'][0]\n\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n            label = labels[classes[i]]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{label}: {scores[i]:.2f}\", (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    return frame\n\n# Main loop for processing video frames\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    input_data = preprocess_image(frame)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n\n    start_time = time.time()\n    interpreter.invoke()\n    end_time = time.time()\n\n    # Get the results\n    output_data = {\n        'detection_boxes': interpreter.get_tensor(output_details[0]['index']),\n        'detection_classes': interpreter.get_tensor(output_details[1]['index']),\n        'detection_scores': interpreter.get_tensor(output_details[2]['index'])\n    }\n\n    frame_with_detections = postprocess_detections(output_data, frame)\n    out.write(frame_with_detections)\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 47.074,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-38-32-106397_chatcmpl-3a10eb01-dbc2-4777-a661-d1dae5e37131",
          "traceId": "df904940",
          "type": "GENERATION",
          "name": "df90_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:38:32.106000+00:00",
          "endTime": "2026-02-06T21:38:53.805000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21699.0,
          "costDetails": {},
          "totalTokens": 1259,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 823,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-38-54-029970_chatcmpl-a9909fea-d865-4e36-afb5-5229dac65569",
          "traceId": "df904940",
          "type": "GENERATION",
          "name": "df90_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:38:54.029000+00:00",
          "endTime": "2026-02-06T21:39:19.180000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25151.0,
          "costDetails": {},
          "totalTokens": 1652,
          "environment": "default",
          "promptTokens": 712,
          "completionTokens": 940,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:38:32.641Z",
      "updatedAt": "2026-02-06T21:40:02.328Z",
      "environment": "default"
    },
    {
      "id": "eed6d572",
      "timestamp": "2026-02-06T21:36:37.113000+00:00",
      "name": "eed6_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Load TFLite model and allocate tensors.\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Video writer setup\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess_image(frame):\n    input_shape = input_details[0]['shape']\n    frame_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n    frame_expanded = np.expand_dims(frame_rgb, axis=0)\n    return frame_expanded\n\ndef draw_detection_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin = int(max(1, (boxes[i][0] * height)))\n            xmin = int(max(1, (boxes[i][1] * width)))\n            ymax = int(min(height, (boxes[i][2] * height)))\n            xmax = int(min(width, (boxes[i][3] * width)))\n            \n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (10, 255, 0), 2)\n            label = '{}: {:.2f}'.format(labels[int(classes[i])], scores[i])\n            labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n            label_ymin = max(ymin, labelSize[1] + 10)\n            cv2.rectangle(frame, (xmin, label_ymin - labelSize[1] - 10), (xmin + labelSize[0], label_ymin + baseLine - 10), (255, 255, 255), cv2.FILLED)\n            cv2.putText(frame, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2)\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    input_data = preprocess_image(frame)\n    \n    # Perform inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes\n    draw_detection_boxes(frame, boxes, classes, scores)\n\n    # Write the frame with the detections to the output video file\n    out.write(frame)\n\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 51.382,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-36-37-115059_chatcmpl-f61cce84-721d-4bc3-ac46-2b701aa75d36",
          "traceId": "eed6d572",
          "type": "GENERATION",
          "name": "eed6_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:36:37.115000+00:00",
          "endTime": "2026-02-06T21:36:59.020000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21905.0,
          "costDetails": {},
          "totalTokens": 1281,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 845,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-36-59-315434_chatcmpl-91d9cf15-b187-4c1e-8840-830cddc102a8",
          "traceId": "eed6d572",
          "type": "GENERATION",
          "name": "eed6_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:36:59.315000+00:00",
          "endTime": "2026-02-06T21:37:28.497000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29182.0,
          "costDetails": {},
          "totalTokens": 1711,
          "environment": "default",
          "promptTokens": 615,
          "completionTokens": 1096,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:36:37.660Z",
      "updatedAt": "2026-02-06T21:38:10.915Z",
      "environment": "default"
    },
    {
      "id": "0544f3e0",
      "timestamp": "2026-02-06T21:35:38.359000+00:00",
      "name": "0544_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\ndef load_labels(filename):\n    with open(filename, 'r') as f:\n        return {i: line.strip() for i, line in enumerate(f.readlines())}\n\nlabels = load_labels(label_path)\n\n# Initialize TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from the given path\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Prepare to write the processed video\nfps = int(cap.get(cv2.CAP_PROP_FPS))\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\ndef preprocess_image(frame):\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    frame_resized = cv2.resize(frame_rgb, (300, 300))\n    input_data = np.expand_dims(frame_resized, axis=0).astype(np.uint8)\n    return input_data\n\ndef postprocess_detection(boxes, classes, scores):\n    detections = []\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels.get(class_id, 'Unknown')\n            detections.append((box, label, scores[i]))\n    return detections\n\n# Main loop\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    \n    input_data = preprocess_image(frame)\n    \n    # Perform inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    \n    # Extract detection results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    \n    detections = postprocess_detection(boxes, classes, scores)\n    \n    # Draw detection results on the frame\n    for (box, label, score) in detections:\n        ymin, xmin, ymax, xmax = box\n        ymin, xmin, ymax, xmax = int(ymin * height), int(xmin * width), int(ymax * height), int(xmax * width)\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n        label_text = f\"{label}: {score:.2f}\"\n        cv2.putText(frame, label_text, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n    \n    # Write the processed frame to output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(f\"Processed video saved to {output_path}\")"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 23.447,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-35-38-360586_chatcmpl-7d06994b-cfce-4016-9ace-dee7a466c901",
          "traceId": "0544f3e0",
          "type": "GENERATION",
          "name": "0544_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:35:38.360000+00:00",
          "endTime": "2026-02-06T21:36:01.807000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23447.0,
          "costDetails": {},
          "totalTokens": 1326,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 890,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:35:38.896Z",
      "updatedAt": "2026-02-06T21:36:16.083Z",
      "environment": "default"
    },
    {
      "id": "3f45c344",
      "timestamp": "2026-02-06T21:33:07.653000+00:00",
      "name": "3f45_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\n\n# Configuration parameters\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Ensure the input tensor is of type UINT8\nif input_details[0]['dtype'] != np.uint8:\n    raise ValueError('Input tensor must be of type UINT8')\n\nheight, width = input_details[0]['shape'][1:3]\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Get video properties to save output video with the same settings\nframe_width = int(cap.get(3))\nframe_height = int(cap.get(4))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame to fit the model's input requirements\n    resized_frame = cv2.resize(frame, (width, height))\n    preprocessed_frame = np.expand_dims(resized_frame, axis=0).astype(np.uint8)\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes and labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            class_id = int(classes[i])\n            label = labels[class_id]\n            box = boxes[i]\n            ymin, xmin, ymax, xmax = box\n\n            # Scale the bounding box to the original image size\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw rectangle and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f\"{label}: {scores[i]:.2f}\", (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n    # Write the frame with detection to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Video processing complete and saved to\", output_path)"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 91.965,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-33-07-655423_chatcmpl-eb25ec5a-95c9-4a8a-bd1a-f8dd0167c6a4",
          "traceId": "3f45c344",
          "type": "GENERATION",
          "name": "3f45_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:33:07.655000+00:00",
          "endTime": "2026-02-06T21:33:32.821000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25166.0,
          "costDetails": {},
          "totalTokens": 1390,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 954,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-33-33-092044_chatcmpl-1cb0f38c-643d-49c4-8902-b40980f05b77",
          "traceId": "3f45c344",
          "type": "GENERATION",
          "name": "3f45_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:33:33.092000+00:00",
          "endTime": "2026-02-06T21:33:54.736000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21644.0,
          "costDetails": {},
          "totalTokens": 1568,
          "environment": "default",
          "promptTokens": 763,
          "completionTokens": 805,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-33-54-979567_chatcmpl-86f82cb1-6538-4886-8e4f-a059584a5bde",
          "traceId": "3f45c344",
          "type": "GENERATION",
          "name": "3f45_psg_gen_attempt#3",
          "startTime": "2026-02-06T21:33:54.979000+00:00",
          "endTime": "2026-02-06T21:34:18.110000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23131.0,
          "costDetails": {},
          "totalTokens": 1477,
          "environment": "default",
          "promptTokens": 603,
          "completionTokens": 874,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-34-18-279391_chatcmpl-464f28e5-0cea-4ede-83c4-09336961fa2c",
          "traceId": "3f45c344",
          "type": "GENERATION",
          "name": "3f45_psg_gen_attempt#4",
          "startTime": "2026-02-06T21:34:18.279000+00:00",
          "endTime": "2026-02-06T21:34:39.620000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21341.0,
          "costDetails": {},
          "totalTokens": 1488,
          "environment": "default",
          "promptTokens": 689,
          "completionTokens": 799,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:33:08.184Z",
      "updatedAt": "2026-02-06T21:35:18.168Z",
      "environment": "default"
    },
    {
      "id": "f3b45764",
      "timestamp": "2026-02-06T21:31:48.528000+00:00",
      "name": "f3b4_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter and allocate tensors\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video file\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Function to preprocess the image\ndef preprocess_image(frame):\n    input_shape = input_details[0]['shape']\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    # Convert to UINT8 as required by the model\n    preprocessed_frame = resized_frame.astype(np.uint8)\n    return np.expand_dims(preprocessed_frame, axis=0)\n\n# Function to draw bounding boxes and labels\ndef draw_detections(frame, boxes, classes, scores):\n    for box, cls, score in zip(boxes[0], classes[0], scores[0]):\n        if score > confidence_threshold:\n            ymin = int(max(1, (box[0] * frame_height)))\n            xmin = int(max(1, (box[1] * frame_width)))\n            ymax = int(min(frame_height, (box[2] * frame_height)))\n            xmax = int(min(frame_width, (box[3] * frame_width)))\n\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            label = f\"{labels[int(cls)]}: {score:.2f}\"\n            cv2.putText(frame, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\n# Main loop\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    preprocessed_frame = preprocess_image(frame)\n    \n    # Set input tensor\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n    \n    # Run inference\n    interpreter.invoke()\n    \n    # Get output tensors\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    \n    # Draw detections on the frame\n    draw_detections(frame, boxes, classes, scores)\n    \n    # Write the processed frame to output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Video processing completed and saved as\", output_path)"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 43.34,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-31-48-530486_chatcmpl-ff2d7adc-f64d-40d5-8198-c600a4f6f214",
          "traceId": "f3b45764",
          "type": "GENERATION",
          "name": "f3b4_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:31:48.530000+00:00",
          "endTime": "2026-02-06T21:32:09.860000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21330.0,
          "costDetails": {},
          "totalTokens": 1242,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 806,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-32-10-113126_chatcmpl-b73c18d4-f90f-47bc-ac5a-5e5a67d9c2d4",
          "traceId": "f3b45764",
          "type": "GENERATION",
          "name": "f3b4_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:32:10.113000+00:00",
          "endTime": "2026-02-06T21:32:31.870000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21757.0,
          "costDetails": {},
          "totalTokens": 1504,
          "environment": "default",
          "promptTokens": 691,
          "completionTokens": 813,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:31:49.054Z",
      "updatedAt": "2026-02-06T21:32:46.286Z",
      "environment": "default"
    },
    {
      "id": "22bde7d7",
      "timestamp": "2026-02-06T21:29:21.577000+00:00",
      "name": "22bd_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\ndef load_labels(label_path):\n    with open(label_path, 'r') as f:\n        labels = {int(line.split(' ', 1)[0]): line.strip().split(' ', 1)[1] for line in f if len(line) > 1}\n    return labels\n\ndef preprocess_image(image, input_size):\n    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image_resized = cv2.resize(image_rgb, (input_size, input_size))\n    input_data = np.expand_dims(image_resized, axis=0)\n    return input_data.astype(np.float32) / 255.0\n\ndef main():\n    # Load TFLite model and allocate tensors.\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Get input and output details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Read the video file from the given path\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise IOError(\"Cannot open video\")\n\n    frame_width = int(cap.get(3))\n    frame_height = int(cap.get(4))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n\n    # Define the codec and create VideoWriter object\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        input_data = preprocess_image(frame, input_details[0]['shape'][1])\n\n        # Set the tensor to point to the input data to be inferred\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        # Run inference\n        interpreter.invoke()\n\n        # Extract results\n        boxes = interpreter.get_tensor(output_details[0]['index'])\n        classes = interpreter.get_tensor(output_details[1]['index'])\n        scores = interpreter.get_tensor(output_details[2]['index'])\n        num_detections = int(interpreter.get_tensor(output_details[3]['index']))\n\n        for i in range(num_detections):\n            if scores[0][i] > confidence_threshold:\n                ymin, xmin, ymax, xmax = boxes[0][i]\n                label = labels[int(classes[0][i])]\n                score = scores[0][i]\n\n                # Convert normalized coordinates to absolute coordinates\n                height, width, _ = frame.shape\n                (xminn, xmaxx, yminn, ymaxx) = (xmin * width, xmax * width, ymin * height, ymax * height)\n\n                cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (0, 255, 0), 2)\n                label_text = f\"{label}: {score:.2f}\"\n                cv2.putText(frame, label_text, (int(xminn), int(yminn) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n\n        # Write the frame with rectangles and labels to the output video\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\nif __name__ == \"__main__\":\n    main()",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206233121_psg_qwen2.5-coder:32b/tmp_20260206233121_psg_qwen2.5-coder:32b.py\", line 90, in <module>\n    main()\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206233121_psg_qwen2.5-coder:32b/tmp_20260206233121_psg_qwen2.5-coder:32b.py\", line 35, in main\n    labels = load_labels(label_path)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206233121_psg_qwen2.5-coder:32b/tmp_20260206233121_psg_qwen2.5-coder:32b.py\", line 16, in load_labels\n    labels = {int(line.split(' ', 1)[0]): line.strip().split(' ', 1)[1] for line in f if len(line) > 1}\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206233121_psg_qwen2.5-coder:32b/tmp_20260206233121_psg_qwen2.5-coder:32b.py\", line 16, in <dictcomp>\n    labels = {int(line.split(' ', 1)[0]): line.strip().split(' ', 1)[1] for line in f if len(line) > 1}\nValueError: invalid literal for int() with base 10: 'person\\n'\n"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 119.761,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-29-21-578763_chatcmpl-cf48e941-7b76-44d0-9407-209bf02b4a25",
          "traceId": "22bde7d7",
          "type": "GENERATION",
          "name": "22bd_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:29:21.578000+00:00",
          "endTime": "2026-02-06T21:29:43.599000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22021.0,
          "costDetails": {},
          "totalTokens": 1272,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 836,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-29-43-800332_chatcmpl-e7a7b03c-9440-433d-98f7-3dd211f90d77",
          "traceId": "22bde7d7",
          "type": "GENERATION",
          "name": "22bd_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:29:43.800000+00:00",
          "endTime": "2026-02-06T21:30:06.633000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22833.0,
          "costDetails": {},
          "totalTokens": 1535,
          "environment": "default",
          "promptTokens": 685,
          "completionTokens": 850,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-30-06-776855_chatcmpl-4df50f88-5803-43c6-ad9a-04e6f67601b9",
          "traceId": "22bde7d7",
          "type": "GENERATION",
          "name": "22bd_psg_gen_attempt#3",
          "startTime": "2026-02-06T21:30:06.776000+00:00",
          "endTime": "2026-02-06T21:30:31.276000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24500.0,
          "costDetails": {},
          "totalTokens": 1662,
          "environment": "default",
          "promptTokens": 746,
          "completionTokens": 916,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-30-31-428023_chatcmpl-c1083bb7-9743-4dd8-883f-27652f3eff35",
          "traceId": "22bde7d7",
          "type": "GENERATION",
          "name": "22bd_psg_gen_attempt#4",
          "startTime": "2026-02-06T21:30:31.428000+00:00",
          "endTime": "2026-02-06T21:30:54.886000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23458.0,
          "costDetails": {},
          "totalTokens": 1634,
          "environment": "default",
          "promptTokens": 753,
          "completionTokens": 881,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-30-55-105721_chatcmpl-e62cabc4-66a9-406d-8ec8-ccb26c2c5e52",
          "traceId": "22bde7d7",
          "type": "GENERATION",
          "name": "22bd_psg_gen_attempt#5",
          "startTime": "2026-02-06T21:30:55.105000+00:00",
          "endTime": "2026-02-06T21:31:21.160000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26055.0,
          "costDetails": {},
          "totalTokens": 1823,
          "environment": "default",
          "promptTokens": 852,
          "completionTokens": 971,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "0ce54135-0d8a-4552-902d-9911c87d35b6",
          "traceId": "22bde7d7",
          "type": "SPAN",
          "name": "error_22_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-06T21:31:21.339000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206233121_psg_qwen2.5-coder:32b/tmp_20260206233121_psg_qwen2.5-coder:32b.py\", line 90, in <module>\n    main()\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206233121_psg_qwen2.5-coder:32b/tmp_20260206233121_psg_qwen2.5-coder:32b.py\", line 35, in main\n    labels = load_labels(label_path)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206233121_psg_qwen2.5-coder:32b/tmp_20260206233121_psg_qwen2.5-coder:32b.py\", line 16, in load_labels\n    labels = {int(line.split(' ', 1)[0]): line.strip().split(' ', 1)[1] for line in f if len(line) > 1}\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260206233121_psg_qwen2.5-coder:32b/tmp_20260206233121_psg_qwen2.5-coder:32b.py\", line 16, in <dictcomp>\n    labels = {int(line.split(' ', 1)[0]): line.strip().split(' ', 1)[1] for line in f if len(line) > 1}\nValueError: invalid literal for int() with base 10: 'person\\n'\n",
          "calculatedTotalCost": 0.0,
          "costDetails": {},
          "totalTokens": 0,
          "environment": "default",
          "promptTokens": 0,
          "completionTokens": 0,
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-06T21:29:22.098Z",
      "updatedAt": "2026-02-06T21:31:27.194Z",
      "environment": "default"
    },
    {
      "id": "e98979d7",
      "timestamp": "2026-02-06T21:28:03.112000+00:00",
      "name": "e989_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize TFLite interpreter and allocate tensors\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(\"Cannot open video\")\n\n# Video writer setup\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Function to preprocess the image\ndef preprocess_image(frame):\n    input_shape = input_details[0]['shape']\n    resized_frame = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    input_data = np.expand_dims(resized_frame, axis=0)\n    # Convert to UINT8 as required by the model\n    input_data = np.uint8(input_data * 255.0)\n    return input_data\n\n# Function to draw bounding boxes on the frame\ndef draw_boxes(frame, boxes, classes, scores):\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            height, width, _ = frame.shape\n            (xminn, xmaxx, yminn, ymaxx) = (xmin * width, xmax * width, ymin * height, ymax * height)\n            cv2.rectangle(frame, (int(xminn), int(yminn)), (int(xmaxx), int(ymaxx)), (0, 255, 0), 2)\n            label = f\"{labels[int(classes[i])]}: {scores[i]:.2f}\"\n            cv2.putText(frame, label, (int(xminn), int(yminn) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Main loop to process each frame\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the image\n    preprocessed_frame = preprocess_image(frame)\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], preprocessed_frame)\n\n    # Run inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw bounding boxes on the frame\n    draw_boxes(frame, boxes, classes, scores)\n\n    # Write the frame to the output video file\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 46.699,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-28-03-114406_chatcmpl-a5b2f2e2-635f-47b0-b85f-98e903fef75f",
          "traceId": "e98979d7",
          "type": "GENERATION",
          "name": "e989_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:28:03.114000+00:00",
          "endTime": "2026-02-06T21:28:27.417000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24303.0,
          "costDetails": {},
          "totalTokens": 1359,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 923,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-28-27-620807_chatcmpl-42e883df-bdf9-45c8-a2bb-841731398f4e",
          "traceId": "e98979d7",
          "type": "GENERATION",
          "name": "e989_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:28:27.620000+00:00",
          "endTime": "2026-02-06T21:28:49.813000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22193.0,
          "costDetails": {},
          "totalTokens": 1516,
          "environment": "default",
          "promptTokens": 686,
          "completionTokens": 830,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:28:03.634Z",
      "updatedAt": "2026-02-06T21:29:00.745Z",
      "environment": "default"
    },
    {
      "id": "c5b26f5d",
      "timestamp": "2026-02-06T21:26:42.477000+00:00",
      "name": "c5b2_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Setup TFLite interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read video from the given path\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise IOError(f\"Could not open video file {input_path}\")\n\n# Prepare to write processed video\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\n# Process each frame\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocessing of the input data\n    input_shape = input_details[0]['shape']\n    height, width = input_shape[1], input_shape[2]\n    img_resized = cv2.resize(frame, (width, height))\n    input_data = np.expand_dims(img_resized, axis=0)\n    # Convert to UINT8 as required by the model\n    if input_details[0]['dtype'] == np.uint8:\n        scale, zero_point = input_details[0]['quantization']\n        input_data = (input_data / scale + zero_point).astype(np.uint8)\n\n    # Inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Output handling\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(int)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    for i in range(len(scores)):\n        if scores[i] >= confidence_threshold:\n            ymin, xmin, ymax, xmax = boxes[i]\n            (left, right, top, bottom) = (xmin * width, xmax * width,\n                                          ymin * height, ymax * height)\n            label = labels[classes[i]]\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {scores[i]:.2f}', (int(left), int(top) - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n\n    # Write the frame with detection to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 44.234,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-26-42-478200_chatcmpl-83132ed3-6cb8-4abf-9573-00cae1501e03",
          "traceId": "c5b26f5d",
          "type": "GENERATION",
          "name": "c5b2_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:26:42.478000+00:00",
          "endTime": "2026-02-06T21:27:05.563000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23085.0,
          "costDetails": {},
          "totalTokens": 1313,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 877,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-27-05-737086_chatcmpl-28b81846-deb3-4922-b1de-395a4d2eb8b9",
          "traceId": "c5b26f5d",
          "type": "GENERATION",
          "name": "c5b2_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:27:05.737000+00:00",
          "endTime": "2026-02-06T21:27:26.712000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20975.0,
          "costDetails": {},
          "totalTokens": 1469,
          "environment": "default",
          "promptTokens": 685,
          "completionTokens": 784,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:26:43.001Z",
      "updatedAt": "2026-02-06T21:27:42.144Z",
      "environment": "default"
    },
    {
      "id": "73e4d909",
      "timestamp": "2026-02-06T21:25:26.811000+00:00",
      "name": "73e4_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load labels\nwith open(label_path, 'r') as f:\n    labels = [line.strip() for line in f.readlines()]\n\n# Initialize the TFLite interpreter\ninterpreter = Interpreter(model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Read the video file\ncap = cv2.VideoCapture(input_path)\n\n# Prepare to write the output video\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\ndef preprocess(frame):\n    # Resize the frame to the input size expected by the model\n    resized_frame = cv2.resize(frame, (input_details[0]['shape'][1], input_details[0]['shape'][2]))\n    # Convert the image to grayscale if needed and normalize to UINT8\n    preprocessed_frame = np.array(resized_frame).astype(np.uint8)\n    return preprocessed_frame\n\ndef draw_detections(frame, boxes, classes, scores):\n    for i in range(len(boxes)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            score = scores[i]\n            label = labels[class_id]\n\n            # Convert normalized coordinates to pixel values\n            ymin, xmin, ymax, xmax = box\n            ymin = int(ymin * frame_height)\n            xmin = int(xmin * frame_width)\n            ymax = int(ymax * frame_height)\n            xmax = int(xmax * frame_width)\n\n            # Draw the bounding box and label on the image\n            cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n            cv2.putText(frame, f'{label}: {score:.2f}', (xmin, ymin - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n\n# Process each frame in the video\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the frame\n    preprocessed_frame = preprocess(frame)\n    input_tensor = np.expand_dims(preprocessed_frame, axis=0)  # Add batch dimension\n\n    # Set the tensor to point to the input data to be inferred\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n\n    # Run the inference\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n\n    # Draw detections on the frame\n    draw_detections(frame, boxes[0], classes[0], scores[0])\n\n    # Write the processed frame to the output video\n    out.write(frame)\n\n# Release resources\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n\nprint(\"Video processing complete. Output saved to\", output_path)"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 44.141,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-25-26-812407_chatcmpl-4c5c5edd-725c-4ea8-a2f0-63f8ca939cbb",
          "traceId": "73e4d909",
          "type": "GENERATION",
          "name": "73e4_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:25:26.812000+00:00",
          "endTime": "2026-02-06T21:25:47.828000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21016.0,
          "costDetails": {},
          "totalTokens": 1232,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 796,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-25-48-044456_chatcmpl-4cc4c03d-cc51-4115-bce7-3209bfad2654",
          "traceId": "73e4d909",
          "type": "GENERATION",
          "name": "73e4_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:25:48.044000+00:00",
          "endTime": "2026-02-06T21:26:10.953000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22909.0,
          "costDetails": {},
          "totalTokens": 1544,
          "environment": "default",
          "promptTokens": 686,
          "completionTokens": 858,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:25:27.335Z",
      "updatedAt": "2026-02-06T21:26:22.038Z",
      "environment": "default"
    },
    {
      "id": "09e36f6a",
      "timestamp": "2026-02-06T21:22:14.936000+00:00",
      "name": "09e3_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter\n\n# CONFIGURATION PARAMETERS\nmodel_path = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path = \"data/object_detection/sheeps.mp4\"\noutput_path = \"results/object_detection/test_results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# Load the label map\nwith open(label_path, 'r') as f:\n    labels = {index: line.strip() for index, line in enumerate(f.readlines())}\n\n# Setup TFLite Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# Get input and output details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Load video from the given path\ncap = cv2.VideoCapture(input_path)\n\nif not cap.isOpened():\n    raise Exception(f\"Error opening video file {input_path}\")\n\n# Get video properties to save processed video\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\n# Define the codec and create VideoWriter object\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Preprocess the input data\n    input_shape = input_details[0]['shape']\n    image_resized = cv2.resize(frame, (input_shape[1], input_shape[2]))\n    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(image_rgb, axis=0).astype(np.uint8)  # Convert to UINT8\n\n    # Run inference\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # Extract results\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n\n    # Draw detection boxes with labels on the frame\n    for i in range(len(scores)):\n        if scores[i] > confidence_threshold:\n            box = boxes[i]\n            class_id = int(classes[i])\n            label = labels[class_id]\n            score = scores[i]\n\n            # Convert box coordinates to image dimensions\n            ymin, xmin, ymax, xmax = box\n            (left, right, top, bottom) = (xmin * frame_width, xmax * frame_width,\n                                          ymin * frame_height, ymax * frame_height)\n\n            # Draw rectangle and label on the frame\n            cv2.rectangle(frame, (int(left), int(top)), (int(right), int(bottom)), (0, 255, 0), 2)\n            label_text = f'{label}: {score:.2f}'\n            cv2.putText(frame, label_text, (int(left), int(top) - 8),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n\n    # Write the frame to the output video\n    out.write(frame)\n\n# Release everything when done\ncap.release()\nout.release()\ncv2.destroyAllWindows()"
      },
      "session_id": "qwen2.5-coder:32b_b5b4_psg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "py_sketch_generator",
        "qwen2.5-coder:32b"
      ],
      "latency": 128.734,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-23-22-14-938253_chatcmpl-fa47ac61-740b-4d6b-88f4-60392d24c6cd",
          "traceId": "09e36f6a",
          "type": "GENERATION",
          "name": "09e3_psg_gen_attempt#1",
          "startTime": "2026-02-06T21:22:14.938000+00:00",
          "endTime": "2026-02-06T21:23:16.610000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 61672.0,
          "costDetails": {},
          "totalTokens": 1410,
          "environment": "default",
          "promptTokens": 436,
          "completionTokens": 974,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-23-17-936517_chatcmpl-7a1cc2d3-dfcd-43b0-bbf1-d9414143b092",
          "traceId": "09e36f6a",
          "type": "GENERATION",
          "name": "09e3_psg_gen_attempt#2",
          "startTime": "2026-02-06T21:23:17.936000+00:00",
          "endTime": "2026-02-06T21:23:37.966000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 20030.0,
          "costDetails": {},
          "totalTokens": 1433,
          "environment": "default",
          "promptTokens": 685,
          "completionTokens": 748,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-23-38-220824_chatcmpl-cf6565cc-09bc-43aa-a595-1f655df240d0",
          "traceId": "09e36f6a",
          "type": "GENERATION",
          "name": "09e3_psg_gen_attempt#3",
          "startTime": "2026-02-06T21:23:38.220000+00:00",
          "endTime": "2026-02-06T21:24:01.222000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23002.0,
          "costDetails": {},
          "totalTokens": 1486,
          "environment": "default",
          "promptTokens": 612,
          "completionTokens": 874,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-24-01-988454_chatcmpl-ce0bacd4-8c95-4759-a10f-712d4b8003b5",
          "traceId": "09e36f6a",
          "type": "GENERATION",
          "name": "09e3_psg_gen_attempt#4",
          "startTime": "2026-02-06T21:24:01.988000+00:00",
          "endTime": "2026-02-06T21:24:23.672000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21684.0,
          "costDetails": {},
          "totalTokens": 1503,
          "environment": "default",
          "promptTokens": 684,
          "completionTokens": 819,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T21:22:15.460Z",
      "updatedAt": "2026-02-06T21:25:05.855Z",
      "environment": "default"
    }
  ],
  "meta": {
    "total_items": 30
  }
}