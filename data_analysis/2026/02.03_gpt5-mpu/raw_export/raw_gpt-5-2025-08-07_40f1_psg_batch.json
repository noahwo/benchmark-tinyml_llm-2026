{
  "data": [
    {
      "id": "32ab2919",
      "timestamp": "2026-02-05T02:42:19.259000+00:00",
      "name": "32ab_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TFLite SSD model.\nIt writes an output video with bounding boxes, labels, and a running temporal mAP (self-supervised)\nestimate displayed on each frame.\n\nPhases implemented according to the provided Programming Guideline:\n- Phase 1: Setup\n- Phase 2: Input Acquisition & Preprocessing Loop\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop (including 4.2 interpretation and 4.3 post-processing)\n- Phase 5: Cleanup\n\nNotes:\n- Temporal mAP is computed self-supervised by treating ground truths as detections from the previous frame,\n  matched to current detections by IoU >= 0.5 and same class. This provides a temporal consistency metric\n  without requiring external annotations.\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Import interpreter literally as specified\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef load_labels(label_path):\n    \"\"\"\n    Load labels from a text file (one label per line).\n    Returns:\n        labels (list of str)\n        label_offset (int): 1 if first label is '???' or 'background', else 0\n    \"\"\"\n    labels = []\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            name = line.strip()\n            if name == '':\n                continue\n            labels.append(name)\n    label_offset = 1 if (len(labels) > 0 and labels[0].strip().lower() in ('???', 'background')) else 0\n    return labels, label_offset\n\ndef ensure_dir(path):\n    \"\"\"Ensure directory exists for the given path.\"\"\"\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\ndef letterbox_resize(image, target_w, target_h):\n    \"\"\"\n    Optionally could letterbox; however many SSD models expect simple resize.\n    For simplicity and performance, use direct resize keeping aspect ratio ignored,\n    as typical SSD Mobilenet was trained on fixed size inputs.\n    \"\"\"\n    return cv2.resize(image, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n\ndef preprocess_frame_bgr_to_model_input(frame_bgr, input_details):\n    \"\"\"\n    Convert BGR frame to the model input tensor based on input_details.\n    - Convert to RGB\n    - Resize to expected input spatial dimensions\n    - Handle dtype conversion and normalization for floating models\n    \"\"\"\n    # Determine input shape: [1, height, width, channels]\n    in_shape = input_details[0]['shape']\n    if len(in_shape) != 4:\n        raise ValueError(f\"Unexpected model input shape: {in_shape}\")\n    in_h, in_w = int(in_shape[1]), int(in_shape[2])\n\n    # Convert BGR to RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # Resize to model input size\n    resized = letterbox_resize(frame_rgb, in_w, in_h)\n\n    # Prepare input tensor\n    input_dtype = input_details[0]['dtype']\n    input_data = resized\n\n    # Phase 2.3: Quantization Handling\n    floating_model = (input_dtype == np.float32)\n    if floating_model:\n        # Normalize to [-1, 1] as per guideline\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # For quantized models, cast to the required dtype\n        input_data = np.asarray(input_data, dtype=input_dtype)\n\n    # Expand dims to [1,H,W,C]\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data, (in_w, in_h), floating_model\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"\n    Compute IoU between two boxes in absolute pixel coordinates: [x1, y1, x2, y2]\n    \"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    inter_w = max(0.0, xB - xA)\n    inter_h = max(0.0, yB - yA)\n    inter_area = inter_w * inter_h\n\n    areaA = max(0.0, (boxA[2] - boxA[0])) * max(0.0, (boxA[3] - boxA[1]))\n    areaB = max(0.0, (boxB[2] - boxB[0])) * max(0.0, (boxB[3] - boxB[1]))\n    denom = (areaA + areaB - inter_area)\n    if denom <= 0.0:\n        return 0.0\n    return inter_area / denom\n\ndef compute_ap(precisions, recalls):\n    \"\"\"\n    Compute Average Precision using the standard all-point interpolation method:\n    1) Append boundary points\n    2) Make precision monotonically non-increasing\n    3) Sum over recall steps of delta_recall * precision_envelope\n\n    Args:\n        precisions: list or np.array of precision values sorted by descending score\n        recalls: list or np.array of recall values sorted by descending score\n\n    Returns:\n        AP (float)\n    \"\"\"\n    if len(precisions) == 0:\n        return 0.0\n\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n\n    # Precision envelope\n    for i in range(len(mpre) - 2, -1, -1):\n        mpre[i] = max(mpre[i], mpre[i + 1])\n\n    # Integrate area under PR curve\n    ap = 0.0\n    for i in range(1, len(mrec)):\n        if mrec[i] != mrec[i - 1]:\n            ap += (mrec[i] - mrec[i - 1]) * mpre[i]\n    return float(ap)\n\ndef compute_map(per_class_scores, per_class_tps, per_class_gt_count):\n    \"\"\"\n    Compute mean Average Precision across classes present (gt_count > 0).\n    The TP/FP flags should correspond to predictions sorted by score DESC per class at evaluation time.\n    Here, we re-sort within this function to ensure correct ordering across frames.\n\n    Args:\n        per_class_scores: dict[class_id] -> list of scores (float)\n        per_class_tps: dict[class_id] -> list of 0/1 flags\n        per_class_gt_count: dict[class_id] -> int count of ground-truth boxes\n\n    Returns:\n        mAP (float)\n    \"\"\"\n    aps = []\n    classes_with_gt = [cid for cid, gtc in per_class_gt_count.items() if gtc > 0]\n    if len(classes_with_gt) == 0:\n        return 0.0\n\n    for cid in classes_with_gt:\n        scores = np.asarray(per_class_scores.get(cid, []), dtype=np.float32)\n        tps = np.asarray(per_class_tps.get(cid, []), dtype=np.int32)\n        gtc = int(per_class_gt_count.get(cid, 0))\n        if gtc <= 0 or scores.size == 0:\n            aps.append(0.0)\n            continue\n\n        # Sort by score descending\n        order = np.argsort(-scores)\n        tps_sorted = tps[order]\n        fps_sorted = 1 - tps_sorted\n\n        cum_tp = np.cumsum(tps_sorted)\n        cum_fp = np.cumsum(fps_sorted)\n\n        # Avoid division by zero\n        denom = (cum_tp + cum_fp)\n        denom[denom == 0] = 1e-9\n        precisions = cum_tp / denom\n        # Recall relative to ground truth count for this class\n        recalls = cum_tp / max(1, gtc)\n\n        ap = compute_ap(precisions, recalls)\n        aps.append(ap)\n\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\ndef color_for_class(cid):\n    \"\"\"\n    Deterministically compute a color (B,G,R) for a given class id.\n    \"\"\"\n    # Simple hash to color mapping\n    r = (37 * (cid + 1)) % 255\n    g = (17 * (cid + 1)) % 255\n    b = (29 * (cid + 1)) % 255\n    return int(b), int(g), int(r)\n\ndef parse_tflite_outputs(interpreter, output_details):\n    \"\"\"\n    Retrieve and parse TFLite SSD outputs into standardized arrays.\n    Returns:\n        boxes: np.ndarray [N,4] (ymin, xmin, ymax, xmax), usually normalized [0,1]\n        classes: np.ndarray [N] of class indices (float or int)\n        scores: np.ndarray [N] of confidence scores [0,1]\n        num: int, number of valid detections (if provided by model), else len(scores)\n    \"\"\"\n    # Phase 4.1: Get Output Tensors\n    outputs = []\n    for od in output_details:\n        outputs.append(interpreter.get_tensor(od['index']))\n    # Flatten output list\n    # Typical: 4 tensors: boxes [1,N,4], classes [1,N], scores [1,N], num [1]\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # First detect boxes by looking for tensor with last dim == 4\n    for arr in outputs:\n        if arr.ndim >= 2 and arr.shape[-1] == 4:\n            boxes = arr\n            break\n    # Detect classes and scores by range and shape\n    tmp_others = [arr for arr in outputs if arr is not boxes]\n    for arr in tmp_others:\n        arr_squeezed = np.squeeze(arr)\n        if arr_squeezed.ndim == 0:\n            # num_detections likely\n            num = int(arr_squeezed)\n        elif arr.dtype in (np.float32, np.float16):\n            # Scores likely in [0,1]\n            maxv = float(np.max(arr_squeezed)) if arr_squeezed.size > 0 else 0.0\n            minv = float(np.min(arr_squeezed)) if arr_squeezed.size > 0 else 0.0\n            if 0.0 <= minv and maxv <= 1.0:\n                scores = arr\n            else:\n                classes = arr\n        elif np.issubdtype(arr.dtype, np.integer):\n            # Could be classes (int) or num\n            if arr.size == 1:\n                num = int(np.squeeze(arr))\n            else:\n                classes = arr\n\n    # Fall-back logic if any is missing\n    if boxes is None:\n        # Try to find something that looks like boxes\n        for arr in outputs:\n            if arr.ndim == 3 and arr.shape[-1] == 4:\n                boxes = arr\n                break\n    if classes is None:\n        # Try to find non-score arrays leftover\n        for arr in outputs:\n            if arr is boxes:\n                continue\n            arr_squeezed = np.squeeze(arr)\n            if arr_squeezed.ndim == 1 and arr_squeezed.size > 1:\n                # decide by range if it's classes or scores\n                maxv = float(np.max(arr_squeezed))\n                minv = float(np.min(arr_squeezed))\n                if not (0.0 <= minv and maxv <= 1.0):\n                    classes = arr\n                    break\n    if scores is None:\n        for arr in outputs:\n            if arr is boxes or arr is classes:\n                continue\n            arr_squeezed = np.squeeze(arr)\n            if arr_squeezed.ndim == 1 and arr_squeezed.size > 1:\n                maxv = float(np.max(arr_squeezed))\n                minv = float(np.min(arr_squeezed))\n                if 0.0 <= minv and maxv <= 1.0:\n                    scores = arr\n                    break\n    if num is None:\n        # If num not provided, infer N from classes or scores\n        if classes is not None:\n            num = int(np.squeeze(classes).shape[0])\n        elif scores is not None:\n            num = int(np.squeeze(scores).shape[0])\n        elif boxes is not None:\n            b = np.squeeze(boxes)\n            if b.ndim == 2:\n                num = int(b.shape[0])\n            else:\n                num = 0\n        else:\n            num = 0\n\n    # Squeeze batch dimension if present\n    if boxes is not None and boxes.ndim >= 3:\n        boxes = boxes[0]\n    elif boxes is not None and boxes.ndim == 2:\n        pass\n    if classes is not None and classes.ndim >= 2:\n        classes = classes[0]\n    if scores is not None and scores.ndim >= 2:\n        scores = scores[0]\n\n    # Slice to num if shapes are longer\n    if boxes is not None and boxes.shape[0] >= num:\n        boxes = boxes[:num]\n    if classes is not None and classes.shape[0] >= num:\n        classes = classes[:num]\n    if scores is not None and scores.shape[0] >= num:\n        scores = scores[:num]\n\n    # Ensure numpy arrays or set defaults\n    boxes = np.asarray(boxes) if boxes is not None else np.zeros((0, 4), dtype=np.float32)\n    classes = np.asarray(classes) if classes is not None else np.zeros((0,), dtype=np.float32)\n    scores = np.asarray(scores) if scores is not None else np.zeros((0,), dtype=np.float32)\n\n    return boxes, classes, scores, num\n\ndef draw_detections(frame, detections, labels, label_offset):\n    \"\"\"\n    Draw bounding boxes and labels on the frame.\n    detections: list of dicts: {'bbox':[x1,y1,x2,y2], 'score':float, 'class_id':int}\n    \"\"\"\n    h, w = frame.shape[:2]\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        cid = int(det['class_id'])\n        score = float(det['score'])\n        color = color_for_class(cid)\n        # Draw rectangle\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness=2)\n        # Label text\n        label_text = None\n        # Map class id to label considering offset if present\n        idx = cid + label_offset\n        if 0 <= idx < len(labels):\n            label_text = labels[idx]\n        else:\n            label_text = f'class_{cid}'\n        text = f\"{label_text} {score:.2f}\"\n        # Put text background\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - baseline), (x1 + tw, y1), color, thickness=-1)\n        cv2.putText(frame, text, (x1, max(0, y1 - baseline)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\ndef clip_box_to_image(xmin, ymin, xmax, ymax, img_w, img_h):\n    \"\"\"\n    Clip bounding box coordinates to image boundaries and convert to int pixel coordinates.\n    \"\"\"\n    x1 = int(np.clip(xmin, 0, img_w - 1))\n    y1 = int(np.clip(ymin, 0, img_h - 1))\n    x2 = int(np.clip(xmax, 0, img_w - 1))\n    y2 = int(np.clip(ymax, 0, img_h - 1))\n    # Ensure x1<=x2, y1<=y2\n    if x2 < x1:\n        x1, x2 = x2, x1\n    if y2 < y1:\n        y1, y2 = y2, y1\n    return x1, y1, x2, y2\n\ndef main():\n    # 1.2 Paths/Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')\n\n    # 1.3 Load Labels (Conditional)\n    labels, label_offset = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    if len(input_details) < 1:\n        print(\"Error: Model has no input tensors.\", file=sys.stderr)\n        sys.exit(1)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: Read a single video file from the given input_path\n    if not os.path.exists(input_path):\n        print(f\"Error: Input video not found at {input_path}\", file=sys.stderr)\n        sys.exit(1)\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file {input_path}\", file=sys.stderr)\n        sys.exit(1)\n    in_fps = cap.get(cv2.CAP_PROP_FPS)\n    if in_fps is None or in_fps <= 1e-3 or np.isnan(in_fps):\n        in_fps = 25.0  # Fallback FPS\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Prepare output writer\n    ensure_dir(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, in_fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        print(f\"Error: Could not open VideoWriter for {output_path}\", file=sys.stderr)\n        cap.release()\n        sys.exit(1)\n\n    # Temporal mAP accumulators\n    iou_threshold = 0.5\n    per_class_scores = {}     # class_id -> list[score]\n    per_class_tps = {}        # class_id -> list[int]\n    per_class_gt_count = {}   # class_id -> int\n    prev_gts = []             # list of dicts: {'bbox':[x1,y1,x2,y2], 'class_id':int}\n\n    frame_count = 0\n    t0 = time.time()\n    final_map = 0.0\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # 2.2 Preprocess Data based on model input details\n        input_data, (in_w, in_h), floating_model = preprocess_frame_bgr_to_model_input(frame_bgr, input_details)\n\n        # Phase 3: Inference\n        # 3.1 Set Input Tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        # 3.2 Run Inference\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling Loop\n        # 4.1 Get Output Tensor(s)\n        boxes_raw, classes_raw, scores_raw, num_raw = parse_tflite_outputs(interpreter, output_details)\n\n        # 4.2 Interpret Results\n        # Convert boxes to pixel coordinates, apply confidence threshold and clipping\n        H, W = frame_bgr.shape[:2]\n        detections = []\n        # Determine if boxes are normalized ([0,1]) by checking value range\n        boxes_are_normalized = False\n        if boxes_raw.size > 0:\n            max_box_val = float(np.max(boxes_raw))\n            boxes_are_normalized = max_box_val <= 1.5  # heuristic\n\n        for i in range(boxes_raw.shape[0]):\n            score = float(scores_raw[i]) if i < scores_raw.shape[0] else 0.0\n            if score < confidence_threshold:\n                continue\n            # Class id as int\n            cls_val = classes_raw[i] if i < classes_raw.shape[0] else 0\n            try:\n                class_id = int(cls_val)\n            except Exception:\n                class_id = int(float(cls_val))\n            # Box format from TFLite SSD: [ymin, xmin, ymax, xmax]\n            ymin, xmin, ymax, xmax = boxes_raw[i].tolist()\n            if boxes_are_normalized:\n                x1 = int(xmin * W)\n                y1 = int(ymin * H)\n                x2 = int(xmax * W)\n                y2 = int(ymax * H)\n            else:\n                x1 = int(xmin)\n                y1 = int(ymin)\n                x2 = int(xmax)\n                y2 = int(ymax)\n            # 4.3 Post-processing: Bounding box clipping\n            x1, y1, x2, y2 = clip_box_to_image(x1, y1, x2, y2, W, H)\n            if x2 <= x1 or y2 <= y1:\n                continue\n            detections.append({\n                'bbox': [x1, y1, x2, y2],\n                'score': score,\n                'class_id': class_id\n            })\n\n        # 4.2 Use labels to map indices to names when drawing\n        draw_detections(frame_bgr, detections, labels, label_offset)\n\n        # Temporal mAP calculation (self-supervised)\n        # Create GTs from previous frame detections (above threshold already)\n        # Evaluate current predictions against prev_gts using IoU >= 0.5 and same class\n        if len(prev_gts) > 0:\n            # Count GTs per class\n            gt_count_frame = {}\n            for gt in prev_gts:\n                cid = int(gt['class_id'])\n                gt_count_frame[cid] = gt_count_frame.get(cid, 0) + 1\n            # Update global GT count\n            for cid, cnt in gt_count_frame.items():\n                per_class_gt_count[cid] = per_class_gt_count.get(cid, 0) + cnt\n\n            # Prepare matching\n            matched_gt_indices_by_class = {}\n            for cid in gt_count_frame.keys():\n                matched_gt_indices_by_class[cid] = set()\n\n            # Sort predictions by score descending for fair evaluation\n            preds_sorted_idx = sorted(range(len(detections)), key=lambda k: -detections[k]['score'])\n\n            for pi in preds_sorted_idx:\n                pred = detections[pi]\n                pcid = int(pred['class_id'])\n                pbox = pred['bbox']\n                pscore = float(pred['score'])\n\n                # Find best GT match of same class with IoU >= threshold that is not matched yet\n                best_iou = 0.0\n                best_gt_idx = -1\n                for gti, gt in enumerate(prev_gts):\n                    gcid = int(gt['class_id'])\n                    if gcid != pcid:\n                        continue\n                    if gti in matched_gt_indices_by_class[pcid]:\n                        continue\n                    iou = iou_xyxy(pbox, gt['bbox'])\n                    if iou >= iou_threshold and iou > best_iou:\n                        best_iou = iou\n                        best_gt_idx = gti\n\n                # Record TP/FP per-class\n                if pcid not in per_class_scores:\n                    per_class_scores[pcid] = []\n                    per_class_tps[pcid] = []\n                per_class_scores[pcid].append(pscore)\n                if best_gt_idx >= 0:\n                    per_class_tps[pcid].append(1)\n                    matched_gt_indices_by_class[pcid].add(best_gt_idx)\n                else:\n                    per_class_tps[pcid].append(0)\n\n            # Compute running mAP\n            final_map = compute_map(per_class_scores, per_class_tps, per_class_gt_count)\n        else:\n            # No GT for the first frame; mAP stays 0.0\n            final_map = 0.0\n\n        # Overlay mAP on the frame\n        map_text = f\"mAP@IoU=0.50 (temporal): {final_map:.3f}\"\n        cv2.putText(frame_bgr, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (5, 245, 5), 2, cv2.LINE_AA)\n\n        # 4.4 Handle Output: write frame to output video\n        writer.write(frame_bgr)\n\n        # 4.5 Loop continuation: prepare GTs for next frame\n        prev_gts = [{'bbox': det['bbox'], 'class_id': det['class_id']} for det in detections]\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n    t1 = time.time()\n\n    # Summary output to console\n    print(f\"Processed {frame_count} frame(s) in {t1 - t0:.2f} s. Average FPS: {frame_count / max(1e-6, (t1 - t0)):.2f}\")\n    print(f\"Output saved to: {output_path}\")\n    print(f\"Final temporal mAP@0.50: {final_map:.3f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/32ab2919",
      "latency": 113.905,
      "total_cost": 0.102301,
      "observations": [
        {
          "id": "time-04-42-19-984368_chatcmpl-D5jbwOk4HYHvC8a9mTgS7jZEcfoEH",
          "traceId": "32ab2919",
          "type": "GENERATION",
          "name": "32ab_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:42:19.984000+00:00",
          "endTime": "2026-02-05T02:44:13.889000+00:00",
          "completionStartTime": "2026-02-05T02:44:13.889000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.102301,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TFLite SSD model.\nIt writes an output video with bounding boxes, labels, and a running temporal mAP (self-supervised)\nestimate displayed on each frame.\n\nPhases implemented according to the provided Programming Guideline:\n- Phase 1: Setup\n- Phase 2: Input Acquisition & Preprocessing Loop\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop (including 4.2 interpretation and 4.3 post-processing)\n- Phase 5: Cleanup\n\nNotes:\n- Temporal mAP is computed self-supervised by treating ground truths as detections from the previous frame,\n  matched to current detections by IoU >= 0.5 and same class. This provides a temporal consistency metric\n  without requiring external annotations.\n\"\"\"\n\nimport os\nimport sys\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Import interpreter literally as specified\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef load_labels(label_path):\n    \"\"\"\n    Load labels from a text file (one label per line).\n    Returns:\n        labels (list of str)\n        label_offset (int): 1 if first label is '???' or 'background', else 0\n    \"\"\"\n    labels = []\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            name = line.strip()\n            if name == '':\n                continue\n            labels.append(name)\n    label_offset = 1 if (len(labels) > 0 and labels[0].strip().lower() in ('???', 'background')) else 0\n    return labels, label_offset\n\ndef ensure_dir(path):\n    \"\"\"Ensure directory exists for the given path.\"\"\"\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\ndef letterbox_resize(image, target_w, target_h):\n    \"\"\"\n    Optionally could letterbox; however many SSD models expect simple resize.\n    For simplicity and performance, use direct resize keeping aspect ratio ignored,\n    as typical SSD Mobilenet was trained on fixed size inputs.\n    \"\"\"\n    return cv2.resize(image, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n\ndef preprocess_frame_bgr_to_model_input(frame_bgr, input_details):\n    \"\"\"\n    Convert BGR frame to the model input tensor based on input_details.\n    - Convert to RGB\n    - Resize to expected input spatial dimensions\n    - Handle dtype conversion and normalization for floating models\n    \"\"\"\n    # Determine input shape: [1, height, width, channels]\n    in_shape = input_details[0]['shape']\n    if len(in_shape) != 4:\n        raise ValueError(f\"Unexpected model input shape: {in_shape}\")\n    in_h, in_w = int(in_shape[1]), int(in_shape[2])\n\n    # Convert BGR to RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # Resize to model input size\n    resized = letterbox_resize(frame_rgb, in_w, in_h)\n\n    # Prepare input tensor\n    input_dtype = input_details[0]['dtype']\n    input_data = resized\n\n    # Phase 2.3: Quantization Handling\n    floating_model = (input_dtype == np.float32)\n    if floating_model:\n        # Normalize to [-1, 1] as per guideline\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # For quantized models, cast to the required dtype\n        input_data = np.asarray(input_data, dtype=input_dtype)\n\n    # Expand dims to [1,H,W,C]\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data, (in_w, in_h), floating_model\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"\n    Compute IoU between two boxes in absolute pixel coordinates: [x1, y1, x2, y2]\n    \"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n\n    inter_w = max(0.0, xB - xA)\n    inter_h = max(0.0, yB - yA)\n    inter_area = inter_w * inter_h\n\n    areaA = max(0.0, (boxA[2] - boxA[0])) * max(0.0, (boxA[3] - boxA[1]))\n    areaB = max(0.0, (boxB[2] - boxB[0])) * max(0.0, (boxB[3] - boxB[1]))\n    denom = (areaA + areaB - inter_area)\n    if denom <= 0.0:\n        return 0.0\n    return inter_area / denom\n\ndef compute_ap(precisions, recalls):\n    \"\"\"\n    Compute Average Precision using the standard all-point interpolation method:\n    1) Append boundary points\n    2) Make precision monotonically non-increasing\n    3) Sum over recall steps of delta_recall * precision_envelope\n\n    Args:\n        precisions: list or np.array of precision values sorted by descending score\n        recalls: list or np.array of recall values sorted by descending score\n\n    Returns:\n        AP (float)\n    \"\"\"\n    if len(precisions) == 0:\n        return 0.0\n\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n\n    # Precision envelope\n    for i in range(len(mpre) - 2, -1, -1):\n        mpre[i] = max(mpre[i], mpre[i + 1])\n\n    # Integrate area under PR curve\n    ap = 0.0\n    for i in range(1, len(mrec)):\n        if mrec[i] != mrec[i - 1]:\n            ap += (mrec[i] - mrec[i - 1]) * mpre[i]\n    return float(ap)\n\ndef compute_map(per_class_scores, per_class_tps, per_class_gt_count):\n    \"\"\"\n    Compute mean Average Precision across classes present (gt_count > 0).\n    The TP/FP flags should correspond to predictions sorted by score DESC per class at evaluation time.\n    Here, we re-sort within this function to ensure correct ordering across frames.\n\n    Args:\n        per_class_scores: dict[class_id] -> list of scores (float)\n        per_class_tps: dict[class_id] -> list of 0/1 flags\n        per_class_gt_count: dict[class_id] -> int count of ground-truth boxes\n\n    Returns:\n        mAP (float)\n    \"\"\"\n    aps = []\n    classes_with_gt = [cid for cid, gtc in per_class_gt_count.items() if gtc > 0]\n    if len(classes_with_gt) == 0:\n        return 0.0\n\n    for cid in classes_with_gt:\n        scores = np.asarray(per_class_scores.get(cid, []), dtype=np.float32)\n        tps = np.asarray(per_class_tps.get(cid, []), dtype=np.int32)\n        gtc = int(per_class_gt_count.get(cid, 0))\n        if gtc <= 0 or scores.size == 0:\n            aps.append(0.0)\n            continue\n\n        # Sort by score descending\n        order = np.argsort(-scores)\n        tps_sorted = tps[order]\n        fps_sorted = 1 - tps_sorted\n\n        cum_tp = np.cumsum(tps_sorted)\n        cum_fp = np.cumsum(fps_sorted)\n\n        # Avoid division by zero\n        denom = (cum_tp + cum_fp)\n        denom[denom == 0] = 1e-9\n        precisions = cum_tp / denom\n        # Recall relative to ground truth count for this class\n        recalls = cum_tp / max(1, gtc)\n\n        ap = compute_ap(precisions, recalls)\n        aps.append(ap)\n\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\ndef color_for_class(cid):\n    \"\"\"\n    Deterministically compute a color (B,G,R) for a given class id.\n    \"\"\"\n    # Simple hash to color mapping\n    r = (37 * (cid + 1)) % 255\n    g = (17 * (cid + 1)) % 255\n    b = (29 * (cid + 1)) % 255\n    return int(b), int(g), int(r)\n\ndef parse_tflite_outputs(interpreter, output_details):\n    \"\"\"\n    Retrieve and parse TFLite SSD outputs into standardized arrays.\n    Returns:\n        boxes: np.ndarray [N,4] (ymin, xmin, ymax, xmax), usually normalized [0,1]\n        classes: np.ndarray [N] of class indices (float or int)\n        scores: np.ndarray [N] of confidence scores [0,1]\n        num: int, number of valid detections (if provided by model), else len(scores)\n    \"\"\"\n    # Phase 4.1: Get Output Tensors\n    outputs = []\n    for od in output_details:\n        outputs.append(interpreter.get_tensor(od['index']))\n    # Flatten output list\n    # Typical: 4 tensors: boxes [1,N,4], classes [1,N], scores [1,N], num [1]\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # First detect boxes by looking for tensor with last dim == 4\n    for arr in outputs:\n        if arr.ndim >= 2 and arr.shape[-1] == 4:\n            boxes = arr\n            break\n    # Detect classes and scores by range and shape\n    tmp_others = [arr for arr in outputs if arr is not boxes]\n    for arr in tmp_others:\n        arr_squeezed = np.squeeze(arr)\n        if arr_squeezed.ndim == 0:\n            # num_detections likely\n            num = int(arr_squeezed)\n        elif arr.dtype in (np.float32, np.float16):\n            # Scores likely in [0,1]\n            maxv = float(np.max(arr_squeezed)) if arr_squeezed.size > 0 else 0.0\n            minv = float(np.min(arr_squeezed)) if arr_squeezed.size > 0 else 0.0\n            if 0.0 <= minv and maxv <= 1.0:\n                scores = arr\n            else:\n                classes = arr\n        elif np.issubdtype(arr.dtype, np.integer):\n            # Could be classes (int) or num\n            if arr.size == 1:\n                num = int(np.squeeze(arr))\n            else:\n                classes = arr\n\n    # Fall-back logic if any is missing\n    if boxes is None:\n        # Try to find something that looks like boxes\n        for arr in outputs:\n            if arr.ndim == 3 and arr.shape[-1] == 4:\n                boxes = arr\n                break\n    if classes is None:\n        # Try to find non-score arrays leftover\n        for arr in outputs:\n            if arr is boxes:\n                continue\n            arr_squeezed = np.squeeze(arr)\n            if arr_squeezed.ndim == 1 and arr_squeezed.size > 1:\n                # decide by range if it's classes or scores\n                maxv = float(np.max(arr_squeezed))\n                minv = float(np.min(arr_squeezed))\n                if not (0.0 <= minv and maxv <= 1.0):\n                    classes = arr\n                    break\n    if scores is None:\n        for arr in outputs:\n            if arr is boxes or arr is classes:\n                continue\n            arr_squeezed = np.squeeze(arr)\n            if arr_squeezed.ndim == 1 and arr_squeezed.size > 1:\n                maxv = float(np.max(arr_squeezed))\n                minv = float(np.min(arr_squeezed))\n                if 0.0 <= minv and maxv <= 1.0:\n                    scores = arr\n                    break\n    if num is None:\n        # If num not provided, infer N from classes or scores\n        if classes is not None:\n            num = int(np.squeeze(classes).shape[0])\n        elif scores is not None:\n            num = int(np.squeeze(scores).shape[0])\n        elif boxes is not None:\n            b = np.squeeze(boxes)\n            if b.ndim == 2:\n                num = int(b.shape[0])\n            else:\n                num = 0\n        else:\n            num = 0\n\n    # Squeeze batch dimension if present\n    if boxes is not None and boxes.ndim >= 3:\n        boxes = boxes[0]\n    elif boxes is not None and boxes.ndim == 2:\n        pass\n    if classes is not None and classes.ndim >= 2:\n        classes = classes[0]\n    if scores is not None and scores.ndim >= 2:\n        scores = scores[0]\n\n    # Slice to num if shapes are longer\n    if boxes is not None and boxes.shape[0] >= num:\n        boxes = boxes[:num]\n    if classes is not None and classes.shape[0] >= num:\n        classes = classes[:num]\n    if scores is not None and scores.shape[0] >= num:\n        scores = scores[:num]\n\n    # Ensure numpy arrays or set defaults\n    boxes = np.asarray(boxes) if boxes is not None else np.zeros((0, 4), dtype=np.float32)\n    classes = np.asarray(classes) if classes is not None else np.zeros((0,), dtype=np.float32)\n    scores = np.asarray(scores) if scores is not None else np.zeros((0,), dtype=np.float32)\n\n    return boxes, classes, scores, num\n\ndef draw_detections(frame, detections, labels, label_offset):\n    \"\"\"\n    Draw bounding boxes and labels on the frame.\n    detections: list of dicts: {'bbox':[x1,y1,x2,y2], 'score':float, 'class_id':int}\n    \"\"\"\n    h, w = frame.shape[:2]\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        cid = int(det['class_id'])\n        score = float(det['score'])\n        color = color_for_class(cid)\n        # Draw rectangle\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness=2)\n        # Label text\n        label_text = None\n        # Map class id to label considering offset if present\n        idx = cid + label_offset\n        if 0 <= idx < len(labels):\n            label_text = labels[idx]\n        else:\n            label_text = f'class_{cid}'\n        text = f\"{label_text} {score:.2f}\"\n        # Put text background\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - baseline), (x1 + tw, y1), color, thickness=-1)\n        cv2.putText(frame, text, (x1, max(0, y1 - baseline)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\ndef clip_box_to_image(xmin, ymin, xmax, ymax, img_w, img_h):\n    \"\"\"\n    Clip bounding box coordinates to image boundaries and convert to int pixel coordinates.\n    \"\"\"\n    x1 = int(np.clip(xmin, 0, img_w - 1))\n    y1 = int(np.clip(ymin, 0, img_h - 1))\n    x2 = int(np.clip(xmax, 0, img_w - 1))\n    y2 = int(np.clip(ymax, 0, img_h - 1))\n    # Ensure x1<=x2, y1<=y2\n    if x2 < x1:\n        x1, x2 = x2, x1\n    if y2 < y1:\n        y1, y2 = y2, y1\n    return x1, y1, x2, y2\n\ndef main():\n    # 1.2 Paths/Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')\n\n    # 1.3 Load Labels (Conditional)\n    labels, label_offset = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    if len(input_details) < 1:\n        print(\"Error: Model has no input tensors.\", file=sys.stderr)\n        sys.exit(1)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: Read a single video file from the given input_path\n    if not os.path.exists(input_path):\n        print(f\"Error: Input video not found at {input_path}\", file=sys.stderr)\n        sys.exit(1)\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"Error: Could not open video file {input_path}\", file=sys.stderr)\n        sys.exit(1)\n    in_fps = cap.get(cv2.CAP_PROP_FPS)\n    if in_fps is None or in_fps <= 1e-3 or np.isnan(in_fps):\n        in_fps = 25.0  # Fallback FPS\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Prepare output writer\n    ensure_dir(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, in_fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        print(f\"Error: Could not open VideoWriter for {output_path}\", file=sys.stderr)\n        cap.release()\n        sys.exit(1)\n\n    # Temporal mAP accumulators\n    iou_threshold = 0.5\n    per_class_scores = {}     # class_id -> list[score]\n    per_class_tps = {}        # class_id -> list[int]\n    per_class_gt_count = {}   # class_id -> int\n    prev_gts = []             # list of dicts: {'bbox':[x1,y1,x2,y2], 'class_id':int}\n\n    frame_count = 0\n    t0 = time.time()\n    final_map = 0.0\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # 2.2 Preprocess Data based on model input details\n        input_data, (in_w, in_h), floating_model = preprocess_frame_bgr_to_model_input(frame_bgr, input_details)\n\n        # Phase 3: Inference\n        # 3.1 Set Input Tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        # 3.2 Run Inference\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling Loop\n        # 4.1 Get Output Tensor(s)\n        boxes_raw, classes_raw, scores_raw, num_raw = parse_tflite_outputs(interpreter, output_details)\n\n        # 4.2 Interpret Results\n        # Convert boxes to pixel coordinates, apply confidence threshold and clipping\n        H, W = frame_bgr.shape[:2]\n        detections = []\n        # Determine if boxes are normalized ([0,1]) by checking value range\n        boxes_are_normalized = False\n        if boxes_raw.size > 0:\n            max_box_val = float(np.max(boxes_raw))\n            boxes_are_normalized = max_box_val <= 1.5  # heuristic\n\n        for i in range(boxes_raw.shape[0]):\n            score = float(scores_raw[i]) if i < scores_raw.shape[0] else 0.0\n            if score < confidence_threshold:\n                continue\n            # Class id as int\n            cls_val = classes_raw[i] if i < classes_raw.shape[0] else 0\n            try:\n                class_id = int(cls_val)\n            except Exception:\n                class_id = int(float(cls_val))\n            # Box format from TFLite SSD: [ymin, xmin, ymax, xmax]\n            ymin, xmin, ymax, xmax = boxes_raw[i].tolist()\n            if boxes_are_normalized:\n                x1 = int(xmin * W)\n                y1 = int(ymin * H)\n                x2 = int(xmax * W)\n                y2 = int(ymax * H)\n            else:\n                x1 = int(xmin)\n                y1 = int(ymin)\n                x2 = int(xmax)\n                y2 = int(ymax)\n            # 4.3 Post-processing: Bounding box clipping\n            x1, y1, x2, y2 = clip_box_to_image(x1, y1, x2, y2, W, H)\n            if x2 <= x1 or y2 <= y1:\n                continue\n            detections.append({\n                'bbox': [x1, y1, x2, y2],\n                'score': score,\n                'class_id': class_id\n            })\n\n        # 4.2 Use labels to map indices to names when drawing\n        draw_detections(frame_bgr, detections, labels, label_offset)\n\n        # Temporal mAP calculation (self-supervised)\n        # Create GTs from previous frame detections (above threshold already)\n        # Evaluate current predictions against prev_gts using IoU >= 0.5 and same class\n        if len(prev_gts) > 0:\n            # Count GTs per class\n            gt_count_frame = {}\n            for gt in prev_gts:\n                cid = int(gt['class_id'])\n                gt_count_frame[cid] = gt_count_frame.get(cid, 0) + 1\n            # Update global GT count\n            for cid, cnt in gt_count_frame.items():\n                per_class_gt_count[cid] = per_class_gt_count.get(cid, 0) + cnt\n\n            # Prepare matching\n            matched_gt_indices_by_class = {}\n            for cid in gt_count_frame.keys():\n                matched_gt_indices_by_class[cid] = set()\n\n            # Sort predictions by score descending for fair evaluation\n            preds_sorted_idx = sorted(range(len(detections)), key=lambda k: -detections[k]['score'])\n\n            for pi in preds_sorted_idx:\n                pred = detections[pi]\n                pcid = int(pred['class_id'])\n                pbox = pred['bbox']\n                pscore = float(pred['score'])\n\n                # Find best GT match of same class with IoU >= threshold that is not matched yet\n                best_iou = 0.0\n                best_gt_idx = -1\n                for gti, gt in enumerate(prev_gts):\n                    gcid = int(gt['class_id'])\n                    if gcid != pcid:\n                        continue\n                    if gti in matched_gt_indices_by_class[pcid]:\n                        continue\n                    iou = iou_xyxy(pbox, gt['bbox'])\n                    if iou >= iou_threshold and iou > best_iou:\n                        best_iou = iou\n                        best_gt_idx = gti\n\n                # Record TP/FP per-class\n                if pcid not in per_class_scores:\n                    per_class_scores[pcid] = []\n                    per_class_tps[pcid] = []\n                per_class_scores[pcid].append(pscore)\n                if best_gt_idx >= 0:\n                    per_class_tps[pcid].append(1)\n                    matched_gt_indices_by_class[pcid].add(best_gt_idx)\n                else:\n                    per_class_tps[pcid].append(0)\n\n            # Compute running mAP\n            final_map = compute_map(per_class_scores, per_class_tps, per_class_gt_count)\n        else:\n            # No GT for the first frame; mAP stays 0.0\n            final_map = 0.0\n\n        # Overlay mAP on the frame\n        map_text = f\"mAP@IoU=0.50 (temporal): {final_map:.3f}\"\n        cv2.putText(frame_bgr, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (5, 245, 5), 2, cv2.LINE_AA)\n\n        # 4.4 Handle Output: write frame to output video\n        writer.write(frame_bgr)\n\n        # 4.5 Loop continuation: prepare GTs for next frame\n        prev_gts = [{'bbox': det['bbox'], 'class_id': det['class_id']} for det in detections]\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n    t1 = time.time()\n\n    # Summary output to console\n    print(f\"Processed {frame_count} frame(s) in {t1 - t0:.2f} s. Average FPS: {frame_count / max(1e-6, (t1 - t0)):.2f}\")\n    print(f\"Output saved to: {output_path}\")\n    print(f\"Final temporal mAP@0.50: {final_map:.3f}\")\n\nif __name__ == \"__main__\":\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 10201,
            "total": 11701,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.102301,
          "latency": 113905.0,
          "timeToFirstToken": 113.905,
          "updatedAt": "2026-02-05T02:44:19.386Z",
          "totalTokens": 11701,
          "costDetails": {
            "total": 0.102301
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 10201,
          "usageDetails": {
            "input": 1500,
            "output": 10201,
            "total": 11701
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:44:14.306Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:45:08.558Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:42:19.804Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "9c1a80e4",
      "timestamp": "2026-02-05T02:40:03.514000+00:00",
      "name": "9c1a_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TensorFlow Lite SSD model.\nIt draws bounding boxes with labels and confidence scores on detected objects and writes the\nresulting annotated video to the specified output path.\n\nPhases implemented per guideline:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (read frames from video file, preprocess)\n- Phase 3: Inference (set input tensor, invoke)\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2: Interpret raw outputs for detection\n  - 4.3: Post-process detections (thresholding, scaling, clipping)\n  - Write annotated frames to output video, overlay label texts and mAP status\n- Phase 5: Cleanup (release resources)\n\nNotes:\n- mAP requires ground truth annotations which are not provided; thus mAP is reported as 'N/A'.\n- This script uses only standard modules (os, time), numpy, cv2, and the mandated ai_edge_litert Interpreter.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1. Imports: Interpreter from ai_edge_litert as per requirement\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef ensure_dir_exists(path):\n    \"\"\"Ensure the directory for the given path exists.\"\"\"\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\ndef load_labels(label_path):\n    \"\"\"Load label map file into a list of strings.\"\"\"\n    labels = []\n    if not os.path.isfile(label_path):\n        return labels\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            name = line.strip()\n            if name:\n                labels.append(name)\n    return labels\n\ndef map_class_to_label(class_id, labels):\n    \"\"\"\n    Map a numeric class ID to a human-readable label.\n    Handles common off-by-one between datasets/models.\n    \"\"\"\n    if not labels:\n        return f\"class_{int(class_id)}\"\n    idx = int(class_id)\n    # Prefer direct index if within range\n    if 0 <= idx < len(labels):\n        return labels[idx]\n    # Fallback: try 1-based indexing\n    if 0 <= (idx - 1) < len(labels):\n        return labels[idx - 1]\n    return f\"class_{idx}\"\n\ndef get_video_writer(output_path, width, height, fps):\n    \"\"\"Create a VideoWriter for mp4 output.\"\"\"\n    ensure_dir_exists(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    return writer\n\ndef preprocess_frame(frame_bgr, input_shape, input_dtype, floating_model):\n    \"\"\"\n    Preprocess frame according to model input requirements.\n    - Resize to model's input spatial dimensions.\n    - Convert BGR to RGB (common for TFLite SSD models).\n    - Normalize if floating model as per guideline.\n    \"\"\"\n    _, in_h, in_w, in_c = input_shape  # Expecting NHWC\n    # Resize and convert color\n    resized = cv2.resize(frame_bgr, (in_w, in_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n    if floating_model:\n        input_data = (np.float32(rgb) - 127.5) / 127.5\n    else:\n        # For quantized models, pass as the expected dtype without normalization.\n        input_data = rgb.astype(input_dtype)\n\n    # Expand to batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    # Ensure dtype matches model\n    input_data = input_data.astype(input_dtype)\n    return input_data\n\ndef parse_detection_outputs(output_details, interpreter):\n    \"\"\"\n    Retrieve and parse model outputs from the interpreter for SSD-style models.\n    Returns:\n      boxes: (N, 4) float32, in normalized [ymin, xmin, ymax, xmax]\n      classes: (N,) int32 class IDs\n      scores: (N,) float32 confidence scores\n      num: int number of valid detections\n    \"\"\"\n    # Retrieve raw outputs\n    raw = []\n    for od in output_details:\n        raw.append(interpreter.get_tensor(od['index']))\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Identify outputs by shape/values\n    for arr in raw:\n        arr_np = np.array(arr)\n        if arr_np.ndim == 3 and arr_np.shape[-1] == 4:\n            # Typical boxes shape: [1, N, 4]\n            boxes = arr_np[0]\n        elif arr_np.ndim == 2:\n            # Typical classes or scores shape: [1, N]\n            candidate = arr_np[0]\n            # Heuristic: scores are float in [0,1]; classes are ints/floats outside [0,1]\n            if np.issubdtype(candidate.dtype, np.floating):\n                # Try to detect scores by range\n                if np.all(candidate >= -1e-3) and np.all(candidate <= 1.0 + 1e-3):\n                    scores = candidate.astype(np.float32)\n                else:\n                    # Could be classes represented as floats\n                    classes = candidate.astype(np.int32)\n            else:\n                classes = candidate.astype(np.int32)\n        elif arr_np.ndim == 1 and arr_np.shape[0] == 1:\n            # num detections\n            try:\n                num = int(arr_np[0])\n            except Exception:\n                pass\n        elif arr_np.ndim == 2 and arr_np.shape == (1, 1):\n            try:\n                num = int(arr_np[0, 0])\n            except Exception:\n                pass\n\n    # Fallbacks if num not provided\n    if num is None:\n        if boxes is not None:\n            num = boxes.shape[0]\n        elif scores is not None:\n            num = scores.shape[0]\n        elif classes is not None:\n            num = classes.shape[0]\n        else:\n            num = 0\n\n    # Truncate arrays to num if longer\n    if boxes is not None and boxes.shape[0] > num:\n        boxes = boxes[:num]\n    if scores is not None and scores.shape[0] > num:\n        scores = scores[:num]\n    if classes is not None and classes.shape[0] > num:\n        classes = classes[:num]\n\n    return boxes, classes, scores, num\n\ndef draw_label_with_background(img, text, origin, font=cv2.FONT_HERSHEY_SIMPLEX,\n                               font_scale=0.5, text_color=(255, 255, 255),\n                               bg_color=(0, 0, 0), thickness=1, padding=2):\n    \"\"\"\n    Draw text with a filled rectangle background for readability.\n    origin: bottom-left corner of the text baseline.\n    \"\"\"\n    (tw, th), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n    x, y = origin\n    # Background rectangle: top-left and bottom-right\n    top_left = (x - padding, y - th - baseline - padding)\n    bottom_right = (x + tw + padding, y + padding)\n    cv2.rectangle(img, top_left, bottom_right, bg_color, thickness=cv2.FILLED)\n    cv2.putText(img, text, (x, y - baseline), font, font_scale, text_color, thickness, cv2.LINE_AA)\n\ndef clip_bbox(xmin, ymin, xmax, ymax, width, height):\n    \"\"\"Clip bounding box coordinates to image bounds.\"\"\"\n    xmin = max(0, min(xmin, width - 1))\n    xmax = max(0, min(xmax, width - 1))\n    ymin = max(0, min(ymin, height - 1))\n    ymax = max(0, min(ymax, height - 1))\n    return int(xmin), int(ymin), int(xmax), int(ymax)\n\ndef main():\n    # Configuration Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    CONF_THRESH = float('0.5')\n\n    # Phase 1.2: Check input path\n    if not os.path.isfile(input_path):\n        raise FileNotFoundError(f\"Input video file not found: {input_path}\")\n\n    # Phase 1.3: Load labels if provided/needed\n    labels = load_labels(label_path)\n\n    # Phase 1.4: Load interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Phase 1.5: Get model details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    if len(input_details) != 1:\n        raise RuntimeError(\"Expected a single input tensor for SSD model.\")\n\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Cannot open input video: {input_path}\")\n\n    # Acquire video properties\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # Safe default\n\n    writer = get_video_writer(output_path, frame_width, frame_height, fps)\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Cannot open VideoWriter for output: {output_path}\")\n\n    # mAP placeholder (no ground truth, so N/A)\n    map_value_text = \"N/A\"\n\n    total_frames = 0\n    total_inference_time = 0.0\n\n    # Start processing frames\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # End of video\n\n        total_frames += 1\n\n        # 2.2 Preprocess\n        input_data = preprocess_frame(frame_bgr, input_shape, input_dtype, floating_model)\n\n        # 3.1 Set input tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        # 3.2 Run inference\n        t0 = time.time()\n        interpreter.invoke()\n        infer_time = (time.time() - t0) * 1000.0  # ms\n        total_inference_time += infer_time\n\n        # 4.1 Get output tensors\n        boxes, classes, scores, num = parse_detection_outputs(output_details, interpreter)\n\n        # 4.2 Interpret Results (generate human-readable detections)\n        detections = []\n        if num is None:\n            num = 0\n        for i in range(num):\n            score = float(scores[i]) if scores is not None else 1.0\n            if score < CONF_THRESH:\n                continue\n            class_id = int(classes[i]) if classes is not None else -1\n            label = map_class_to_label(class_id, labels)\n\n            if boxes is not None:\n                # SSD box format: [ymin, xmin, ymax, xmax] normalized\n                ymin, xmin, ymax, xmax = boxes[i]\n            else:\n                ymin = xmin = 0.0\n                ymax = xmax = 1.0\n\n            detections.append({\n                \"bbox_norm\": (float(xmin), float(ymin), float(xmax), float(ymax)),\n                \"score\": score,\n                \"class_id\": class_id,\n                \"label\": label\n            })\n\n        # 4.3 Post-processing: threshold already applied, now scaling and clipping\n        # Draw detections on the frame\n        for det in detections:\n            xmin_n, ymin_n, xmax_n, ymax_n = det[\"bbox_norm\"]\n\n            # Scale to absolute pixel coordinates\n            x1 = int(xmin_n * frame_width)\n            y1 = int(ymin_n * frame_height)\n            x2 = int(xmax_n * frame_width)\n            y2 = int(ymax_n * frame_height)\n\n            # Clip to image bounds\n            x1, y1, x2, y2 = clip_bbox(x1, y1, x2, y2, frame_width, frame_height)\n\n            # Draw rectangle\n            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            # Label with score\n            label_text = f\"{det['label']}: {det['score']:.2f}\"\n            draw_label_with_background(frame_bgr, label_text, (x1, max(0, y1 - 5)),\n                                       font_scale=0.5, text_color=(255, 255, 255),\n                                       bg_color=(0, 128, 0), thickness=1, padding=3)\n\n        # Overlay mAP information (N/A due to missing ground truth)\n        info_text = f\"mAP: {map_value_text}\"\n        draw_label_with_background(frame_bgr, info_text, (10, 25),\n                                   font_scale=0.6, text_color=(255, 255, 255),\n                                   bg_color=(50, 50, 50), thickness=1, padding=4)\n\n        # Optional: overlay FPS (inference only)\n        avg_infer_ms = infer_time\n        fps_infer = 1000.0 / avg_infer_ms if avg_infer_ms > 0 else 0.0\n        fps_text = f\"Infer: {avg_infer_ms:.1f} ms ({fps_infer:.1f} FPS)\"\n        draw_label_with_background(frame_bgr, fps_text, (10, 50),\n                                   font_scale=0.6, text_color=(255, 255, 255),\n                                   bg_color=(50, 50, 50), thickness=1, padding=4)\n\n        # 4.4 Handle Output: write annotated frame to output video\n        writer.write(frame_bgr)\n\n        # 4.5 Loop Continuation: continue until frames exhausted\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n\n    # Report summary to console\n    if total_frames > 0:\n        avg_infer_time = total_inference_time / total_frames\n        print(f\"Processed {total_frames} frames.\")\n        print(f\"Average inference time: {avg_infer_time:.2f} ms per frame.\")\n        print(f\"Output written to: {output_path}\")\n        print(\"mAP: N/A (Ground truth annotations not provided; cannot compute mAP.)\")\n    else:\n        print(\"No frames processed. Please verify the input video file.\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/9c1a80e4",
      "latency": 79.919,
      "total_cost": 0.076621,
      "observations": [
        {
          "id": "time-04-40-04-183827_chatcmpl-D5jZkPDT4D9fTzlPgoo7SRokgnMoi",
          "traceId": "9c1a80e4",
          "type": "GENERATION",
          "name": "9c1a_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:40:04.183000+00:00",
          "endTime": "2026-02-05T02:41:24.102000+00:00",
          "completionStartTime": "2026-02-05T02:41:24.102000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.07662100000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TensorFlow Lite SSD model.\nIt draws bounding boxes with labels and confidence scores on detected objects and writes the\nresulting annotated video to the specified output path.\n\nPhases implemented per guideline:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (read frames from video file, preprocess)\n- Phase 3: Inference (set input tensor, invoke)\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2: Interpret raw outputs for detection\n  - 4.3: Post-process detections (thresholding, scaling, clipping)\n  - Write annotated frames to output video, overlay label texts and mAP status\n- Phase 5: Cleanup (release resources)\n\nNotes:\n- mAP requires ground truth annotations which are not provided; thus mAP is reported as 'N/A'.\n- This script uses only standard modules (os, time), numpy, cv2, and the mandated ai_edge_litert Interpreter.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1. Imports: Interpreter from ai_edge_litert as per requirement\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef ensure_dir_exists(path):\n    \"\"\"Ensure the directory for the given path exists.\"\"\"\n    os.makedirs(os.path.dirname(path), exist_ok=True)\n\ndef load_labels(label_path):\n    \"\"\"Load label map file into a list of strings.\"\"\"\n    labels = []\n    if not os.path.isfile(label_path):\n        return labels\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            name = line.strip()\n            if name:\n                labels.append(name)\n    return labels\n\ndef map_class_to_label(class_id, labels):\n    \"\"\"\n    Map a numeric class ID to a human-readable label.\n    Handles common off-by-one between datasets/models.\n    \"\"\"\n    if not labels:\n        return f\"class_{int(class_id)}\"\n    idx = int(class_id)\n    # Prefer direct index if within range\n    if 0 <= idx < len(labels):\n        return labels[idx]\n    # Fallback: try 1-based indexing\n    if 0 <= (idx - 1) < len(labels):\n        return labels[idx - 1]\n    return f\"class_{idx}\"\n\ndef get_video_writer(output_path, width, height, fps):\n    \"\"\"Create a VideoWriter for mp4 output.\"\"\"\n    ensure_dir_exists(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    return writer\n\ndef preprocess_frame(frame_bgr, input_shape, input_dtype, floating_model):\n    \"\"\"\n    Preprocess frame according to model input requirements.\n    - Resize to model's input spatial dimensions.\n    - Convert BGR to RGB (common for TFLite SSD models).\n    - Normalize if floating model as per guideline.\n    \"\"\"\n    _, in_h, in_w, in_c = input_shape  # Expecting NHWC\n    # Resize and convert color\n    resized = cv2.resize(frame_bgr, (in_w, in_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n    if floating_model:\n        input_data = (np.float32(rgb) - 127.5) / 127.5\n    else:\n        # For quantized models, pass as the expected dtype without normalization.\n        input_data = rgb.astype(input_dtype)\n\n    # Expand to batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    # Ensure dtype matches model\n    input_data = input_data.astype(input_dtype)\n    return input_data\n\ndef parse_detection_outputs(output_details, interpreter):\n    \"\"\"\n    Retrieve and parse model outputs from the interpreter for SSD-style models.\n    Returns:\n      boxes: (N, 4) float32, in normalized [ymin, xmin, ymax, xmax]\n      classes: (N,) int32 class IDs\n      scores: (N,) float32 confidence scores\n      num: int number of valid detections\n    \"\"\"\n    # Retrieve raw outputs\n    raw = []\n    for od in output_details:\n        raw.append(interpreter.get_tensor(od['index']))\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Identify outputs by shape/values\n    for arr in raw:\n        arr_np = np.array(arr)\n        if arr_np.ndim == 3 and arr_np.shape[-1] == 4:\n            # Typical boxes shape: [1, N, 4]\n            boxes = arr_np[0]\n        elif arr_np.ndim == 2:\n            # Typical classes or scores shape: [1, N]\n            candidate = arr_np[0]\n            # Heuristic: scores are float in [0,1]; classes are ints/floats outside [0,1]\n            if np.issubdtype(candidate.dtype, np.floating):\n                # Try to detect scores by range\n                if np.all(candidate >= -1e-3) and np.all(candidate <= 1.0 + 1e-3):\n                    scores = candidate.astype(np.float32)\n                else:\n                    # Could be classes represented as floats\n                    classes = candidate.astype(np.int32)\n            else:\n                classes = candidate.astype(np.int32)\n        elif arr_np.ndim == 1 and arr_np.shape[0] == 1:\n            # num detections\n            try:\n                num = int(arr_np[0])\n            except Exception:\n                pass\n        elif arr_np.ndim == 2 and arr_np.shape == (1, 1):\n            try:\n                num = int(arr_np[0, 0])\n            except Exception:\n                pass\n\n    # Fallbacks if num not provided\n    if num is None:\n        if boxes is not None:\n            num = boxes.shape[0]\n        elif scores is not None:\n            num = scores.shape[0]\n        elif classes is not None:\n            num = classes.shape[0]\n        else:\n            num = 0\n\n    # Truncate arrays to num if longer\n    if boxes is not None and boxes.shape[0] > num:\n        boxes = boxes[:num]\n    if scores is not None and scores.shape[0] > num:\n        scores = scores[:num]\n    if classes is not None and classes.shape[0] > num:\n        classes = classes[:num]\n\n    return boxes, classes, scores, num\n\ndef draw_label_with_background(img, text, origin, font=cv2.FONT_HERSHEY_SIMPLEX,\n                               font_scale=0.5, text_color=(255, 255, 255),\n                               bg_color=(0, 0, 0), thickness=1, padding=2):\n    \"\"\"\n    Draw text with a filled rectangle background for readability.\n    origin: bottom-left corner of the text baseline.\n    \"\"\"\n    (tw, th), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n    x, y = origin\n    # Background rectangle: top-left and bottom-right\n    top_left = (x - padding, y - th - baseline - padding)\n    bottom_right = (x + tw + padding, y + padding)\n    cv2.rectangle(img, top_left, bottom_right, bg_color, thickness=cv2.FILLED)\n    cv2.putText(img, text, (x, y - baseline), font, font_scale, text_color, thickness, cv2.LINE_AA)\n\ndef clip_bbox(xmin, ymin, xmax, ymax, width, height):\n    \"\"\"Clip bounding box coordinates to image bounds.\"\"\"\n    xmin = max(0, min(xmin, width - 1))\n    xmax = max(0, min(xmax, width - 1))\n    ymin = max(0, min(ymin, height - 1))\n    ymax = max(0, min(ymax, height - 1))\n    return int(xmin), int(ymin), int(xmax), int(ymax)\n\ndef main():\n    # Configuration Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    CONF_THRESH = float('0.5')\n\n    # Phase 1.2: Check input path\n    if not os.path.isfile(input_path):\n        raise FileNotFoundError(f\"Input video file not found: {input_path}\")\n\n    # Phase 1.3: Load labels if provided/needed\n    labels = load_labels(label_path)\n\n    # Phase 1.4: Load interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # Phase 1.5: Get model details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    if len(input_details) != 1:\n        raise RuntimeError(\"Expected a single input tensor for SSD model.\")\n\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Cannot open input video: {input_path}\")\n\n    # Acquire video properties\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # Safe default\n\n    writer = get_video_writer(output_path, frame_width, frame_height, fps)\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Cannot open VideoWriter for output: {output_path}\")\n\n    # mAP placeholder (no ground truth, so N/A)\n    map_value_text = \"N/A\"\n\n    total_frames = 0\n    total_inference_time = 0.0\n\n    # Start processing frames\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # End of video\n\n        total_frames += 1\n\n        # 2.2 Preprocess\n        input_data = preprocess_frame(frame_bgr, input_shape, input_dtype, floating_model)\n\n        # 3.1 Set input tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        # 3.2 Run inference\n        t0 = time.time()\n        interpreter.invoke()\n        infer_time = (time.time() - t0) * 1000.0  # ms\n        total_inference_time += infer_time\n\n        # 4.1 Get output tensors\n        boxes, classes, scores, num = parse_detection_outputs(output_details, interpreter)\n\n        # 4.2 Interpret Results (generate human-readable detections)\n        detections = []\n        if num is None:\n            num = 0\n        for i in range(num):\n            score = float(scores[i]) if scores is not None else 1.0\n            if score < CONF_THRESH:\n                continue\n            class_id = int(classes[i]) if classes is not None else -1\n            label = map_class_to_label(class_id, labels)\n\n            if boxes is not None:\n                # SSD box format: [ymin, xmin, ymax, xmax] normalized\n                ymin, xmin, ymax, xmax = boxes[i]\n            else:\n                ymin = xmin = 0.0\n                ymax = xmax = 1.0\n\n            detections.append({\n                \"bbox_norm\": (float(xmin), float(ymin), float(xmax), float(ymax)),\n                \"score\": score,\n                \"class_id\": class_id,\n                \"label\": label\n            })\n\n        # 4.3 Post-processing: threshold already applied, now scaling and clipping\n        # Draw detections on the frame\n        for det in detections:\n            xmin_n, ymin_n, xmax_n, ymax_n = det[\"bbox_norm\"]\n\n            # Scale to absolute pixel coordinates\n            x1 = int(xmin_n * frame_width)\n            y1 = int(ymin_n * frame_height)\n            x2 = int(xmax_n * frame_width)\n            y2 = int(ymax_n * frame_height)\n\n            # Clip to image bounds\n            x1, y1, x2, y2 = clip_bbox(x1, y1, x2, y2, frame_width, frame_height)\n\n            # Draw rectangle\n            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            # Label with score\n            label_text = f\"{det['label']}: {det['score']:.2f}\"\n            draw_label_with_background(frame_bgr, label_text, (x1, max(0, y1 - 5)),\n                                       font_scale=0.5, text_color=(255, 255, 255),\n                                       bg_color=(0, 128, 0), thickness=1, padding=3)\n\n        # Overlay mAP information (N/A due to missing ground truth)\n        info_text = f\"mAP: {map_value_text}\"\n        draw_label_with_background(frame_bgr, info_text, (10, 25),\n                                   font_scale=0.6, text_color=(255, 255, 255),\n                                   bg_color=(50, 50, 50), thickness=1, padding=4)\n\n        # Optional: overlay FPS (inference only)\n        avg_infer_ms = infer_time\n        fps_infer = 1000.0 / avg_infer_ms if avg_infer_ms > 0 else 0.0\n        fps_text = f\"Infer: {avg_infer_ms:.1f} ms ({fps_infer:.1f} FPS)\"\n        draw_label_with_background(frame_bgr, fps_text, (10, 50),\n                                   font_scale=0.6, text_color=(255, 255, 255),\n                                   bg_color=(50, 50, 50), thickness=1, padding=4)\n\n        # 4.4 Handle Output: write annotated frame to output video\n        writer.write(frame_bgr)\n\n        # 4.5 Loop Continuation: continue until frames exhausted\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n\n    # Report summary to console\n    if total_frames > 0:\n        avg_infer_time = total_inference_time / total_frames\n        print(f\"Processed {total_frames} frames.\")\n        print(f\"Average inference time: {avg_infer_time:.2f} ms per frame.\")\n        print(f\"Output written to: {output_path}\")\n        print(\"mAP: N/A (Ground truth annotations not provided; cannot compute mAP.)\")\n    else:\n        print(\"No frames processed. Please verify the input video file.\")\n\nif __name__ == \"__main__\":\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 7633,
            "total": 9133,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.076621,
          "latency": 79919.0,
          "timeToFirstToken": 79.919,
          "updatedAt": "2026-02-05T02:41:29.360Z",
          "totalTokens": 9133,
          "costDetails": {
            "total": 0.076621
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 7633,
          "usageDetails": {
            "input": 1500,
            "output": 7633,
            "total": 9133
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:41:24.264Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:42:13.814Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:40:04.069Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "6dc35e5e",
      "timestamp": "2026-02-05T02:37:25.718000+00:00",
      "name": "6dc3_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single video file using a TFLite SSD model,\ndraws bounding boxes and labels on detected objects, and writes the annotated video\nto the specified output path. It also computes and overlays a running \"mAP (proxy)\"\nmetric due to the absence of ground-truth annotations.\n\nPhases implemented as per Programming Guidelines:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (read frames from the given video)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results\n  - 4.3 Post-processing (confidence thresholding, coordinate scaling, clipping)\n- Phase 5: Cleanup\n\nNote on mAP:\n- Since ground-truth annotations are not provided, we compute a proxy mAP by averaging\n  the sorted detection scores per class (treated as a rough \"AP\" proxy) and then\n  averaging across classes (mAP). This proxy is intended only for relative, informal\n  feedback during inference and not as a formal evaluation metric.\n\"\"\"\n\n# =========================\n# Phase 1: Setup\n# =========================\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Per Programming Guideline 1.1: Import Interpreter literally as below\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2. Paths/Parameters (from CONFIGURATION PARAMETERS)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD  = 0.5\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\ndef load_labels(label_path):\n    \"\"\"\n    1.3. Load Labels if label path is provided.\n    Returns:\n        labels (list[str]): list of label strings\n        label_offset (int): offset to apply for class index mapping (e.g., 1 if first label is '???')\n    \"\"\"\n    labels = []\n    if label_path and os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            labels = [line.strip() for line in f.readlines() if line.strip() != '']\n    else:\n        print(f\"Warning: Label file not found at '{label_path}'. Proceeding with IDs as labels.\")\n        labels = []\n    # Common in TF Lite: first label may be '???'\n    label_offset = 1 if len(labels) > 0 and labels[0].strip().lower() in ('???', 'background') else 0\n    return labels, label_offset\n\ndef initialize_interpreter(model_path):\n    \"\"\"\n    1.4. Load Interpreter and allocate tensors, return input/output details.\n    \"\"\"\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    return interpreter, input_details, output_details\n\ndef get_input_shape_dtype(input_details):\n    \"\"\"\n    1.5. Extract input tensor properties (shape, dtype, index).\n    Returns:\n        input_index, input_height, input_width, input_channels, input_dtype, floating_model (bool)\n    \"\"\"\n    if not input_details or len(input_details) == 0:\n        raise RuntimeError(\"No input tensor details found in the model.\")\n    input_index = input_details[0]['index']\n    shape = input_details[0]['shape']\n    if len(shape) != 4:\n        raise ValueError(f\"Unexpected input tensor shape: {shape}, expected [1, H, W, C].\")\n    input_height, input_width, input_channels = int(shape[1]), int(shape[2]), int(shape[3])\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n    return input_index, input_height, input_width, input_channels, input_dtype, floating_model\n\ndef parse_detection_outputs(interpreter, output_details):\n    \"\"\"\n    4.1. Retrieve raw output tensors and attempt to parse them into standard detection outputs.\n    Tries to be robust across common SSD Mobilenet TFLite detection models.\n\n    Returns:\n        boxes: np.ndarray of shape [N, 4] in order [ymin, xmin, ymax, xmax]\n        classes: np.ndarray of shape [N], int32\n        scores: np.ndarray of shape [N], float32\n        num_detections: int\n    \"\"\"\n    raw_outputs = []\n    for od in output_details:\n        raw_outputs.append(interpreter.get_tensor(od['index']))\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Try to use names if provided\n    for od, out in zip(output_details, raw_outputs):\n        name = od.get('name', '').lower()\n        shp = out.shape\n        if out.ndim == 3 and shp[-1] == 4:\n            boxes = out\n        elif 'box' in name and out.ndim in (2, 3) and shp[-1] == 4:\n            boxes = out\n        elif 'score' in name or 'scores' in name:\n            scores = out\n        elif 'class' in name or 'classes' in name:\n            classes = out\n        elif 'num' in name:\n            num = out\n\n    # If names weren't helpful, infer by shape/value ranges\n    if boxes is None:\n        for out in raw_outputs:\n            if out.ndim == 3 and out.shape[-1] == 4:\n                boxes = out\n                break\n    if scores is None or classes is None:\n        for out in raw_outputs:\n            if out.ndim == 2:\n                # Heuristic: scores are in [0,1]\n                if out.size > 0 and np.amin(out) >= 0.0 and np.amax(out) <= 1.0:\n                    scores = out\n                else:\n                    classes = out\n    if num is None:\n        for out in raw_outputs:\n            if out.ndim == 1 and out.size == 1:\n                num = out\n                break\n\n    # Squeeze to remove batch dimension if present\n    if boxes is not None:\n        boxes = np.squeeze(boxes)\n    if classes is not None:\n        classes = np.squeeze(classes)\n    if scores is not None:\n        scores = np.squeeze(scores)\n    if num is not None:\n        num_detections = int(np.squeeze(num).astype(np.int32))\n    else:\n        # Fallback if num_detections not provided\n        num_detections = int(scores.shape[0]) if scores is not None else 0\n\n    # Final type casting\n    if classes is None:\n        classes = np.zeros((num_detections,), dtype=np.int32)\n    else:\n        classes = classes.astype(np.int32, copy=False)\n    if scores is None:\n        scores = np.zeros((num_detections,), dtype=np.float32)\n    else:\n        scores = scores.astype(np.float32, copy=False)\n\n    # Ensure boxes shape [N, 4]\n    if boxes is None:\n        boxes = np.zeros((num_detections, 4), dtype=np.float32)\n    else:\n        boxes = boxes.astype(np.float32, copy=False)\n        if boxes.ndim == 1 and boxes.size == 4:\n            boxes = boxes.reshape((1, 4))\n        if boxes.shape[0] != num_detections:\n            # Align length if discrepancy\n            min_len = min(boxes.shape[0], num_detections, scores.shape[0])\n            boxes = boxes[:min_len]\n            classes = classes[:min_len]\n            scores = scores[:min_len]\n            num_detections = min_len\n\n    return boxes, classes, scores, num_detections\n\ndef scale_and_clip_box(box, frame_w, frame_h, input_w=None, input_h=None):\n    \"\"\"\n    Scales normalized or absolute boxes to frame coordinates and clips to image bounds.\n    box format: [ymin, xmin, ymax, xmax]\n    \"\"\"\n    ymin, xmin, ymax, xmax = box\n    # Detect normalization: if any coord > 1.0 -> assume absolute in input image space\n    if max(abs(ymin), abs(xmin), abs(ymax), abs(xmax)) <= 1.0:\n        y1 = int(round(ymin * frame_h))\n        x1 = int(round(xmin * frame_w))\n        y2 = int(round(ymax * frame_h))\n        x2 = int(round(xmax * frame_w))\n    else:\n        # If absolute (likely in model input coordinates), scale if input size known\n        if input_w is not None and input_h is not None and input_w > 0 and input_h > 0:\n            scale_x = frame_w / float(input_w)\n            scale_y = frame_h / float(input_h)\n            y1 = int(round(ymin * scale_y))\n            x1 = int(round(xmin * scale_x))\n            y2 = int(round(ymax * scale_y))\n            x2 = int(round(xmax * scale_x))\n        else:\n            y1 = int(round(ymin))\n            x1 = int(round(xmin))\n            y2 = int(round(ymax))\n            x2 = int(round(xmax))\n\n    # Clip to valid ranges\n    x1 = max(0, min(x1, frame_w - 1))\n    y1 = max(0, min(y1, frame_h - 1))\n    x2 = max(0, min(x2, frame_w - 1))\n    y2 = max(0, min(y2, frame_h - 1))\n    return x1, y1, x2, y2\n\ndef color_for_class(class_id):\n    \"\"\"\n    Deterministic color for a given class id.\n    \"\"\"\n    return (int(37 * class_id) % 255, int(17 * class_id) % 255, int(29 * class_id) % 255)\n\ndef compute_map_proxy(scores_per_class):\n    \"\"\"\n    Compute a proxy mAP without ground-truth:\n    - For each class, sort detection scores descending and compute mean(score) as AP proxy.\n    - mAP is the mean of per-class AP proxies over classes that have detections.\n\n    Returns:\n        mAP_proxy (float)\n    \"\"\"\n    if not scores_per_class:\n        return 0.0\n    ap_values = []\n    for cls_id, s_list in scores_per_class.items():\n        if len(s_list) == 0:\n            continue\n        sorted_scores = sorted(s_list, reverse=True)\n        ap_proxy = float(np.mean(sorted_scores))\n        ap_values.append(ap_proxy)\n    if len(ap_values) == 0:\n        return 0.0\n    return float(np.mean(ap_values))\n\ndef main():\n    # Load labels\n    labels, label_offset = load_labels(LABEL_PATH)\n\n    # Initialize interpreter and get model details\n    interpreter, input_details, output_details = initialize_interpreter(MODEL_PATH)\n    input_index, in_h, in_w, in_c, input_dtype, floating_model = get_input_shape_dtype(input_details)\n\n    # 2.1. Acquire Input Data: Open the specified video file\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: Could not open input video: {INPUT_PATH}\")\n        return\n\n    # Retrieve input video properties\n    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # reasonable default fallback\n\n    # Prepare VideoWriter for output\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_w, orig_h))\n    if not writer.isOpened():\n        print(f\"Error: Could not open output video for writing: {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # Accumulator for mAP(proxy)\n    scores_per_class = {}  # dict: class_id -> list of scores\n\n    frame_count = 0\n    t0 = time.time()\n\n    # =========================\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # =========================\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # end of video\n\n        frame_count += 1\n        frame_h, frame_w = frame_bgr.shape[:2]\n\n        # 2.2. Preprocess Data to match model input: BGR -> RGB, resize, batch, dtype handling\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        resized_rgb = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n        input_data = np.expand_dims(resized_rgb, axis=0)\n\n        # 2.3. Quantization Handling\n        if floating_model:\n            input_data = (input_data.astype(np.float32) - 127.5) / 127.5\n        else:\n            input_data = input_data.astype(input_dtype, copy=False)\n\n        # =========================\n        # Phase 3: Inference\n        # =========================\n        interpreter.set_tensor(input_index, input_data)\n        interpreter.invoke()\n\n        # =========================\n        # Phase 4: Output Interpretation & Handling Loop\n        # =========================\n\n        # 4.1. Get Output Tensor(s)\n        boxes, classes, scores, num_detections = parse_detection_outputs(interpreter, output_details)\n\n        # 4.2. Interpret Results\n        # Map class indices to labels, and prepare annotations\n        annotated = frame_bgr.copy()\n        detections_this_frame = 0\n\n        # 4.3. Post-processing: thresholding, coordinate scaling, clipping\n        for i in range(num_detections):\n            score = float(scores[i])\n            if score < CONFIDENCE_THRESHOLD:\n                continue\n            cls_id = int(classes[i])\n            # Map label with offset if needed\n            label_text = None\n            mapped_index = cls_id + (-label_offset) if label_offset > 0 else cls_id\n            if labels and 0 <= mapped_index < len(labels):\n                label_text = labels[mapped_index]\n            else:\n                label_text = f\"id:{cls_id}\"\n\n            # Scale and clip boxes\n            x1, y1, x2, y2 = scale_and_clip_box(boxes[i], frame_w, frame_h, in_w, in_h)\n\n            # Draw bounding box and label\n            color = color_for_class(cls_id)\n            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n            label_str = f\"{label_text}: {score:.2f}\"\n            # Put label background for readability\n            (tw, th), baseline = cv2.getTextSize(label_str, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            y_text = max(0, y1 - 5)\n            cv2.rectangle(annotated, (x1, y_text - th - baseline), (x1 + tw, y_text + baseline), color, -1)\n            cv2.putText(annotated, label_str, (x1, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n            # Accumulate scores per class for mAP(proxy)\n            if cls_id not in scores_per_class:\n                scores_per_class[cls_id] = []\n            scores_per_class[cls_id].append(score)\n            detections_this_frame += 1\n\n        # Compute running mAP(proxy)\n        mAP_proxy = compute_map_proxy(scores_per_class)\n\n        # 4.4. Handle Output: Overlay mAP(proxy) and write frame\n        overlay_text = f\"mAP(proxy): {mAP_proxy:.3f} | Detections: {detections_this_frame}\"\n        cv2.putText(annotated, overlay_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 0), 3, cv2.LINE_AA)\n        cv2.putText(annotated, overlay_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 255, 255), 1, cv2.LINE_AA)\n        writer.write(annotated)\n\n        # 4.5. Loop Continuation: continues until video ends\n\n    # =========================\n    # Phase 5: Cleanup\n    # =========================\n    cap.release()\n    writer.release()\n\n    total_time = time.time() - t0\n    final_map = compute_map_proxy(scores_per_class)\n    print(f\"Processed {frame_count} frames in {total_time:.2f}s ({(frame_count / total_time) if total_time > 0 else 0:.2f} FPS).\")\n    print(f\"Final mAP(proxy): {final_map:.4f}\")\n    print(f\"Annotated output saved to: {OUTPUT_PATH}\")\n\nif __name__ == '__main__':\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/6dc35e5e",
      "latency": 101.279,
      "total_cost": 0.075545,
      "observations": [
        {
          "id": "time-04-37-26-378605_chatcmpl-D5jXCmeXxXRh45h4JbbW3IvPZgFrw",
          "traceId": "6dc35e5e",
          "type": "GENERATION",
          "name": "6dc3_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:37:26.378000+00:00",
          "endTime": "2026-02-05T02:39:07.657000+00:00",
          "completionStartTime": "2026-02-05T02:39:07.657000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.075545,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single video file using a TFLite SSD model,\ndraws bounding boxes and labels on detected objects, and writes the annotated video\nto the specified output path. It also computes and overlays a running \"mAP (proxy)\"\nmetric due to the absence of ground-truth annotations.\n\nPhases implemented as per Programming Guidelines:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (read frames from the given video)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results\n  - 4.3 Post-processing (confidence thresholding, coordinate scaling, clipping)\n- Phase 5: Cleanup\n\nNote on mAP:\n- Since ground-truth annotations are not provided, we compute a proxy mAP by averaging\n  the sorted detection scores per class (treated as a rough \"AP\" proxy) and then\n  averaging across classes (mAP). This proxy is intended only for relative, informal\n  feedback during inference and not as a formal evaluation metric.\n\"\"\"\n\n# =========================\n# Phase 1: Setup\n# =========================\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Per Programming Guideline 1.1: Import Interpreter literally as below\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2. Paths/Parameters (from CONFIGURATION PARAMETERS)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONFIDENCE_THRESHOLD = float('0.5')\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\ndef load_labels(label_path):\n    \"\"\"\n    1.3. Load Labels if label path is provided.\n    Returns:\n        labels (list[str]): list of label strings\n        label_offset (int): offset to apply for class index mapping (e.g., 1 if first label is '???')\n    \"\"\"\n    labels = []\n    if label_path and os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            labels = [line.strip() for line in f.readlines() if line.strip() != '']\n    else:\n        print(f\"Warning: Label file not found at '{label_path}'. Proceeding with IDs as labels.\")\n        labels = []\n    # Common in TF Lite: first label may be '???'\n    label_offset = 1 if len(labels) > 0 and labels[0].strip().lower() in ('???', 'background') else 0\n    return labels, label_offset\n\ndef initialize_interpreter(model_path):\n    \"\"\"\n    1.4. Load Interpreter and allocate tensors, return input/output details.\n    \"\"\"\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    return interpreter, input_details, output_details\n\ndef get_input_shape_dtype(input_details):\n    \"\"\"\n    1.5. Extract input tensor properties (shape, dtype, index).\n    Returns:\n        input_index, input_height, input_width, input_channels, input_dtype, floating_model (bool)\n    \"\"\"\n    if not input_details or len(input_details) == 0:\n        raise RuntimeError(\"No input tensor details found in the model.\")\n    input_index = input_details[0]['index']\n    shape = input_details[0]['shape']\n    if len(shape) != 4:\n        raise ValueError(f\"Unexpected input tensor shape: {shape}, expected [1, H, W, C].\")\n    input_height, input_width, input_channels = int(shape[1]), int(shape[2]), int(shape[3])\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n    return input_index, input_height, input_width, input_channels, input_dtype, floating_model\n\ndef parse_detection_outputs(interpreter, output_details):\n    \"\"\"\n    4.1. Retrieve raw output tensors and attempt to parse them into standard detection outputs.\n    Tries to be robust across common SSD Mobilenet TFLite detection models.\n\n    Returns:\n        boxes: np.ndarray of shape [N, 4] in order [ymin, xmin, ymax, xmax]\n        classes: np.ndarray of shape [N], int32\n        scores: np.ndarray of shape [N], float32\n        num_detections: int\n    \"\"\"\n    raw_outputs = []\n    for od in output_details:\n        raw_outputs.append(interpreter.get_tensor(od['index']))\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Try to use names if provided\n    for od, out in zip(output_details, raw_outputs):\n        name = od.get('name', '').lower()\n        shp = out.shape\n        if out.ndim == 3 and shp[-1] == 4:\n            boxes = out\n        elif 'box' in name and out.ndim in (2, 3) and shp[-1] == 4:\n            boxes = out\n        elif 'score' in name or 'scores' in name:\n            scores = out\n        elif 'class' in name or 'classes' in name:\n            classes = out\n        elif 'num' in name:\n            num = out\n\n    # If names weren't helpful, infer by shape/value ranges\n    if boxes is None:\n        for out in raw_outputs:\n            if out.ndim == 3 and out.shape[-1] == 4:\n                boxes = out\n                break\n    if scores is None or classes is None:\n        for out in raw_outputs:\n            if out.ndim == 2:\n                # Heuristic: scores are in [0,1]\n                if out.size > 0 and np.amin(out) >= 0.0 and np.amax(out) <= 1.0:\n                    scores = out\n                else:\n                    classes = out\n    if num is None:\n        for out in raw_outputs:\n            if out.ndim == 1 and out.size == 1:\n                num = out\n                break\n\n    # Squeeze to remove batch dimension if present\n    if boxes is not None:\n        boxes = np.squeeze(boxes)\n    if classes is not None:\n        classes = np.squeeze(classes)\n    if scores is not None:\n        scores = np.squeeze(scores)\n    if num is not None:\n        num_detections = int(np.squeeze(num).astype(np.int32))\n    else:\n        # Fallback if num_detections not provided\n        num_detections = int(scores.shape[0]) if scores is not None else 0\n\n    # Final type casting\n    if classes is None:\n        classes = np.zeros((num_detections,), dtype=np.int32)\n    else:\n        classes = classes.astype(np.int32, copy=False)\n    if scores is None:\n        scores = np.zeros((num_detections,), dtype=np.float32)\n    else:\n        scores = scores.astype(np.float32, copy=False)\n\n    # Ensure boxes shape [N, 4]\n    if boxes is None:\n        boxes = np.zeros((num_detections, 4), dtype=np.float32)\n    else:\n        boxes = boxes.astype(np.float32, copy=False)\n        if boxes.ndim == 1 and boxes.size == 4:\n            boxes = boxes.reshape((1, 4))\n        if boxes.shape[0] != num_detections:\n            # Align length if discrepancy\n            min_len = min(boxes.shape[0], num_detections, scores.shape[0])\n            boxes = boxes[:min_len]\n            classes = classes[:min_len]\n            scores = scores[:min_len]\n            num_detections = min_len\n\n    return boxes, classes, scores, num_detections\n\ndef scale_and_clip_box(box, frame_w, frame_h, input_w=None, input_h=None):\n    \"\"\"\n    Scales normalized or absolute boxes to frame coordinates and clips to image bounds.\n    box format: [ymin, xmin, ymax, xmax]\n    \"\"\"\n    ymin, xmin, ymax, xmax = box\n    # Detect normalization: if any coord > 1.0 -> assume absolute in input image space\n    if max(abs(ymin), abs(xmin), abs(ymax), abs(xmax)) <= 1.0:\n        y1 = int(round(ymin * frame_h))\n        x1 = int(round(xmin * frame_w))\n        y2 = int(round(ymax * frame_h))\n        x2 = int(round(xmax * frame_w))\n    else:\n        # If absolute (likely in model input coordinates), scale if input size known\n        if input_w is not None and input_h is not None and input_w > 0 and input_h > 0:\n            scale_x = frame_w / float(input_w)\n            scale_y = frame_h / float(input_h)\n            y1 = int(round(ymin * scale_y))\n            x1 = int(round(xmin * scale_x))\n            y2 = int(round(ymax * scale_y))\n            x2 = int(round(xmax * scale_x))\n        else:\n            y1 = int(round(ymin))\n            x1 = int(round(xmin))\n            y2 = int(round(ymax))\n            x2 = int(round(xmax))\n\n    # Clip to valid ranges\n    x1 = max(0, min(x1, frame_w - 1))\n    y1 = max(0, min(y1, frame_h - 1))\n    x2 = max(0, min(x2, frame_w - 1))\n    y2 = max(0, min(y2, frame_h - 1))\n    return x1, y1, x2, y2\n\ndef color_for_class(class_id):\n    \"\"\"\n    Deterministic color for a given class id.\n    \"\"\"\n    return (int(37 * class_id) % 255, int(17 * class_id) % 255, int(29 * class_id) % 255)\n\ndef compute_map_proxy(scores_per_class):\n    \"\"\"\n    Compute a proxy mAP without ground-truth:\n    - For each class, sort detection scores descending and compute mean(score) as AP proxy.\n    - mAP is the mean of per-class AP proxies over classes that have detections.\n\n    Returns:\n        mAP_proxy (float)\n    \"\"\"\n    if not scores_per_class:\n        return 0.0\n    ap_values = []\n    for cls_id, s_list in scores_per_class.items():\n        if len(s_list) == 0:\n            continue\n        sorted_scores = sorted(s_list, reverse=True)\n        ap_proxy = float(np.mean(sorted_scores))\n        ap_values.append(ap_proxy)\n    if len(ap_values) == 0:\n        return 0.0\n    return float(np.mean(ap_values))\n\ndef main():\n    # Load labels\n    labels, label_offset = load_labels(LABEL_PATH)\n\n    # Initialize interpreter and get model details\n    interpreter, input_details, output_details = initialize_interpreter(MODEL_PATH)\n    input_index, in_h, in_w, in_c, input_dtype, floating_model = get_input_shape_dtype(input_details)\n\n    # 2.1. Acquire Input Data: Open the specified video file\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: Could not open input video: {INPUT_PATH}\")\n        return\n\n    # Retrieve input video properties\n    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # reasonable default fallback\n\n    # Prepare VideoWriter for output\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_w, orig_h))\n    if not writer.isOpened():\n        print(f\"Error: Could not open output video for writing: {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # Accumulator for mAP(proxy)\n    scores_per_class = {}  # dict: class_id -> list of scores\n\n    frame_count = 0\n    t0 = time.time()\n\n    # =========================\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # =========================\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # end of video\n\n        frame_count += 1\n        frame_h, frame_w = frame_bgr.shape[:2]\n\n        # 2.2. Preprocess Data to match model input: BGR -> RGB, resize, batch, dtype handling\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        resized_rgb = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n        input_data = np.expand_dims(resized_rgb, axis=0)\n\n        # 2.3. Quantization Handling\n        if floating_model:\n            input_data = (input_data.astype(np.float32) - 127.5) / 127.5\n        else:\n            input_data = input_data.astype(input_dtype, copy=False)\n\n        # =========================\n        # Phase 3: Inference\n        # =========================\n        interpreter.set_tensor(input_index, input_data)\n        interpreter.invoke()\n\n        # =========================\n        # Phase 4: Output Interpretation & Handling Loop\n        # =========================\n\n        # 4.1. Get Output Tensor(s)\n        boxes, classes, scores, num_detections = parse_detection_outputs(interpreter, output_details)\n\n        # 4.2. Interpret Results\n        # Map class indices to labels, and prepare annotations\n        annotated = frame_bgr.copy()\n        detections_this_frame = 0\n\n        # 4.3. Post-processing: thresholding, coordinate scaling, clipping\n        for i in range(num_detections):\n            score = float(scores[i])\n            if score < CONFIDENCE_THRESHOLD:\n                continue\n            cls_id = int(classes[i])\n            # Map label with offset if needed\n            label_text = None\n            mapped_index = cls_id + (-label_offset) if label_offset > 0 else cls_id\n            if labels and 0 <= mapped_index < len(labels):\n                label_text = labels[mapped_index]\n            else:\n                label_text = f\"id:{cls_id}\"\n\n            # Scale and clip boxes\n            x1, y1, x2, y2 = scale_and_clip_box(boxes[i], frame_w, frame_h, in_w, in_h)\n\n            # Draw bounding box and label\n            color = color_for_class(cls_id)\n            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)\n            label_str = f\"{label_text}: {score:.2f}\"\n            # Put label background for readability\n            (tw, th), baseline = cv2.getTextSize(label_str, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            y_text = max(0, y1 - 5)\n            cv2.rectangle(annotated, (x1, y_text - th - baseline), (x1 + tw, y_text + baseline), color, -1)\n            cv2.putText(annotated, label_str, (x1, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n            # Accumulate scores per class for mAP(proxy)\n            if cls_id not in scores_per_class:\n                scores_per_class[cls_id] = []\n            scores_per_class[cls_id].append(score)\n            detections_this_frame += 1\n\n        # Compute running mAP(proxy)\n        mAP_proxy = compute_map_proxy(scores_per_class)\n\n        # 4.4. Handle Output: Overlay mAP(proxy) and write frame\n        overlay_text = f\"mAP(proxy): {mAP_proxy:.3f} | Detections: {detections_this_frame}\"\n        cv2.putText(annotated, overlay_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 0, 0), 3, cv2.LINE_AA)\n        cv2.putText(annotated, overlay_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255, 255, 255), 1, cv2.LINE_AA)\n        writer.write(annotated)\n\n        # 4.5. Loop Continuation: continues until video ends\n\n    # =========================\n    # Phase 5: Cleanup\n    # =========================\n    cap.release()\n    writer.release()\n\n    total_time = time.time() - t0\n    final_map = compute_map_proxy(scores_per_class)\n    print(f\"Processed {frame_count} frames in {total_time:.2f}s ({(frame_count / total_time) if total_time > 0 else 0:.2f} FPS).\")\n    print(f\"Final mAP(proxy): {final_map:.4f}\")\n    print(f\"Annotated output saved to: {OUTPUT_PATH}\")\n\nif __name__ == '__main__':\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 7367,
            "total": 8867,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.075545,
          "latency": 101279.0,
          "timeToFirstToken": 101.279,
          "updatedAt": "2026-02-05T02:39:13.655Z",
          "totalTokens": 8867,
          "costDetails": {
            "total": 0.075545
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 7367,
          "usageDetails": {
            "input": 1500,
            "output": 7367,
            "total": 8867
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:39:08.615Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:39:58.009Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:37:26.261Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "dba3b065",
      "timestamp": "2026-02-05T02:28:39.050000+00:00",
      "name": "dba3_py_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nDescription:\n- Read a single video file from the given input_path.\n- Run TFLite SSD detection on each frame using ai_edge_litert Interpreter.\n- Draw rectangles and labels for detected objects above the confidence threshold.\n- Calculate and overlay a proxy mAP(mean average precision) per entire processed video.\n- Write annotated video to the provided output_path.\n\nPhases:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video read, resize, normalize/cast)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results: parse outputs, map labels\n  - 4.3 Post-processing: thresholding, scaling to pixels, clipping, NMS for proxy mAP construction\n  - 4.4 Write annotated frames to output video\n- Phase 5: Cleanup\n\nNotes:\n- Only standard libraries (os, time, numpy, cv2) are used alongside the required ai_edge_litert interpreter.\n\"\"\"\n\n# Phase 1: Setup\n# 1.1 Imports\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Import Interpreter literally as required\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (use provided paths and parameters exactly)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (needed for output interpretation)\ndef load_labels(label_path):\n    labels_list = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels_list.append(line)\n    return labels_list\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter and allocate tensors\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input tensor properties\nif len(input_details) == 0:\n    raise RuntimeError(\"No input tensors found in the model.\")\n\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Validate and derive model input size\nif not (len(input_shape) == 4 and int(input_shape[-1]) == 3):\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\nmodel_in_h  = \"gpt-5-2025-08-07\"\nmodel_in_w = int(input_shape[2])  # Width\n\n# ------------------ Helper utilities ------------------\n\ndef class_to_label(cid, labels_list):\n    \"\"\"\n    Robust mapping from class id to label string (handles off-by-one differences).\n    Many SSD label files are 1-based; TFLite outputs are 0-based floats.\n    \"\"\"\n    if not labels_list:\n        return str(cid)\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return str(cid)\n\ndef get_output_tensors(interp, out_details):\n    \"\"\"\n    Extract boxes, classes, scores, num_detections from TFLite SSD outputs.\n    Returns:\n      boxes: np.ndarray [N, 4] normalized [ymin, xmin, ymax, xmax]\n      classes: np.ndarray [N] int\n      scores: np.ndarray [N] float\n      num_det: int\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Try name-based mapping first\n    for od in out_details:\n        name = str(od.get('name', '')).lower()\n        ary = interp.get_tensor(od['index'])\n        arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n        if 'box' in name or 'bbox' in name:\n            boxes = np.asarray(arr, dtype=np.float32)\n        elif 'class' in name:\n            classes = np.asarray(arr).astype(np.int32)\n        elif 'score' in name or 'confidence' in name:\n            scores = np.asarray(arr).astype(np.float32)\n        elif ('num' in name) and ('detection' in name):\n            num_det = int(np.round(float(ary.squeeze())))\n\n    # Fallbacks by shape/dtype if anything missing\n    if boxes is None:\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n            if arr.ndim >= 2 and arr.shape[-1] == 4:\n                boxes = np.asarray(arr, dtype=np.float32)\n                break\n\n    if scores is None or classes is None or num_det is None:\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n            if scores is None and arr.ndim >= 1 and arr.dtype in (np.float32, np.float64):\n                # Candidate for scores if range in [0,1]\n                mn = float(np.min(arr)) if arr.size else 0.0\n                mx = float(np.max(arr)) if arr.size else 0.0\n                if 0.0 <= mn and mx <= 1.0:\n                    scores = np.asarray(arr, dtype=np.float32)\n                    continue\n            if classes is None and arr.ndim >= 1 and arr.dtype in (np.int32, np.int64, np.float32, np.float64):\n                if arr.dtype in (np.float32, np.float64):\n                    classes = np.asarray(arr, dtype=np.float32).astype(np.int32)\n                else:\n                    classes = np.asarray(arr).astype(np.int32)\n            if num_det is None and arr.size == 1:\n                num_det = int(np.round(float(arr.squeeze())))\n\n    # Defaults and trimming\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if num_det is None:\n        num_det = min(len(boxes), len(scores), len(classes))\n\n    n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\ndef preprocess_frame(frame_bgr):\n    \"\"\"\n    Phase 2: Preprocess Data\n    - Convert BGR to RGB\n    - Resize to model input size (model_in_w, model_in_h)\n    - Normalize if floating model: (x - 127.5)/127.5\n    - Cast to input dtype if quantized\n    - Add batch dimension [1, H, W, 3]\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # cv2.resize expects size as (width, height)\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n    if floating_model:\n        input_data = (np.float32(resized) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(resized, dtype=input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef class_color(cid):\n    \"\"\"Deterministic color for a given class id.\"\"\"\n    rng = np.random.RandomState(seed=(int(cid) * 9973) % (2**32))\n    color = rng.randint(0, 256, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef bbox_clip_xyxy(box, w, h):\n    \"\"\"Clip [x1,y1,x2,y2] to image bounds and ensure integer coords.\"\"\"\n    x1, y1, x2, y2 = box\n    x1 = int(max(0, min(round(x1), w - 1)))\n    y1 = int(max(0, min(round(y1), h - 1)))\n    x2 = int(max(0, min(round(x2), w - 1)))\n    y2 = int(max(0, min(round(y2), h - 1)))\n    if x2 < x1:\n        x1, x2 = x2, x1\n    if y2 < y1:\n        y1, y2 = y2, y1\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"IoU for [x1,y1,x2,y2] boxes.\"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    w = max(0.0, xB - xA)\n    h = max(0.0, yB - yA)\n    inter = w * h\n    areaA = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n    areaB = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n    denom = areaA + areaB - inter + 1e-9\n    return inter / denom\n\ndef nms_xyxy(boxes, scores, iou_thresh=0.5):\n    \"\"\"Plain NMS for XYXY boxes; returns kept indices.\"\"\"\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    areas = np.maximum(0.0, x2 - x1) * np.maximum(0.0, y2 - y1)\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_proxy_map(predictions_by_class, iou_thresh=0.5):\n    \"\"\"\n    Compute proxy mAP@0.5 IoU:\n    - For each class and frame, create pseudo ground-truth via NMS on that frame's predictions.\n    - Evaluate AP per class by matching predictions to pseudo GT at IoU>=threshold.\n    - Return mean AP across classes with at least one pseudo GT.\n    \"\"\"\n    per_class_ap = {}\n    valid_classes = []\n\n    for cid, preds in predictions_by_class.items():\n        # Group predictions by frame\n        by_frame = {}\n        for p in preds:\n            f = p['frame']\n            by_frame.setdefault(f, []).append({'box': p['box'], 'score': p['score']})\n\n        # Build pseudo GT via NMS\n        pseudo_gt = {}\n        total_gt = 0\n        for f, plist in by_frame.items():\n            if not plist:\n                pseudo_gt[f] = []\n                continue\n            boxes = [pp['box'] for pp in plist]\n            scores = [pp['score'] for pp in plist]\n            keep_idx = nms_xyxy(boxes, scores, iou_thresh=0.5)\n            gt_boxes = [boxes[k] for k in keep_idx]\n            pseudo_gt[f] = gt_boxes\n            total_gt += len(gt_boxes)\n\n        if total_gt == 0:\n            continue\n\n        valid_classes.append(cid)\n\n        # Flatten and sort predictions by score desc\n        flat_preds = [{'frame': p['frame'], 'box': p['box'], 'score': p['score']} for p in preds]\n        flat_preds.sort(key=lambda x: -x['score'])\n\n        # Track GT assignments per frame\n        gt_matched = {f: np.zeros(len(pseudo_gt[f]), dtype=np.uint8) for f in pseudo_gt}\n\n        tps, fps = [], []\n        for p in flat_preds:\n            f = p['frame']\n            box = p['box']\n            gt_boxes = pseudo_gt.get(f, [])\n            best_iou, best_idx = 0.0, -1\n            for gi, gtb in enumerate(gt_boxes):\n                if gt_matched[f][gi] == 1:\n                    continue\n                iou = iou_xyxy(box, gtb)\n                if iou >= iou_thresh and iou > best_iou:\n                    best_iou = iou\n                    best_idx = gi\n            if best_idx >= 0:\n                gt_matched[f][best_idx] = 1\n                tps.append(1)\n                fps.append(0)\n            else:\n                tps.append(0)\n                fps.append(1)\n\n        if len(tps) == 0:\n            per_class_ap[cid] = 0.0\n            continue\n\n        tps = np.array(tps, dtype=np.float32)\n        fps = np.array(fps, dtype=np.float32)\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / max(total_gt, 1e-9)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0.0, 1.0, 11):\n            p_at_r = precision[recall >= r]\n            p_val = float(np.max(p_at_r)) if p_at_r.size > 0 else 0.0\n            ap += p_val\n        ap /= 11.0\n        per_class_ap[cid] = ap\n\n    mAP = float(np.mean([per_class_ap[c] for c in valid_classes])) if len(valid_classes) > 0 else 0.0\n    return mAP, per_class_ap\n\n# Phase 2: Input Acquisition & Preprocessing Loop (Read a single video file)\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video file not found at: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Prepare output writer with same resolution and fps as input\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or np.isnan(in_fps) or in_fps <= 1e-3:\n    in_fps = 30.0  # fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n# Prepare aggregators for proxy mAP\npredictions_by_class = {}  # cid -> list of {'frame': idx, 'box': [x1,y1,x2,y2], 'score': s}\n\n# Processing loop\nframe_index = 0\nt_start = time.time()\nlast_map = 0.0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    H, W = frame.shape[:2]\n\n    # 2.2 Preprocess Data for the model\n    input_data = preprocess_frame(frame)\n\n    # Phase 3: Inference\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, n_det = get_output_tensors(interpreter, output_details)\n\n    # 4.2 Interpret Results: map labels, prepare detections\n    draw_detections = []       # detections above threshold for drawing\n    all_preds_this_frame = []  # all detections for proxy mAP aggregation\n\n    # 4.3 Post-processing: confidence thresholding, coordinate scaling, clipping\n    for i in range(n_det):\n        score = float(scores_raw[i])\n        cls_id = int(classes_raw[i])\n\n        # boxes are normalized [ymin, xmin, ymax, xmax]; clip to [0,1] before scaling\n        ymin, xmin, ymax, xmax = [float(v) for v in boxes_norm[i]]\n        ymin = min(max(ymin, 0.0), 1.0)\n        xmin = min(max(xmin, 0.0), 1.0)\n        ymax = min(max(ymax, 0.0), 1.0)\n        xmax = min(max(xmax, 0.0), 1.0)\n\n        # Scale to pixel coordinates\n        x1 = xmin * W\n        y1 = ymin * H\n        x2 = xmax * W\n        y2 = ymax * H\n\n        # Clip to valid image bounds and ensure proper ordering\n        x1, y1, x2, y2 = bbox_clip_xyxy([x1, y1, x2, y2], W, H)\n\n        # Aggregate all predictions for proxy mAP (no threshold here)\n        all_preds_this_frame.append({'cid': cls_id, 'box': [x1, y1, x2, y2], 'score': score})\n\n        # Keep only confident detections for visualization\n        if score >= CONF_THRESHOLD:\n            label_text = class_to_label(cls_id, labels)\n            draw_detections.append({\n                'cid': cls_id,\n                'label': label_text,\n                'score': score,\n                'box': [x1, y1, x2, y2]\n            })\n\n    # Update aggregator for proxy mAP\n    for p in all_preds_this_frame:\n        cid = p['cid']\n        predictions_by_class.setdefault(cid, []).append({\n            'frame': frame_index,\n            'box': p['box'],\n            'score': p['score']\n        })\n\n    # Draw detections on the frame\n    for det in draw_detections:\n        x1, y1, x2, y2 = det['box']\n        color = class_color(det['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{det['label']}:{det['score']:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(th + 4, y1)\n        cv2.rectangle(frame, (x1, y_text - th - 4), (x1 + tw + 2, y_text), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y_text - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Compute and overlay proxy mAP after current frame\n    last_map, _ = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n    map_text = f\"mAP@0.5 (proxy): {last_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    out_writer.write(frame)\n\n    # 4.5 Loop continuation\n    frame_index += 1\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - t_start\nfps_proc = (frame_index / elapsed) if elapsed > 0 else 0.0\n\n# Final proxy mAP computed on all frames\nfinal_map, per_class_ap = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n\n# Console summary\nprint(\"Processing complete.\")\nprint(f\"Input Method: Read a single video file from the given input_path\")\nprint(f\"Output Method: Output video with rectangles and labels plus calculated mAP\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}, Processing FPS: {fps_proc:.2f}\")\nprint(f\"Estimated mAP@0.5 (proxy, no ground-truth): {final_map:.4f}\")\n\nif len(per_class_ap) > 0:\n    items = sorted(per_class_ap.items(), key=lambda kv: -kv[1])\n    top_k = min(5, len(items))\n    print(\"Top class APs (proxy):\")\n    for i in range(top_k):\n        cid, ap = items[i]\n        label_name = class_to_label(int(cid), labels)\n        print(f\"  {label_name} (id={cid}): AP={ap:.4f}\")\nelse:\n    print(\"No pseudo-GT could be formed; proxy mAP is 0.0.\")\n\n# -------------------------------------------------------------------------\n# REPORT OF THE FIX FOR THE LAST ERROR:\n# The prior error came from assigning model_in_h to a string, causing cv2.resize\n# to receive an invalid dsize. This script correctly sets:\n#   model_in_h = int(input_shape[1])  # Height\n#   model_in_w = int(input_shape[2])  # Width\n# ensuring cv2.resize gets a tuple of integers (width, height).\n# -------------------------------------------------------------------------",
        "last_error": "Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043714_psg_gpt-5-2025-08-07/tmp_20260205043714_psg_gpt-5-2025-08-07.py\", line 374, in <module>\n    input_data = preprocess_frame(frame)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043714_psg_gpt-5-2025-08-07/tmp_20260205043714_psg_gpt-5-2025-08-07.py\", line 182, in preprocess_frame\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\ncv2.error: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n>  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n\n"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/dba3b065",
      "latency": 515.273,
      "total_cost": 0.3983265,
      "observations": [
        {
          "id": "time-04-28-39-774022_chatcmpl-D5jOiA2gVsIiILLxkqOXalEaD7nfI",
          "traceId": "dba3b065",
          "type": "GENERATION",
          "name": "dba3_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:28:39.774000+00:00",
          "endTime": "2026-02-05T02:31:55.205000+00:00",
          "completionStartTime": "2026-02-05T02:31:55.205000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998272
              }
            },
            "litellm_response_cost": 0.106645,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a given video file using a TFLite SSD MobileNet model,\ndraws detection bounding boxes and labels on each frame, and writes the annotated video to an output file.\nAdditionally, it computes and overlays an estimated mAP@0.5 IoU (proxy without ground-truth) using a pseudo-GT\napproach derived via NMS per frame.\n\nPhases implemented per guideline:\n- Phase 1: Setup (imports, paths, label loading, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (reading frames from video file, preprocessing for model)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2: Interpret results (labels, detection parsing)\n  - 4.3: Post-processing (confidence thresholding, coordinate scaling, clipping, NMS for proxy mAP)\n  - 4.4: Write output video with overlays (boxes, labels, mAP)\n- Phase 5: Cleanup (release video resources)\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Interpreter must be imported literally as specified\nfrom ai_edge_litert.interpreter import Interpreter  # TFLite interpreter (LiteRT)\n\n# 1.2 Paths/Parameters (from configuration)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = float('0.5')  # Confidence Threshold\n\n# Utility: Ensure directories exist for output\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (if needed)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Store input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected shape: [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Helper: Parse model outputs robustly\ndef get_output_tensors(interp, out_details):\n    \"\"\"\n    Attempt to extract boxes, classes, scores, num_detections from TFLite detection model outputs.\n    Returns:\n      boxes: np.ndarray [N, 4] in normalized [ymin, xmin, ymax, xmax]\n      classes: np.ndarray [N] (int)\n      scores: np.ndarray [N] (float)\n      num_det: int\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # First pass: detect by name if available\n    for od in out_details:\n        name = str(od.get('name', '')).lower()\n        ary = interp.get_tensor(od['index'])\n        # Normalize dimensions: often [1, N, ...]\n        if ary.ndim == 3 and ary.shape[0] == 1:\n            ary_use = ary[0]\n        elif ary.ndim == 2 and ary.shape[0] == 1:\n            ary_use = ary[0]\n        else:\n            ary_use = ary\n\n        if 'box' in name or 'bbox' in name:\n            boxes = ary_use\n        elif 'class' in name:\n            classes = ary_use.astype(np.int32)\n        elif 'score' in name:\n            scores = ary_use.astype(np.float32)\n        elif 'num' in name and 'detection' in name:\n            num_det = int(np.round(float(ary.squeeze())))\n\n    # Fallback: infer by shape/dtype\n    if boxes is None or classes is None or scores is None or num_det is None:\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            # Try infer boxes\n            if boxes is None and ary.ndim >= 2 and ary.shape[-1] == 4 and ary.size >= 4:\n                boxes = ary[0] if ary.ndim > 2 else ary\n                continue\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            if ary.ndim >= 2 and ary.shape[-1] != 4:\n                arr = ary[0] if ary.ndim > 2 else ary\n                # Try infer scores/classes\n                if scores is None and arr.dtype in (np.float32, np.float64):\n                    # Scores typically in [0, 1]\n                    mx = float(np.max(arr)) if arr.size > 0 else 0.0\n                    mn = float(np.min(arr)) if arr.size > 0 else 0.0\n                    if 0.0 <= mn and mx <= 1.0:\n                        scores = arr.astype(np.float32)\n                        continue\n                if classes is None:\n                    classes = arr.astype(np.int32)\n                    continue\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            if num_det is None and ary.size == 1:\n                num_det = int(np.round(float(ary.squeeze())))\n                continue\n\n    # Final sanity setup\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if num_det is None:\n        num_det = min(len(scores), len(classes), len(boxes))\n\n    # Clip to num_det if arrays are larger\n    n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\n# Helper: Map class index to label string robustly\ndef class_to_label(cid, labels_list):\n    if not labels_list:\n        return str(cid)\n    # Prefer direct mapping if in range\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    # Fallback to off-by-one\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return str(cid)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n# 2.1 Acquire Input Data: Open the video file from INPUT_PATH\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video file not found at: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Retrieve input video properties\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or np.isnan(in_fps) or in_fps <= 1e-3:\n    in_fps = 30.0  # Fallback FPS\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\ntotal_frames_est = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# VideoWriter for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n# Determine expected input size for the model\n# Typically [1, H, W, 3]\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\nmodel_in_h, model_in_w = int(input_shape[1]), int(input_shape[2])\n\ndef preprocess_frame(frame_bgr):\n    \"\"\"\n    - Convert BGR to RGB\n    - Resize to model input size\n    - Convert to required dtype and normalize if floating model\n    - Add batch dimension\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n    input_data = resized\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(input_data, dtype=input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n# Helper: color for a given class id (deterministic)\ndef class_color(cid):\n    rng = np.random.RandomState(seed=(cid * 9973) % 2**32)\n    color = rng.randint(0, 256, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\n# 4.3 Post-processing helpers (IoU, NMS, mAP estimation)\ndef bbox_clip_xyxy(box, w, h):\n    x1, y1, x2, y2 = box\n    x1 = max(0, min(x1, w - 1))\n    y1 = max(0, min(y1, h - 1))\n    x2 = max(0, min(x2, w - 1))\n    y2 = max(0, min(y2, h - 1))\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    # box: [x1,y1,x2,y2]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0.0, xB - xA)\n    inter_h = max(0.0, yB - yA)\n    inter = inter_w * inter_h\n    areaA = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n    areaB = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n    denom = areaA + areaB - inter + 1e-9\n    return inter / denom\n\ndef nms_xyxy(boxes, scores, iou_thresh=0.5):\n    \"\"\"\n    Non-Maximum Suppression for boxes in [x1,y1,x2,y2]\n    Returns indices of boxes kept.\n    \"\"\"\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = np.maximum(0.0, x2 - x1) * np.maximum(0.0, y2 - y1)\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_proxy_map(predictions_by_class, iou_thresh=0.5):\n    \"\"\"\n    Compute an estimated mAP@0.5 IoU across all classes using a pseudo ground-truth:\n    - For each frame and class, apply NMS to the set of all detections -> pseudo-GT boxes.\n    - Evaluate AP per class by matching predictions to pseudo-GT on a per-frame basis via IoU >= threshold.\n    - Compute mean over classes with at least one pseudo-GT.\n    predictions_by_class:\n      dict[class_id] = list of dicts with keys: 'frame', 'box'=[x1,y1,x2,y2], 'score'\n    Returns:\n      mAP (float), dict of per-class AP (class_id -> AP)\n    \"\"\"\n    per_class_ap = {}\n    classes_with_gt = []\n\n    # Build per-class, per-frame predictions for GT creation\n    for cid, preds in predictions_by_class.items():\n        # per frame predictions\n        frame_dict = {}\n        for p in preds:\n            f = p['frame']\n            frame_dict.setdefault(f, []).append({'box': p['box'], 'score': p['score']})\n\n        # Build pseudo-GT via NMS\n        pseudo_gt = {}  # frame -> list of boxes\n        total_gt = 0\n        for f, plist in frame_dict.items():\n            if not plist:\n                pseudo_gt[f] = []\n                continue\n            boxes = [pp['box'] for pp in plist]\n            scores = [pp['score'] for pp in plist]\n            keep_idx = nms_xyxy(boxes, scores, iou_thresh=0.5)\n            gt_boxes = [boxes[k] for k in keep_idx]\n            pseudo_gt[f] = gt_boxes\n            total_gt += len(gt_boxes)\n\n        if total_gt == 0:\n            # No GT for this class, skip\n            continue\n\n        classes_with_gt.append(cid)\n\n        # Flatten predictions and sort by score\n        flat_preds = [{'frame': p['frame'], 'box': p['box'], 'score': p['score']} for p in preds]\n        flat_preds.sort(key=lambda x: -x['score'])\n\n        # Matched flags per frame for GT\n        gt_matched = {f: np.zeros(len(pseudo_gt[f]), dtype=np.uint8) for f in pseudo_gt}\n\n        tps = []\n        fps = []\n        for p in flat_preds:\n            f = p['frame']\n            box = p['box']\n            # Match against GT in the same frame\n            gt_boxes = pseudo_gt.get(f, [])\n            best_iou = 0.0\n            best_idx = -1\n            for gi, gtb in enumerate(gt_boxes):\n                if gt_matched[f][gi] == 1:\n                    continue\n                iou = iou_xyxy(box, gtb)\n                if iou >= iou_thresh and iou > best_iou:\n                    best_iou = iou\n                    best_idx = gi\n            if best_idx >= 0:\n                gt_matched[f][best_idx] = 1\n                tps.append(1)\n                fps.append(0)\n            else:\n                tps.append(0)\n                fps.append(1)\n\n        if len(tps) == 0:\n            per_class_ap[cid] = 0.0\n            continue\n\n        tps = np.array(tps, dtype=np.float32)\n        fps = np.array(fps, dtype=np.float32)\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / max(total_gt, 1e-9)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0.0, 1.0, 11):\n            p_at_r = precision[recall >= r]\n            p_val = float(np.max(p_at_r)) if p_at_r.size > 0 else 0.0\n            ap += p_val\n        ap /= 11.0\n        per_class_ap[cid] = ap\n\n    if len(classes_with_gt) == 0:\n        mAP = 0.0\n    else:\n        mAP = float(np.mean([per_class_ap[c] for c in classes_with_gt]))\n    return mAP, per_class_ap\n\n# Aggregation for proxy mAP across frames\npredictions_by_class = {}  # cid -> list of {'frame': idx, 'box': [x1,y1,x2,y2], 'score': s}\n\n# Phase 2.4 Loop Control + Phases 3 & 4\nframe_index = 0\nt0 = time.time()\nlast_map = 0.0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    H, W = frame.shape[:2]\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame(frame)\n\n    # 3.1 Set Input Tensors\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, n_det = get_output_tensors(interpreter, output_details)\n\n    # 4.2 Interpret Results: scale boxes, map labels\n    # boxes_norm are [N,4] in [ymin,xmin,ymax,xmax], normalized [0,1]\n    draw_detections = []\n    all_preds_this_frame = []  # For proxy mAP aggregation (include all detections, not only above threshold)\n\n    for i in range(n_det):\n        score = float(scores_raw[i])\n        cls_id = int(classes_raw[i])\n        ymin, xmin, ymax, xmax = boxes_norm[i].tolist()\n\n        # 4.3 Post-processing: scale to pixel coords and clip\n        x1 = int(round(xmin * W))\n        y1 = int(round(ymin * H))\n        x2 = int(round(xmax * W))\n        y2 = int(round(ymax * H))\n        x1, y1, x2, y2 = bbox_clip_xyxy([x1, y1, x2, y2], W, H)\n\n        # Store all predictions for proxy mAP computation\n        all_preds_this_frame.append({'cid': cls_id, 'box': [x1, y1, x2, y2], 'score': score})\n\n        # Only draw if above confidence threshold\n        if score >= CONF_THRESHOLD:\n            label_text = class_to_label(cls_id, labels)\n            draw_detections.append({\n                'cid': cls_id,\n                'label': label_text,\n                'score': score,\n                'box': [x1, y1, x2, y2]\n            })\n\n    # Aggregate predictions for proxy mAP\n    for p in all_preds_this_frame:\n        cid = p['cid']\n        predictions_by_class.setdefault(cid, []).append({\n            'frame': frame_index,\n            'box': p['box'],\n            'score': p['score']\n        })\n\n    # Draw detections\n    for det in draw_detections:\n        x1, y1, x2, y2 = det['box']\n        color = class_color(det['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{det['label']}:{det['score']:.2f}\"\n        # Text background for readability\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - 4), (x1 + tw + 2, y1), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y1 - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Compute and overlay proxy mAP after incorporating this frame\n    last_map, _ = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n    map_text = f\"mAP@0.5 (proxy): {last_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write the annotated frame\n    out_writer.write(frame)\n\n    frame_index += 1\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - t0\nfps_proc = (frame_index / elapsed) if elapsed > 0 else 0.0\n\n# Final proxy mAP computation on all collected predictions (already computed per-frame)\nfinal_map, per_class_ap = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n\n# Print a concise summary\nprint(\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}, Processing FPS: {fps_proc:.2f}\")\nprint(f\"Estimated mAP@0.5 (proxy, no ground-truth): {final_map:.4f}\")\n\n# Optionally, list top few classes with AP values (if labels available)\nif len(per_class_ap) > 0:\n    # Sort classes by AP descending\n    items = sorted(per_class_ap.items(), key=lambda kv: -kv[1])\n    top_k = min(5, len(items))\n    print(\"Top class APs (proxy):\")\n    for i in range(top_k):\n        cid, ap = items[i]\n        label_name = class_to_label(int(cid), labels)\n        print(f\"  {label_name} (id={cid}): AP={ap:.4f}\")\nelse:\n    print(\"No pseudo-GT could be formed (insufficient detections); proxy mAP is 0.0.\")\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 10477,
            "total": 11977,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.106645,
          "latency": 195431.0,
          "timeToFirstToken": 195.431,
          "updatedAt": "2026-02-05T02:32:01.055Z",
          "totalTokens": 11977,
          "costDetails": {
            "total": 0.106645
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 10477,
          "usageDetails": {
            "input": 1500,
            "output": 10477,
            "total": 11977
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:31:55.931Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-31-56-121752_chatcmpl-D5jRs5TTVX1RHLK5738DyR66fdV9T",
          "traceId": "dba3b065",
          "type": "GENERATION",
          "name": "dba3_psg_gen_attempt#2",
          "startTime": "2026-02-05T02:31:56.121000+00:00",
          "endTime": "2026-02-05T02:33:11.681000+00:00",
          "completionStartTime": "2026-02-05T02:33:11.681000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\n- The previous Python script generated has error as logged under ### CAUSED ERROR BEFORE ###.\n- The \"CAUSED ERROR BEFORE\" lists the error message and your previous faulty code caused the error is under ### EXECUTED CODE BEFORE ###. \n- *MAKE SURE YOU USED THE PROVIDED PATHS AND PARAMETERS*\n\n### INSTRUCTIONS ###\n- Regenerate the complete script, adhering strictly to Application Description and Core Requirements.\n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### provided below, REWRITE the complete Python script for the application 'Object Detection via a video file' to avoid the error.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- Input Method Description: 'Read a single video file from the given input_path'\n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path = 'models/ssd-mobilenet_v1/labelmap.txt'  \n- input_path = 'data/object_detection/sheeps.mp4'  \n- output_path = 'results/object_detection/test_results/sheeps_detections.mp4' \n- confidence_threshold = '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n### CAUSED ERROR BEFORE ###\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043155_psg_gpt-5-2025-08-07/tmp_20260205043155_psg_gpt-5-2025-08-07.py\", line 192, in <module>\n    model_in_h, model_in_w  = \"gpt-5-2025-08-07\"\nValueError: too many values to unpack (expected 2)\n\n\n### EXECUTED CODE BEFORE ###\n```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a given video file using a TFLite SSD MobileNet model,\ndraws detection bounding boxes and labels on each frame, and writes the annotated video to an output file.\nAdditionally, it computes and overlays an estimated mAP@0.5 IoU (proxy without ground-truth) using a pseudo-GT\napproach derived via NMS per frame.\n\nPhases implemented per guideline:\n- Phase 1: Setup (imports, paths, label loading, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (reading frames from video file, preprocessing for model)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2: Interpret results (labels, detection parsing)\n  - 4.3: Post-processing (confidence thresholding, coordinate scaling, clipping, NMS for proxy mAP)\n  - 4.4: Write output video with overlays (boxes, labels, mAP)\n- Phase 5: Cleanup (release video resources)\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Interpreter must be imported literally as specified\nfrom ai_edge_litert.interpreter import Interpreter  # TFLite interpreter (LiteRT)\n\n# 1.2 Paths/Parameters (from configuration)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')  # Confidence Threshold\n\n# Utility: Ensure directories exist for output\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (if needed)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Store input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected shape: [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Helper: Parse model outputs robustly\ndef get_output_tensors(interp, out_details):\n    \"\"\"\n    Attempt to extract boxes, classes, scores, num_detections from TFLite detection model outputs.\n    Returns:\n      boxes: np.ndarray [N, 4] in normalized [ymin, xmin, ymax, xmax]\n      classes: np.ndarray [N] (int)\n      scores: np.ndarray [N] (float)\n      num_det: int\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # First pass: detect by name if available\n    for od in out_details:\n        name = str(od.get('name', '')).lower()\n        ary = interp.get_tensor(od['index'])\n        # Normalize dimensions: often [1, N, ...]\n        if ary.ndim == 3 and ary.shape[0] == 1:\n            ary_use = ary[0]\n        elif ary.ndim == 2 and ary.shape[0] == 1:\n            ary_use = ary[0]\n        else:\n            ary_use = ary\n\n        if 'box' in name or 'bbox' in name:\n            boxes = ary_use\n        elif 'class' in name:\n            classes = ary_use.astype(np.int32)\n        elif 'score' in name:\n            scores = ary_use.astype(np.float32)\n        elif 'num' in name and 'detection' in name:\n            num_det = int(np.round(float(ary.squeeze())))\n\n    # Fallback: infer by shape/dtype\n    if boxes is None or classes is None or scores is None or num_det is None:\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            # Try infer boxes\n            if boxes is None and ary.ndim >= 2 and ary.shape[-1] == 4 and ary.size >= 4:\n                boxes = ary[0] if ary.ndim > 2 else ary\n                continue\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            if ary.ndim >= 2 and ary.shape[-1] != 4:\n                arr = ary[0] if ary.ndim > 2 else ary\n                # Try infer scores/classes\n                if scores is None and arr.dtype in (np.float32, np.float64):\n                    # Scores typically in [0, 1]\n                    mx = float(np.max(arr)) if arr.size > 0 else 0.0\n                    mn = float(np.min(arr)) if arr.size > 0 else 0.0\n                    if 0.0 <= mn and mx <= 1.0:\n                        scores = arr.astype(np.float32)\n                        continue\n                if classes is None:\n                    classes = arr.astype(np.int32)\n                    continue\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            if num_det is None and ary.size == 1:\n                num_det = int(np.round(float(ary.squeeze())))\n                continue\n\n    # Final sanity setup\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if num_det is None:\n        num_det = min(len(scores), len(classes), len(boxes))\n\n    # Clip to num_det if arrays are larger\n    n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\n# Helper: Map class index to label string robustly\ndef class_to_label(cid, labels_list):\n    if not labels_list:\n        return str(cid)\n    # Prefer direct mapping if in range\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    # Fallback to off-by-one\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return str(cid)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n# 2.1 Acquire Input Data: Open the video file from INPUT_PATH\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video file not found at: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Retrieve input video properties\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or np.isnan(in_fps) or in_fps <= 1e-3:\n    in_fps = 30.0  # Fallback FPS\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\ntotal_frames_est = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# VideoWriter for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n# Determine expected input size for the model\n# Typically [1, H, W, 3]\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\nmodel_in_h, model_in_w  = \"gpt-5-2025-08-07\"\n\ndef preprocess_frame(frame_bgr):\n    \"\"\"\n    - Convert BGR to RGB\n    - Resize to model input size\n    - Convert to required dtype and normalize if floating model\n    - Add batch dimension\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n    input_data = resized\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(input_data, dtype=input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n# Helper: color for a given class id (deterministic)\ndef class_color(cid):\n    rng = np.random.RandomState(seed=(cid * 9973) % 2**32)\n    color = rng.randint(0, 256, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\n# 4.3 Post-processing helpers (IoU, NMS, mAP estimation)\ndef bbox_clip_xyxy(box, w, h):\n    x1, y1, x2, y2 = box\n    x1 = max(0, min(x1, w - 1))\n    y1 = max(0, min(y1, h - 1))\n    x2 = max(0, min(x2, w - 1))\n    y2 = max(0, min(y2, h - 1))\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    # box: [x1,y1,x2,y2]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0.0, xB - xA)\n    inter_h = max(0.0, yB - yA)\n    inter = inter_w * inter_h\n    areaA = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n    areaB = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n    denom = areaA + areaB - inter + 1e-9\n    return inter / denom\n\ndef nms_xyxy(boxes, scores, iou_thresh=0.5):\n    \"\"\"\n    Non-Maximum Suppression for boxes in [x1,y1,x2,y2]\n    Returns indices of boxes kept.\n    \"\"\"\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = np.maximum(0.0, x2 - x1) * np.maximum(0.0, y2 - y1)\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_proxy_map(predictions_by_class, iou_thresh=0.5):\n    \"\"\"\n    Compute an estimated mAP@0.5 IoU across all classes using a pseudo ground-truth:\n    - For each frame and class, apply NMS to the set of all detections -> pseudo-GT boxes.\n    - Evaluate AP per class by matching predictions to pseudo-GT on a per-frame basis via IoU >= threshold.\n    - Compute mean over classes with at least one pseudo-GT.\n    predictions_by_class:\n      dict[class_id] = list of dicts with keys: 'frame', 'box'=[x1,y1,x2,y2], 'score'\n    Returns:\n      mAP (float), dict of per-class AP (class_id -> AP)\n    \"\"\"\n    per_class_ap = {}\n    classes_with_gt = []\n\n    # Build per-class, per-frame predictions for GT creation\n    for cid, preds in predictions_by_class.items():\n        # per frame predictions\n        frame_dict = {}\n        for p in preds:\n            f = p['frame']\n            frame_dict.setdefault(f, []).append({'box': p['box'], 'score': p['score']})\n\n        # Build pseudo-GT via NMS\n        pseudo_gt = {}  # frame -> list of boxes\n        total_gt = 0\n        for f, plist in frame_dict.items():\n            if not plist:\n                pseudo_gt[f] = []\n                continue\n            boxes = [pp['box'] for pp in plist]\n            scores = [pp['score'] for pp in plist]\n            keep_idx = nms_xyxy(boxes, scores, iou_thresh=0.5)\n            gt_boxes = [boxes[k] for k in keep_idx]\n            pseudo_gt[f] = gt_boxes\n            total_gt += len(gt_boxes)\n\n        if total_gt == 0:\n            # No GT for this class, skip\n            continue\n\n        classes_with_gt.append(cid)\n\n        # Flatten predictions and sort by score\n        flat_preds = [{'frame': p['frame'], 'box': p['box'], 'score': p['score']} for p in preds]\n        flat_preds.sort(key=lambda x: -x['score'])\n\n        # Matched flags per frame for GT\n        gt_matched = {f: np.zeros(len(pseudo_gt[f]), dtype=np.uint8) for f in pseudo_gt}\n\n        tps = []\n        fps = []\n        for p in flat_preds:\n            f = p['frame']\n            box = p['box']\n            # Match against GT in the same frame\n            gt_boxes = pseudo_gt.get(f, [])\n            best_iou = 0.0\n            best_idx = -1\n            for gi, gtb in enumerate(gt_boxes):\n                if gt_matched[f][gi] == 1:\n                    continue\n                iou = iou_xyxy(box, gtb)\n                if iou >= iou_thresh and iou > best_iou:\n                    best_iou = iou\n                    best_idx = gi\n            if best_idx >= 0:\n                gt_matched[f][best_idx] = 1\n                tps.append(1)\n                fps.append(0)\n            else:\n                tps.append(0)\n                fps.append(1)\n\n        if len(tps) == 0:\n            per_class_ap[cid] = 0.0\n            continue\n\n        tps = np.array(tps, dtype=np.float32)\n        fps = np.array(fps, dtype=np.float32)\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / max(total_gt, 1e-9)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0.0, 1.0, 11):\n            p_at_r = precision[recall >= r]\n            p_val = float(np.max(p_at_r)) if p_at_r.size > 0 else 0.0\n            ap += p_val\n        ap /= 11.0\n        per_class_ap[cid] = ap\n\n    if len(classes_with_gt) == 0:\n        mAP = 0.0\n    else:\n        mAP = float(np.mean([per_class_ap[c] for c in classes_with_gt]))\n    return mAP, per_class_ap\n\n# Aggregation for proxy mAP across frames\npredictions_by_class = {}  # cid -> list of {'frame': idx, 'box': [x1,y1,x2,y2], 'score': s}\n\n# Phase 2.4 Loop Control + Phases 3 & 4\nframe_index = 0\nt0 = time.time()\nlast_map = 0.0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    H, W = frame.shape[:2]\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame(frame)\n\n    # 3.1 Set Input Tensors\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, n_det = get_output_tensors(interpreter, output_details)\n\n    # 4.2 Interpret Results: scale boxes, map labels\n    # boxes_norm are [N,4] in [ymin,xmin,ymax,xmax], normalized [0,1]\n    draw_detections = []\n    all_preds_this_frame = []  # For proxy mAP aggregation (include all detections, not only above threshold)\n\n    for i in range(n_det):\n        score = float(scores_raw[i])\n        cls_id = int(classes_raw[i])\n        ymin, xmin, ymax, xmax = boxes_norm[i].tolist()\n\n        # 4.3 Post-processing: scale to pixel coords and clip\n        x1 = int(round(xmin * W))\n        y1 = int(round(ymin * H))\n        x2 = int(round(xmax * W))\n        y2 = int(round(ymax * H))\n        x1, y1, x2, y2 = bbox_clip_xyxy([x1, y1, x2, y2], W, H)\n\n        # Store all predictions for proxy mAP computation\n        all_preds_this_frame.append({'cid': cls_id, 'box': [x1, y1, x2, y2], 'score': score})\n\n        # Only draw if above confidence threshold\n        if score >= CONF_THRESHOLD:\n            label_text = class_to_label(cls_id, labels)\n            draw_detections.append({\n                'cid': cls_id,\n                'label': label_text,\n                'score': score,\n                'box': [x1, y1, x2, y2]\n            })\n\n    # Aggregate predictions for proxy mAP\n    for p in all_preds_this_frame:\n        cid = p['cid']\n        predictions_by_class.setdefault(cid, []).append({\n            'frame': frame_index,\n            'box': p['box'],\n            'score': p['score']\n        })\n\n    # Draw detections\n    for det in draw_detections:\n        x1, y1, x2, y2 = det['box']\n        color = class_color(det['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{det['label']}:{det['score']:.2f}\"\n        # Text background for readability\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - 4), (x1 + tw + 2, y1), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y1 - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Compute and overlay proxy mAP after incorporating this frame\n    last_map, _ = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n    map_text = f\"mAP@0.5 (proxy): {last_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write the annotated frame\n    out_writer.write(frame)\n\n    frame_index += 1\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - t0\nfps_proc = (frame_index / elapsed) if elapsed > 0 else 0.0\n\n# Final proxy mAP computation on all collected predictions (already computed per-frame)\nfinal_map, per_class_ap = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n\n# Print a concise summary\nprint(\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}, Processing FPS: {fps_proc:.2f}\")\nprint(f\"Estimated mAP@0.5 (proxy, no ground-truth): {final_map:.4f}\")\n\n# Optionally, list top few classes with AP values (if labels available)\nif len(per_class_ap) > 0:\n    # Sort classes by AP descending\n    items = sorted(per_class_ap.items(), key=lambda kv: -kv[1])\n    top_k = min(5, len(items))\n    print(\"Top class APs (proxy):\")\n    for i in range(top_k):\n        cid, ap = items[i]\n        label_name = class_to_label(int(cid), labels)\n        print(f\"  {label_name} (id={cid}): AP={ap:.4f}\")\nelse:\n    print(\"No pseudo-GT could be formed (insufficient detections); proxy mAP is 0.0.\")\n```\n\n### OUTPUT TEMPLATE ###\n```python\n<complete_code>\n```\n<REPORT THE FIX OF THE LAST ERROR>\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 993669
              }
            },
            "litellm_response_cost": 0.07042500000000002,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nDescription:\n- Reads a single video file from the given input_path.\n- Runs TFLite SSD detection on each frame.\n- Draws rectangles and labels for detected objects above the confidence threshold.\n- Calculates and overlays a proxy mAP@0.5 IoU (no GT; derived via per-class/frame NMS).\n- Writes annotated video to the provided output_path.\n\nPhases:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video read, resize, normalize/cast)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results: parse outputs, map labels\n  - 4.3 Post-processing: thresholding, scaling to pixels, clipping, NMS for proxy mAP\n  - 4.4 Write annotated frames to output video\n- Phase 5: Cleanup\n\nNote:\n- Only standard libraries and cv2 are used intentionally for video processing, as required.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Import Interpreter literally as required\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (use provided paths and parameters)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = float('0.5')\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (needed for output interpretation)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter and allocate tensors\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Validate and derive model input size (fix for prior error)\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\nmodel_in_h = int(input_shape[1])\nmodel_in_w = int(input_shape[2])\n\n# ------------------ Helper utilities ------------------\n\ndef class_to_label(cid, labels_list):\n    \"\"\"Robust mapping from class id to label string (handles off-by-one variants).\"\"\"\n    if not labels_list:\n        return str(cid)\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return str(cid)\n\ndef get_output_tensors(interp, out_details):\n    \"\"\"\n    Extract boxes, classes, scores, num_detections from TFLite SSD outputs.\n    Returns:\n      boxes: np.ndarray [N, 4] normalized [ymin, xmin, ymax, xmax]\n      classes: np.ndarray [N] int\n      scores: np.ndarray [N] float\n      num_det: int\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Try name-based mapping\n    for od in out_details:\n        name = str(od.get('name', '')).lower()\n        ary = interp.get_tensor(od['index'])\n        if ary.ndim > 1 and ary.shape[0] == 1:\n            arr = ary[0]\n        else:\n            arr = ary\n        if 'box' in name or 'bbox' in name:\n            boxes = arr\n        elif 'class' in name:\n            classes = arr.astype(np.int32)\n        elif 'score' in name:\n            scores = arr.astype(np.float32)\n        elif ('num' in name) and ('detection' in name):\n            num_det = int(np.round(float(ary.squeeze())))\n\n    # Fallback inference by shape/dtype if needed\n    if boxes is None or classes is None or scores is None or num_det is None:\n        # Boxes\n        if boxes is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.ndim >= 2 and ary.shape[-1] == 4:\n                    boxes = ary[0] if ary.ndim > 2 else ary\n                    break\n        # Scores and classes\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n            if scores is None and arr.dtype in (np.float32, np.float64) and arr.ndim >= 1:\n                mn = float(np.min(arr)) if arr.size else 0.0\n                mx = float(np.max(arr)) if arr.size else 0.0\n                if 0.0 <= mn and mx <= 1.0:\n                    scores = arr.astype(np.float32)\n                    continue\n            if classes is None and arr.ndim >= 1 and arr.dtype in (np.int32, np.int64, np.float32, np.float64):\n                if arr.dtype in (np.float32, np.float64):\n                    classes = arr.astype(np.int32)\n                else:\n                    classes = arr.astype(np.int32)\n        # Num detections\n        if num_det is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.size == 1:\n                    num_det = int(np.round(float(ary.squeeze())))\n                    break\n\n    # Defaults and clipping\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if num_det is None:\n        num_det = min(len(boxes), len(scores), len(classes))\n\n    n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\ndef preprocess_frame(frame_bgr):\n    \"\"\"\n    Phase 2: Preprocess Data\n    - Convert BGR to RGB\n    - Resize to model input size (model_in_w, model_in_h)\n    - Normalize if floating model: (x - 127.5)/127.5\n    - Cast to input dtype if quantized\n    - Add batch dimension [1, H, W, 3]\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n    if floating_model:\n        input_data = (np.float32(resized) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(resized, dtype=input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef class_color(cid):\n    \"\"\"Deterministic color for a given class id.\"\"\"\n    rng = np.random.RandomState(seed=(cid * 9973) % (2**32))\n    color = rng.randint(0, 256, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef bbox_clip_xyxy(box, w, h):\n    \"\"\"Clip [x1,y1,x2,y2] to image bounds.\"\"\"\n    x1, y1, x2, y2 = box\n    x1 = max(0, min(int(x1), w - 1))\n    y1 = max(0, min(int(y1), h - 1))\n    x2 = max(0, min(int(x2), w - 1))\n    y2 = max(0, min(int(y2), h - 1))\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"IoU for [x1,y1,x2,y2] boxes.\"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    w = max(0.0, xB - xA)\n    h = max(0.0, yB - yA)\n    inter = w * h\n    areaA = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n    areaB = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n    denom = areaA + areaB - inter + 1e-9\n    return inter / denom\n\ndef nms_xyxy(boxes, scores, iou_thresh=0.5):\n    \"\"\"Plain NMS for XYXY boxes; returns kept indices.\"\"\"\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    areas = np.maximum(0.0, x2 - x1) * np.maximum(0.0, y2 - y1)\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_proxy_map(predictions_by_class, iou_thresh=0.5):\n    \"\"\"\n    Compute proxy mAP@0.5 IoU:\n    - For each class and frame, pseudo GT via NMS on that frame's predictions.\n    - Evaluate AP per class by matching predictions to pseudo GT at IoU>=threshold.\n    - Return mean AP across classes with at least one pseudo GT.\n    \"\"\"\n    per_class_ap = {}\n    valid_classes = []\n\n    for cid, preds in predictions_by_class.items():\n        # Group predictions by frame\n        by_frame = {}\n        for p in preds:\n            f = p['frame']\n            by_frame.setdefault(f, []).append({'box': p['box'], 'score': p['score']})\n\n        # Build pseudo GT via NMS\n        pseudo_gt = {}\n        total_gt = 0\n        for f, plist in by_frame.items():\n            if not plist:\n                pseudo_gt[f] = []\n                continue\n            boxes = [pp['box'] for pp in plist]\n            scores = [pp['score'] for pp in plist]\n            keep_idx = nms_xyxy(boxes, scores, iou_thresh=0.5)\n            gt_boxes = [boxes[k] for k in keep_idx]\n            pseudo_gt[f] = gt_boxes\n            total_gt += len(gt_boxes)\n\n        if total_gt == 0:\n            continue\n\n        valid_classes.append(cid)\n\n        # Flatten and sort predictions by score desc\n        flat_preds = [{'frame': p['frame'], 'box': p['box'], 'score': p['score']} for p in preds]\n        flat_preds.sort(key=lambda x: -x['score'])\n\n        # Track GT assignments per frame\n        gt_matched = {f: np.zeros(len(pseudo_gt[f]), dtype=np.uint8) for f in pseudo_gt}\n\n        tps, fps = [], []\n        for p in flat_preds:\n            f = p['frame']\n            box = p['box']\n            gt_boxes = pseudo_gt.get(f, [])\n            best_iou, best_idx = 0.0, -1\n            for gi, gtb in enumerate(gt_boxes):\n                if gt_matched[f][gi] == 1:\n                    continue\n                iou = iou_xyxy(box, gtb)\n                if iou >= iou_thresh and iou > best_iou:\n                    best_iou = iou\n                    best_idx = gi\n            if best_idx >= 0:\n                gt_matched[f][best_idx] = 1\n                tps.append(1)\n                fps.append(0)\n            else:\n                tps.append(0)\n                fps.append(1)\n\n        if len(tps) == 0:\n            per_class_ap[cid] = 0.0\n            continue\n\n        tps = np.array(tps, dtype=np.float32)\n        fps = np.array(fps, dtype=np.float32)\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / max(total_gt, 1e-9)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0.0, 1.0, 11):\n            p_at_r = precision[recall >= r]\n            p_val = float(np.max(p_at_r)) if p_at_r.size > 0 else 0.0\n            ap += p_val\n        ap /= 11.0\n        per_class_ap[cid] = ap\n\n    mAP = float(np.mean([per_class_ap[c] for c in valid_classes])) if len(valid_classes) > 0 else 0.0\n    return mAP, per_class_ap\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n# 2.1 Acquire Input Data: Read a single video file from INPUT_PATH\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video file not found at: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Prepare output writer with same resolution and fps\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or np.isnan(in_fps) or in_fps <= 1e-3:\n    in_fps = 30.0  # reasonable fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n# Prepare aggregators for proxy mAP\npredictions_by_class = {}  # cid -> list of {'frame': idx, 'box': [x1,y1,x2,y2], 'score': s}\n\n# Processing loop\nframe_index = 0\nt_start = time.time()\nlast_map = 0.0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    H, W = frame.shape[:2]\n\n    # 2.2 Preprocess Data for the model\n    input_data = preprocess_frame(frame)\n\n    # Phase 3: Inference\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, n_det = get_output_tensors(interpreter, output_details)\n\n    # 4.2 Interpret Results: map labels, prepare detections\n    draw_detections = []     # detections above threshold for drawing\n    all_preds_this_frame = []  # all detections for proxy mAP aggregation\n\n    # 4.3 Post-processing: confidence thresholding, scaling, clipping\n    for i in range(n_det):\n        score = float(scores_raw[i])\n        cls_id = int(classes_raw[i])\n\n        # boxes are normalized [ymin, xmin, ymax, xmax]\n        ymin, xmin, ymax, xmax = boxes_norm[i].tolist()\n\n        # Scale to pixel coordinates\n        x1 = xmin * W\n        y1 = ymin * H\n        x2 = xmax * W\n        y2 = ymax * H\n\n        # Clip to valid image bounds\n        x1, y1, x2, y2 = bbox_clip_xyxy([x1, y1, x2, y2], W, H)\n\n        # Aggregate all predictions for proxy mAP (no threshold here)\n        all_preds_this_frame.append({'cid': cls_id, 'box': [x1, y1, x2, y2], 'score': score})\n\n        # Keep only confident detections for visualization\n        if score >= CONF_THRESHOLD:\n            label_text = class_to_label(cls_id, labels)\n            draw_detections.append({\n                'cid': cls_id,\n                'label': label_text,\n                'score': score,\n                'box': [x1, y1, x2, y2]\n            })\n\n    # Update aggregator for proxy mAP\n    for p in all_preds_this_frame:\n        cid = p['cid']\n        predictions_by_class.setdefault(cid, []).append({\n            'frame': frame_index,\n            'box': p['box'],\n            'score': p['score']\n        })\n\n    # Draw detections on the frame\n    for det in draw_detections:\n        x1, y1, x2, y2 = det['box']\n        color = class_color(det['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{det['label']}:{det['score']:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(th + 4, y1)  # ensure visible\n        cv2.rectangle(frame, (x1, y_text - th - 4), (x1 + tw + 2, y_text), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y_text - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Compute and overlay proxy mAP after current frame\n    last_map, _ = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n    map_text = f\"mAP@0.5 (proxy): {last_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    out_writer.write(frame)\n\n    # 4.5 Loop continuation\n    frame_index += 1\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - t_start\nfps_proc = (frame_index / elapsed) if elapsed > 0 else 0.0\n\n# Final proxy mAP computed on all frames\nfinal_map, per_class_ap = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n\n# Console summary\nprint(\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}, Processing FPS: {fps_proc:.2f}\")\nprint(f\"Estimated mAP@0.5 (proxy, no ground-truth): {final_map:.4f}\")\n\nif len(per_class_ap) > 0:\n    # Show top few classes by AP\n    items = sorted(per_class_ap.items(), key=lambda kv: -kv[1])\n    top_k = min(5, len(items))\n    print(\"Top class APs (proxy):\")\n    for i in range(top_k):\n        cid, ap = items[i]\n        label_name = class_to_label(int(cid), labels)\n        print(f\"  {label_name} (id={cid}): AP={ap:.4f}\")\nelse:\n    print(\"No pseudo-GT could be formed; proxy mAP is 0.0.\")\n\n# ------------------------------------------------------------------------------------\n# REPORT OF FIX:\n# Fixed the prior error where model_in_h, model_in_w were incorrectly assigned to a string.\n# Now they are properly derived from input_shape as:\n#   model_in_h = int(input_shape[1])\n#   model_in_w = int(input_shape[2])\n# This resolves the \"too many values to unpack\" ValueError and ensures correct resizing.\n# ------------------------------------------------------------------------------------\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 6756,
            "output": 6198,
            "total": 12954,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.070425,
          "latency": 75560.0,
          "timeToFirstToken": 75.56,
          "updatedAt": "2026-02-05T02:33:17.241Z",
          "totalTokens": 12954,
          "costDetails": {
            "total": 0.070425
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 6198,
          "usageDetails": {
            "input": 6756,
            "output": 6198,
            "total": 12954
          },
          "unit": "TOKENS",
          "promptTokens": 6756,
          "environment": "default",
          "createdAt": "2026-02-05T02:33:12.203Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-33-12-528612_chatcmpl-D5jT6SoimeoMr5oxVSnirqcY6iQTl",
          "traceId": "dba3b065",
          "type": "GENERATION",
          "name": "dba3_psg_gen_attempt#3",
          "startTime": "2026-02-05T02:33:12.528000+00:00",
          "endTime": "2026-02-05T02:34:32.072000+00:00",
          "completionStartTime": "2026-02-05T02:34:32.072000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\n- The previous Python script generated has error as logged under ### CAUSED ERROR BEFORE ###.\n- The \"CAUSED ERROR BEFORE\" lists the error message and your previous faulty code caused the error is under ### EXECUTED CODE BEFORE ###. \n- *MAKE SURE YOU USED THE PROVIDED PATHS AND PARAMETERS*\n\n### INSTRUCTIONS ###\n- Regenerate the complete script, adhering strictly to Application Description and Core Requirements.\n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### provided below, REWRITE the complete Python script for the application 'Object Detection via a video file' to avoid the error.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- Input Method Description: 'Read a single video file from the given input_path'\n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path = 'models/ssd-mobilenet_v1/labelmap.txt'  \n- input_path = 'data/object_detection/sheeps.mp4'  \n- output_path = 'results/object_detection/test_results/sheeps_detections.mp4' \n- confidence_threshold = '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n### CAUSED ERROR BEFORE ###\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043311_psg_gpt-5-2025-08-07/tmp_20260205043311_psg_gpt-5-2025-08-07.py\", line 370, in <module>\n    input_data = preprocess_frame(frame)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043311_psg_gpt-5-2025-08-07/tmp_20260205043311_psg_gpt-5-2025-08-07.py\", line 181, in preprocess_frame\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\ncv2.error: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n>  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n\n\n\n### EXECUTED CODE BEFORE ###\n```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nDescription:\n- Reads a single video file from the given input_path.\n- Runs TFLite SSD detection on each frame.\n- Draws rectangles and labels for detected objects above the confidence threshold.\n- Calculates and overlays a proxy mAP@0.5 IoU (no GT; derived via per-class/frame NMS).\n- Writes annotated video to the provided output_path.\n\nPhases:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video read, resize, normalize/cast)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results: parse outputs, map labels\n  - 4.3 Post-processing: thresholding, scaling to pixels, clipping, NMS for proxy mAP\n  - 4.4 Write annotated frames to output video\n- Phase 5: Cleanup\n\nNote:\n- Only standard libraries and cv2 are used intentionally for video processing, as required.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Import Interpreter literally as required\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (use provided paths and parameters)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (needed for output interpretation)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter and allocate tensors\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Validate and derive model input size (fix for prior error)\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\nmodel_in_h  = \"gpt-5-2025-08-07\"\nmodel_in_w = int(input_shape[2])\n\n# ------------------ Helper utilities ------------------\n\ndef class_to_label(cid, labels_list):\n    \"\"\"Robust mapping from class id to label string (handles off-by-one variants).\"\"\"\n    if not labels_list:\n        return str(cid)\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return str(cid)\n\ndef get_output_tensors(interp, out_details):\n    \"\"\"\n    Extract boxes, classes, scores, num_detections from TFLite SSD outputs.\n    Returns:\n      boxes: np.ndarray [N, 4] normalized [ymin, xmin, ymax, xmax]\n      classes: np.ndarray [N] int\n      scores: np.ndarray [N] float\n      num_det: int\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Try name-based mapping\n    for od in out_details:\n        name = str(od.get('name', '')).lower()\n        ary = interp.get_tensor(od['index'])\n        if ary.ndim > 1 and ary.shape[0] == 1:\n            arr = ary[0]\n        else:\n            arr = ary\n        if 'box' in name or 'bbox' in name:\n            boxes = arr\n        elif 'class' in name:\n            classes = arr.astype(np.int32)\n        elif 'score' in name:\n            scores = arr.astype(np.float32)\n        elif ('num' in name) and ('detection' in name):\n            num_det = int(np.round(float(ary.squeeze())))\n\n    # Fallback inference by shape/dtype if needed\n    if boxes is None or classes is None or scores is None or num_det is None:\n        # Boxes\n        if boxes is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.ndim >= 2 and ary.shape[-1] == 4:\n                    boxes = ary[0] if ary.ndim > 2 else ary\n                    break\n        # Scores and classes\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n            if scores is None and arr.dtype in (np.float32, np.float64) and arr.ndim >= 1:\n                mn = float(np.min(arr)) if arr.size else 0.0\n                mx = float(np.max(arr)) if arr.size else 0.0\n                if 0.0 <= mn and mx <= 1.0:\n                    scores = arr.astype(np.float32)\n                    continue\n            if classes is None and arr.ndim >= 1 and arr.dtype in (np.int32, np.int64, np.float32, np.float64):\n                if arr.dtype in (np.float32, np.float64):\n                    classes = arr.astype(np.int32)\n                else:\n                    classes = arr.astype(np.int32)\n        # Num detections\n        if num_det is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.size == 1:\n                    num_det = int(np.round(float(ary.squeeze())))\n                    break\n\n    # Defaults and clipping\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if num_det is None:\n        num_det = min(len(boxes), len(scores), len(classes))\n\n    n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\ndef preprocess_frame(frame_bgr):\n    \"\"\"\n    Phase 2: Preprocess Data\n    - Convert BGR to RGB\n    - Resize to model input size (model_in_w, model_in_h)\n    - Normalize if floating model: (x - 127.5)/127.5\n    - Cast to input dtype if quantized\n    - Add batch dimension [1, H, W, 3]\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n    if floating_model:\n        input_data = (np.float32(resized) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(resized, dtype=input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef class_color(cid):\n    \"\"\"Deterministic color for a given class id.\"\"\"\n    rng = np.random.RandomState(seed=(cid * 9973) % (2**32))\n    color = rng.randint(0, 256, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef bbox_clip_xyxy(box, w, h):\n    \"\"\"Clip [x1,y1,x2,y2] to image bounds.\"\"\"\n    x1, y1, x2, y2 = box\n    x1 = max(0, min(int(x1), w - 1))\n    y1 = max(0, min(int(y1), h - 1))\n    x2 = max(0, min(int(x2), w - 1))\n    y2 = max(0, min(int(y2), h - 1))\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"IoU for [x1,y1,x2,y2] boxes.\"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    w = max(0.0, xB - xA)\n    h = max(0.0, yB - yA)\n    inter = w * h\n    areaA = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n    areaB = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n    denom = areaA + areaB - inter + 1e-9\n    return inter / denom\n\ndef nms_xyxy(boxes, scores, iou_thresh=0.5):\n    \"\"\"Plain NMS for XYXY boxes; returns kept indices.\"\"\"\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    areas = np.maximum(0.0, x2 - x1) * np.maximum(0.0, y2 - y1)\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_proxy_map(predictions_by_class, iou_thresh=0.5):\n    \"\"\"\n    Compute proxy mAP@0.5 IoU:\n    - For each class and frame, pseudo GT via NMS on that frame's predictions.\n    - Evaluate AP per class by matching predictions to pseudo GT at IoU>=threshold.\n    - Return mean AP across classes with at least one pseudo GT.\n    \"\"\"\n    per_class_ap = {}\n    valid_classes = []\n\n    for cid, preds in predictions_by_class.items():\n        # Group predictions by frame\n        by_frame = {}\n        for p in preds:\n            f = p['frame']\n            by_frame.setdefault(f, []).append({'box': p['box'], 'score': p['score']})\n\n        # Build pseudo GT via NMS\n        pseudo_gt = {}\n        total_gt = 0\n        for f, plist in by_frame.items():\n            if not plist:\n                pseudo_gt[f] = []\n                continue\n            boxes = [pp['box'] for pp in plist]\n            scores = [pp['score'] for pp in plist]\n            keep_idx = nms_xyxy(boxes, scores, iou_thresh=0.5)\n            gt_boxes = [boxes[k] for k in keep_idx]\n            pseudo_gt[f] = gt_boxes\n            total_gt += len(gt_boxes)\n\n        if total_gt == 0:\n            continue\n\n        valid_classes.append(cid)\n\n        # Flatten and sort predictions by score desc\n        flat_preds = [{'frame': p['frame'], 'box': p['box'], 'score': p['score']} for p in preds]\n        flat_preds.sort(key=lambda x: -x['score'])\n\n        # Track GT assignments per frame\n        gt_matched = {f: np.zeros(len(pseudo_gt[f]), dtype=np.uint8) for f in pseudo_gt}\n\n        tps, fps = [], []\n        for p in flat_preds:\n            f = p['frame']\n            box = p['box']\n            gt_boxes = pseudo_gt.get(f, [])\n            best_iou, best_idx = 0.0, -1\n            for gi, gtb in enumerate(gt_boxes):\n                if gt_matched[f][gi] == 1:\n                    continue\n                iou = iou_xyxy(box, gtb)\n                if iou >= iou_thresh and iou > best_iou:\n                    best_iou = iou\n                    best_idx = gi\n            if best_idx >= 0:\n                gt_matched[f][best_idx] = 1\n                tps.append(1)\n                fps.append(0)\n            else:\n                tps.append(0)\n                fps.append(1)\n\n        if len(tps) == 0:\n            per_class_ap[cid] = 0.0\n            continue\n\n        tps = np.array(tps, dtype=np.float32)\n        fps = np.array(fps, dtype=np.float32)\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / max(total_gt, 1e-9)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0.0, 1.0, 11):\n            p_at_r = precision[recall >= r]\n            p_val = float(np.max(p_at_r)) if p_at_r.size > 0 else 0.0\n            ap += p_val\n        ap /= 11.0\n        per_class_ap[cid] = ap\n\n    mAP = float(np.mean([per_class_ap[c] for c in valid_classes])) if len(valid_classes) > 0 else 0.0\n    return mAP, per_class_ap\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n# 2.1 Acquire Input Data: Read a single video file from INPUT_PATH\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video file not found at: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Prepare output writer with same resolution and fps\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or np.isnan(in_fps) or in_fps <= 1e-3:\n    in_fps = 30.0  # reasonable fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n# Prepare aggregators for proxy mAP\npredictions_by_class = {}  # cid -> list of {'frame': idx, 'box': [x1,y1,x2,y2], 'score': s}\n\n# Processing loop\nframe_index = 0\nt_start = time.time()\nlast_map = 0.0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    H, W = frame.shape[:2]\n\n    # 2.2 Preprocess Data for the model\n    input_data = preprocess_frame(frame)\n\n    # Phase 3: Inference\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, n_det = get_output_tensors(interpreter, output_details)\n\n    # 4.2 Interpret Results: map labels, prepare detections\n    draw_detections = []     # detections above threshold for drawing\n    all_preds_this_frame = []  # all detections for proxy mAP aggregation\n\n    # 4.3 Post-processing: confidence thresholding, scaling, clipping\n    for i in range(n_det):\n        score = float(scores_raw[i])\n        cls_id = int(classes_raw[i])\n\n        # boxes are normalized [ymin, xmin, ymax, xmax]\n        ymin, xmin, ymax, xmax = boxes_norm[i].tolist()\n\n        # Scale to pixel coordinates\n        x1 = xmin * W\n        y1 = ymin * H\n        x2 = xmax * W\n        y2 = ymax * H\n\n        # Clip to valid image bounds\n        x1, y1, x2, y2 = bbox_clip_xyxy([x1, y1, x2, y2], W, H)\n\n        # Aggregate all predictions for proxy mAP (no threshold here)\n        all_preds_this_frame.append({'cid': cls_id, 'box': [x1, y1, x2, y2], 'score': score})\n\n        # Keep only confident detections for visualization\n        if score >= CONF_THRESHOLD:\n            label_text = class_to_label(cls_id, labels)\n            draw_detections.append({\n                'cid': cls_id,\n                'label': label_text,\n                'score': score,\n                'box': [x1, y1, x2, y2]\n            })\n\n    # Update aggregator for proxy mAP\n    for p in all_preds_this_frame:\n        cid = p['cid']\n        predictions_by_class.setdefault(cid, []).append({\n            'frame': frame_index,\n            'box': p['box'],\n            'score': p['score']\n        })\n\n    # Draw detections on the frame\n    for det in draw_detections:\n        x1, y1, x2, y2 = det['box']\n        color = class_color(det['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{det['label']}:{det['score']:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(th + 4, y1)  # ensure visible\n        cv2.rectangle(frame, (x1, y_text - th - 4), (x1 + tw + 2, y_text), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y_text - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Compute and overlay proxy mAP after current frame\n    last_map, _ = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n    map_text = f\"mAP@0.5 (proxy): {last_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    out_writer.write(frame)\n\n    # 4.5 Loop continuation\n    frame_index += 1\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - t_start\nfps_proc = (frame_index / elapsed) if elapsed > 0 else 0.0\n\n# Final proxy mAP computed on all frames\nfinal_map, per_class_ap = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n\n# Console summary\nprint(\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}, Processing FPS: {fps_proc:.2f}\")\nprint(f\"Estimated mAP@0.5 (proxy, no ground-truth): {final_map:.4f}\")\n\nif len(per_class_ap) > 0:\n    # Show top few classes by AP\n    items = sorted(per_class_ap.items(), key=lambda kv: -kv[1])\n    top_k = min(5, len(items))\n    print(\"Top class APs (proxy):\")\n    for i in range(top_k):\n        cid, ap = items[i]\n        label_name = class_to_label(int(cid), labels)\n        print(f\"  {label_name} (id={cid}): AP={ap:.4f}\")\nelse:\n    print(\"No pseudo-GT could be formed; proxy mAP is 0.0.\")\n\n# ------------------------------------------------------------------------------------\n# REPORT OF FIX:\n# Fixed the prior error where model_in_h, model_in_w were incorrectly assigned to a string.\n# Now they are properly derived from input_shape as:\n#   model_in_h = int(input_shape[1])\n#   model_in_w = int(input_shape[2])\n# This resolves the \"too many values to unpack\" ValueError and ensures correct resizing.\n# ------------------------------------------------------------------------------------\n```\n\n### OUTPUT TEMPLATE ###\n```python\n<complete_code>\n```\n<REPORT THE FIX OF THE LAST ERROR>\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 993604
              }
            },
            "litellm_response_cost": 0.07540325,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nDescription:\n- Reads a single video file from the given input_path.\n- Runs TFLite SSD detection on each frame.\n- Draws rectangles and labels for detected objects above the confidence threshold.\n- Calculates and overlays a proxy mAP@0.5 IoU (no GT; derived via per-class/frame NMS).\n- Writes annotated video to the provided output_path.\n\nPhases:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video read, resize, normalize/cast)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results: parse outputs, map labels\n  - 4.3 Post-processing: thresholding, scaling to pixels, clipping, NMS for proxy mAP\n  - 4.4 Write annotated frames to output video\n- Phase 5: Cleanup\n\nNote:\n- Only standard libraries and cv2 are used intentionally for video processing, as required.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Import Interpreter literally as required\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (use provided paths and parameters)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = float('0.5')\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (needed for output interpretation)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter and allocate tensors\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Validate and derive model input size\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\nmodel_in_h = int(input_shape[1])\nmodel_in_w = int(input_shape[2])\n\n# ------------------ Helper utilities ------------------\n\ndef class_to_label(cid, labels_list):\n    \"\"\"Robust mapping from class id to label string (handles off-by-one variants).\"\"\"\n    if not labels_list:\n        return str(cid)\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return str(cid)\n\ndef get_output_tensors(interp, out_details):\n    \"\"\"\n    Extract boxes, classes, scores, num_detections from TFLite SSD outputs.\n    Returns:\n      boxes: np.ndarray [N, 4] normalized [ymin, xmin, ymax, xmax]\n      classes: np.ndarray [N] int\n      scores: np.ndarray [N] float\n      num_det: int\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Try name-based mapping\n    for od in out_details:\n        name = str(od.get('name', '')).lower()\n        ary = interp.get_tensor(od['index'])\n        if ary.ndim > 1 and ary.shape[0] == 1:\n            arr = ary[0]\n        else:\n            arr = ary\n        if 'box' in name or 'bbox' in name:\n            boxes = arr\n        elif 'class' in name:\n            classes = arr.astype(np.int32)\n        elif 'score' in name:\n            scores = arr.astype(np.float32)\n        elif ('num' in name) and ('detection' in name):\n            num_det = int(np.round(float(ary.squeeze())))\n\n    # Fallback inference by shape/dtype if needed\n    if boxes is None or classes is None or scores is None or num_det is None:\n        # Boxes\n        if boxes is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.ndim >= 2 and ary.shape[-1] == 4:\n                    boxes = ary[0] if ary.ndim > 2 else ary\n                    break\n        # Scores and classes\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n            if scores is None and arr.dtype in (np.float32, np.float64) and arr.ndim >= 1:\n                mn = float(np.min(arr)) if arr.size else 0.0\n                mx = float(np.max(arr)) if arr.size else 0.0\n                if 0.0 <= mn and mx <= 1.0:\n                    scores = arr.astype(np.float32)\n                    continue\n            if classes is None and arr.ndim >= 1 and arr.dtype in (np.int32, np.int64, np.float32, np.float64):\n                if arr.dtype in (np.float32, np.float64):\n                    classes = arr.astype(np.int32)\n                else:\n                    classes = arr.astype(np.int32)\n        # Num detections\n        if num_det is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.size == 1:\n                    num_det = int(np.round(float(ary.squeeze())))\n                    break\n\n    # Defaults and clipping\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if num_det is None:\n        num_det = min(len(boxes), len(scores), len(classes))\n\n    n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\ndef preprocess_frame(frame_bgr):\n    \"\"\"\n    Phase 2: Preprocess Data\n    - Convert BGR to RGB\n    - Resize to model input size (model_in_w, model_in_h)\n    - Normalize if floating model: (x - 127.5)/127.5\n    - Cast to input dtype if quantized\n    - Add batch dimension [1, H, W, 3]\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # cv2.resize expects (width, height)\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n    if floating_model:\n        input_data = (np.float32(resized) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(resized, dtype=input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef class_color(cid):\n    \"\"\"Deterministic color for a given class id.\"\"\"\n    rng = np.random.RandomState(seed=(cid * 9973) % (2**32))\n    color = rng.randint(0, 256, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef bbox_clip_xyxy(box, w, h):\n    \"\"\"Clip [x1,y1,x2,y2] to image bounds and ensure integer coords.\"\"\"\n    x1, y1, x2, y2 = box\n    x1 = int(max(0, min(round(x1), w - 1)))\n    y1 = int(max(0, min(round(y1), h - 1)))\n    x2 = int(max(0, min(round(x2), w - 1)))\n    y2 = int(max(0, min(round(y2), h - 1)))\n    # Ensure proper ordering\n    if x2 < x1:\n        x1, x2 = x2, x1\n    if y2 < y1:\n        y1, y2 = y2, y1\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"IoU for [x1,y1,x2,y2] boxes.\"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    w = max(0.0, xB - xA)\n    h = max(0.0, yB - yA)\n    inter = w * h\n    areaA = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n    areaB = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n    denom = areaA + areaB - inter + 1e-9\n    return inter / denom\n\ndef nms_xyxy(boxes, scores, iou_thresh=0.5):\n    \"\"\"Plain NMS for XYXY boxes; returns kept indices.\"\"\"\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    areas = np.maximum(0.0, x2 - x1) * np.maximum(0.0, y2 - y1)\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_proxy_map(predictions_by_class, iou_thresh=0.5):\n    \"\"\"\n    Compute proxy mAP@0.5 IoU:\n    - For each class and frame, pseudo GT via NMS on that frame's predictions.\n    - Evaluate AP per class by matching predictions to pseudo GT at IoU>=threshold.\n    - Return mean AP across classes with at least one pseudo GT.\n    \"\"\"\n    per_class_ap = {}\n    valid_classes = []\n\n    for cid, preds in predictions_by_class.items():\n        # Group predictions by frame\n        by_frame = {}\n        for p in preds:\n            f = p['frame']\n            by_frame.setdefault(f, []).append({'box': p['box'], 'score': p['score']})\n\n        # Build pseudo GT via NMS\n        pseudo_gt = {}\n        total_gt = 0\n        for f, plist in by_frame.items():\n            if not plist:\n                pseudo_gt[f] = []\n                continue\n            boxes = [pp['box'] for pp in plist]\n            scores = [pp['score'] for pp in plist]\n            keep_idx = nms_xyxy(boxes, scores, iou_thresh=0.5)\n            gt_boxes = [boxes[k] for k in keep_idx]\n            pseudo_gt[f] = gt_boxes\n            total_gt += len(gt_boxes)\n\n        if total_gt == 0:\n            continue\n\n        valid_classes.append(cid)\n\n        # Flatten and sort predictions by score desc\n        flat_preds = [{'frame': p['frame'], 'box': p['box'], 'score': p['score']} for p in preds]\n        flat_preds.sort(key=lambda x: -x['score'])\n\n        # Track GT assignments per frame\n        gt_matched = {f: np.zeros(len(pseudo_gt[f]), dtype=np.uint8) for f in pseudo_gt}\n\n        tps, fps = [], []\n        for p in flat_preds:\n            f = p['frame']\n            box = p['box']\n            gt_boxes = pseudo_gt.get(f, [])\n            best_iou, best_idx = 0.0, -1\n            for gi, gtb in enumerate(gt_boxes):\n                if gt_matched[f][gi] == 1:\n                    continue\n                iou = iou_xyxy(box, gtb)\n                if iou >= iou_thresh and iou > best_iou:\n                    best_iou = iou\n                    best_idx = gi\n            if best_idx >= 0:\n                gt_matched[f][best_idx] = 1\n                tps.append(1)\n                fps.append(0)\n            else:\n                tps.append(0)\n                fps.append(1)\n\n        if len(tps) == 0:\n            per_class_ap[cid] = 0.0\n            continue\n\n        tps = np.array(tps, dtype=np.float32)\n        fps = np.array(fps, dtype=np.float32)\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / max(total_gt, 1e-9)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0.0, 1.0, 11):\n            p_at_r = precision[recall >= r]\n            p_val = float(np.max(p_at_r)) if p_at_r.size > 0 else 0.0\n            ap += p_val\n        ap /= 11.0\n        per_class_ap[cid] = ap\n\n    mAP = float(np.mean([per_class_ap[c] for c in valid_classes])) if len(valid_classes) > 0 else 0.0\n    return mAP, per_class_ap\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n# 2.1 Acquire Input Data: Read a single video file from INPUT_PATH\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video file not found at: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Prepare output writer with same resolution and fps\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or np.isnan(in_fps) or in_fps <= 1e-3:\n    in_fps = 30.0  # reasonable fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n# Prepare aggregators for proxy mAP\npredictions_by_class = {}  # cid -> list of {'frame': idx, 'box': [x1,y1,x2,y2], 'score': s}\n\n# Processing loop\nframe_index = 0\nt_start = time.time()\nlast_map = 0.0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    H, W = frame.shape[:2]\n\n    # 2.2 Preprocess Data for the model\n    input_data = preprocess_frame(frame)\n\n    # Phase 3: Inference\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, n_det = get_output_tensors(interpreter, output_details)\n\n    # 4.2 Interpret Results: map labels, prepare detections\n    draw_detections = []       # detections above threshold for drawing\n    all_preds_this_frame = []  # all detections for proxy mAP aggregation\n\n    # 4.3 Post-processing: confidence thresholding, coordinate scaling, clipping\n    for i in range(n_det):\n        score = float(scores_raw[i])\n        cls_id = int(classes_raw[i])\n\n        # boxes are normalized [ymin, xmin, ymax, xmax]; clip to [0,1] before scaling\n        ymin, xmin, ymax, xmax = boxes_norm[i].tolist()\n        ymin = min(max(float(ymin), 0.0), 1.0)\n        xmin = min(max(float(xmin), 0.0), 1.0)\n        ymax = min(max(float(ymax), 0.0), 1.0)\n        xmax = min(max(float(xmax), 0.0), 1.0)\n\n        # Scale to pixel coordinates\n        x1 = xmin * W\n        y1 = ymin * H\n        x2 = xmax * W\n        y2 = ymax * H\n\n        # Clip to valid image bounds and ensure correct ordering\n        x1, y1, x2, y2 = bbox_clip_xyxy([x1, y1, x2, y2], W, H)\n\n        # Aggregate all predictions for proxy mAP (no threshold here)\n        all_preds_this_frame.append({'cid': cls_id, 'box': [x1, y1, x2, y2], 'score': score})\n\n        # Keep only confident detections for visualization\n        if score >= CONF_THRESHOLD:\n            label_text = class_to_label(cls_id, labels)\n            draw_detections.append({\n                'cid': cls_id,\n                'label': label_text,\n                'score': score,\n                'box': [x1, y1, x2, y2]\n            })\n\n    # Update aggregator for proxy mAP\n    for p in all_preds_this_frame:\n        cid = p['cid']\n        predictions_by_class.setdefault(cid, []).append({\n            'frame': frame_index,\n            'box': p['box'],\n            'score': p['score']\n        })\n\n    # Draw detections on the frame\n    for det in draw_detections:\n        x1, y1, x2, y2 = det['box']\n        color = class_color(det['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{det['label']}:{det['score']:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(th + 4, y1)\n        cv2.rectangle(frame, (x1, y_text - th - 4), (x1 + tw + 2, y_text), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y_text - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Compute and overlay proxy mAP after current frame\n    last_map, _ = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n    map_text = f\"mAP@0.5 (proxy): {last_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    out_writer.write(frame)\n\n    # 4.5 Loop continuation\n    frame_index += 1\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - t_start\nfps_proc = (frame_index / elapsed) if elapsed > 0 else 0.0\n\n# Final proxy mAP computed on all frames\nfinal_map, per_class_ap = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n\n# Console summary\nprint(\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}, Processing FPS: {fps_proc:.2f}\")\nprint(f\"Estimated mAP@0.5 (proxy, no ground-truth): {final_map:.4f}\")\n\nif len(per_class_ap) > 0:\n    # Show top few classes by AP\n    items = sorted(per_class_ap.items(), key=lambda kv: -kv[1])\n    top_k = min(5, len(items))\n    print(\"Top class APs (proxy):\")\n    for i in range(top_k):\n        cid, ap = items[i]\n        label_name = class_to_label(int(cid), labels)\n        print(f\"  {label_name} (id={cid}): AP={ap:.4f}\")\nelse:\n    print(\"No pseudo-GT could be formed; proxy mAP is 0.0.\")\n```\n\nFixed the previous error by correctly deriving model input dimensions from the interpreter's input tensor shape:\n- Replaced the incorrect string assignment with:\n  model_in_h = int(input_shape[1])\n  model_in_w = int(input_shape[2])\nThis ensures cv2.resize receives a valid (width, height) tuple of integers, preventing the dsize type error. Additionally, added clipping of normalized box coordinates to [0,1] before scaling.",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 6785,
            "output": 6865,
            "total": 13650,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07540325,
          "latency": 79544.0,
          "timeToFirstToken": 79.544,
          "updatedAt": "2026-02-05T02:34:37.435Z",
          "totalTokens": 13650,
          "costDetails": {
            "total": 0.07540325
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 6865,
          "usageDetails": {
            "input": 6785,
            "output": 6865,
            "total": 13650
          },
          "unit": "TOKENS",
          "promptTokens": 6785,
          "environment": "default",
          "createdAt": "2026-02-05T02:34:32.316Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-34-32-983832_chatcmpl-D5jUPf1SjlGVwxdfbqJBFWRxToN5h",
          "traceId": "dba3b065",
          "type": "GENERATION",
          "name": "dba3_psg_gen_attempt#4",
          "startTime": "2026-02-05T02:34:32.983000+00:00",
          "endTime": "2026-02-05T02:35:55.582000+00:00",
          "completionStartTime": "2026-02-05T02:35:55.582000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\n- The previous Python script generated has error as logged under ### CAUSED ERROR BEFORE ###.\n- The \"CAUSED ERROR BEFORE\" lists the error message and your previous faulty code caused the error is under ### EXECUTED CODE BEFORE ###. \n- *MAKE SURE YOU USED THE PROVIDED PATHS AND PARAMETERS*\n\n### INSTRUCTIONS ###\n- Regenerate the complete script, adhering strictly to Application Description and Core Requirements.\n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### provided below, REWRITE the complete Python script for the application 'Object Detection via a video file' to avoid the error.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- Input Method Description: 'Read a single video file from the given input_path'\n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path = 'models/ssd-mobilenet_v1/labelmap.txt'  \n- input_path = 'data/object_detection/sheeps.mp4'  \n- output_path = 'results/object_detection/test_results/sheeps_detections.mp4' \n- confidence_threshold = '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n### CAUSED ERROR BEFORE ###\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043432_psg_gpt-5-2025-08-07/tmp_20260205043432_psg_gpt-5-2025-08-07.py\", line 376, in <module>\n    input_data = preprocess_frame(frame)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043432_psg_gpt-5-2025-08-07/tmp_20260205043432_psg_gpt-5-2025-08-07.py\", line 182, in preprocess_frame\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\ncv2.error: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n>  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n\n\n\n### EXECUTED CODE BEFORE ###\n```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nDescription:\n- Reads a single video file from the given input_path.\n- Runs TFLite SSD detection on each frame.\n- Draws rectangles and labels for detected objects above the confidence threshold.\n- Calculates and overlays a proxy mAP@0.5 IoU (no GT; derived via per-class/frame NMS).\n- Writes annotated video to the provided output_path.\n\nPhases:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video read, resize, normalize/cast)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results: parse outputs, map labels\n  - 4.3 Post-processing: thresholding, scaling to pixels, clipping, NMS for proxy mAP\n  - 4.4 Write annotated frames to output video\n- Phase 5: Cleanup\n\nNote:\n- Only standard libraries and cv2 are used intentionally for video processing, as required.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Import Interpreter literally as required\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (use provided paths and parameters)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (needed for output interpretation)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter and allocate tensors\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Validate and derive model input size\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\nmodel_in_h  = \"gpt-5-2025-08-07\"\nmodel_in_w = int(input_shape[2])\n\n# ------------------ Helper utilities ------------------\n\ndef class_to_label(cid, labels_list):\n    \"\"\"Robust mapping from class id to label string (handles off-by-one variants).\"\"\"\n    if not labels_list:\n        return str(cid)\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return str(cid)\n\ndef get_output_tensors(interp, out_details):\n    \"\"\"\n    Extract boxes, classes, scores, num_detections from TFLite SSD outputs.\n    Returns:\n      boxes: np.ndarray [N, 4] normalized [ymin, xmin, ymax, xmax]\n      classes: np.ndarray [N] int\n      scores: np.ndarray [N] float\n      num_det: int\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Try name-based mapping\n    for od in out_details:\n        name = str(od.get('name', '')).lower()\n        ary = interp.get_tensor(od['index'])\n        if ary.ndim > 1 and ary.shape[0] == 1:\n            arr = ary[0]\n        else:\n            arr = ary\n        if 'box' in name or 'bbox' in name:\n            boxes = arr\n        elif 'class' in name:\n            classes = arr.astype(np.int32)\n        elif 'score' in name:\n            scores = arr.astype(np.float32)\n        elif ('num' in name) and ('detection' in name):\n            num_det = int(np.round(float(ary.squeeze())))\n\n    # Fallback inference by shape/dtype if needed\n    if boxes is None or classes is None or scores is None or num_det is None:\n        # Boxes\n        if boxes is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.ndim >= 2 and ary.shape[-1] == 4:\n                    boxes = ary[0] if ary.ndim > 2 else ary\n                    break\n        # Scores and classes\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n            if scores is None and arr.dtype in (np.float32, np.float64) and arr.ndim >= 1:\n                mn = float(np.min(arr)) if arr.size else 0.0\n                mx = float(np.max(arr)) if arr.size else 0.0\n                if 0.0 <= mn and mx <= 1.0:\n                    scores = arr.astype(np.float32)\n                    continue\n            if classes is None and arr.ndim >= 1 and arr.dtype in (np.int32, np.int64, np.float32, np.float64):\n                if arr.dtype in (np.float32, np.float64):\n                    classes = arr.astype(np.int32)\n                else:\n                    classes = arr.astype(np.int32)\n        # Num detections\n        if num_det is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.size == 1:\n                    num_det = int(np.round(float(ary.squeeze())))\n                    break\n\n    # Defaults and clipping\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if num_det is None:\n        num_det = min(len(boxes), len(scores), len(classes))\n\n    n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\ndef preprocess_frame(frame_bgr):\n    \"\"\"\n    Phase 2: Preprocess Data\n    - Convert BGR to RGB\n    - Resize to model input size (model_in_w, model_in_h)\n    - Normalize if floating model: (x - 127.5)/127.5\n    - Cast to input dtype if quantized\n    - Add batch dimension [1, H, W, 3]\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # cv2.resize expects (width, height)\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n    if floating_model:\n        input_data = (np.float32(resized) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(resized, dtype=input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef class_color(cid):\n    \"\"\"Deterministic color for a given class id.\"\"\"\n    rng = np.random.RandomState(seed=(cid * 9973) % (2**32))\n    color = rng.randint(0, 256, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef bbox_clip_xyxy(box, w, h):\n    \"\"\"Clip [x1,y1,x2,y2] to image bounds and ensure integer coords.\"\"\"\n    x1, y1, x2, y2 = box\n    x1 = int(max(0, min(round(x1), w - 1)))\n    y1 = int(max(0, min(round(y1), h - 1)))\n    x2 = int(max(0, min(round(x2), w - 1)))\n    y2 = int(max(0, min(round(y2), h - 1)))\n    # Ensure proper ordering\n    if x2 < x1:\n        x1, x2 = x2, x1\n    if y2 < y1:\n        y1, y2 = y2, y1\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"IoU for [x1,y1,x2,y2] boxes.\"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    w = max(0.0, xB - xA)\n    h = max(0.0, yB - yA)\n    inter = w * h\n    areaA = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n    areaB = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n    denom = areaA + areaB - inter + 1e-9\n    return inter / denom\n\ndef nms_xyxy(boxes, scores, iou_thresh=0.5):\n    \"\"\"Plain NMS for XYXY boxes; returns kept indices.\"\"\"\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    areas = np.maximum(0.0, x2 - x1) * np.maximum(0.0, y2 - y1)\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_proxy_map(predictions_by_class, iou_thresh=0.5):\n    \"\"\"\n    Compute proxy mAP@0.5 IoU:\n    - For each class and frame, pseudo GT via NMS on that frame's predictions.\n    - Evaluate AP per class by matching predictions to pseudo GT at IoU>=threshold.\n    - Return mean AP across classes with at least one pseudo GT.\n    \"\"\"\n    per_class_ap = {}\n    valid_classes = []\n\n    for cid, preds in predictions_by_class.items():\n        # Group predictions by frame\n        by_frame = {}\n        for p in preds:\n            f = p['frame']\n            by_frame.setdefault(f, []).append({'box': p['box'], 'score': p['score']})\n\n        # Build pseudo GT via NMS\n        pseudo_gt = {}\n        total_gt = 0\n        for f, plist in by_frame.items():\n            if not plist:\n                pseudo_gt[f] = []\n                continue\n            boxes = [pp['box'] for pp in plist]\n            scores = [pp['score'] for pp in plist]\n            keep_idx = nms_xyxy(boxes, scores, iou_thresh=0.5)\n            gt_boxes = [boxes[k] for k in keep_idx]\n            pseudo_gt[f] = gt_boxes\n            total_gt += len(gt_boxes)\n\n        if total_gt == 0:\n            continue\n\n        valid_classes.append(cid)\n\n        # Flatten and sort predictions by score desc\n        flat_preds = [{'frame': p['frame'], 'box': p['box'], 'score': p['score']} for p in preds]\n        flat_preds.sort(key=lambda x: -x['score'])\n\n        # Track GT assignments per frame\n        gt_matched = {f: np.zeros(len(pseudo_gt[f]), dtype=np.uint8) for f in pseudo_gt}\n\n        tps, fps = [], []\n        for p in flat_preds:\n            f = p['frame']\n            box = p['box']\n            gt_boxes = pseudo_gt.get(f, [])\n            best_iou, best_idx = 0.0, -1\n            for gi, gtb in enumerate(gt_boxes):\n                if gt_matched[f][gi] == 1:\n                    continue\n                iou = iou_xyxy(box, gtb)\n                if iou >= iou_thresh and iou > best_iou:\n                    best_iou = iou\n                    best_idx = gi\n            if best_idx >= 0:\n                gt_matched[f][best_idx] = 1\n                tps.append(1)\n                fps.append(0)\n            else:\n                tps.append(0)\n                fps.append(1)\n\n        if len(tps) == 0:\n            per_class_ap[cid] = 0.0\n            continue\n\n        tps = np.array(tps, dtype=np.float32)\n        fps = np.array(fps, dtype=np.float32)\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / max(total_gt, 1e-9)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0.0, 1.0, 11):\n            p_at_r = precision[recall >= r]\n            p_val = float(np.max(p_at_r)) if p_at_r.size > 0 else 0.0\n            ap += p_val\n        ap /= 11.0\n        per_class_ap[cid] = ap\n\n    mAP = float(np.mean([per_class_ap[c] for c in valid_classes])) if len(valid_classes) > 0 else 0.0\n    return mAP, per_class_ap\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n# 2.1 Acquire Input Data: Read a single video file from INPUT_PATH\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video file not found at: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Prepare output writer with same resolution and fps\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or np.isnan(in_fps) or in_fps <= 1e-3:\n    in_fps = 30.0  # reasonable fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n# Prepare aggregators for proxy mAP\npredictions_by_class = {}  # cid -> list of {'frame': idx, 'box': [x1,y1,x2,y2], 'score': s}\n\n# Processing loop\nframe_index = 0\nt_start = time.time()\nlast_map = 0.0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    H, W = frame.shape[:2]\n\n    # 2.2 Preprocess Data for the model\n    input_data = preprocess_frame(frame)\n\n    # Phase 3: Inference\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, n_det = get_output_tensors(interpreter, output_details)\n\n    # 4.2 Interpret Results: map labels, prepare detections\n    draw_detections = []       # detections above threshold for drawing\n    all_preds_this_frame = []  # all detections for proxy mAP aggregation\n\n    # 4.3 Post-processing: confidence thresholding, coordinate scaling, clipping\n    for i in range(n_det):\n        score = float(scores_raw[i])\n        cls_id = int(classes_raw[i])\n\n        # boxes are normalized [ymin, xmin, ymax, xmax]; clip to [0,1] before scaling\n        ymin, xmin, ymax, xmax = boxes_norm[i].tolist()\n        ymin = min(max(float(ymin), 0.0), 1.0)\n        xmin = min(max(float(xmin), 0.0), 1.0)\n        ymax = min(max(float(ymax), 0.0), 1.0)\n        xmax = min(max(float(xmax), 0.0), 1.0)\n\n        # Scale to pixel coordinates\n        x1 = xmin * W\n        y1 = ymin * H\n        x2 = xmax * W\n        y2 = ymax * H\n\n        # Clip to valid image bounds and ensure correct ordering\n        x1, y1, x2, y2 = bbox_clip_xyxy([x1, y1, x2, y2], W, H)\n\n        # Aggregate all predictions for proxy mAP (no threshold here)\n        all_preds_this_frame.append({'cid': cls_id, 'box': [x1, y1, x2, y2], 'score': score})\n\n        # Keep only confident detections for visualization\n        if score >= CONF_THRESHOLD:\n            label_text = class_to_label(cls_id, labels)\n            draw_detections.append({\n                'cid': cls_id,\n                'label': label_text,\n                'score': score,\n                'box': [x1, y1, x2, y2]\n            })\n\n    # Update aggregator for proxy mAP\n    for p in all_preds_this_frame:\n        cid = p['cid']\n        predictions_by_class.setdefault(cid, []).append({\n            'frame': frame_index,\n            'box': p['box'],\n            'score': p['score']\n        })\n\n    # Draw detections on the frame\n    for det in draw_detections:\n        x1, y1, x2, y2 = det['box']\n        color = class_color(det['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{det['label']}:{det['score']:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(th + 4, y1)\n        cv2.rectangle(frame, (x1, y_text - th - 4), (x1 + tw + 2, y_text), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y_text - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Compute and overlay proxy mAP after current frame\n    last_map, _ = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n    map_text = f\"mAP@0.5 (proxy): {last_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    out_writer.write(frame)\n\n    # 4.5 Loop continuation\n    frame_index += 1\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - t_start\nfps_proc = (frame_index / elapsed) if elapsed > 0 else 0.0\n\n# Final proxy mAP computed on all frames\nfinal_map, per_class_ap = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n\n# Console summary\nprint(\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}, Processing FPS: {fps_proc:.2f}\")\nprint(f\"Estimated mAP@0.5 (proxy, no ground-truth): {final_map:.4f}\")\n\nif len(per_class_ap) > 0:\n    # Show top few classes by AP\n    items = sorted(per_class_ap.items(), key=lambda kv: -kv[1])\n    top_k = min(5, len(items))\n    print(\"Top class APs (proxy):\")\n    for i in range(top_k):\n        cid, ap = items[i]\n        label_name = class_to_label(int(cid), labels)\n        print(f\"  {label_name} (id={cid}): AP={ap:.4f}\")\nelse:\n    print(\"No pseudo-GT could be formed; proxy mAP is 0.0.\")\n```\n\n### OUTPUT TEMPLATE ###\n```python\n<complete_code>\n```\n<REPORT THE FIX OF THE LAST ERROR>\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 993624
              }
            },
            "litellm_response_cost": 0.07252325000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nDescription:\n- Reads a single video file from the given input_path.\n- Runs TFLite SSD detection on each frame.\n- Draws rectangles and labels for detected objects above the confidence threshold.\n- Calculates and overlays a proxy mAP@0.5 IoU (no GT; derived via per-class/frame NMS).\n- Writes annotated video to the provided output_path.\n\nPhases:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video read, resize, normalize/cast)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results: parse outputs, map labels\n  - 4.3 Post-processing: thresholding, scaling to pixels, clipping, NMS for proxy mAP\n  - 4.4 Write annotated frames to output video\n- Phase 5: Cleanup\n\nNote:\n- Only standard libraries and cv2 are used intentionally for video processing, as required.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Import Interpreter literally as required\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (use provided paths and parameters)\nMODEL_PATH   = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH   = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH   = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH  = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = float('0.5')\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (needed for output interpretation)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter and allocate tensors\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Validate and derive model input size\nif len(input_shape) != 4 or int(input_shape[-1]) != 3:\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\nmodel_in_h = int(input_shape[1])\nmodel_in_w = int(input_shape[2])\n\n# ------------------ Helper utilities ------------------\n\ndef class_to_label(cid, labels_list):\n    \"\"\"Robust mapping from class id to label string (handles off-by-one variants).\"\"\"\n    if not labels_list:\n        return str(cid)\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return str(cid)\n\ndef get_output_tensors(interp, out_details):\n    \"\"\"\n    Extract boxes, classes, scores, num_detections from TFLite SSD outputs.\n    Returns:\n      boxes: np.ndarray [N, 4] normalized [ymin, xmin, ymax, xmax]\n      classes: np.ndarray [N] int\n      scores: np.ndarray [N] float\n      num_det: int\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Try name-based mapping\n    for od in out_details:\n        name = str(od.get('name', '')).lower()\n        ary = interp.get_tensor(od['index'])\n        if ary.ndim > 1 and ary.shape[0] == 1:\n            arr = ary[0]\n        else:\n            arr = ary\n        if 'box' in name or 'bbox' in name:\n            boxes = arr\n        elif 'class' in name:\n            classes = arr.astype(np.int32)\n        elif 'score' in name:\n            scores = arr.astype(np.float32)\n        elif ('num' in name) and ('detection' in name):\n            num_det = int(np.round(float(ary.squeeze())))\n\n    # Fallback inference by shape/dtype if needed\n    if boxes is None or classes is None or scores is None or num_det is None:\n        # Boxes\n        if boxes is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.ndim >= 2 and ary.shape[-1] == 4:\n                    boxes = ary[0] if ary.ndim > 2 else ary\n                    break\n        # Scores and classes\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n            if scores is None and arr.dtype in (np.float32, np.float64) and arr.ndim >= 1:\n                mn = float(np.min(arr)) if arr.size else 0.0\n                mx = float(np.max(arr)) if arr.size else 0.0\n                if 0.0 <= mn and mx <= 1.0:\n                    scores = arr.astype(np.float32)\n                    continue\n            if classes is None and arr.ndim >= 1 and arr.dtype in (np.int32, np.int64, np.float32, np.float64):\n                if arr.dtype in (np.float32, np.float64):\n                    classes = arr.astype(np.int32)\n                else:\n                    classes = arr.astype(np.int32)\n        # Num detections\n        if num_det is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.size == 1:\n                    num_det = int(np.round(float(ary.squeeze())))\n                    break\n\n    # Defaults and clipping\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if num_det is None:\n        num_det = min(len(boxes), len(scores), len(classes))\n\n    n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\ndef preprocess_frame(frame_bgr):\n    \"\"\"\n    Phase 2: Preprocess Data\n    - Convert BGR to RGB\n    - Resize to model input size (model_in_w, model_in_h)\n    - Normalize if floating model: (x - 127.5)/127.5\n    - Cast to input dtype if quantized\n    - Add batch dimension [1, H, W, 3]\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # cv2.resize expects (width, height)\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n    if floating_model:\n        input_data = (np.float32(resized) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(resized, dtype=input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef class_color(cid):\n    \"\"\"Deterministic color for a given class id.\"\"\"\n    rng = np.random.RandomState(seed=(cid * 9973) % (2**32))\n    color = rng.randint(0, 256, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef bbox_clip_xyxy(box, w, h):\n    \"\"\"Clip [x1,y1,x2,y2] to image bounds and ensure integer coords.\"\"\"\n    x1, y1, x2, y2 = box\n    x1 = int(max(0, min(round(x1), w - 1)))\n    y1 = int(max(0, min(round(y1), h - 1)))\n    x2 = int(max(0, min(round(x2), w - 1)))\n    y2 = int(max(0, min(round(y2), h - 1)))\n    # Ensure proper ordering\n    if x2 < x1:\n        x1, x2 = x2, x1\n    if y2 < y1:\n        y1, y2 = y2, y1\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"IoU for [x1,y1,x2,y2] boxes.\"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    w = max(0.0, xB - xA)\n    h = max(0.0, yB - yA)\n    inter = w * h\n    areaA = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n    areaB = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n    denom = areaA + areaB - inter + 1e-9\n    return inter / denom\n\ndef nms_xyxy(boxes, scores, iou_thresh=0.5):\n    \"\"\"Plain NMS for XYXY boxes; returns kept indices.\"\"\"\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    areas = np.maximum(0.0, x2 - x1) * np.maximum(0.0, y2 - y1)\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_proxy_map(predictions_by_class, iou_thresh=0.5):\n    \"\"\"\n    Compute proxy mAP@0.5 IoU:\n    - For each class and frame, pseudo GT via NMS on that frame's predictions.\n    - Evaluate AP per class by matching predictions to pseudo GT at IoU>=threshold.\n    - Return mean AP across classes with at least one pseudo GT.\n    \"\"\"\n    per_class_ap = {}\n    valid_classes = []\n\n    for cid, preds in predictions_by_class.items():\n        # Group predictions by frame\n        by_frame = {}\n        for p in preds:\n            f = p['frame']\n            by_frame.setdefault(f, []).append({'box': p['box'], 'score': p['score']})\n\n        # Build pseudo GT via NMS\n        pseudo_gt = {}\n        total_gt = 0\n        for f, plist in by_frame.items():\n            if not plist:\n                pseudo_gt[f] = []\n                continue\n            boxes = [pp['box'] for pp in plist]\n            scores = [pp['score'] for pp in plist]\n            keep_idx = nms_xyxy(boxes, scores, iou_thresh=0.5)\n            gt_boxes = [boxes[k] for k in keep_idx]\n            pseudo_gt[f] = gt_boxes\n            total_gt += len(gt_boxes)\n\n        if total_gt == 0:\n            continue\n\n        valid_classes.append(cid)\n\n        # Flatten and sort predictions by score desc\n        flat_preds = [{'frame': p['frame'], 'box': p['box'], 'score': p['score']} for p in preds]\n        flat_preds.sort(key=lambda x: -x['score'])\n\n        # Track GT assignments per frame\n        gt_matched = {f: np.zeros(len(pseudo_gt[f]), dtype=np.uint8) for f in pseudo_gt}\n\n        tps, fps = [], []\n        for p in flat_preds:\n            f = p['frame']\n            box = p['box']\n            gt_boxes = pseudo_gt.get(f, [])\n            best_iou, best_idx = 0.0, -1\n            for gi, gtb in enumerate(gt_boxes):\n                if gt_matched[f][gi] == 1:\n                    continue\n                iou = iou_xyxy(box, gtb)\n                if iou >= iou_thresh and iou > best_iou:\n                    best_iou = iou\n                    best_idx = gi\n            if best_idx >= 0:\n                gt_matched[f][best_idx] = 1\n                tps.append(1)\n                fps.append(0)\n            else:\n                tps.append(0)\n                fps.append(1)\n\n        if len(tps) == 0:\n            per_class_ap[cid] = 0.0\n            continue\n\n        tps = np.array(tps, dtype=np.float32)\n        fps = np.array(fps, dtype=np.float32)\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / max(total_gt, 1e-9)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0.0, 1.0, 11):\n            p_at_r = precision[recall >= r]\n            p_val = float(np.max(p_at_r)) if p_at_r.size > 0 else 0.0\n            ap += p_val\n        ap /= 11.0\n        per_class_ap[cid] = ap\n\n    mAP = float(np.mean([per_class_ap[c] for c in valid_classes])) if len(valid_classes) > 0 else 0.0\n    return mAP, per_class_ap\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n# 2.1 Acquire Input Data: Read a single video file from INPUT_PATH\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video file not found at: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Prepare output writer with same resolution and fps\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or np.isnan(in_fps) or in_fps <= 1e-3:\n    in_fps = 30.0  # reasonable fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n# Prepare aggregators for proxy mAP\npredictions_by_class = {}  # cid -> list of {'frame': idx, 'box': [x1,y1,x2,y2], 'score': s}\n\n# Processing loop\nframe_index = 0\nt_start = time.time()\nlast_map = 0.0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    H, W = frame.shape[:2]\n\n    # 2.2 Preprocess Data for the model\n    input_data = preprocess_frame(frame)\n\n    # Phase 3: Inference\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, n_det = get_output_tensors(interpreter, output_details)\n\n    # 4.2 Interpret Results: map labels, prepare detections\n    draw_detections = []       # detections above threshold for drawing\n    all_preds_this_frame = []  # all detections for proxy mAP aggregation\n\n    # 4.3 Post-processing: confidence thresholding, coordinate scaling, clipping\n    for i in range(n_det):\n        score = float(scores_raw[i])\n        cls_id = int(classes_raw[i])\n\n        # boxes are normalized [ymin, xmin, ymax, xmax]; clip to [0,1] before scaling\n        ymin, xmin, ymax, xmax = boxes_norm[i].tolist()\n        ymin = min(max(float(ymin), 0.0), 1.0)\n        xmin = min(max(float(xmin), 0.0), 1.0)\n        ymax = min(max(float(ymax), 0.0), 1.0)\n        xmax = min(max(float(xmax), 0.0), 1.0)\n\n        # Scale to pixel coordinates\n        x1 = xmin * W\n        y1 = ymin * H\n        x2 = xmax * W\n        y2 = ymax * H\n\n        # Clip to valid image bounds and ensure correct ordering\n        x1, y1, x2, y2 = bbox_clip_xyxy([x1, y1, x2, y2], W, H)\n\n        # Aggregate all predictions for proxy mAP (no threshold here)\n        all_preds_this_frame.append({'cid': cls_id, 'box': [x1, y1, x2, y2], 'score': score})\n\n        # Keep only confident detections for visualization\n        if score >= CONF_THRESHOLD:\n            label_text = class_to_label(cls_id, labels)\n            draw_detections.append({\n                'cid': cls_id,\n                'label': label_text,\n                'score': score,\n                'box': [x1, y1, x2, y2]\n            })\n\n    # Update aggregator for proxy mAP\n    for p in all_preds_this_frame:\n        cid = p['cid']\n        predictions_by_class.setdefault(cid, []).append({\n            'frame': frame_index,\n            'box': p['box'],\n            'score': p['score']\n        })\n\n    # Draw detections on the frame\n    for det in draw_detections:\n        x1, y1, x2, y2 = det['box']\n        color = class_color(det['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{det['label']}:{det['score']:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(th + 4, y1)\n        cv2.rectangle(frame, (x1, y_text - th - 4), (x1 + tw + 2, y_text), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y_text - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Compute and overlay proxy mAP after current frame\n    last_map, _ = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n    map_text = f\"mAP@0.5 (proxy): {last_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    out_writer.write(frame)\n\n    # 4.5 Loop continuation\n    frame_index += 1\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - t_start\nfps_proc = (frame_index / elapsed) if elapsed > 0 else 0.0\n\n# Final proxy mAP computed on all frames\nfinal_map, per_class_ap = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n\n# Console summary\nprint(\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}, Processing FPS: {fps_proc:.2f}\")\nprint(f\"Estimated mAP@0.5 (proxy, no ground-truth): {final_map:.4f}\")\n\nif len(per_class_ap) > 0:\n    # Show top few classes by AP\n    items = sorted(per_class_ap.items(), key=lambda kv: -kv[1])\n    top_k = min(5, len(items))\n    print(\"Top class APs (proxy):\")\n    for i in range(top_k):\n        cid, ap = items[i]\n        label_name = class_to_label(int(cid), labels)\n        print(f\"  {label_name} (id={cid}): AP={ap:.4f}\")\nelse:\n    print(\"No pseudo-GT could be formed; proxy mAP is 0.0.\")\n\n# ---- REPORT OF FIX FOR LAST ERROR ----\n# The previous error occurred because model_in_h was set to a string (\"gpt-5-2025-08-07\"),\n# causing cv2.resize to receive an invalid dsize type. This script correctly derives\n# model_in_h and model_in_w as integers from input_shape:\n#   model_in_h = int(input_shape[1])\n#   model_in_w = int(input_shape[2])\n# ensuring cv2.resize receives a valid (width, height) tuple of integers.\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 6849,
            "output": 6569,
            "total": 13418,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07252325,
          "latency": 82599.0,
          "timeToFirstToken": 82.599,
          "updatedAt": "2026-02-05T02:36:01.450Z",
          "totalTokens": 13418,
          "costDetails": {
            "total": 0.07252325
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 6569,
          "usageDetails": {
            "input": 6849,
            "output": 6569,
            "total": 13418
          },
          "unit": "TOKENS",
          "promptTokens": 6849,
          "environment": "default",
          "createdAt": "2026-02-05T02:35:56.403Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-04-35-56-360477_chatcmpl-D5jVkfSL0HyYwLZcXIY4wKZBW3pQr",
          "traceId": "dba3b065",
          "type": "GENERATION",
          "name": "dba3_psg_gen_attempt#5",
          "startTime": "2026-02-05T02:35:56.360000+00:00",
          "endTime": "2026-02-05T02:37:14.864000+00:00",
          "completionStartTime": "2026-02-05T02:37:14.864000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\n- The previous Python script generated has error as logged under ### CAUSED ERROR BEFORE ###.\n- The \"CAUSED ERROR BEFORE\" lists the error message and your previous faulty code caused the error is under ### EXECUTED CODE BEFORE ###. \n- *MAKE SURE YOU USED THE PROVIDED PATHS AND PARAMETERS*\n\n### INSTRUCTIONS ###\n- Regenerate the complete script, adhering strictly to Application Description and Core Requirements.\n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### provided below, REWRITE the complete Python script for the application 'Object Detection via a video file' to avoid the error.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- Input Method Description: 'Read a single video file from the given input_path'\n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path = 'models/ssd-mobilenet_v1/labelmap.txt'  \n- input_path = 'data/object_detection/sheeps.mp4'  \n- output_path = 'results/object_detection/test_results/sheeps_detections.mp4' \n- confidence_threshold = '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n### CAUSED ERROR BEFORE ###\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043555_psg_gpt-5-2025-08-07/tmp_20260205043555_psg_gpt-5-2025-08-07.py\", line 376, in <module>\n    input_data = preprocess_frame(frame)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043555_psg_gpt-5-2025-08-07/tmp_20260205043555_psg_gpt-5-2025-08-07.py\", line 182, in preprocess_frame\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\ncv2.error: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n>  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n\n\n\n### EXECUTED CODE BEFORE ###\n```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nDescription:\n- Reads a single video file from the given input_path.\n- Runs TFLite SSD detection on each frame.\n- Draws rectangles and labels for detected objects above the confidence threshold.\n- Calculates and overlays a proxy mAP@0.5 IoU (no GT; derived via per-class/frame NMS).\n- Writes annotated video to the provided output_path.\n\nPhases:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video read, resize, normalize/cast)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results: parse outputs, map labels\n  - 4.3 Post-processing: thresholding, scaling to pixels, clipping, NMS for proxy mAP\n  - 4.4 Write annotated frames to output video\n- Phase 5: Cleanup\n\nNote:\n- Only standard libraries and cv2 are used intentionally for video processing, as required.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Import Interpreter literally as required\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (use provided paths and parameters)\nMODEL_PATH    = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH    = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH    = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH   = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (needed for output interpretation)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter and allocate tensors\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Validate and derive model input size\nif len(input_shape) != 4 or int(input_shape[-1]) != 3:\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\nmodel_in_h  = \"gpt-5-2025-08-07\"\nmodel_in_w = int(input_shape[2])\n\n# ------------------ Helper utilities ------------------\n\ndef class_to_label(cid, labels_list):\n    \"\"\"Robust mapping from class id to label string (handles off-by-one variants).\"\"\"\n    if not labels_list:\n        return str(cid)\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return str(cid)\n\ndef get_output_tensors(interp, out_details):\n    \"\"\"\n    Extract boxes, classes, scores, num_detections from TFLite SSD outputs.\n    Returns:\n      boxes: np.ndarray [N, 4] normalized [ymin, xmin, ymax, xmax]\n      classes: np.ndarray [N] int\n      scores: np.ndarray [N] float\n      num_det: int\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Try name-based mapping\n    for od in out_details:\n        name = str(od.get('name', '')).lower()\n        ary = interp.get_tensor(od['index'])\n        if ary.ndim > 1 and ary.shape[0] == 1:\n            arr = ary[0]\n        else:\n            arr = ary\n        if 'box' in name or 'bbox' in name:\n            boxes = arr\n        elif 'class' in name:\n            classes = arr.astype(np.int32)\n        elif 'score' in name:\n            scores = arr.astype(np.float32)\n        elif ('num' in name) and ('detection' in name):\n            num_det = int(np.round(float(ary.squeeze())))\n\n    # Fallback inference by shape/dtype if needed\n    if boxes is None or classes is None or scores is None or num_det is None:\n        # Boxes\n        if boxes is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.ndim >= 2 and ary.shape[-1] == 4:\n                    boxes = ary[0] if ary.ndim > 2 else ary\n                    break\n        # Scores and classes\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n            if scores is None and arr.dtype in (np.float32, np.float64) and arr.ndim >= 1:\n                mn = float(np.min(arr)) if arr.size else 0.0\n                mx = float(np.max(arr)) if arr.size else 0.0\n                if 0.0 <= mn and mx <= 1.0:\n                    scores = arr.astype(np.float32)\n                    continue\n            if classes is None and arr.ndim >= 1 and arr.dtype in (np.int32, np.int64, np.float32, np.float64):\n                if arr.dtype in (np.float32, np.float64):\n                    classes = arr.astype(np.int32)\n                else:\n                    classes = arr.astype(np.int32)\n        # Num detections\n        if num_det is None:\n            for od in out_details:\n                ary = interp.get_tensor(od['index'])\n                if ary.size == 1:\n                    num_det = int(np.round(float(ary.squeeze())))\n                    break\n\n    # Defaults and clipping\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if num_det is None:\n        num_det = min(len(boxes), len(scores), len(classes))\n\n    n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\ndef preprocess_frame(frame_bgr):\n    \"\"\"\n    Phase 2: Preprocess Data\n    - Convert BGR to RGB\n    - Resize to model input size (model_in_w, model_in_h)\n    - Normalize if floating model: (x - 127.5)/127.5\n    - Cast to input dtype if quantized\n    - Add batch dimension [1, H, W, 3]\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # cv2.resize expects (width, height)\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n    if floating_model:\n        input_data = (np.float32(resized) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(resized, dtype=input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef class_color(cid):\n    \"\"\"Deterministic color for a given class id.\"\"\"\n    rng = np.random.RandomState(seed=(cid * 9973) % (2**32))\n    color = rng.randint(0, 256, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef bbox_clip_xyxy(box, w, h):\n    \"\"\"Clip [x1,y1,x2,y2] to image bounds and ensure integer coords.\"\"\"\n    x1, y1, x2, y2 = box\n    x1 = int(max(0, min(round(x1), w - 1)))\n    y1 = int(max(0, min(round(y1), h - 1)))\n    x2 = int(max(0, min(round(x2), w - 1)))\n    y2 = int(max(0, min(round(y2), h - 1)))\n    # Ensure proper ordering\n    if x2 < x1:\n        x1, x2 = x2, x1\n    if y2 < y1:\n        y1, y2 = y2, y1\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"IoU for [x1,y1,x2,y2] boxes.\"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    w = max(0.0, xB - xA)\n    h = max(0.0, yB - yA)\n    inter = w * h\n    areaA = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n    areaB = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n    denom = areaA + areaB - inter + 1e-9\n    return inter / denom\n\ndef nms_xyxy(boxes, scores, iou_thresh=0.5):\n    \"\"\"Plain NMS for XYXY boxes; returns kept indices.\"\"\"\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    areas = np.maximum(0.0, x2 - x1) * np.maximum(0.0, y2 - y1)\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_proxy_map(predictions_by_class, iou_thresh=0.5):\n    \"\"\"\n    Compute proxy mAP@0.5 IoU:\n    - For each class and frame, pseudo GT via NMS on that frame's predictions.\n    - Evaluate AP per class by matching predictions to pseudo GT at IoU>=threshold.\n    - Return mean AP across classes with at least one pseudo GT.\n    \"\"\"\n    per_class_ap = {}\n    valid_classes = []\n\n    for cid, preds in predictions_by_class.items():\n        # Group predictions by frame\n        by_frame = {}\n        for p in preds:\n            f = p['frame']\n            by_frame.setdefault(f, []).append({'box': p['box'], 'score': p['score']})\n\n        # Build pseudo GT via NMS\n        pseudo_gt = {}\n        total_gt = 0\n        for f, plist in by_frame.items():\n            if not plist:\n                pseudo_gt[f] = []\n                continue\n            boxes = [pp['box'] for pp in plist]\n            scores = [pp['score'] for pp in plist]\n            keep_idx = nms_xyxy(boxes, scores, iou_thresh=0.5)\n            gt_boxes = [boxes[k] for k in keep_idx]\n            pseudo_gt[f] = gt_boxes\n            total_gt += len(gt_boxes)\n\n        if total_gt == 0:\n            continue\n\n        valid_classes.append(cid)\n\n        # Flatten and sort predictions by score desc\n        flat_preds = [{'frame': p['frame'], 'box': p['box'], 'score': p['score']} for p in preds]\n        flat_preds.sort(key=lambda x: -x['score'])\n\n        # Track GT assignments per frame\n        gt_matched = {f: np.zeros(len(pseudo_gt[f]), dtype=np.uint8) for f in pseudo_gt}\n\n        tps, fps = [], []\n        for p in flat_preds:\n            f = p['frame']\n            box = p['box']\n            gt_boxes = pseudo_gt.get(f, [])\n            best_iou, best_idx = 0.0, -1\n            for gi, gtb in enumerate(gt_boxes):\n                if gt_matched[f][gi] == 1:\n                    continue\n                iou = iou_xyxy(box, gtb)\n                if iou >= iou_thresh and iou > best_iou:\n                    best_iou = iou\n                    best_idx = gi\n            if best_idx >= 0:\n                gt_matched[f][best_idx] = 1\n                tps.append(1)\n                fps.append(0)\n            else:\n                tps.append(0)\n                fps.append(1)\n\n        if len(tps) == 0:\n            per_class_ap[cid] = 0.0\n            continue\n\n        tps = np.array(tps, dtype=np.float32)\n        fps = np.array(fps, dtype=np.float32)\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / max(total_gt, 1e-9)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0.0, 1.0, 11):\n            p_at_r = precision[recall >= r]\n            p_val = float(np.max(p_at_r)) if p_at_r.size > 0 else 0.0\n            ap += p_val\n        ap /= 11.0\n        per_class_ap[cid] = ap\n\n    mAP = float(np.mean([per_class_ap[c] for c in valid_classes])) if len(valid_classes) > 0 else 0.0\n    return mAP, per_class_ap\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n# 2.1 Acquire Input Data: Read a single video file from INPUT_PATH\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video file not found at: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Prepare output writer with same resolution and fps\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or np.isnan(in_fps) or in_fps <= 1e-3:\n    in_fps = 30.0  # reasonable fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n# Prepare aggregators for proxy mAP\npredictions_by_class = {}  # cid -> list of {'frame': idx, 'box': [x1,y1,x2,y2], 'score': s}\n\n# Processing loop\nframe_index = 0\nt_start = time.time()\nlast_map = 0.0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    H, W = frame.shape[:2]\n\n    # 2.2 Preprocess Data for the model\n    input_data = preprocess_frame(frame)\n\n    # Phase 3: Inference\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, n_det = get_output_tensors(interpreter, output_details)\n\n    # 4.2 Interpret Results: map labels, prepare detections\n    draw_detections = []       # detections above threshold for drawing\n    all_preds_this_frame = []  # all detections for proxy mAP aggregation\n\n    # 4.3 Post-processing: confidence thresholding, coordinate scaling, clipping\n    for i in range(n_det):\n        score = float(scores_raw[i])\n        cls_id = int(classes_raw[i])\n\n        # boxes are normalized [ymin, xmin, ymax, xmax]; clip to [0,1] before scaling\n        ymin, xmin, ymax, xmax = boxes_norm[i].tolist()\n        ymin = min(max(float(ymin), 0.0), 1.0)\n        xmin = min(max(float(xmin), 0.0), 1.0)\n        ymax = min(max(float(ymax), 0.0), 1.0)\n        xmax = min(max(float(xmax), 0.0), 1.0)\n\n        # Scale to pixel coordinates\n        x1 = xmin * W\n        y1 = ymin * H\n        x2 = xmax * W\n        y2 = ymax * H\n\n        # Clip to valid image bounds and ensure correct ordering\n        x1, y1, x2, y2 = bbox_clip_xyxy([x1, y1, x2, y2], W, H)\n\n        # Aggregate all predictions for proxy mAP (no threshold here)\n        all_preds_this_frame.append({'cid': cls_id, 'box': [x1, y1, x2, y2], 'score': score})\n\n        # Keep only confident detections for visualization\n        if score >= CONF_THRESHOLD:\n            label_text = class_to_label(cls_id, labels)\n            draw_detections.append({\n                'cid': cls_id,\n                'label': label_text,\n                'score': score,\n                'box': [x1, y1, x2, y2]\n            })\n\n    # Update aggregator for proxy mAP\n    for p in all_preds_this_frame:\n        cid = p['cid']\n        predictions_by_class.setdefault(cid, []).append({\n            'frame': frame_index,\n            'box': p['box'],\n            'score': p['score']\n        })\n\n    # Draw detections on the frame\n    for det in draw_detections:\n        x1, y1, x2, y2 = det['box']\n        color = class_color(det['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{det['label']}:{det['score']:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(th + 4, y1)\n        cv2.rectangle(frame, (x1, y_text - th - 4), (x1 + tw + 2, y_text), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y_text - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Compute and overlay proxy mAP after current frame\n    last_map, _ = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n    map_text = f\"mAP@0.5 (proxy): {last_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    out_writer.write(frame)\n\n    # 4.5 Loop continuation\n    frame_index += 1\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - t_start\nfps_proc = (frame_index / elapsed) if elapsed > 0 else 0.0\n\n# Final proxy mAP computed on all frames\nfinal_map, per_class_ap = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n\n# Console summary\nprint(\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}, Processing FPS: {fps_proc:.2f}\")\nprint(f\"Estimated mAP@0.5 (proxy, no ground-truth): {final_map:.4f}\")\n\nif len(per_class_ap) > 0:\n    # Show top few classes by AP\n    items = sorted(per_class_ap.items(), key=lambda kv: -kv[1])\n    top_k = min(5, len(items))\n    print(\"Top class APs (proxy):\")\n    for i in range(top_k):\n        cid, ap = items[i]\n        label_name = class_to_label(int(cid), labels)\n        print(f\"  {label_name} (id={cid}): AP={ap:.4f}\")\nelse:\n    print(\"No pseudo-GT could be formed; proxy mAP is 0.0.\")\n\n# ---- REPORT OF FIX FOR LAST ERROR ----\n# The previous error occurred because model_in_h was set to a string (\"gpt-5-2025-08-07\"),\n# causing cv2.resize to receive an invalid dsize type. This script correctly derives\n# model_in_h and model_in_w as integers from input_shape:\n#   model_in_h = int(input_shape[1])\n#   model_in_w = int(input_shape[2])\n# ensuring cv2.resize receives a valid (width, height) tuple of integers.\n```\n\n### OUTPUT TEMPLATE ###\n```python\n<complete_code>\n```\n<REPORT THE FIX OF THE LAST ERROR>\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 993515
              }
            },
            "litellm_response_cost": 0.07333,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nDescription:\n- Read a single video file from the given input_path.\n- Run TFLite SSD detection on each frame using ai_edge_litert Interpreter.\n- Draw rectangles and labels for detected objects above the confidence threshold.\n- Calculate and overlay a proxy mAP(mean average precision) per entire processed video.\n- Write annotated video to the provided output_path.\n\nPhases:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video read, resize, normalize/cast)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2 Interpret Results: parse outputs, map labels\n  - 4.3 Post-processing: thresholding, scaling to pixels, clipping, NMS for proxy mAP construction\n  - 4.4 Write annotated frames to output video\n- Phase 5: Cleanup\n\nNotes:\n- Only standard libraries (os, time, numpy, cv2) are used alongside the required ai_edge_litert interpreter.\n\"\"\"\n\n# Phase 1: Setup\n# 1.1 Imports\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Import Interpreter literally as required\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (use provided paths and parameters exactly)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = float('0.5')\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (needed for output interpretation)\ndef load_labels(label_path):\n    labels_list = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels_list.append(line)\n    return labels_list\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter and allocate tensors\nif not os.path.isfile(MODEL_PATH):\n    raise FileNotFoundError(f\"Model file not found at: {MODEL_PATH}\")\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input tensor properties\nif len(input_details) == 0:\n    raise RuntimeError(\"No input tensors found in the model.\")\n\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Validate and derive model input size\nif not (len(input_shape) == 4 and int(input_shape[-1]) == 3):\n    raise ValueError(f\"Unexpected input tensor shape: {input_shape}\")\nmodel_in_h = int(input_shape[1])  # Height\nmodel_in_w = int(input_shape[2])  # Width\n\n# ------------------ Helper utilities ------------------\n\ndef class_to_label(cid, labels_list):\n    \"\"\"\n    Robust mapping from class id to label string (handles off-by-one differences).\n    Many SSD label files are 1-based; TFLite outputs are 0-based floats.\n    \"\"\"\n    if not labels_list:\n        return str(cid)\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return str(cid)\n\ndef get_output_tensors(interp, out_details):\n    \"\"\"\n    Extract boxes, classes, scores, num_detections from TFLite SSD outputs.\n    Returns:\n      boxes: np.ndarray [N, 4] normalized [ymin, xmin, ymax, xmax]\n      classes: np.ndarray [N] int\n      scores: np.ndarray [N] float\n      num_det: int\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Try name-based mapping first\n    for od in out_details:\n        name = str(od.get('name', '')).lower()\n        ary = interp.get_tensor(od['index'])\n        arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n        if 'box' in name or 'bbox' in name:\n            boxes = np.asarray(arr, dtype=np.float32)\n        elif 'class' in name:\n            classes = np.asarray(arr).astype(np.int32)\n        elif 'score' in name or 'confidence' in name:\n            scores = np.asarray(arr).astype(np.float32)\n        elif ('num' in name) and ('detection' in name):\n            num_det = int(np.round(float(ary.squeeze())))\n\n    # Fallbacks by shape/dtype if anything missing\n    if boxes is None:\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n            if arr.ndim >= 2 and arr.shape[-1] == 4:\n                boxes = np.asarray(arr, dtype=np.float32)\n                break\n\n    if scores is None or classes is None or num_det is None:\n        for od in out_details:\n            ary = interp.get_tensor(od['index'])\n            arr = ary[0] if (ary.ndim > 1 and ary.shape[0] == 1) else ary\n            if scores is None and arr.ndim >= 1 and arr.dtype in (np.float32, np.float64):\n                # Candidate for scores if range in [0,1]\n                mn = float(np.min(arr)) if arr.size else 0.0\n                mx = float(np.max(arr)) if arr.size else 0.0\n                if 0.0 <= mn and mx <= 1.0:\n                    scores = np.asarray(arr, dtype=np.float32)\n                    continue\n            if classes is None and arr.ndim >= 1 and arr.dtype in (np.int32, np.int64, np.float32, np.float64):\n                if arr.dtype in (np.float32, np.float64):\n                    classes = np.asarray(arr, dtype=np.float32).astype(np.int32)\n                else:\n                    classes = np.asarray(arr).astype(np.int32)\n            if num_det is None and arr.size == 1:\n                num_det = int(np.round(float(arr.squeeze())))\n\n    # Defaults and trimming\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if num_det is None:\n        num_det = min(len(boxes), len(scores), len(classes))\n\n    n = min(num_det, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\ndef preprocess_frame(frame_bgr):\n    \"\"\"\n    Phase 2: Preprocess Data\n    - Convert BGR to RGB\n    - Resize to model input size (model_in_w, model_in_h)\n    - Normalize if floating model: (x - 127.5)/127.5\n    - Cast to input dtype if quantized\n    - Add batch dimension [1, H, W, 3]\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # cv2.resize expects size as (width, height)\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\n    if floating_model:\n        input_data = (np.float32(resized) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(resized, dtype=input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef class_color(cid):\n    \"\"\"Deterministic color for a given class id.\"\"\"\n    rng = np.random.RandomState(seed=(int(cid) * 9973) % (2**32))\n    color = rng.randint(0, 256, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef bbox_clip_xyxy(box, w, h):\n    \"\"\"Clip [x1,y1,x2,y2] to image bounds and ensure integer coords.\"\"\"\n    x1, y1, x2, y2 = box\n    x1 = int(max(0, min(round(x1), w - 1)))\n    y1 = int(max(0, min(round(y1), h - 1)))\n    x2 = int(max(0, min(round(x2), w - 1)))\n    y2 = int(max(0, min(round(y2), h - 1)))\n    if x2 < x1:\n        x1, x2 = x2, x1\n    if y2 < y1:\n        y1, y2 = y2, y1\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"IoU for [x1,y1,x2,y2] boxes.\"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    w = max(0.0, xB - xA)\n    h = max(0.0, yB - yA)\n    inter = w * h\n    areaA = max(0.0, boxA[2] - boxA[0]) * max(0.0, boxA[3] - boxA[1])\n    areaB = max(0.0, boxB[2] - boxB[0]) * max(0.0, boxB[3] - boxB[1])\n    denom = areaA + areaB - inter + 1e-9\n    return inter / denom\n\ndef nms_xyxy(boxes, scores, iou_thresh=0.5):\n    \"\"\"Plain NMS for XYXY boxes; returns kept indices.\"\"\"\n    if len(boxes) == 0:\n        return []\n    boxes = np.asarray(boxes, dtype=np.float32)\n    scores = np.asarray(scores, dtype=np.float32)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    areas = np.maximum(0.0, x2 - x1) * np.maximum(0.0, y2 - y1)\n    order = scores.argsort()[::-1]\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n        w = np.maximum(0.0, xx2 - xx1)\n        h = np.maximum(0.0, yy2 - yy1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_proxy_map(predictions_by_class, iou_thresh=0.5):\n    \"\"\"\n    Compute proxy mAP@0.5 IoU:\n    - For each class and frame, create pseudo ground-truth via NMS on that frame's predictions.\n    - Evaluate AP per class by matching predictions to pseudo GT at IoU>=threshold.\n    - Return mean AP across classes with at least one pseudo GT.\n    \"\"\"\n    per_class_ap = {}\n    valid_classes = []\n\n    for cid, preds in predictions_by_class.items():\n        # Group predictions by frame\n        by_frame = {}\n        for p in preds:\n            f = p['frame']\n            by_frame.setdefault(f, []).append({'box': p['box'], 'score': p['score']})\n\n        # Build pseudo GT via NMS\n        pseudo_gt = {}\n        total_gt = 0\n        for f, plist in by_frame.items():\n            if not plist:\n                pseudo_gt[f] = []\n                continue\n            boxes = [pp['box'] for pp in plist]\n            scores = [pp['score'] for pp in plist]\n            keep_idx = nms_xyxy(boxes, scores, iou_thresh=0.5)\n            gt_boxes = [boxes[k] for k in keep_idx]\n            pseudo_gt[f] = gt_boxes\n            total_gt += len(gt_boxes)\n\n        if total_gt == 0:\n            continue\n\n        valid_classes.append(cid)\n\n        # Flatten and sort predictions by score desc\n        flat_preds = [{'frame': p['frame'], 'box': p['box'], 'score': p['score']} for p in preds]\n        flat_preds.sort(key=lambda x: -x['score'])\n\n        # Track GT assignments per frame\n        gt_matched = {f: np.zeros(len(pseudo_gt[f]), dtype=np.uint8) for f in pseudo_gt}\n\n        tps, fps = [], []\n        for p in flat_preds:\n            f = p['frame']\n            box = p['box']\n            gt_boxes = pseudo_gt.get(f, [])\n            best_iou, best_idx = 0.0, -1\n            for gi, gtb in enumerate(gt_boxes):\n                if gt_matched[f][gi] == 1:\n                    continue\n                iou = iou_xyxy(box, gtb)\n                if iou >= iou_thresh and iou > best_iou:\n                    best_iou = iou\n                    best_idx = gi\n            if best_idx >= 0:\n                gt_matched[f][best_idx] = 1\n                tps.append(1)\n                fps.append(0)\n            else:\n                tps.append(0)\n                fps.append(1)\n\n        if len(tps) == 0:\n            per_class_ap[cid] = 0.0\n            continue\n\n        tps = np.array(tps, dtype=np.float32)\n        fps = np.array(fps, dtype=np.float32)\n        cum_tp = np.cumsum(tps)\n        cum_fp = np.cumsum(fps)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / max(total_gt, 1e-9)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0.0, 1.0, 11):\n            p_at_r = precision[recall >= r]\n            p_val = float(np.max(p_at_r)) if p_at_r.size > 0 else 0.0\n            ap += p_val\n        ap /= 11.0\n        per_class_ap[cid] = ap\n\n    mAP = float(np.mean([per_class_ap[c] for c in valid_classes])) if len(valid_classes) > 0 else 0.0\n    return mAP, per_class_ap\n\n# Phase 2: Input Acquisition & Preprocessing Loop (Read a single video file)\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video file not found at: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Prepare output writer with same resolution and fps as input\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif not in_fps or np.isnan(in_fps) or in_fps <= 1e-3:\n    in_fps = 30.0  # fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n# Prepare aggregators for proxy mAP\npredictions_by_class = {}  # cid -> list of {'frame': idx, 'box': [x1,y1,x2,y2], 'score': s}\n\n# Processing loop\nframe_index = 0\nt_start = time.time()\nlast_map = 0.0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    H, W = frame.shape[:2]\n\n    # 2.2 Preprocess Data for the model\n    input_data = preprocess_frame(frame)\n\n    # Phase 3: Inference\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, n_det = get_output_tensors(interpreter, output_details)\n\n    # 4.2 Interpret Results: map labels, prepare detections\n    draw_detections = []       # detections above threshold for drawing\n    all_preds_this_frame = []  # all detections for proxy mAP aggregation\n\n    # 4.3 Post-processing: confidence thresholding, coordinate scaling, clipping\n    for i in range(n_det):\n        score = float(scores_raw[i])\n        cls_id = int(classes_raw[i])\n\n        # boxes are normalized [ymin, xmin, ymax, xmax]; clip to [0,1] before scaling\n        ymin, xmin, ymax, xmax = [float(v) for v in boxes_norm[i]]\n        ymin = min(max(ymin, 0.0), 1.0)\n        xmin = min(max(xmin, 0.0), 1.0)\n        ymax = min(max(ymax, 0.0), 1.0)\n        xmax = min(max(xmax, 0.0), 1.0)\n\n        # Scale to pixel coordinates\n        x1 = xmin * W\n        y1 = ymin * H\n        x2 = xmax * W\n        y2 = ymax * H\n\n        # Clip to valid image bounds and ensure proper ordering\n        x1, y1, x2, y2 = bbox_clip_xyxy([x1, y1, x2, y2], W, H)\n\n        # Aggregate all predictions for proxy mAP (no threshold here)\n        all_preds_this_frame.append({'cid': cls_id, 'box': [x1, y1, x2, y2], 'score': score})\n\n        # Keep only confident detections for visualization\n        if score >= CONF_THRESHOLD:\n            label_text = class_to_label(cls_id, labels)\n            draw_detections.append({\n                'cid': cls_id,\n                'label': label_text,\n                'score': score,\n                'box': [x1, y1, x2, y2]\n            })\n\n    # Update aggregator for proxy mAP\n    for p in all_preds_this_frame:\n        cid = p['cid']\n        predictions_by_class.setdefault(cid, []).append({\n            'frame': frame_index,\n            'box': p['box'],\n            'score': p['score']\n        })\n\n    # Draw detections on the frame\n    for det in draw_detections:\n        x1, y1, x2, y2 = det['box']\n        color = class_color(det['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{det['label']}:{det['score']:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        y_text = max(th + 4, y1)\n        cv2.rectangle(frame, (x1, y_text - th - 4), (x1 + tw + 2, y_text), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y_text - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Compute and overlay proxy mAP after current frame\n    last_map, _ = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n    map_text = f\"mAP@0.5 (proxy): {last_map:.3f}\"\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    out_writer.write(frame)\n\n    # 4.5 Loop continuation\n    frame_index += 1\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - t_start\nfps_proc = (frame_index / elapsed) if elapsed > 0 else 0.0\n\n# Final proxy mAP computed on all frames\nfinal_map, per_class_ap = compute_proxy_map(predictions_by_class, iou_thresh=0.5)\n\n# Console summary\nprint(\"Processing complete.\")\nprint(f\"Input Method: Read a single video file from the given input_path\")\nprint(f\"Output Method: Output video with rectangles and labels plus calculated mAP\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}, Processing FPS: {fps_proc:.2f}\")\nprint(f\"Estimated mAP@0.5 (proxy, no ground-truth): {final_map:.4f}\")\n\nif len(per_class_ap) > 0:\n    items = sorted(per_class_ap.items(), key=lambda kv: -kv[1])\n    top_k = min(5, len(items))\n    print(\"Top class APs (proxy):\")\n    for i in range(top_k):\n        cid, ap = items[i]\n        label_name = class_to_label(int(cid), labels)\n        print(f\"  {label_name} (id={cid}): AP={ap:.4f}\")\nelse:\n    print(\"No pseudo-GT could be formed; proxy mAP is 0.0.\")\n\n# -------------------------------------------------------------------------\n# REPORT OF THE FIX FOR THE LAST ERROR:\n# The prior error came from assigning model_in_h to a string, causing cv2.resize\n# to receive an invalid dsize. This script correctly sets:\n#   model_in_h = int(input_shape[1])  # Height\n#   model_in_w = int(input_shape[2])  # Width\n# ensuring cv2.resize gets a tuple of integers (width, height).\n# -------------------------------------------------------------------------\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 6960,
            "output": 6463,
            "total": 13423,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07333,
          "latency": 78504.0,
          "timeToFirstToken": 78.504,
          "updatedAt": "2026-02-05T02:37:20.537Z",
          "totalTokens": 13423,
          "costDetails": {
            "total": 0.07333
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 6463,
          "usageDetails": {
            "input": 6960,
            "output": 6463,
            "total": 13423
          },
          "unit": "TOKENS",
          "promptTokens": 6960,
          "environment": "default",
          "createdAt": "2026-02-05T02:37:15.462Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "a1c5c464-ab33-4343-8c89-aafcafa97e49",
          "traceId": "dba3b065",
          "type": "SPAN",
          "name": "error_db_psg_failure_signal_py_sketch_generator",
          "startTime": "2026-02-05T02:37:15.047000+00:00",
          "metadata": {},
          "usage": {
            "input": 0,
            "output": 0,
            "total": 0,
            "unit": "TOKENS"
          },
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Max retries reached with failure. Last error from execution: INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043714_psg_gpt-5-2025-08-07/tmp_20260205043714_psg_gpt-5-2025-08-07.py\", line 374, in <module>\n    input_data = preprocess_frame(frame)\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205043714_psg_gpt-5-2025-08-07/tmp_20260205043714_psg_gpt-5-2025-08-07.py\", line 182, in preprocess_frame\n    resized = cv2.resize(rgb, (model_in_w, model_in_h), interpolation=cv2.INTER_LINEAR)\ncv2.error: OpenCV(4.10.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n>  - Can't parse 'dsize'. Sequence item with index 1 has a wrong type\n\n",
          "calculatedTotalCost": 0.0,
          "updatedAt": "2026-02-05T02:37:20.439Z",
          "totalTokens": 0,
          "costDetails": {},
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 0,
          "usageDetails": {},
          "unit": "TOKENS",
          "promptTokens": 0,
          "environment": "default",
          "createdAt": "2026-02-05T02:37:15.409Z",
          "endTime": null,
          "completionStartTime": null,
          "model": null,
          "modelParameters": null,
          "input": null,
          "version": null,
          "output": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null,
          "timeToFirstToken": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:37:20.530Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:28:39.599Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "d37904d4",
      "timestamp": "2026-02-05T02:25:32.336000+00:00",
      "name": "d379_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TFLite SSD model\nand writes an annotated output video with bounding boxes and labels. It also computes\na running proxy mAP (mean Average Precision) using pseudo ground truth derived from\nhigh-confidence detections (>= 0.8) and evaluates predictions at the configured\nconfidence threshold (>= 0.5) at IoU=0.5.\n\nNotes:\n- Interpreter is imported exactly as required: from ai_edge_litert.interpreter import Interpreter\n- Only standard Python libraries and cv2/numpy are used.\n- Phases 2, 4.2, and 4.3 are explicitly implemented as per the Programming Guidelines.\n\nProxy mAP explanation (due to lack of ground-truth annotations):\n- Pseudo-GT is formed from detections with confidence >= 0.8, with per-class NMS to reduce duplicates.\n- Predictions are detections with confidence >= 0.5 (as configured).\n- For each class, predictions are matched to pseudo-GT by IoU >= 0.5 using a greedy strategy.\n- AP is computed as area under precision-recall curve; mAP is the mean AP across classes that have at least 1 pseudo-GT instance.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.1 Imports: Interpreter must be imported literally as below\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from configuration)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD  = 0.5\nPSEUDO_GT_THRESHOLD = 0.8            # pseudo ground-truth threshold for mAP\nIOU_THRESHOLD = 0.5                  # IoU threshold for evaluation and NMS\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\ndef load_labels(path):\n    labels = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef get_label_name(labels, class_id):\n    \"\"\"\n    Attempts to map model class_id to a label name robustly.\n    Many SSD MobileNet TFLite models use 0-based class indices; some use 1-based.\n    \"\"\"\n    idx = int(class_id)\n    # Prefer 0-based\n    if 0 <= idx < len(labels):\n        return labels[idx]\n    # Try 1-based fallback\n    idx1 = idx - 1\n    if 0 <= idx1 < len(labels):\n        return labels[idx1]\n    return f'class_{int(class_id)}'\n\n# 1.3 Load Labels\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Retrieve input shape and dtype\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # e.g., [1, height, width, 3]\ninput_dtype = input_details[0]['dtype']\ninput_height = int(input_shape[1])\ninput_width = int(input_shape[2])\nfloating_model = (input_dtype == np.float32)\n\n# =========================\n# Utility Functions\n# =========================\n\ndef preprocess_frame(frame_bgr, target_width, target_height, floating):\n    \"\"\"\n    Resize and normalize the frame for model input.\n    Converts BGR (OpenCV) to RGB as commonly expected by TFLite SSD models.\n    Returns input_data shaped [1, H, W, 3] of dtype matching the model input.\n    \"\"\"\n    img_resized = cv2.resize(frame_bgr, (target_width, target_height))\n    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n\n    if floating:\n        input_data = img_rgb.astype(np.float32)\n        input_data = (input_data - 127.5) / 127.5\n    else:\n        input_data = img_rgb.astype(np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef identify_outputs(interpreter, output_details):\n    \"\"\"\n    Retrieve raw output tensors and reliably identify:\n    - boxes: (1, N, 4)\n    - classes: (1, N)\n    - scores: (1, N)\n    - num_detections: (1)\n    Uses shape and value ranges to distinguish classes vs scores.\n    \"\"\"\n    out_tensors = []\n    for od in output_details:\n        out_tensors.append(interpreter.get_tensor(od['index']))\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # First find boxes and num by shape\n    for t in out_tensors:\n        shp = t.shape\n        if len(shp) == 3 and shp[-1] == 4:\n            boxes = t\n        elif len(shp) == 1 and shp[0] == 1:\n            num = t\n\n    # Remaining two are classes and scores\n    cand_1d = [t for t in out_tensors if len(t.shape) == 2]\n    if len(cand_1d) == 2:\n        a, b = cand_1d[0], cand_1d[1]\n        # Determine which is scores based on value ranges [0,1]\n        a_vals = a.flatten()\n        b_vals = b.flatten()\n        a_is_scores = np.mean((a_vals >= 0.0) & (a_vals <= 1.0)) > 0.9\n        b_is_scores = np.mean((b_vals >= 0.0) & (b_vals <= 1.0)) > 0.9\n        # Assign based on heuristic\n        if a_is_scores and not b_is_scores:\n            scores, classes = a, b\n        elif b_is_scores and not a_is_scores:\n            scores, classes = b, a\n        else:\n            # If ambiguous, choose by max value: classes likely have max > 1\n            if np.max(a_vals) > np.max(b_vals):\n                classes, scores = a, b\n            else:\n                classes, scores = b, a\n\n    return boxes, classes, scores, num\n\ndef clip_box(box, w, h):\n    \"\"\"\n    Clip [xmin, ymin, xmax, ymax] to image bounds.\n    \"\"\"\n    x1, y1, x2, y2 = box\n    x1 = max(0, min(int(x1), w - 1))\n    x2 = max(0, min(int(x2), w - 1))\n    y1 = max(0, min(int(y1), h - 1))\n    y2 = max(0, min(int(y2), h - 1))\n    # Ensure proper ordering after clipping\n    if x2 < x1:\n        x1, x2 = x2, x1\n    if y2 < y1:\n        y1, y2 = y2, y1\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"\n    Intersection-over-Union for boxes in [xmin, ymin, xmax, ymax] format.\n    \"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0, xB - xA + 1)\n    inter_h = max(0, yB - yA + 1)\n    inter_area = inter_w * inter_h\n    areaA = max(0, (boxA[2] - boxA[0] + 1)) * max(0, (boxA[3] - boxA[1] + 1))\n    areaB = max(0, (boxB[2] - boxB[0] + 1)) * max(0, (boxB[3] - boxB[1] + 1))\n    denom = float(areaA + areaB - inter_area) if (areaA + areaB - inter_area) > 0 else 1.0\n    return inter_area / denom\n\ndef nms_per_class(boxes, scores, iou_thr=0.5):\n    \"\"\"\n    Non-maximum suppression for a single class.\n    boxes: list of [xmin, ymin, xmax, ymax]\n    scores: list of float\n    Returns indices kept.\n    \"\"\"\n    if len(boxes) == 0:\n        return []\n\n    idxs = np.argsort(-np.array(scores))\n    keep = []\n    boxes_arr = np.array(boxes, dtype=np.float32)\n\n    while len(idxs) > 0:\n        i = idxs[0]\n        keep.append(i)\n        if len(idxs) == 1:\n            break\n        ious = []\n        for j in idxs[1:]:\n            ious.append(iou_xyxy(boxes_arr[i], boxes_arr[j]))\n        ious = np.array(ious)\n        remain = np.where(ious <= iou_thr)[0]\n        idxs = idxs[remain + 1]\n    return keep\n\ndef compute_map_pseudo_gt(gts_by_class, preds_by_class, iou_thr=0.5):\n    \"\"\"\n    Compute mAP across classes using pseudo ground truth.\n    gts_by_class: dict[class_id] -> dict[frame_idx] -> list of boxes\n    preds_by_class: dict[class_id] -> list of tuples (frame_idx, score, box)\n    Returns (mAP, AP_per_class_dict, num_classes_contributing)\n    \"\"\"\n    ap_dict = {}\n    contributing = 0\n    for cls_id in sorted(set(list(gts_by_class.keys()) + list(preds_by_class.keys()))):\n        gt_frames = gts_by_class.get(cls_id, {})\n        preds = preds_by_class.get(cls_id, [])\n        # Count GT instances\n        npos = sum(len(boxes) for boxes in gt_frames.values())\n        if npos == 0:\n            # No GT for this class; skip in mean\n            ap_dict[cls_id] = 0.0\n            continue\n\n        contributing += 1\n        # Sort predictions by descending score\n        preds_sorted = sorted(preds, key=lambda x: -x[1])\n        # Track matches per frame\n        gt_matched = {fidx: np.zeros(len(gt_frames[fidx]), dtype=bool) for fidx in gt_frames}\n\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n\n        for i, (fidx, score, pbox) in enumerate(preds_sorted):\n            if fidx not in gt_frames or len(gt_frames[fidx]) == 0:\n                fp[i] = 1.0\n                continue\n            gt_boxes = gt_frames[fidx]\n            ious = np.array([iou_xyxy(pbox, g) for g in gt_boxes], dtype=np.float32)\n            max_iou_idx = int(np.argmax(ious))\n            max_iou = float(ious[max_iou_idx])\n            if max_iou >= iou_thr and not gt_matched[fidx][max_iou_idx]:\n                tp[i] = 1.0\n                gt_matched[fidx][max_iou_idx] = True\n            else:\n                fp[i] = 1.0\n\n        # Precision-Recall\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        rec = cum_tp / float(npos)\n        prec = cum_tp / np.maximum(cum_tp + cum_fp, 1e-12)\n\n        # AP as area under precision envelope\n        mrec = np.concatenate(([0.0], rec, [1.0]))\n        mpre = np.concatenate(([0.0], prec, [0.0]))\n        for i in range(len(mpre) - 2, -1, -1):\n            mpre[i] = max(mpre[i], mpre[i + 1])\n        # Sum over recall steps\n        ap = 0.0\n        for i in range(len(mrec) - 1):\n            if mrec[i + 1] != mrec[i]:\n                ap += (mrec[i + 1] - mrec[i]) * mpre[i + 1]\n        ap_dict[cls_id] = ap\n\n    if contributing == 0:\n        mAP = 0.0\n    else:\n        mAP = float(np.mean([ap_dict[c] for c in ap_dict if c in gts_by_class and sum(len(v) for v in gts_by_class[c].values()) > 0]))\n    return mAP, ap_dict, contributing\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# 2.1 Acquire Input Data: Read a single video file from the given input_path\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Get video properties\norig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0:\n    fps = 25.0  # fallback\n\n# Prepare VideoWriter for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_width, orig_height))\nif not writer.isOpened():\n    cap.release()\n    raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\nprint(\"Starting inference on video...\")\nprint(f\"Model: {MODEL_PATH}\")\nprint(f\"Labels: {LABEL_PATH}\")\nprint(f\"Input: {INPUT_PATH}\")\nprint(f\"Output: {OUTPUT_PATH}\")\nprint(f\"Confidence threshold (predictions): {CONFIDENCE_THRESHOLD}\")\nprint(f\"Pseudo-GT threshold: {PSEUDO_GT_THRESHOLD}, IoU threshold: {IOU_THRESHOLD}\")\n\n# Data structures for proxy mAP computation\n# gts_by_class: dict[class_id] -> dict[frame_idx] -> list of boxes\ngts_by_class = {}\n# preds_by_class: dict[class_id] -> list of tuples (frame_idx, score, box)\npreds_by_class = {}\n\nframe_index = 0\nstart_time = time.time()\n\n# 2.4 Loop through frames\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break\n\n    # 2.2 Preprocess Data (resize/normalize based on input_details; store as input_data)\n    input_data = preprocess_frame(frame_bgr, input_width, input_height, floating_model)\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling Loop\n    # =========================\n    # 4.1 Get Output Tensors\n    boxes_raw, classes_raw, scores_raw, num_raw = identify_outputs(interpreter, output_details)\n\n    # Extract and reshape outputs\n    if boxes_raw is None or classes_raw is None or scores_raw is None:\n        # Clean up resources before raising\n        cap.release()\n        writer.release()\n        raise RuntimeError(\"Failed to identify output tensors (boxes/classes/scores).\")\n\n    boxes = np.squeeze(boxes_raw)       # shape (N, 4) normalized [ymin, xmin, ymax, xmax]\n    classes = np.squeeze(classes_raw)   # shape (N,)\n    scores = np.squeeze(scores_raw)     # shape (N,)\n    if num_raw is not None:\n        num_detections = int(np.squeeze(num_raw).astype(np.int32))\n        boxes = boxes[:num_detections]\n        classes = classes[:num_detections]\n        scores = scores[:num_detections]\n\n    # 4.2 Interpret Results: convert to absolute coords, map classes to labels\n    detections = []\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score <= 0.0:\n            continue\n        # Decode normalized boxes to absolute pixel coords on original frame\n        ymin, xmin, ymax, xmax = boxes[i]\n        x1 = int(xmin * orig_width)\n        y1 = int(ymin * orig_height)\n        x2 = int(xmax * orig_width)\n        y2 = int(ymax * orig_height)\n        # 4.3 Post-processing: clipping and thresholding later\n        box_xyxy = clip_box([x1, y1, x2, y2], orig_width, orig_height)\n        class_id = int(classes[i])\n        label_name = get_label_name(labels, class_id)\n        detections.append({\n            'class_id': class_id,\n            'label': label_name,\n            'score': score,\n            'box': box_xyxy\n        })\n\n    # 4.3 Post-processing: apply confidence thresholding for predictions; bounding box clipping already applied\n    # Prepare predictions (>= CONFIDENCE_THRESHOLD) and pseudo-GT (>= PSEUDO_GT_THRESHOLD)\n    # Pseudo-GT per class with NMS to reduce duplicates\n    frame_pseudo_gt_by_class = {}\n    for det in detections:\n        cid = det['class_id']\n        if det['score'] >= PSEUDO_GT_THRESHOLD:\n            frame_pseudo_gt_by_class.setdefault(cid, {'boxes': [], 'scores': []})\n            frame_pseudo_gt_by_class[cid]['boxes'].append(det['box'])\n            frame_pseudo_gt_by_class[cid]['scores'].append(det['score'])\n\n    # Apply per-class NMS for pseudo-GT\n    for cid, val in frame_pseudo_gt_by_class.items():\n        boxes_c = val['boxes']\n        scores_c = val['scores']\n        keep_idx = nms_per_class(boxes_c, scores_c, iou_thr=IOU_THRESHOLD)\n        kept_boxes = [boxes_c[k] for k in keep_idx]\n        # Update global GT dict\n        gts_by_class.setdefault(cid, {})\n        gts_by_class[cid][frame_index] = kept_boxes\n\n    # Prepare predictions list per class\n    for det in detections:\n        if det['score'] >= CONFIDENCE_THRESHOLD:\n            cid = det['class_id']\n            preds_by_class.setdefault(cid, [])\n            preds_by_class[cid].append((frame_index, det['score'], det['box']))\n\n    # 4.2 Continue: draw detections that pass prediction threshold\n    # Draw rectangle and label text on frame\n    for det in detections:\n        if det['score'] >= CONFIDENCE_THRESHOLD:\n            x1, y1, x2, y2 = det['box']\n            label = det['label']\n            score = det['score']\n            # Draw bounding box\n            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            # Put label and score\n            text = f\"{label}: {score:.2f}\"\n            (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame_bgr, (x1, y1 - th - baseline), (x1 + tw, y1), (0, 255, 0), -1)\n            cv2.putText(frame_bgr, text, (x1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Compute running proxy mAP and overlay on the frame\n    mAP_val, ap_dict, contributing_classes = compute_map_pseudo_gt(gts_by_class, preds_by_class, iou_thr=IOU_THRESHOLD)\n    map_text = f\"mAP (pseudo-GT@{PSEUDO_GT_THRESHOLD:.1f}, IoU@{IOU_THRESHOLD:.1f}): {mAP_val:.3f}\"\n    if contributing_classes == 0:\n        map_text += \" [insufficient pseudo-GT]\"\n    cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 50, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    writer.write(frame_bgr)\n\n    # 4.5 Loop Continuation\n    frame_index += 1\n    if frame_index % 30 == 0:\n        elapsed = time.time() - start_time\n        print(f\"Processed {frame_index} frames in {elapsed:.1f}s; Current proxy mAP: {mAP_val:.3f}\")\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nwriter.release()\ntotal_time = time.time() - start_time\n\n# Final mAP computation and reporting\nfinal_mAP, final_ap_dict, contributing_classes = compute_map_pseudo_gt(gts_by_class, preds_by_class, iou_thr=IOU_THRESHOLD)\nprint(\"\\nInference complete.\")\nprint(f\"Total frames processed: {frame_index}\")\nprint(f\"Total time: {total_time:.2f}s, FPS (processing): {frame_index / max(total_time, 1e-6):.2f}\")\nprint(f\"Final proxy mAP (pseudo-GT@{PSEUDO_GT_THRESHOLD:.1f}, IoU@{IOU_THRESHOLD:.1f}): {final_mAP:.4f}\")\nif contributing_classes == 0:\n    print(\"Note: No pseudo ground truth instances were found; mAP is not meaningful.\")\nelse:\n    # Optionally, list AP for top few classes present\n    present_classes = sorted([cid for cid in final_ap_dict if cid in gts_by_class and sum(len(v) for v in gts_by_class[cid].values()) > 0])\n    print(\"Per-class AP (for classes with pseudo-GT):\")\n    for cid in present_classes[:10]:  # limit printout\n        print(f\"  {get_label_name(labels, cid)} (id {cid}): AP={final_ap_dict[cid]:.4f}\")\nprint(f\"Annotated video saved to: {OUTPUT_PATH}\")"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/d37904d4",
      "latency": 124.945,
      "total_cost": 0.093651,
      "observations": [
        {
          "id": "time-04-25-32-982126_chatcmpl-D5jLhmGFHa7rX9BhdPVlBncljUloe",
          "traceId": "d37904d4",
          "type": "GENERATION",
          "name": "d379_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:25:32.982000+00:00",
          "endTime": "2026-02-05T02:27:37.927000+00:00",
          "completionStartTime": "2026-02-05T02:27:37.927000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.09365100000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TFLite SSD model\nand writes an annotated output video with bounding boxes and labels. It also computes\na running proxy mAP (mean Average Precision) using pseudo ground truth derived from\nhigh-confidence detections (>= 0.8) and evaluates predictions at the configured\nconfidence threshold (>= 0.5) at IoU=0.5.\n\nNotes:\n- Interpreter is imported exactly as required: from ai_edge_litert.interpreter import Interpreter\n- Only standard Python libraries and cv2/numpy are used.\n- Phases 2, 4.2, and 4.3 are explicitly implemented as per the Programming Guidelines.\n\nProxy mAP explanation (due to lack of ground-truth annotations):\n- Pseudo-GT is formed from detections with confidence >= 0.8, with per-class NMS to reduce duplicates.\n- Predictions are detections with confidence >= 0.5 (as configured).\n- For each class, predictions are matched to pseudo-GT by IoU >= 0.5 using a greedy strategy.\n- AP is computed as area under precision-recall curve; mAP is the mean AP across classes that have at least 1 pseudo-GT instance.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.1 Imports: Interpreter must be imported literally as below\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from configuration)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONFIDENCE_THRESHOLD = float('0.5')  # predictions threshold\nPSEUDO_GT_THRESHOLD = 0.8            # pseudo ground-truth threshold for mAP\nIOU_THRESHOLD = 0.5                  # IoU threshold for evaluation and NMS\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\ndef load_labels(path):\n    labels = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef get_label_name(labels, class_id):\n    \"\"\"\n    Attempts to map model class_id to a label name robustly.\n    Many SSD MobileNet TFLite models use 0-based class indices; some use 1-based.\n    \"\"\"\n    idx = int(class_id)\n    # Prefer 0-based\n    if 0 <= idx < len(labels):\n        return labels[idx]\n    # Try 1-based fallback\n    idx1 = idx - 1\n    if 0 <= idx1 < len(labels):\n        return labels[idx1]\n    return f'class_{int(class_id)}'\n\n# 1.3 Load Labels\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Retrieve input shape and dtype\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # e.g., [1, height, width, 3]\ninput_dtype = input_details[0]['dtype']\ninput_height = int(input_shape[1])\ninput_width = int(input_shape[2])\nfloating_model = (input_dtype == np.float32)\n\n# =========================\n# Utility Functions\n# =========================\n\ndef preprocess_frame(frame_bgr, target_width, target_height, floating):\n    \"\"\"\n    Resize and normalize the frame for model input.\n    Converts BGR (OpenCV) to RGB as commonly expected by TFLite SSD models.\n    Returns input_data shaped [1, H, W, 3] of dtype matching the model input.\n    \"\"\"\n    img_resized = cv2.resize(frame_bgr, (target_width, target_height))\n    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n\n    if floating:\n        input_data = img_rgb.astype(np.float32)\n        input_data = (input_data - 127.5) / 127.5\n    else:\n        input_data = img_rgb.astype(np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef identify_outputs(interpreter, output_details):\n    \"\"\"\n    Retrieve raw output tensors and reliably identify:\n    - boxes: (1, N, 4)\n    - classes: (1, N)\n    - scores: (1, N)\n    - num_detections: (1)\n    Uses shape and value ranges to distinguish classes vs scores.\n    \"\"\"\n    out_tensors = []\n    for od in output_details:\n        out_tensors.append(interpreter.get_tensor(od['index']))\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # First find boxes and num by shape\n    for t in out_tensors:\n        shp = t.shape\n        if len(shp) == 3 and shp[-1] == 4:\n            boxes = t\n        elif len(shp) == 1 and shp[0] == 1:\n            num = t\n\n    # Remaining two are classes and scores\n    cand_1d = [t for t in out_tensors if len(t.shape) == 2]\n    if len(cand_1d) == 2:\n        a, b = cand_1d[0], cand_1d[1]\n        # Determine which is scores based on value ranges [0,1]\n        a_vals = a.flatten()\n        b_vals = b.flatten()\n        a_is_scores = np.mean((a_vals >= 0.0) & (a_vals <= 1.0)) > 0.9\n        b_is_scores = np.mean((b_vals >= 0.0) & (b_vals <= 1.0)) > 0.9\n        # Assign based on heuristic\n        if a_is_scores and not b_is_scores:\n            scores, classes = a, b\n        elif b_is_scores and not a_is_scores:\n            scores, classes = b, a\n        else:\n            # If ambiguous, choose by max value: classes likely have max > 1\n            if np.max(a_vals) > np.max(b_vals):\n                classes, scores = a, b\n            else:\n                classes, scores = b, a\n\n    return boxes, classes, scores, num\n\ndef clip_box(box, w, h):\n    \"\"\"\n    Clip [xmin, ymin, xmax, ymax] to image bounds.\n    \"\"\"\n    x1, y1, x2, y2 = box\n    x1 = max(0, min(int(x1), w - 1))\n    x2 = max(0, min(int(x2), w - 1))\n    y1 = max(0, min(int(y1), h - 1))\n    y2 = max(0, min(int(y2), h - 1))\n    # Ensure proper ordering after clipping\n    if x2 < x1:\n        x1, x2 = x2, x1\n    if y2 < y1:\n        y1, y2 = y2, y1\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    \"\"\"\n    Intersection-over-Union for boxes in [xmin, ymin, xmax, ymax] format.\n    \"\"\"\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0, xB - xA + 1)\n    inter_h = max(0, yB - yA + 1)\n    inter_area = inter_w * inter_h\n    areaA = max(0, (boxA[2] - boxA[0] + 1)) * max(0, (boxA[3] - boxA[1] + 1))\n    areaB = max(0, (boxB[2] - boxB[0] + 1)) * max(0, (boxB[3] - boxB[1] + 1))\n    denom = float(areaA + areaB - inter_area) if (areaA + areaB - inter_area) > 0 else 1.0\n    return inter_area / denom\n\ndef nms_per_class(boxes, scores, iou_thr=0.5):\n    \"\"\"\n    Non-maximum suppression for a single class.\n    boxes: list of [xmin, ymin, xmax, ymax]\n    scores: list of float\n    Returns indices kept.\n    \"\"\"\n    if len(boxes) == 0:\n        return []\n\n    idxs = np.argsort(-np.array(scores))\n    keep = []\n    boxes_arr = np.array(boxes, dtype=np.float32)\n\n    while len(idxs) > 0:\n        i = idxs[0]\n        keep.append(i)\n        if len(idxs) == 1:\n            break\n        ious = []\n        for j in idxs[1:]:\n            ious.append(iou_xyxy(boxes_arr[i], boxes_arr[j]))\n        ious = np.array(ious)\n        remain = np.where(ious <= iou_thr)[0]\n        idxs = idxs[remain + 1]\n    return keep\n\ndef compute_map_pseudo_gt(gts_by_class, preds_by_class, iou_thr=0.5):\n    \"\"\"\n    Compute mAP across classes using pseudo ground truth.\n    gts_by_class: dict[class_id] -> dict[frame_idx] -> list of boxes\n    preds_by_class: dict[class_id] -> list of tuples (frame_idx, score, box)\n    Returns (mAP, AP_per_class_dict, num_classes_contributing)\n    \"\"\"\n    ap_dict = {}\n    contributing = 0\n    for cls_id in sorted(set(list(gts_by_class.keys()) + list(preds_by_class.keys()))):\n        gt_frames = gts_by_class.get(cls_id, {})\n        preds = preds_by_class.get(cls_id, [])\n        # Count GT instances\n        npos = sum(len(boxes) for boxes in gt_frames.values())\n        if npos == 0:\n            # No GT for this class; skip in mean\n            ap_dict[cls_id] = 0.0\n            continue\n\n        contributing += 1\n        # Sort predictions by descending score\n        preds_sorted = sorted(preds, key=lambda x: -x[1])\n        # Track matches per frame\n        gt_matched = {fidx: np.zeros(len(gt_frames[fidx]), dtype=bool) for fidx in gt_frames}\n\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n\n        for i, (fidx, score, pbox) in enumerate(preds_sorted):\n            if fidx not in gt_frames or len(gt_frames[fidx]) == 0:\n                fp[i] = 1.0\n                continue\n            gt_boxes = gt_frames[fidx]\n            ious = np.array([iou_xyxy(pbox, g) for g in gt_boxes], dtype=np.float32)\n            max_iou_idx = int(np.argmax(ious))\n            max_iou = float(ious[max_iou_idx])\n            if max_iou >= iou_thr and not gt_matched[fidx][max_iou_idx]:\n                tp[i] = 1.0\n                gt_matched[fidx][max_iou_idx] = True\n            else:\n                fp[i] = 1.0\n\n        # Precision-Recall\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        rec = cum_tp / float(npos)\n        prec = cum_tp / np.maximum(cum_tp + cum_fp, 1e-12)\n\n        # AP as area under precision envelope\n        mrec = np.concatenate(([0.0], rec, [1.0]))\n        mpre = np.concatenate(([0.0], prec, [0.0]))\n        for i in range(len(mpre) - 2, -1, -1):\n            mpre[i] = max(mpre[i], mpre[i + 1])\n        # Sum over recall steps\n        ap = 0.0\n        for i in range(len(mrec) - 1):\n            if mrec[i + 1] != mrec[i]:\n                ap += (mrec[i + 1] - mrec[i]) * mpre[i + 1]\n        ap_dict[cls_id] = ap\n\n    if contributing == 0:\n        mAP = 0.0\n    else:\n        mAP = float(np.mean([ap_dict[c] for c in ap_dict if c in gts_by_class and sum(len(v) for v in gts_by_class[c].values()) > 0]))\n    return mAP, ap_dict, contributing\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# 2.1 Acquire Input Data: Read a single video file from the given input_path\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Get video properties\norig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0:\n    fps = 25.0  # fallback\n\n# Prepare VideoWriter for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_width, orig_height))\nif not writer.isOpened():\n    cap.release()\n    raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\nprint(\"Starting inference on video...\")\nprint(f\"Model: {MODEL_PATH}\")\nprint(f\"Labels: {LABEL_PATH}\")\nprint(f\"Input: {INPUT_PATH}\")\nprint(f\"Output: {OUTPUT_PATH}\")\nprint(f\"Confidence threshold (predictions): {CONFIDENCE_THRESHOLD}\")\nprint(f\"Pseudo-GT threshold: {PSEUDO_GT_THRESHOLD}, IoU threshold: {IOU_THRESHOLD}\")\n\n# Data structures for proxy mAP computation\n# gts_by_class: dict[class_id] -> dict[frame_idx] -> list of boxes\ngts_by_class = {}\n# preds_by_class: dict[class_id] -> list of tuples (frame_idx, score, box)\npreds_by_class = {}\n\nframe_index = 0\nstart_time = time.time()\n\n# 2.4 Loop through frames\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break\n\n    # 2.2 Preprocess Data (resize/normalize based on input_details; store as input_data)\n    input_data = preprocess_frame(frame_bgr, input_width, input_height, floating_model)\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling Loop\n    # =========================\n    # 4.1 Get Output Tensors\n    boxes_raw, classes_raw, scores_raw, num_raw = identify_outputs(interpreter, output_details)\n\n    # Extract and reshape outputs\n    if boxes_raw is None or classes_raw is None or scores_raw is None:\n        # Clean up resources before raising\n        cap.release()\n        writer.release()\n        raise RuntimeError(\"Failed to identify output tensors (boxes/classes/scores).\")\n\n    boxes = np.squeeze(boxes_raw)       # shape (N, 4) normalized [ymin, xmin, ymax, xmax]\n    classes = np.squeeze(classes_raw)   # shape (N,)\n    scores = np.squeeze(scores_raw)     # shape (N,)\n    if num_raw is not None:\n        num_detections = int(np.squeeze(num_raw).astype(np.int32))\n        boxes = boxes[:num_detections]\n        classes = classes[:num_detections]\n        scores = scores[:num_detections]\n\n    # 4.2 Interpret Results: convert to absolute coords, map classes to labels\n    detections = []\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score <= 0.0:\n            continue\n        # Decode normalized boxes to absolute pixel coords on original frame\n        ymin, xmin, ymax, xmax = boxes[i]\n        x1 = int(xmin * orig_width)\n        y1 = int(ymin * orig_height)\n        x2 = int(xmax * orig_width)\n        y2 = int(ymax * orig_height)\n        # 4.3 Post-processing: clipping and thresholding later\n        box_xyxy = clip_box([x1, y1, x2, y2], orig_width, orig_height)\n        class_id = int(classes[i])\n        label_name = get_label_name(labels, class_id)\n        detections.append({\n            'class_id': class_id,\n            'label': label_name,\n            'score': score,\n            'box': box_xyxy\n        })\n\n    # 4.3 Post-processing: apply confidence thresholding for predictions; bounding box clipping already applied\n    # Prepare predictions (>= CONFIDENCE_THRESHOLD) and pseudo-GT (>= PSEUDO_GT_THRESHOLD)\n    # Pseudo-GT per class with NMS to reduce duplicates\n    frame_pseudo_gt_by_class = {}\n    for det in detections:\n        cid = det['class_id']\n        if det['score'] >= PSEUDO_GT_THRESHOLD:\n            frame_pseudo_gt_by_class.setdefault(cid, {'boxes': [], 'scores': []})\n            frame_pseudo_gt_by_class[cid]['boxes'].append(det['box'])\n            frame_pseudo_gt_by_class[cid]['scores'].append(det['score'])\n\n    # Apply per-class NMS for pseudo-GT\n    for cid, val in frame_pseudo_gt_by_class.items():\n        boxes_c = val['boxes']\n        scores_c = val['scores']\n        keep_idx = nms_per_class(boxes_c, scores_c, iou_thr=IOU_THRESHOLD)\n        kept_boxes = [boxes_c[k] for k in keep_idx]\n        # Update global GT dict\n        gts_by_class.setdefault(cid, {})\n        gts_by_class[cid][frame_index] = kept_boxes\n\n    # Prepare predictions list per class\n    for det in detections:\n        if det['score'] >= CONFIDENCE_THRESHOLD:\n            cid = det['class_id']\n            preds_by_class.setdefault(cid, [])\n            preds_by_class[cid].append((frame_index, det['score'], det['box']))\n\n    # 4.2 Continue: draw detections that pass prediction threshold\n    # Draw rectangle and label text on frame\n    for det in detections:\n        if det['score'] >= CONFIDENCE_THRESHOLD:\n            x1, y1, x2, y2 = det['box']\n            label = det['label']\n            score = det['score']\n            # Draw bounding box\n            cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            # Put label and score\n            text = f\"{label}: {score:.2f}\"\n            (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame_bgr, (x1, y1 - th - baseline), (x1 + tw, y1), (0, 255, 0), -1)\n            cv2.putText(frame_bgr, text, (x1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Compute running proxy mAP and overlay on the frame\n    mAP_val, ap_dict, contributing_classes = compute_map_pseudo_gt(gts_by_class, preds_by_class, iou_thr=IOU_THRESHOLD)\n    map_text = f\"mAP (pseudo-GT@{PSEUDO_GT_THRESHOLD:.1f}, IoU@{IOU_THRESHOLD:.1f}): {mAP_val:.3f}\"\n    if contributing_classes == 0:\n        map_text += \" [insufficient pseudo-GT]\"\n    cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 50, 255), 2, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    writer.write(frame_bgr)\n\n    # 4.5 Loop Continuation\n    frame_index += 1\n    if frame_index % 30 == 0:\n        elapsed = time.time() - start_time\n        print(f\"Processed {frame_index} frames in {elapsed:.1f}s; Current proxy mAP: {mAP_val:.3f}\")\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nwriter.release()\ntotal_time = time.time() - start_time\n\n# Final mAP computation and reporting\nfinal_mAP, final_ap_dict, contributing_classes = compute_map_pseudo_gt(gts_by_class, preds_by_class, iou_thr=IOU_THRESHOLD)\nprint(\"\\nInference complete.\")\nprint(f\"Total frames processed: {frame_index}\")\nprint(f\"Total time: {total_time:.2f}s, FPS (processing): {frame_index / max(total_time, 1e-6):.2f}\")\nprint(f\"Final proxy mAP (pseudo-GT@{PSEUDO_GT_THRESHOLD:.1f}, IoU@{IOU_THRESHOLD:.1f}): {final_mAP:.4f}\")\nif contributing_classes == 0:\n    print(\"Note: No pseudo ground truth instances were found; mAP is not meaningful.\")\nelse:\n    # Optionally, list AP for top few classes present\n    present_classes = sorted([cid for cid in final_ap_dict if cid in gts_by_class and sum(len(v) for v in gts_by_class[cid].values()) > 0])\n    print(\"Per-class AP (for classes with pseudo-GT):\")\n    for cid in present_classes[:10]:  # limit printout\n        print(f\"  {get_label_name(labels, cid)} (id {cid}): AP={final_ap_dict[cid]:.4f}\")\nprint(f\"Annotated video saved to: {OUTPUT_PATH}\")\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 9336,
            "total": 10836,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.093651,
          "latency": 124945.0,
          "timeToFirstToken": 124.945,
          "updatedAt": "2026-02-05T02:27:43.405Z",
          "totalTokens": 10836,
          "costDetails": {
            "total": 0.093651
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 9336,
          "usageDetails": {
            "input": 1500,
            "output": 9336,
            "total": 10836
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:27:38.279Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:28:33.583Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:25:32.894Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "769b6961",
      "timestamp": "2026-02-05T02:21:21.566000+00:00",
      "name": "769b_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Interpreter must be imported literally as specified\ntry:\n    from ai_edge_litert.interpreter import Interpreter\nexcept Exception as e:\n    raise SystemExit(\"Failed to import ai_edge_litert Interpreter. Ensure the 'ai_edge_litert' package is available. Error: {}\".format(e))\n\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    else:\n        # Fallback to an empty list if labels file is missing (script remains runnable)\n        labels = []\n    return labels\n\ndef ensure_dir(path):\n    directory = os.path.dirname(path)\n    if directory and not os.path.isdir(directory):\n        os.makedirs(directory, exist_ok=True)\n\ndef get_input_size_and_format(input_details):\n    shape = input_details[0]['shape']\n    # Default to NHWC if possible\n    if len(shape) == 4:\n        if shape[3] == 3:\n            # NHWC\n            height, width, channels = int(shape[1]), int(shape[2]), int(shape[3])\n            data_format = 'NHWC'\n        elif shape[1] == 3:\n            # NCHW\n            height, width, channels = int(shape[2]), int(shape[3]), int(shape[1])\n            data_format = 'NCHW'\n        else:\n            # Fallback assume NHWC (most TFLite models)\n            height, width, channels = int(shape[1]), int(shape[2]), 3\n            data_format = 'NHWC'\n    else:\n        raise SystemExit(\"Unexpected input tensor shape: {}\".format(shape))\n    return height, width, channels, data_format\n\ndef preprocess_frame(frame_bgr, in_h, in_w, dtype, data_format):\n    # Convert BGR to RGB as SSD MobileNet typically expects RGB input\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (in_w, in_h))\n    input_data = resized\n    # Expand to batch and reorder if necessary\n    if data_format == 'NHWC':\n        input_data = np.expand_dims(input_data, axis=0)\n    else:\n        # NCHW\n        input_data = np.transpose(input_data, (2, 0, 1))  # HWC -> CHW\n        input_data = np.expand_dims(input_data, axis=0)\n    # Convert dtype as required\n    if dtype == np.float32:\n        input_data = input_data.astype(np.float32)\n        # Phase 2.3 Quantization Handling - normalize for floating model\n        input_data = (input_data - 127.5) / 127.5\n    else:\n        input_data = input_data.astype(dtype)\n    return input_data\n\ndef iou(boxA, boxB):\n    # Boxes: [xmin, ymin, xmax, ymax]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interW = max(0, xB - xA)\n    interH = max(0, yB - yA)\n    interArea = interW * interH\n    areaA = max(0, boxA[2] - boxA[0]) * max(0, boxA[3] - boxA[1])\n    areaB = max(0, boxB[2] - boxB[0]) * max(0, boxB[3] - boxB[1])\n    denom = areaA + areaB - interArea\n    if denom <= 0:\n        return 0.0\n    return interArea / denom\n\ndef cluster_and_mark_tp_fp(dets, iou_threshold=0.5):\n    \"\"\"\n    dets: list of dicts: {'score': float, 'box': [xmin, ymin, xmax, ymax]}\n    Returns:\n      dets_sorted: same det dicts with additional 'is_tp' boolean, sorted by score desc\n      n_gt: number of pseudo ground truth objects (clusters)\n    \"\"\"\n    if not dets:\n        return [], 0\n    # Sort detections by score descending\n    dets_sorted = sorted(dets, key=lambda d: d['score'], reverse=True)\n    clusters = []  # each cluster: {'box': [..], 'members': [det_indices]}\n    # Assign detections to clusters; first member of each cluster is TP, others are FP\n    for det in dets_sorted:\n        assigned = False\n        for cl in clusters:\n            if iou(det['box'], cl['box']) > iou_threshold:\n                det['is_tp'] = False\n                cl['members'].append(det)\n                assigned = True\n                break\n        if not assigned:\n            det['is_tp'] = True\n            clusters.append({'box': det['box'], 'members': [det]})\n    n_gt = len(clusters)\n    return dets_sorted, n_gt\n\ndef compute_ap(dets_sorted, n_gt):\n    \"\"\"\n    Compute Average Precision using precision envelope integration.\n    dets_sorted: list of det dicts with 'is_tp' key sorted by score desc\n    n_gt: number of pseudo ground truth objects\n    Returns AP float or None if not computable (n_gt == 0 and no detections)\n    \"\"\"\n    if n_gt == 0:\n        if len(dets_sorted) == 0:\n            return None  # No positives and no predictions: skip for averaging\n        else:\n            return 0.0   # No ground truth but predictions exist: AP=0\n    tp = np.array([1 if d['is_tp'] else 0 for d in dets_sorted], dtype=np.float32)\n    fp = 1.0 - tp\n    tp_cum = np.cumsum(tp)\n    fp_cum = np.cumsum(fp)\n    precision = tp_cum / np.maximum(tp_cum + fp_cum, 1e-12)\n    recall = tp_cum / float(n_gt)\n    # Precision envelope\n    # Add boundary points\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([0.0], precision, [0.0]))\n    for i in range(len(mpre) - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n    # Integrate area under curve where recall changes\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n    return float(ap)\n\ndef color_for_class(class_id):\n    # Deterministic color from class_id\n    np.random.seed(class_id + 12345)\n    color = tuple(int(c) for c in np.random.randint(0, 255, size=3))\n    # OpenCV uses BGR\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef get_label_name(labels, class_id):\n    if 0 <= class_id < len(labels):\n        return labels[class_id]\n    return \"id_{}\".format(class_id)\n\ndef main():\n    # 1.2 Paths/Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')\n\n    # 1.3 Load Labels if available and relevant\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    if not os.path.isfile(model_path):\n        raise SystemExit(\"Model file not found at: {}\".format(model_path))\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    input_index = input_details[0]['index']\n    input_dtype = input_details[0]['dtype']\n    in_h, in_w, in_c, data_format = get_input_size_and_format(input_details)\n    floating_model = (input_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: single video file\n    if not os.path.isfile(input_path):\n        raise SystemExit(\"Input video file not found at: {}\".format(input_path))\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise SystemExit(\"Failed to open input video: {}\".format(input_path))\n\n    # Prepare output writer\n    ensure_dir(output_path)\n    # Try to use original FPS and size; fallback if unavailable\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 25.0\n    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    if orig_w <= 0 or orig_h <= 0:\n        # Read a frame to determine size\n        ret, test_frame = cap.read()\n        if not ret:\n            cap.release()\n            raise SystemExit(\"Could not read frames from video.\")\n        orig_h, orig_w = test_frame.shape[:2]\n        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # reset to start\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (orig_w, orig_h))\n    if not writer.isOpened():\n        cap.release()\n        raise SystemExit(\"Failed to open output video writer at: {}\".format(output_path))\n\n    # Variables for output mapping determination (boxes/classes/scores/num_dets)\n    mapping_determined = False\n    boxes_idx = None\n    classes_idx = None\n    scores_idx = None\n    num_idx = None\n\n    # Variables for mAP accumulation across frames and classes\n    per_class_ap_sum = {}\n    per_class_ap_count = {}\n\n    frame_index = 0\n    start_time = time.time()\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # End of video\n\n        # 2.2 Preprocess Data\n        input_data = preprocess_frame(frame_bgr, in_h, in_w, input_dtype, data_format)\n\n        # 3.1 Set Input Tensor\n        interpreter.set_tensor(input_index, input_data)\n        # 3.2 Run Inference\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling\n        # 4.1 Get Output Tensors\n        outputs = [interpreter.get_tensor(out['index']) for out in output_details]\n\n        # Determine mapping on first frame using shapes and value ranges\n        if not mapping_determined:\n            for i, arr in enumerate(outputs):\n                a = np.array(arr)\n                if a.size == 1:\n                    num_idx = i\n                elif a.ndim == 3 and a.shape[-1] == 4:\n                    boxes_idx = i\n            # For classes and scores, both are shape (1, N). Distinguish by value range.\n            candidates = [i for i in range(len(outputs)) if i not in (boxes_idx, num_idx)]\n            for i in candidates:\n                arr = np.array(outputs[i]).astype(np.float32)\n                # scores are within [0,1], classes usually > 1\n                if np.all(arr >= 0.0) and np.all(arr <= 1.0):\n                    scores_idx = i\n                else:\n                    classes_idx = i\n            # Fallback if still ambiguous: try common order [boxes, classes, scores, num]\n            if boxes_idx is None or classes_idx is None or scores_idx is None or num_idx is None:\n                if len(outputs) >= 4:\n                    boxes_idx = 0 if boxes_idx is None else boxes_idx\n                    classes_idx = 1 if classes_idx is None else classes_idx\n                    scores_idx = 2 if scores_idx is None else scores_idx\n                    num_idx = 3 if num_idx is None else num_idx\n                else:\n                    cap.release()\n                    writer.release()\n                    raise SystemExit(\"Could not determine TFLite detection output mappings.\")\n            mapping_determined = True\n\n        # Extract tensors using determined indices\n        boxes = np.array(outputs[boxes_idx])[0]  # shape [N, 4], normalized [ymin, xmin, ymax, xmax]\n        classes = np.array(outputs[classes_idx])[0]\n        scores = np.array(outputs[scores_idx])[0]\n        num_dets = int(np.array(outputs[num_idx]).reshape(-1)[0]) if outputs[num_idx].size >= 1 else len(scores)\n        num_dets = min(num_dets, boxes.shape[0], scores.shape[0], classes.shape[0])\n\n        # 4.2 Interpret Results: collect detections per class with bounding boxes and labels\n        detections_per_class = {}  # class_id -> list of det dicts {'score': float, 'box': [xmin, ymin, xmax, ymax]}\n        drawn_boxes = []  # boxes to draw (top of clusters or all detections), list of tuples (box, label, score)\n        for j in range(num_dets):\n            score = float(scores[j])\n            if score < confidence_threshold:\n                continue\n            cls_id = int(classes[j])\n            # Scale normalized box to pixel coordinates\n            ymin, xmin, ymax, xmax = boxes[j]\n            xmin = int(max(0, min(orig_w - 1, xmin * orig_w)))\n            xmax = int(max(0, min(orig_w - 1, xmax * orig_w)))\n            ymin = int(max(0, min(orig_h - 1, ymin * orig_h)))\n            ymax = int(max(0, min(orig_h - 1, ymax * orig_h)))\n            # 4.3 Post-processing: ensure valid coordinates (clip and correct if needed)\n            if xmax < xmin: xmin, xmax = xmax, xmin\n            if ymax < ymin: ymin, ymax = ymax, ymin\n            box = [xmin, ymin, xmax, ymax]\n            # Accumulate detections per class for mAP computation\n            if cls_id not in detections_per_class:\n                detections_per_class[cls_id] = []\n            detections_per_class[cls_id].append({'score': score, 'box': box})\n            # For drawing: collect now, actual selection (top of clusters) will be determined below\n            drawn_boxes.append((box, cls_id, score))\n\n        # Compute per-class AP for this frame using pseudo GT via clustering duplicates\n        frame_ap_per_class = {}\n        boxes_to_draw = []  # top-of-cluster boxes to draw to avoid duplicates\n        for cls_id, dets in detections_per_class.items():\n            dets_sorted, n_gt = cluster_and_mark_tp_fp(dets, iou_threshold=0.5)\n            ap = compute_ap(dets_sorted, n_gt)\n            if ap is not None:\n                frame_ap_per_class[cls_id] = ap\n                # Update running averages\n                per_class_ap_sum[cls_id] = per_class_ap_sum.get(cls_id, 0.0) + ap\n                per_class_ap_count[cls_id] = per_class_ap_count.get(cls_id, 0) + 1\n            # For drawing: draw only top of each cluster (is_tp True)\n            for det in dets_sorted:\n                if det.get('is_tp', False):\n                    boxes_to_draw.append((det['box'], cls_id, det['score']))\n\n        # Compute running mAP across classes seen so far\n        running_map = 0.0\n        cls_with_stats = [cid for cid, cnt in per_class_ap_count.items() if cnt > 0]\n        if len(cls_with_stats) > 0:\n            per_class_avgs = [per_class_ap_sum[cid] / per_class_ap_count[cid] for cid in cls_with_stats]\n            running_map = float(np.mean(per_class_avgs))\n\n        # 4.4 Handle Output: Draw results and mAP onto the frame and write to output video\n        # Draw rectangles and labels\n        for box, cls_id, score in boxes_to_draw:\n            xmin, ymin, xmax, ymax = box\n            color = color_for_class(cls_id)\n            cv2.rectangle(frame_bgr, (xmin, ymin), (xmax, ymax), color, 2)\n            label = get_label_name(labels, cls_id)\n            caption = \"{}: {:.2f}\".format(label, score)\n            # Text background\n            (tw, th), _ = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame_bgr, (xmin, max(0, ymin - th - 6)), (xmin + tw + 4, ymin), color, -1)\n            cv2.putText(frame_bgr, caption, (xmin + 2, ymin - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Overlay running mAP on the frame\n        map_text = \"mAP: {:.3f}\".format(running_map)\n        cv2.rectangle(frame_bgr, (5, 5), (5 + 200, 5 + 30), (0, 0, 0), -1)\n        cv2.putText(frame_bgr, map_text, (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n        # Write frame to output video\n        writer.write(frame_bgr)\n\n        frame_index += 1\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n\n    # Print final mAP to console\n    if len([cid for cid, cnt in per_class_ap_count.items() if cnt > 0]) > 0:\n        per_class_avgs = {cid: (per_class_ap_sum[cid] / per_class_ap_count[cid]) for cid in per_class_ap_count if per_class_ap_count[cid] > 0}\n        final_map = float(np.mean(list(per_class_avgs.values())))\n        print(\"Final mAP over processed video (proxy via duplicate clustering): {:.4f}\".format(final_map))\n        # Optionally print per-class AP averages\n        for cid, avg_ap in sorted(per_class_avgs.items(), key=lambda x: x[0]):\n            print(\"Class {} ({}): AP_avg = {:.4f}\".format(cid, get_label_name(labels, cid), avg_ap))\n    else:\n        print(\"No detections above threshold were found; mAP could not be computed.\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/769b6961",
      "latency": 194.53,
      "total_cost": 0.102861,
      "observations": [
        {
          "id": "time-04-21-22-236150_chatcmpl-D5jHeewaipLywU02jTxsLpzNnSEuq",
          "traceId": "769b6961",
          "type": "GENERATION",
          "name": "769b_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:21:22.236000+00:00",
          "endTime": "2026-02-05T02:24:36.766000+00:00",
          "completionStartTime": "2026-02-05T02:24:36.766000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.10286100000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Interpreter must be imported literally as specified\ntry:\n    from ai_edge_litert.interpreter import Interpreter\nexcept Exception as e:\n    raise SystemExit(\"Failed to import ai_edge_litert Interpreter. Ensure the 'ai_edge_litert' package is available. Error: {}\".format(e))\n\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    else:\n        # Fallback to an empty list if labels file is missing (script remains runnable)\n        labels = []\n    return labels\n\ndef ensure_dir(path):\n    directory = os.path.dirname(path)\n    if directory and not os.path.isdir(directory):\n        os.makedirs(directory, exist_ok=True)\n\ndef get_input_size_and_format(input_details):\n    shape = input_details[0]['shape']\n    # Default to NHWC if possible\n    if len(shape) == 4:\n        if shape[3] == 3:\n            # NHWC\n            height, width, channels = int(shape[1]), int(shape[2]), int(shape[3])\n            data_format = 'NHWC'\n        elif shape[1] == 3:\n            # NCHW\n            height, width, channels = int(shape[2]), int(shape[3]), int(shape[1])\n            data_format = 'NCHW'\n        else:\n            # Fallback assume NHWC (most TFLite models)\n            height, width, channels = int(shape[1]), int(shape[2]), 3\n            data_format = 'NHWC'\n    else:\n        raise SystemExit(\"Unexpected input tensor shape: {}\".format(shape))\n    return height, width, channels, data_format\n\ndef preprocess_frame(frame_bgr, in_h, in_w, dtype, data_format):\n    # Convert BGR to RGB as SSD MobileNet typically expects RGB input\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (in_w, in_h))\n    input_data = resized\n    # Expand to batch and reorder if necessary\n    if data_format == 'NHWC':\n        input_data = np.expand_dims(input_data, axis=0)\n    else:\n        # NCHW\n        input_data = np.transpose(input_data, (2, 0, 1))  # HWC -> CHW\n        input_data = np.expand_dims(input_data, axis=0)\n    # Convert dtype as required\n    if dtype == np.float32:\n        input_data = input_data.astype(np.float32)\n        # Phase 2.3 Quantization Handling - normalize for floating model\n        input_data = (input_data - 127.5) / 127.5\n    else:\n        input_data = input_data.astype(dtype)\n    return input_data\n\ndef iou(boxA, boxB):\n    # Boxes: [xmin, ymin, xmax, ymax]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interW = max(0, xB - xA)\n    interH = max(0, yB - yA)\n    interArea = interW * interH\n    areaA = max(0, boxA[2] - boxA[0]) * max(0, boxA[3] - boxA[1])\n    areaB = max(0, boxB[2] - boxB[0]) * max(0, boxB[3] - boxB[1])\n    denom = areaA + areaB - interArea\n    if denom <= 0:\n        return 0.0\n    return interArea / denom\n\ndef cluster_and_mark_tp_fp(dets, iou_threshold=0.5):\n    \"\"\"\n    dets: list of dicts: {'score': float, 'box': [xmin, ymin, xmax, ymax]}\n    Returns:\n      dets_sorted: same det dicts with additional 'is_tp' boolean, sorted by score desc\n      n_gt: number of pseudo ground truth objects (clusters)\n    \"\"\"\n    if not dets:\n        return [], 0\n    # Sort detections by score descending\n    dets_sorted = sorted(dets, key=lambda d: d['score'], reverse=True)\n    clusters = []  # each cluster: {'box': [..], 'members': [det_indices]}\n    # Assign detections to clusters; first member of each cluster is TP, others are FP\n    for det in dets_sorted:\n        assigned = False\n        for cl in clusters:\n            if iou(det['box'], cl['box']) > iou_threshold:\n                det['is_tp'] = False\n                cl['members'].append(det)\n                assigned = True\n                break\n        if not assigned:\n            det['is_tp'] = True\n            clusters.append({'box': det['box'], 'members': [det]})\n    n_gt = len(clusters)\n    return dets_sorted, n_gt\n\ndef compute_ap(dets_sorted, n_gt):\n    \"\"\"\n    Compute Average Precision using precision envelope integration.\n    dets_sorted: list of det dicts with 'is_tp' key sorted by score desc\n    n_gt: number of pseudo ground truth objects\n    Returns AP float or None if not computable (n_gt == 0 and no detections)\n    \"\"\"\n    if n_gt == 0:\n        if len(dets_sorted) == 0:\n            return None  # No positives and no predictions: skip for averaging\n        else:\n            return 0.0   # No ground truth but predictions exist: AP=0\n    tp = np.array([1 if d['is_tp'] else 0 for d in dets_sorted], dtype=np.float32)\n    fp = 1.0 - tp\n    tp_cum = np.cumsum(tp)\n    fp_cum = np.cumsum(fp)\n    precision = tp_cum / np.maximum(tp_cum + fp_cum, 1e-12)\n    recall = tp_cum / float(n_gt)\n    # Precision envelope\n    # Add boundary points\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([0.0], precision, [0.0]))\n    for i in range(len(mpre) - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n    # Integrate area under curve where recall changes\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n    return float(ap)\n\ndef color_for_class(class_id):\n    # Deterministic color from class_id\n    np.random.seed(class_id + 12345)\n    color = tuple(int(c) for c in np.random.randint(0, 255, size=3))\n    # OpenCV uses BGR\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef get_label_name(labels, class_id):\n    if 0 <= class_id < len(labels):\n        return labels[class_id]\n    return \"id_{}\".format(class_id)\n\ndef main():\n    # 1.2 Paths/Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')\n\n    # 1.3 Load Labels if available and relevant\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    if not os.path.isfile(model_path):\n        raise SystemExit(\"Model file not found at: {}\".format(model_path))\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    input_index = input_details[0]['index']\n    input_dtype = input_details[0]['dtype']\n    in_h, in_w, in_c, data_format = get_input_size_and_format(input_details)\n    floating_model = (input_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: single video file\n    if not os.path.isfile(input_path):\n        raise SystemExit(\"Input video file not found at: {}\".format(input_path))\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise SystemExit(\"Failed to open input video: {}\".format(input_path))\n\n    # Prepare output writer\n    ensure_dir(output_path)\n    # Try to use original FPS and size; fallback if unavailable\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 25.0\n    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    if orig_w <= 0 or orig_h <= 0:\n        # Read a frame to determine size\n        ret, test_frame = cap.read()\n        if not ret:\n            cap.release()\n            raise SystemExit(\"Could not read frames from video.\")\n        orig_h, orig_w = test_frame.shape[:2]\n        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # reset to start\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (orig_w, orig_h))\n    if not writer.isOpened():\n        cap.release()\n        raise SystemExit(\"Failed to open output video writer at: {}\".format(output_path))\n\n    # Variables for output mapping determination (boxes/classes/scores/num_dets)\n    mapping_determined = False\n    boxes_idx = None\n    classes_idx = None\n    scores_idx = None\n    num_idx = None\n\n    # Variables for mAP accumulation across frames and classes\n    per_class_ap_sum = {}\n    per_class_ap_count = {}\n\n    frame_index = 0\n    start_time = time.time()\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # End of video\n\n        # 2.2 Preprocess Data\n        input_data = preprocess_frame(frame_bgr, in_h, in_w, input_dtype, data_format)\n\n        # 3.1 Set Input Tensor\n        interpreter.set_tensor(input_index, input_data)\n        # 3.2 Run Inference\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling\n        # 4.1 Get Output Tensors\n        outputs = [interpreter.get_tensor(out['index']) for out in output_details]\n\n        # Determine mapping on first frame using shapes and value ranges\n        if not mapping_determined:\n            for i, arr in enumerate(outputs):\n                a = np.array(arr)\n                if a.size == 1:\n                    num_idx = i\n                elif a.ndim == 3 and a.shape[-1] == 4:\n                    boxes_idx = i\n            # For classes and scores, both are shape (1, N). Distinguish by value range.\n            candidates = [i for i in range(len(outputs)) if i not in (boxes_idx, num_idx)]\n            for i in candidates:\n                arr = np.array(outputs[i]).astype(np.float32)\n                # scores are within [0,1], classes usually > 1\n                if np.all(arr >= 0.0) and np.all(arr <= 1.0):\n                    scores_idx = i\n                else:\n                    classes_idx = i\n            # Fallback if still ambiguous: try common order [boxes, classes, scores, num]\n            if boxes_idx is None or classes_idx is None or scores_idx is None or num_idx is None:\n                if len(outputs) >= 4:\n                    boxes_idx = 0 if boxes_idx is None else boxes_idx\n                    classes_idx = 1 if classes_idx is None else classes_idx\n                    scores_idx = 2 if scores_idx is None else scores_idx\n                    num_idx = 3 if num_idx is None else num_idx\n                else:\n                    cap.release()\n                    writer.release()\n                    raise SystemExit(\"Could not determine TFLite detection output mappings.\")\n            mapping_determined = True\n\n        # Extract tensors using determined indices\n        boxes = np.array(outputs[boxes_idx])[0]  # shape [N, 4], normalized [ymin, xmin, ymax, xmax]\n        classes = np.array(outputs[classes_idx])[0]\n        scores = np.array(outputs[scores_idx])[0]\n        num_dets = int(np.array(outputs[num_idx]).reshape(-1)[0]) if outputs[num_idx].size >= 1 else len(scores)\n        num_dets = min(num_dets, boxes.shape[0], scores.shape[0], classes.shape[0])\n\n        # 4.2 Interpret Results: collect detections per class with bounding boxes and labels\n        detections_per_class = {}  # class_id -> list of det dicts {'score': float, 'box': [xmin, ymin, xmax, ymax]}\n        drawn_boxes = []  # boxes to draw (top of clusters or all detections), list of tuples (box, label, score)\n        for j in range(num_dets):\n            score = float(scores[j])\n            if score < confidence_threshold:\n                continue\n            cls_id = int(classes[j])\n            # Scale normalized box to pixel coordinates\n            ymin, xmin, ymax, xmax = boxes[j]\n            xmin = int(max(0, min(orig_w - 1, xmin * orig_w)))\n            xmax = int(max(0, min(orig_w - 1, xmax * orig_w)))\n            ymin = int(max(0, min(orig_h - 1, ymin * orig_h)))\n            ymax = int(max(0, min(orig_h - 1, ymax * orig_h)))\n            # 4.3 Post-processing: ensure valid coordinates (clip and correct if needed)\n            if xmax < xmin: xmin, xmax = xmax, xmin\n            if ymax < ymin: ymin, ymax = ymax, ymin\n            box = [xmin, ymin, xmax, ymax]\n            # Accumulate detections per class for mAP computation\n            if cls_id not in detections_per_class:\n                detections_per_class[cls_id] = []\n            detections_per_class[cls_id].append({'score': score, 'box': box})\n            # For drawing: collect now, actual selection (top of clusters) will be determined below\n            drawn_boxes.append((box, cls_id, score))\n\n        # Compute per-class AP for this frame using pseudo GT via clustering duplicates\n        frame_ap_per_class = {}\n        boxes_to_draw = []  # top-of-cluster boxes to draw to avoid duplicates\n        for cls_id, dets in detections_per_class.items():\n            dets_sorted, n_gt = cluster_and_mark_tp_fp(dets, iou_threshold=0.5)\n            ap = compute_ap(dets_sorted, n_gt)\n            if ap is not None:\n                frame_ap_per_class[cls_id] = ap\n                # Update running averages\n                per_class_ap_sum[cls_id] = per_class_ap_sum.get(cls_id, 0.0) + ap\n                per_class_ap_count[cls_id] = per_class_ap_count.get(cls_id, 0) + 1\n            # For drawing: draw only top of each cluster (is_tp True)\n            for det in dets_sorted:\n                if det.get('is_tp', False):\n                    boxes_to_draw.append((det['box'], cls_id, det['score']))\n\n        # Compute running mAP across classes seen so far\n        running_map = 0.0\n        cls_with_stats = [cid for cid, cnt in per_class_ap_count.items() if cnt > 0]\n        if len(cls_with_stats) > 0:\n            per_class_avgs = [per_class_ap_sum[cid] / per_class_ap_count[cid] for cid in cls_with_stats]\n            running_map = float(np.mean(per_class_avgs))\n\n        # 4.4 Handle Output: Draw results and mAP onto the frame and write to output video\n        # Draw rectangles and labels\n        for box, cls_id, score in boxes_to_draw:\n            xmin, ymin, xmax, ymax = box\n            color = color_for_class(cls_id)\n            cv2.rectangle(frame_bgr, (xmin, ymin), (xmax, ymax), color, 2)\n            label = get_label_name(labels, cls_id)\n            caption = \"{}: {:.2f}\".format(label, score)\n            # Text background\n            (tw, th), _ = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame_bgr, (xmin, max(0, ymin - th - 6)), (xmin + tw + 4, ymin), color, -1)\n            cv2.putText(frame_bgr, caption, (xmin + 2, ymin - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Overlay running mAP on the frame\n        map_text = \"mAP: {:.3f}\".format(running_map)\n        cv2.rectangle(frame_bgr, (5, 5), (5 + 200, 5 + 30), (0, 0, 0), -1)\n        cv2.putText(frame_bgr, map_text, (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv2.LINE_AA)\n\n        # Write frame to output video\n        writer.write(frame_bgr)\n\n        frame_index += 1\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n\n    # Print final mAP to console\n    if len([cid for cid, cnt in per_class_ap_count.items() if cnt > 0]) > 0:\n        per_class_avgs = {cid: (per_class_ap_sum[cid] / per_class_ap_count[cid]) for cid in per_class_ap_count if per_class_ap_count[cid] > 0}\n        final_map = float(np.mean(list(per_class_avgs.values())))\n        print(\"Final mAP over processed video (proxy via duplicate clustering): {:.4f}\".format(final_map))\n        # Optionally print per-class AP averages\n        for cid, avg_ap in sorted(per_class_avgs.items(), key=lambda x: x[0]):\n            print(\"Class {} ({}): AP_avg = {:.4f}\".format(cid, get_label_name(labels, cid), avg_ap))\n    else:\n        print(\"No detections above threshold were found; mAP could not be computed.\")\n\nif __name__ == \"__main__\":\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 10257,
            "total": 11757,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.102861,
          "latency": 194530.0,
          "timeToFirstToken": 194.53,
          "updatedAt": "2026-02-05T02:24:42.421Z",
          "totalTokens": 11757,
          "costDetails": {
            "total": 0.102861
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 10257,
          "usageDetails": {
            "input": 1500,
            "output": 10257,
            "total": 11757
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:24:37.384Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:25:26.880Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:21:22.127Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "02ee4d22",
      "timestamp": "2026-02-05T02:18:37.923000+00:00",
      "name": "02ee_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script loads a TFLite SSD MobileNet model and performs object detection on a single input\nvideo file. It writes an output video with bounding boxes and labels overlaid. It also computes\nand overlays a proxy mAP value (mean confidence of kept detections) since ground-truth annotations\nare not provided for true mAP computation.\n\nPhases implemented per Programming Guideline:\n- Phase 1: Setup (imports, paths, load labels, load interpreter, get model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (read video file, preprocess frames)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2: Interpret Results (map classes to labels, structure detections)\n  - 4.3: Post-processing (thresholding, coordinate scaling, clipping)\n- Phase 5: Cleanup (release resources)\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# -----------------------------\n# Phase 1: Setup\n# -----------------------------\n# 1.1 Imports: Interpreter per instructions\nfrom ai_edge_litert.interpreter import Interpreter  # NOTE: as required by the guideline\n\n# 1.2 Paths/Parameters\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD_STR  = 0.5\nCONFIDENCE_THRESHOLD = float(CONFIDENCE_THRESHOLD_STR)\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (if provided and relevant)\ndef load_labels(label_path):\n    labels = []\n    if label_path and os.path.exists(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name:\n                    labels.append(name)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # e.g., [1, height, width, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    raise ValueError(f\"Unexpected model input shape: {input_shape}. Expected [1, H, W, 3].\")\n\nin_h, in_w = int(input_shape[1]), int(input_shape[2])\n\n# -----------------------------\n# Utility functions\n# -----------------------------\ndef preprocess_frame_bgr_to_input(frame_bgr, target_w, target_h, dtype, floating):\n    \"\"\"\n    Convert BGR frame to model input tensor:\n    - Resize to (target_w, target_h)\n    - Convert BGR->RGB\n    - Add batch dimension\n    - Cast to dtype\n    - Normalize if floating model: (x - 127.5) / 127.5\n    \"\"\"\n    resized = cv2.resize(frame_bgr, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(rgb, axis=0)\n    if floating:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # For quantized models, keep as uint8\n        input_data = input_data.astype(dtype, copy=False)\n    return input_data\n\ndef extract_detection_outputs(interpreter, output_details):\n    \"\"\"\n    Extract TFLite SSD detection outputs robustly, handling variations.\n    Expected tensors:\n      - boxes: [1, num, 4] or [num, 4]\n      - classes: [1, num] or [num]\n      - scores: [1, num] or [num]\n      - num_detections: [1] or scalar\n    Returns:\n      boxes (np.ndarray [N, 4], normalized [ymin, xmin, ymax, xmax]),\n      classes (np.ndarray [N], int),\n      scores (np.ndarray [N], float),\n      num (int)\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Collect raw outputs\n    raw_outs = []\n    for od in output_details:\n        arr = interpreter.get_tensor(od['index'])\n        # Remove batch dimension if present\n        squeezed = np.squeeze(arr)\n        raw_outs.append(squeezed)\n\n    # Heuristic assignment\n    for arr in raw_outs:\n        if arr.ndim == 2 and arr.shape[1] == 4:\n            boxes = arr.astype(np.float32, copy=False)\n        elif arr.ndim == 1 and arr.size > 1:\n            # Distinguish scores vs classes\n            # Scores are floats in [0, 1]; classes are typically floats of integer ids\n            arr_f32 = arr.astype(np.float32, copy=False)\n            if np.all((arr_f32 >= 0.0) & (arr_f32 <= 1.0)):\n                scores = arr_f32\n            else:\n                classes = arr.astype(np.int32, copy=False)\n        elif arr.ndim == 0 or arr.size == 1:\n            # num_detections may be scalar\n            try:\n                num = int(float(arr.reshape(-1)[0]))\n            except Exception:\n                pass\n\n    # If classes still float (e.g., floats close to ints), cast\n    if classes is None:\n        # Try to find remaining 1D that wasn't assigned as scores\n        for arr in raw_outs:\n            if arr.ndim == 1 and arr.size > 1:\n                arr_f32 = arr.astype(np.float32, copy=False)\n                if not np.all((arr_f32 >= 0.0) & (arr_f32 <= 1.0)):\n                    classes = arr.astype(np.int32, copy=False)\n\n    # If num is not provided, infer from boxes or scores\n    candidate_ns = []\n    if boxes is not None:\n        candidate_ns.append(boxes.shape[0])\n    if scores is not None:\n        candidate_ns.append(scores.shape[0])\n    if classes is not None:\n        candidate_ns.append(classes.shape[0])\n    if num is None and candidate_ns:\n        num = int(min(candidate_ns))\n\n    # Sanity checks and trimming to num\n    if num is None:\n        num = 0\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n\n    n = min(num, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\ndef clip_box(xmin, ymin, xmax, ymax, img_w, img_h):\n    xmin = max(0, min(img_w - 1, xmin))\n    ymin = max(0, min(img_h - 1, ymin))\n    xmax = max(0, min(img_w - 1, xmax))\n    ymax = max(0, min(img_h - 1, ymax))\n    return int(xmin), int(ymin), int(xmax), int(ymax)\n\ndef class_id_to_name(cid, labels_list):\n    \"\"\"\n    SSD MobileNet v1 COCO models typically output class IDs starting at 1.\n    Map safely to label name if available.\n    \"\"\"\n    if labels_list:\n        # Try 1-based mapping first\n        if 1 <= cid <= len(labels_list):\n            return labels_list[cid - 1]\n        # Fallback to 0-based if above fails\n        if 0 <= cid < len(labels_list):\n            return labels_list[cid]\n    return f\"id:{cid}\"\n\n# Proxy mAP: mean of confidences above threshold across processed frames\nclass ProxymAP:\n    def __init__(self):\n        self.confidences = []\n\n    def update(self, confs):\n        # confs: iterable of confidence scores for kept detections in a frame\n        for c in confs:\n            if np.isfinite(c):\n                self.confidences.append(float(c))\n\n    def value(self):\n        if not self.confidences:\n            return 0.0\n        return float(np.mean(self.confidences))\n\n# -----------------------------\n# Phase 2: Input Acquisition & Preprocessing Loop\n# -----------------------------\n# 2.1 Acquire Input Data: read a single video file from input_path\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise FileNotFoundError(f\"Cannot open input video: {INPUT_PATH}\")\n\n# Video properties\nfps = cap.get(cv2.CAP_PROP_FPS)\nif not fps or fps <= 0 or np.isnan(fps):\n    fps = 30.0  # fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Prepare VideoWriter\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Cannot open output video for writing: {OUTPUT_PATH}\")\n\n# Initialize stats\nproxy_map = ProxymAP()\ntotal_frames = 0\ntotal_kept_dets = 0\nt0 = time.time()\n\n# Processing loop for single video file\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break  # end of video\n\n    total_frames += 1\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame_bgr_to_input(\n        frame_bgr, target_w=in_w, target_h=in_h, dtype=input_dtype, floating=floating_model\n    )\n\n    # 2.3 Quantization Handling done inside preprocess (normalization for floating model)\n\n    # -----------------------------\n    # Phase 3: Inference\n    # -----------------------------\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # -----------------------------\n    # Phase 4: Output Interpretation & Handling Loop\n    # -----------------------------\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, det_count = extract_detection_outputs(interpreter, output_details)\n\n    # 4.2 Interpret Results\n    # Structure detections, map class IDs to names\n    kept_confidences = []\n    for i in range(det_count):\n        score = float(scores_raw[i]) if i < len(scores_raw) else 0.0\n        if score < CONFIDENCE_THRESHOLD or not np.isfinite(score):\n            continue  # filter low confidence\n\n        cid = int(classes_raw[i]) if i < len(classes_raw) else -1\n        label_name = class_id_to_name(cid, labels)\n\n        # 4.3 Post-processing: scale and clip boxes\n        # boxes_norm are [ymin, xmin, ymax, xmax] normalized [0, 1]\n        ymin, xmin, ymax, xmax = boxes_norm[i] if i < len(boxes_norm) else (0, 0, 0, 0)\n        xmin_abs = xmin * frame_width\n        xmax_abs = xmax * frame_width\n        ymin_abs = ymin * frame_height\n        ymax_abs = ymax * frame_height\n\n        x1, y1, x2, y2 = clip_box(xmin_abs, ymin_abs, xmax_abs, ymax_abs, frame_width, frame_height)\n\n        # Draw bounding box and label on frame\n        color = (0, 255, 0)  # green box\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, thickness=2)\n        label_text = f\"{label_name} {score:.2f}\"\n        # Background rectangle for text\n        (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame_bgr, (x1, y1 - th - baseline - 4), (x1 + tw + 2, y1), (0, 0, 0), cv2.FILLED)\n        cv2.putText(frame_bgr, label_text, (x1 + 1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        kept_confidences.append(score)\n        total_kept_dets += 1\n\n    # Update proxy mAP (mean confidence of kept detections)\n    proxy_map.update(kept_confidences)\n\n    # 4.4 Handle Output: overlay stats and write to output video\n    # Compute running FPS\n    elapsed = time.time() - t0\n    running_fps = total_frames / elapsed if elapsed > 0 else 0.0\n    # Overlay information\n    overlay_lines = [\n        f\"Frame: {total_frames}\",\n        f\"Detections(>= {CONFIDENCE_THRESHOLD:.2f}): {len(kept_confidences)}\",\n        f\"mAP (proxy mean conf): {proxy_map.value():.3f}\",\n        f\"FPS: {running_fps:.2f}\"\n    ]\n    y_base = 20\n    for li, text in enumerate(overlay_lines):\n        cv2.putText(frame_bgr, text, (10, y_base + li * 18), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 0, 255), 2, cv2.LINE_AA)\n\n    out_writer.write(frame_bgr)\n\n# 4.5 Loop ended\n\n# -----------------------------\n# Phase 5: Cleanup\n# -----------------------------\ncap.release()\nout_writer.release()\n\n# Final summary print\nprint(\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Total frames processed: {total_frames}\")\nprint(f\"Total detections kept (score >= {CONFIDENCE_THRESHOLD:.2f}): {total_kept_dets}\")\nprint(f\"Proxy mAP (mean confidence of kept detections): {proxy_map.value():.4f}\")"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/02ee4d22",
      "latency": 107.007,
      "total_cost": 0.063981,
      "observations": [
        {
          "id": "time-04-18-38-606798_chatcmpl-D5jF1q8rIZbmgphTokyz7FMi48fMg",
          "traceId": "02ee4d22",
          "type": "GENERATION",
          "name": "02ee_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:18:38.606000+00:00",
          "endTime": "2026-02-05T02:20:25.613000+00:00",
          "completionStartTime": "2026-02-05T02:20:25.613000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.06398100000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script loads a TFLite SSD MobileNet model and performs object detection on a single input\nvideo file. It writes an output video with bounding boxes and labels overlaid. It also computes\nand overlays a proxy mAP value (mean confidence of kept detections) since ground-truth annotations\nare not provided for true mAP computation.\n\nPhases implemented per Programming Guideline:\n- Phase 1: Setup (imports, paths, load labels, load interpreter, get model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (read video file, preprocess frames)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop\n  - 4.2: Interpret Results (map classes to labels, structure detections)\n  - 4.3: Post-processing (thresholding, coordinate scaling, clipping)\n- Phase 5: Cleanup (release resources)\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# -----------------------------\n# Phase 1: Setup\n# -----------------------------\n# 1.1 Imports: Interpreter per instructions\nfrom ai_edge_litert.interpreter import Interpreter  # NOTE: as required by the guideline\n\n# 1.2 Paths/Parameters\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONFIDENCE_THRESHOLD_STR = '0.5'  # provided as string, parse to float\nCONFIDENCE_THRESHOLD = float(CONFIDENCE_THRESHOLD_STR)\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (if provided and relevant)\ndef load_labels(label_path):\n    labels = []\n    if label_path and os.path.exists(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name:\n                    labels.append(name)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # e.g., [1, height, width, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    raise ValueError(f\"Unexpected model input shape: {input_shape}. Expected [1, H, W, 3].\")\n\nin_h, in_w = int(input_shape[1]), int(input_shape[2])\n\n# -----------------------------\n# Utility functions\n# -----------------------------\ndef preprocess_frame_bgr_to_input(frame_bgr, target_w, target_h, dtype, floating):\n    \"\"\"\n    Convert BGR frame to model input tensor:\n    - Resize to (target_w, target_h)\n    - Convert BGR->RGB\n    - Add batch dimension\n    - Cast to dtype\n    - Normalize if floating model: (x - 127.5) / 127.5\n    \"\"\"\n    resized = cv2.resize(frame_bgr, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(rgb, axis=0)\n    if floating:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # For quantized models, keep as uint8\n        input_data = input_data.astype(dtype, copy=False)\n    return input_data\n\ndef extract_detection_outputs(interpreter, output_details):\n    \"\"\"\n    Extract TFLite SSD detection outputs robustly, handling variations.\n    Expected tensors:\n      - boxes: [1, num, 4] or [num, 4]\n      - classes: [1, num] or [num]\n      - scores: [1, num] or [num]\n      - num_detections: [1] or scalar\n    Returns:\n      boxes (np.ndarray [N, 4], normalized [ymin, xmin, ymax, xmax]),\n      classes (np.ndarray [N], int),\n      scores (np.ndarray [N], float),\n      num (int)\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Collect raw outputs\n    raw_outs = []\n    for od in output_details:\n        arr = interpreter.get_tensor(od['index'])\n        # Remove batch dimension if present\n        squeezed = np.squeeze(arr)\n        raw_outs.append(squeezed)\n\n    # Heuristic assignment\n    for arr in raw_outs:\n        if arr.ndim == 2 and arr.shape[1] == 4:\n            boxes = arr.astype(np.float32, copy=False)\n        elif arr.ndim == 1 and arr.size > 1:\n            # Distinguish scores vs classes\n            # Scores are floats in [0, 1]; classes are typically floats of integer ids\n            arr_f32 = arr.astype(np.float32, copy=False)\n            if np.all((arr_f32 >= 0.0) & (arr_f32 <= 1.0)):\n                scores = arr_f32\n            else:\n                classes = arr.astype(np.int32, copy=False)\n        elif arr.ndim == 0 or arr.size == 1:\n            # num_detections may be scalar\n            try:\n                num = int(float(arr.reshape(-1)[0]))\n            except Exception:\n                pass\n\n    # If classes still float (e.g., floats close to ints), cast\n    if classes is None:\n        # Try to find remaining 1D that wasn't assigned as scores\n        for arr in raw_outs:\n            if arr.ndim == 1 and arr.size > 1:\n                arr_f32 = arr.astype(np.float32, copy=False)\n                if not np.all((arr_f32 >= 0.0) & (arr_f32 <= 1.0)):\n                    classes = arr.astype(np.int32, copy=False)\n\n    # If num is not provided, infer from boxes or scores\n    candidate_ns = []\n    if boxes is not None:\n        candidate_ns.append(boxes.shape[0])\n    if scores is not None:\n        candidate_ns.append(scores.shape[0])\n    if classes is not None:\n        candidate_ns.append(classes.shape[0])\n    if num is None and candidate_ns:\n        num = int(min(candidate_ns))\n\n    # Sanity checks and trimming to num\n    if num is None:\n        num = 0\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n\n    n = min(num, boxes.shape[0], scores.shape[0], classes.shape[0])\n    boxes = boxes[:n]\n    scores = scores[:n]\n    classes = classes[:n]\n\n    return boxes, classes, scores, n\n\ndef clip_box(xmin, ymin, xmax, ymax, img_w, img_h):\n    xmin = max(0, min(img_w - 1, xmin))\n    ymin = max(0, min(img_h - 1, ymin))\n    xmax = max(0, min(img_w - 1, xmax))\n    ymax = max(0, min(img_h - 1, ymax))\n    return int(xmin), int(ymin), int(xmax), int(ymax)\n\ndef class_id_to_name(cid, labels_list):\n    \"\"\"\n    SSD MobileNet v1 COCO models typically output class IDs starting at 1.\n    Map safely to label name if available.\n    \"\"\"\n    if labels_list:\n        # Try 1-based mapping first\n        if 1 <= cid <= len(labels_list):\n            return labels_list[cid - 1]\n        # Fallback to 0-based if above fails\n        if 0 <= cid < len(labels_list):\n            return labels_list[cid]\n    return f\"id:{cid}\"\n\n# Proxy mAP: mean of confidences above threshold across processed frames\nclass ProxymAP:\n    def __init__(self):\n        self.confidences = []\n\n    def update(self, confs):\n        # confs: iterable of confidence scores for kept detections in a frame\n        for c in confs:\n            if np.isfinite(c):\n                self.confidences.append(float(c))\n\n    def value(self):\n        if not self.confidences:\n            return 0.0\n        return float(np.mean(self.confidences))\n\n# -----------------------------\n# Phase 2: Input Acquisition & Preprocessing Loop\n# -----------------------------\n# 2.1 Acquire Input Data: read a single video file from input_path\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise FileNotFoundError(f\"Cannot open input video: {INPUT_PATH}\")\n\n# Video properties\nfps = cap.get(cv2.CAP_PROP_FPS)\nif not fps or fps <= 0 or np.isnan(fps):\n    fps = 30.0  # fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Prepare VideoWriter\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    raise RuntimeError(f\"Cannot open output video for writing: {OUTPUT_PATH}\")\n\n# Initialize stats\nproxy_map = ProxymAP()\ntotal_frames = 0\ntotal_kept_dets = 0\nt0 = time.time()\n\n# Processing loop for single video file\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break  # end of video\n\n    total_frames += 1\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame_bgr_to_input(\n        frame_bgr, target_w=in_w, target_h=in_h, dtype=input_dtype, floating=floating_model\n    )\n\n    # 2.3 Quantization Handling done inside preprocess (normalization for floating model)\n\n    # -----------------------------\n    # Phase 3: Inference\n    # -----------------------------\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_index, input_data)\n\n    # 3.2 Run Inference\n    interpreter.invoke()\n\n    # -----------------------------\n    # Phase 4: Output Interpretation & Handling Loop\n    # -----------------------------\n    # 4.1 Get Output Tensors\n    boxes_norm, classes_raw, scores_raw, det_count = extract_detection_outputs(interpreter, output_details)\n\n    # 4.2 Interpret Results\n    # Structure detections, map class IDs to names\n    kept_confidences = []\n    for i in range(det_count):\n        score = float(scores_raw[i]) if i < len(scores_raw) else 0.0\n        if score < CONFIDENCE_THRESHOLD or not np.isfinite(score):\n            continue  # filter low confidence\n\n        cid = int(classes_raw[i]) if i < len(classes_raw) else -1\n        label_name = class_id_to_name(cid, labels)\n\n        # 4.3 Post-processing: scale and clip boxes\n        # boxes_norm are [ymin, xmin, ymax, xmax] normalized [0, 1]\n        ymin, xmin, ymax, xmax = boxes_norm[i] if i < len(boxes_norm) else (0, 0, 0, 0)\n        xmin_abs = xmin * frame_width\n        xmax_abs = xmax * frame_width\n        ymin_abs = ymin * frame_height\n        ymax_abs = ymax * frame_height\n\n        x1, y1, x2, y2 = clip_box(xmin_abs, ymin_abs, xmax_abs, ymax_abs, frame_width, frame_height)\n\n        # Draw bounding box and label on frame\n        color = (0, 255, 0)  # green box\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, thickness=2)\n        label_text = f\"{label_name} {score:.2f}\"\n        # Background rectangle for text\n        (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame_bgr, (x1, y1 - th - baseline - 4), (x1 + tw + 2, y1), (0, 0, 0), cv2.FILLED)\n        cv2.putText(frame_bgr, label_text, (x1 + 1, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        kept_confidences.append(score)\n        total_kept_dets += 1\n\n    # Update proxy mAP (mean confidence of kept detections)\n    proxy_map.update(kept_confidences)\n\n    # 4.4 Handle Output: overlay stats and write to output video\n    # Compute running FPS\n    elapsed = time.time() - t0\n    running_fps = total_frames / elapsed if elapsed > 0 else 0.0\n    # Overlay information\n    overlay_lines = [\n        f\"Frame: {total_frames}\",\n        f\"Detections(>= {CONFIDENCE_THRESHOLD:.2f}): {len(kept_confidences)}\",\n        f\"mAP (proxy mean conf): {proxy_map.value():.3f}\",\n        f\"FPS: {running_fps:.2f}\"\n    ]\n    y_base = 20\n    for li, text in enumerate(overlay_lines):\n        cv2.putText(frame_bgr, text, (10, y_base + li * 18), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 0, 255), 2, cv2.LINE_AA)\n\n    out_writer.write(frame_bgr)\n\n# 4.5 Loop ended\n\n# -----------------------------\n# Phase 5: Cleanup\n# -----------------------------\ncap.release()\nout_writer.release()\n\n# Final summary print\nprint(\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Total frames processed: {total_frames}\")\nprint(f\"Total detections kept (score >= {CONFIDENCE_THRESHOLD:.2f}): {total_kept_dets}\")\nprint(f\"Proxy mAP (mean confidence of kept detections): {proxy_map.value():.4f}\")\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 6369,
            "total": 7869,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.063981,
          "latency": 107007.0,
          "timeToFirstToken": 107.007,
          "updatedAt": "2026-02-05T02:20:30.790Z",
          "totalTokens": 7869,
          "costDetails": {
            "total": 0.063981
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 6369,
          "usageDetails": {
            "input": 1500,
            "output": 6369,
            "total": 7869
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:20:25.744Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:21:16.848Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:18:38.502Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "9fd74b85",
      "timestamp": "2026-02-05T02:16:00.208000+00:00",
      "name": "9fd7_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single video file using a TensorFlow Lite model\nvia ai_edge_litert Interpreter, draws bounding boxes and labels on detected objects, and\nwrites the annotated result to an output video. It also computes and overlays a proxy mAP\n(mean Average Precision) metric based on mean confidence per detected class (no ground truth).\n\nPhases implemented per Programming Guidelines:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video frames preprocessing)\n- Phase 3: Inference (invocation per frame)\n- Phase 4: Output Interpretation & Handling (parsing outputs, thresholding, clipping; writing annotated video and mAP proxy)\n- Phase 5: Cleanup (release resources)\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1. Imports: Import interpreter literally by specified API\nfrom ai_edge_litert.interpreter import Interpreter  # per instruction\n\ndef load_labels(label_path):\n    labels = []\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef ensure_dir_for_file(path):\n    dir_path = os.path.dirname(path)\n    if dir_path and not os.path.exists(dir_path):\n        os.makedirs(dir_path, exist_ok=True)\n\ndef get_tflite_output_tensors(interpreter, output_details):\n    # Retrieve output tensors in the order provided\n    outputs = []\n    for od in output_details:\n        outputs.append(interpreter.get_tensor(od['index']))\n    return outputs\n\ndef identify_detection_tensors(outputs):\n    \"\"\"\n    Identify SSD MobileNet outputs:\n    Typically:\n      - boxes: [1, num_detections, 4]\n      - classes: [1, num_detections]\n      - scores: [1, num_detections]\n      - num_detections: [1]\n    This function uses heuristics to map them.\n    Returns: boxes, classes, scores, num_detections\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Flatten outer batch dimension where applicable for easier handling\n    for out in outputs:\n        arr = out\n        shape = arr.shape\n        # num_detections: usually shape == (1,) or arr.size == 1\n        if arr.size == 1:\n            num_det = int(np.squeeze(arr).astype(np.int32))\n            continue\n        # boxes: last dim == 4\n        if len(shape) == 3 and shape[-1] == 4:\n            boxes = np.squeeze(arr, axis=0)\n            continue\n        # classes or scores: shape [1, N]\n        if len(shape) == 2 and shape[0] == 1:\n            vals = np.squeeze(arr, axis=0)\n            # scores are in [0,1]\n            maxv = float(np.max(vals)) if vals.size > 0 else 0.0\n            minv = float(np.min(vals)) if vals.size > 0 else 0.0\n            if 0.0 <= minv and maxv <= 1.0:\n                # likely scores\n                # But classes could be small ints too; if all ints and within 0..1 it's ambiguous for N=1\n                # Prefer to assign scores first, then classes\n                if scores is None:\n                    scores = vals\n                else:\n                    # already assigned, so this must be classes\n                    classes = vals\n            else:\n                classes = vals\n            continue\n\n    # Fallbacks: if num_det is None, deduce from shapes present\n    if num_det is None:\n        if boxes is not None:\n            num_det = boxes.shape[0]\n        elif scores is not None:\n            num_det = scores.shape[0]\n        elif classes is not None:\n            num_det = classes.shape[0]\n        else:\n            num_det = 0\n\n    # Ensure all are present with consistent sizes\n    if boxes is None:\n        boxes = np.zeros((num_det, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((num_det,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((num_det,), dtype=np.float32)\n\n    # Clip arrays to num_det if larger\n    boxes = boxes[:num_det]\n    scores = scores[:num_det]\n    classes = classes[:num_det]\n\n    return boxes, classes, scores, num_det\n\ndef draw_detections(frame, detections, labels, color=(0, 255, 0), thickness=2, font_scale=0.5):\n    \"\"\"\n    Draw bounding boxes and labels on the frame.\n    detections: list of dicts with keys: x1, y1, x2, y2, score, class_id, label\n    \"\"\"\n    for det in detections:\n        x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n        score = det['score']\n        class_id = det['class_id']\n        label_text = det['label']\n\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n        text = f\"{label_text}: {score:.2f}\"\n        # Draw filled rect for text background\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 1)\n        cv2.rectangle(frame, (x1, max(0, y1 - th - baseline - 3)), (x1 + tw + 2, y1), (0, 0, 0), -1)\n        cv2.putText(frame, text, (x1 + 1, y1 - 3), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), 1, cv2.LINE_AA)\n\ndef overlay_info(frame, map_value, frame_idx, total_frames, color=(255, 0, 0)):\n    \"\"\"\n    Overlay proxy mAP and progress information on the frame.\n    \"\"\"\n    h, w = frame.shape[:2]\n    text1 = f\"mAP(proxy): {map_value:.4f}\"\n    text2 = f\"Frame: {frame_idx}/{total_frames if total_frames>0 else '?'}\"\n    cv2.putText(frame, text1, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n    cv2.putText(frame, text2, (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n\ndef compute_proxy_map(per_class_scores):\n    \"\"\"\n    Compute a proxy mAP without ground truth:\n    - For each class that has detections, compute AP_proxy = mean(confidence scores)\n    - mAP_proxy = mean(AP_proxy over classes with any detections)\n    Returns mAP_proxy (float). If no detections observed, returns 0.0.\n    \"\"\"\n    if not per_class_scores:\n        return 0.0\n    ap_list = []\n    for cls_id, scores in per_class_scores.items():\n        if len(scores) > 0:\n            ap_list.append(float(np.mean(scores)))\n    if len(ap_list) == 0:\n        return 0.0\n    return float(np.mean(ap_list))\n\ndef main():\n    # 1.2. Paths/Parameters (from configuration parameters)\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')\n\n    # 1.3. Load Labels (Conditional)\n    labels = []\n    if label_path and os.path.exists(label_path):\n        labels = load_labels(label_path)\n    else:\n        labels = []  # In case labels file missing, fall back to class IDs\n\n    # 1.4. Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5. Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Assume single input tensor\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']  # e.g., [1, 300, 300, 3]\n    input_dtype = input_details[0]['dtype']\n    input_height = int(input_shape[1])\n    input_width = int(input_shape[2])\n    floating_model = (input_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1. Acquire Input Data (read a single video file from input_path)\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    # Retrieve input video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # default fallback\n\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Prepare output writer\n    ensure_dir_for_file(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video writer: {output_path}\")\n\n    # For proxy mAP calculation across the whole video\n    per_class_scores = {}  # dict: class_id -> list of scores\n\n    frame_index = 0\n    start_time = time.time()\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # End of video\n        frame_index += 1\n\n        # 2.2. Preprocess Data\n        # Convert BGR to RGB for TFLite SSD models\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        resized = cv2.resize(frame_rgb, (input_width, input_height), interpolation=cv2.INTER_LINEAR)\n\n        # Prepare input tensor [1, H, W, 3]\n        input_data = np.expand_dims(resized, axis=0)\n\n        # 2.3. Quantization Handling\n        if floating_model:\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n        else:\n            # Ensure uint8 for quantized models\n            input_data = np.uint8(input_data)\n\n        # Phase 3: Inference\n        # 3.1. Set Input Tensor\n        interpreter.set_tensor(input_index, input_data)\n        # 3.2. Run Inference\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling Loop\n        # 4.1. Get Output Tensors\n        outputs = get_tflite_output_tensors(interpreter, output_details)\n\n        # 4.2. Interpret Results for object detection\n        boxes, classes, scores, num_det = identify_detection_tensors(outputs)\n\n        # 4.3. Post-processing: apply confidence thresholding, coordinate scaling, and clipping\n        detections = []\n        for i in range(num_det):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n            cls_id = int(classes[i])\n            label_name = labels[cls_id] if (0 <= cls_id < len(labels)) else f\"id_{cls_id}\"\n\n            # boxes are in normalized coordinates [ymin, xmin, ymax, xmax]\n            y_min, x_min, y_max, x_max = boxes[i].tolist()\n\n            # Clip to [0,1]\n            y_min = max(0.0, min(1.0, y_min))\n            x_min = max(0.0, min(1.0, x_min))\n            y_max = max(0.0, min(1.0, y_max))\n            x_max = max(0.0, min(1.0, x_max))\n\n            # Scale to pixel coordinates\n            x1 = int(round(x_min * frame_width))\n            y1 = int(round(y_min * frame_height))\n            x2 = int(round(x_max * frame_width))\n            y2 = int(round(y_max * frame_height))\n\n            # Ensure valid rectangle within frame bounds\n            x1 = max(0, min(frame_width - 1, x1))\n            y1 = max(0, min(frame_height - 1, y1))\n            x2 = max(0, min(frame_width - 1, x2))\n            y2 = max(0, min(frame_height - 1, y2))\n\n            # Discard invalid boxes where min > max after rounding/clipping\n            if x2 <= x1 or y2 <= y1:\n                continue\n\n            detections.append({\n                'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,\n                'score': score, 'class_id': cls_id, 'label': label_name\n            })\n\n            # Accumulate scores per class for proxy mAP\n            if cls_id not in per_class_scores:\n                per_class_scores[cls_id] = []\n            per_class_scores[cls_id].append(score)\n\n        # Compute proxy mAP up to current frame\n        map_proxy = compute_proxy_map(per_class_scores)\n\n        # 4.4. Handle Output: draw and write to file\n        annotated = frame_bgr.copy()\n        draw_detections(annotated, detections, labels)\n        overlay_info(annotated, map_proxy, frame_index, total_frames)\n        writer.write(annotated)\n\n        # 4.5. Loop Continuation: continue until video ends\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n    elapsed = time.time() - start_time\n\n    # Print summary\n    print(\"Processing complete.\")\n    print(f\"Input video: {input_path}\")\n    print(f\"Output video: {output_path}\")\n    print(f\"Frames processed: {frame_index}\")\n    print(f\"Elapsed time: {elapsed:.2f}s\")\n    final_map = compute_proxy_map(per_class_scores)\n    print(f\"Final mAP (proxy, mean of per-class mean confidences): {final_map:.4f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/9fd74b85",
      "latency": 101.886,
      "total_cost": 0.063461,
      "observations": [
        {
          "id": "time-04-16-00-904357_chatcmpl-D5jCTpwCYIg9AJ64G0t3g08hTTW56",
          "traceId": "9fd74b85",
          "type": "GENERATION",
          "name": "9fd7_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:16:00.904000+00:00",
          "endTime": "2026-02-05T02:17:42.790000+00:00",
          "completionStartTime": "2026-02-05T02:17:42.790000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.063461,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single video file using a TensorFlow Lite model\nvia ai_edge_litert Interpreter, draws bounding boxes and labels on detected objects, and\nwrites the annotated result to an output video. It also computes and overlays a proxy mAP\n(mean Average Precision) metric based on mean confidence per detected class (no ground truth).\n\nPhases implemented per Programming Guidelines:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video frames preprocessing)\n- Phase 3: Inference (invocation per frame)\n- Phase 4: Output Interpretation & Handling (parsing outputs, thresholding, clipping; writing annotated video and mAP proxy)\n- Phase 5: Cleanup (release resources)\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1. Imports: Import interpreter literally by specified API\nfrom ai_edge_litert.interpreter import Interpreter  # per instruction\n\ndef load_labels(label_path):\n    labels = []\n    with open(label_path, 'r', encoding='utf-8') as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                labels.append(line)\n    return labels\n\ndef ensure_dir_for_file(path):\n    dir_path = os.path.dirname(path)\n    if dir_path and not os.path.exists(dir_path):\n        os.makedirs(dir_path, exist_ok=True)\n\ndef get_tflite_output_tensors(interpreter, output_details):\n    # Retrieve output tensors in the order provided\n    outputs = []\n    for od in output_details:\n        outputs.append(interpreter.get_tensor(od['index']))\n    return outputs\n\ndef identify_detection_tensors(outputs):\n    \"\"\"\n    Identify SSD MobileNet outputs:\n    Typically:\n      - boxes: [1, num_detections, 4]\n      - classes: [1, num_detections]\n      - scores: [1, num_detections]\n      - num_detections: [1]\n    This function uses heuristics to map them.\n    Returns: boxes, classes, scores, num_detections\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Flatten outer batch dimension where applicable for easier handling\n    for out in outputs:\n        arr = out\n        shape = arr.shape\n        # num_detections: usually shape == (1,) or arr.size == 1\n        if arr.size == 1:\n            num_det = int(np.squeeze(arr).astype(np.int32))\n            continue\n        # boxes: last dim == 4\n        if len(shape) == 3 and shape[-1] == 4:\n            boxes = np.squeeze(arr, axis=0)\n            continue\n        # classes or scores: shape [1, N]\n        if len(shape) == 2 and shape[0] == 1:\n            vals = np.squeeze(arr, axis=0)\n            # scores are in [0,1]\n            maxv = float(np.max(vals)) if vals.size > 0 else 0.0\n            minv = float(np.min(vals)) if vals.size > 0 else 0.0\n            if 0.0 <= minv and maxv <= 1.0:\n                # likely scores\n                # But classes could be small ints too; if all ints and within 0..1 it's ambiguous for N=1\n                # Prefer to assign scores first, then classes\n                if scores is None:\n                    scores = vals\n                else:\n                    # already assigned, so this must be classes\n                    classes = vals\n            else:\n                classes = vals\n            continue\n\n    # Fallbacks: if num_det is None, deduce from shapes present\n    if num_det is None:\n        if boxes is not None:\n            num_det = boxes.shape[0]\n        elif scores is not None:\n            num_det = scores.shape[0]\n        elif classes is not None:\n            num_det = classes.shape[0]\n        else:\n            num_det = 0\n\n    # Ensure all are present with consistent sizes\n    if boxes is None:\n        boxes = np.zeros((num_det, 4), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((num_det,), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((num_det,), dtype=np.float32)\n\n    # Clip arrays to num_det if larger\n    boxes = boxes[:num_det]\n    scores = scores[:num_det]\n    classes = classes[:num_det]\n\n    return boxes, classes, scores, num_det\n\ndef draw_detections(frame, detections, labels, color=(0, 255, 0), thickness=2, font_scale=0.5):\n    \"\"\"\n    Draw bounding boxes and labels on the frame.\n    detections: list of dicts with keys: x1, y1, x2, y2, score, class_id, label\n    \"\"\"\n    for det in detections:\n        x1, y1, x2, y2 = det['x1'], det['y1'], det['x2'], det['y2']\n        score = det['score']\n        class_id = det['class_id']\n        label_text = det['label']\n\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness)\n        text = f\"{label_text}: {score:.2f}\"\n        # Draw filled rect for text background\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, 1)\n        cv2.rectangle(frame, (x1, max(0, y1 - th - baseline - 3)), (x1 + tw + 2, y1), (0, 0, 0), -1)\n        cv2.putText(frame, text, (x1 + 1, y1 - 3), cv2.FONT_HERSHEY_SIMPLEX, font_scale, (255, 255, 255), 1, cv2.LINE_AA)\n\ndef overlay_info(frame, map_value, frame_idx, total_frames, color=(255, 0, 0)):\n    \"\"\"\n    Overlay proxy mAP and progress information on the frame.\n    \"\"\"\n    h, w = frame.shape[:2]\n    text1 = f\"mAP(proxy): {map_value:.4f}\"\n    text2 = f\"Frame: {frame_idx}/{total_frames if total_frames>0 else '?'}\"\n    cv2.putText(frame, text1, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n    cv2.putText(frame, text2, (10, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n\ndef compute_proxy_map(per_class_scores):\n    \"\"\"\n    Compute a proxy mAP without ground truth:\n    - For each class that has detections, compute AP_proxy = mean(confidence scores)\n    - mAP_proxy = mean(AP_proxy over classes with any detections)\n    Returns mAP_proxy (float). If no detections observed, returns 0.0.\n    \"\"\"\n    if not per_class_scores:\n        return 0.0\n    ap_list = []\n    for cls_id, scores in per_class_scores.items():\n        if len(scores) > 0:\n            ap_list.append(float(np.mean(scores)))\n    if len(ap_list) == 0:\n        return 0.0\n    return float(np.mean(ap_list))\n\ndef main():\n    # 1.2. Paths/Parameters (from configuration parameters)\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')\n\n    # 1.3. Load Labels (Conditional)\n    labels = []\n    if label_path and os.path.exists(label_path):\n        labels = load_labels(label_path)\n    else:\n        labels = []  # In case labels file missing, fall back to class IDs\n\n    # 1.4. Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5. Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Assume single input tensor\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']  # e.g., [1, 300, 300, 3]\n    input_dtype = input_details[0]['dtype']\n    input_height = int(input_shape[1])\n    input_width = int(input_shape[2])\n    floating_model = (input_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1. Acquire Input Data (read a single video file from input_path)\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    # Retrieve input video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # default fallback\n\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Prepare output writer\n    ensure_dir_for_file(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video writer: {output_path}\")\n\n    # For proxy mAP calculation across the whole video\n    per_class_scores = {}  # dict: class_id -> list of scores\n\n    frame_index = 0\n    start_time = time.time()\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # End of video\n        frame_index += 1\n\n        # 2.2. Preprocess Data\n        # Convert BGR to RGB for TFLite SSD models\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        resized = cv2.resize(frame_rgb, (input_width, input_height), interpolation=cv2.INTER_LINEAR)\n\n        # Prepare input tensor [1, H, W, 3]\n        input_data = np.expand_dims(resized, axis=0)\n\n        # 2.3. Quantization Handling\n        if floating_model:\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n        else:\n            # Ensure uint8 for quantized models\n            input_data = np.uint8(input_data)\n\n        # Phase 3: Inference\n        # 3.1. Set Input Tensor\n        interpreter.set_tensor(input_index, input_data)\n        # 3.2. Run Inference\n        interpreter.invoke()\n\n        # Phase 4: Output Interpretation & Handling Loop\n        # 4.1. Get Output Tensors\n        outputs = get_tflite_output_tensors(interpreter, output_details)\n\n        # 4.2. Interpret Results for object detection\n        boxes, classes, scores, num_det = identify_detection_tensors(outputs)\n\n        # 4.3. Post-processing: apply confidence thresholding, coordinate scaling, and clipping\n        detections = []\n        for i in range(num_det):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n            cls_id = int(classes[i])\n            label_name = labels[cls_id] if (0 <= cls_id < len(labels)) else f\"id_{cls_id}\"\n\n            # boxes are in normalized coordinates [ymin, xmin, ymax, xmax]\n            y_min, x_min, y_max, x_max = boxes[i].tolist()\n\n            # Clip to [0,1]\n            y_min = max(0.0, min(1.0, y_min))\n            x_min = max(0.0, min(1.0, x_min))\n            y_max = max(0.0, min(1.0, y_max))\n            x_max = max(0.0, min(1.0, x_max))\n\n            # Scale to pixel coordinates\n            x1 = int(round(x_min * frame_width))\n            y1 = int(round(y_min * frame_height))\n            x2 = int(round(x_max * frame_width))\n            y2 = int(round(y_max * frame_height))\n\n            # Ensure valid rectangle within frame bounds\n            x1 = max(0, min(frame_width - 1, x1))\n            y1 = max(0, min(frame_height - 1, y1))\n            x2 = max(0, min(frame_width - 1, x2))\n            y2 = max(0, min(frame_height - 1, y2))\n\n            # Discard invalid boxes where min > max after rounding/clipping\n            if x2 <= x1 or y2 <= y1:\n                continue\n\n            detections.append({\n                'x1': x1, 'y1': y1, 'x2': x2, 'y2': y2,\n                'score': score, 'class_id': cls_id, 'label': label_name\n            })\n\n            # Accumulate scores per class for proxy mAP\n            if cls_id not in per_class_scores:\n                per_class_scores[cls_id] = []\n            per_class_scores[cls_id].append(score)\n\n        # Compute proxy mAP up to current frame\n        map_proxy = compute_proxy_map(per_class_scores)\n\n        # 4.4. Handle Output: draw and write to file\n        annotated = frame_bgr.copy()\n        draw_detections(annotated, detections, labels)\n        overlay_info(annotated, map_proxy, frame_index, total_frames)\n        writer.write(annotated)\n\n        # 4.5. Loop Continuation: continue until video ends\n\n    # Phase 5: Cleanup\n    cap.release()\n    writer.release()\n    elapsed = time.time() - start_time\n\n    # Print summary\n    print(\"Processing complete.\")\n    print(f\"Input video: {input_path}\")\n    print(f\"Output video: {output_path}\")\n    print(f\"Frames processed: {frame_index}\")\n    print(f\"Elapsed time: {elapsed:.2f}s\")\n    final_map = compute_proxy_map(per_class_scores)\n    print(f\"Final mAP (proxy, mean of per-class mean confidences): {final_map:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 6317,
            "total": 7817,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.063461,
          "latency": 101886.0,
          "timeToFirstToken": 101.886,
          "updatedAt": "2026-02-05T02:17:48.185Z",
          "totalTokens": 7817,
          "costDetails": {
            "total": 0.063461
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 6317,
          "usageDetails": {
            "input": 1500,
            "output": 6317,
            "total": 7817
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:17:43.067Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:18:33.140Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:16:00.966Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "130f3a1e",
      "timestamp": "2026-02-05T02:11:53.517000+00:00",
      "name": "130f_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TFLite SSD model,\ndraws bounding boxes and labels on detected objects, and saves an annotated output video.\nIt also calculates a proxy mAP (mean Average Precision) over the processed video by\ntreating Non-Maximum Suppressed (NMS) detections as pseudo ground truth and evaluating\nraw model proposals against them.\n\nPhases implemented following the provided Programming Guideline:\n- Phase 1: Setup (imports, paths, load labels, load interpreter, get model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (read video frames, preprocess)\n- Phase 3: Inference (set input tensors and invoke)\n- Phase 4: Output Interpretation & Handling Loop\n    4.1 Get output tensors\n    4.2 Interpret results (map indices to labels, structure detections)\n    4.3 Post-processing (thresholding, NMS, coordinate scaling/clipping)\n    4.4 Handle output (draw and write annotated frames, compute and overlay mAP)\n    4.5 Loop continuation\n- Phase 5: Cleanup (release video resources)\n\"\"\"\n\n# =========================\n# Phase 1: Setup\n# =========================\n# 1.1 Imports\nimport os\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESH = float('0.5')\n\n# Utility: Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels\ndef load_labels(path):\n    labels = []\n    if os.path.isfile(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name:\n                    labels.append(name)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input properties (assuming single input tensor)\nin_shape = input_details[0]['shape']  # e.g., [1, H, W, 3]\nin_height, in_width = int(in_shape[1]), int(in_shape[2])\nin_dtype = input_details[0]['dtype']\n\n# Helper: Determine which output tensor corresponds to boxes/scores/classes/num\ndef map_output_indices(interpreter, output_details):\n    \"\"\"\n    After a first inference, map output tensor indices for:\n    - boxes: shape [1, N, 4]\n    - classes: shape [1, N]\n    - scores: shape [1, N]\n    - num_detections: scalar (1) or [1]\n    \"\"\"\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n    boxes_idx = classes_idx = scores_idx = num_idx = None\n\n    # Identify boxes by last dim == 4\n    for i, od in enumerate(output_details):\n        shp = od['shape']\n        if len(shp) == 3 and shp[-1] == 4:\n            boxes_idx = i\n            break\n\n    # Identify num_detections by being scalar or having single element\n    for i, od in enumerate(output_details):\n        shp = od['shape']\n        total_elems = int(np.prod(shp))\n        if total_elems == 1:\n            num_idx = i\n            break\n\n    # Identify classes and scores among remaining [1, N]\n    # Heuristic: classes have values generally > 1, scores within [0,1]\n    cand = []\n    for i, od in enumerate(output_details):\n        if i in [boxes_idx, num_idx]:\n            continue\n        shp = od['shape']\n        if len(shp) == 2 and shp[0] == 1:\n            cand.append(i)\n\n    # If both present, decide by value ranges after a dry inference\n    if len(cand) == 2:\n        a = outputs[cand[0]].flatten()\n        b = outputs[cand[1]].flatten()\n        # Decide which looks like scores (max <= 1.0) vs classes\n        def is_scores(arr):\n            if arr.size == 0:\n                return True\n            mx = np.nanmax(arr)\n            mn = np.nanmin(arr)\n            return (mx <= 1.0001 and mn >= 0.0)\n        if is_scores(a) and not is_scores(b):\n            scores_idx, classes_idx = cand[0], cand[1]\n        elif is_scores(b) and not is_scores(a):\n            scores_idx, classes_idx = cand[1], cand[0]\n        else:\n            # Fallback: assign arbitrarily, will still work for many models\n            scores_idx, classes_idx = cand[0], cand[1]\n    elif len(cand) == 1:\n        # Some models may skip num_detections; assume the remaining is scores, and classes absent (unlikely)\n        scores_idx = cand[0]\n        classes_idx = None\n    else:\n        # Fallback, though standard SSD should have both\n        scores_idx = classes_idx = None\n\n    return boxes_idx, classes_idx, scores_idx, num_idx\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# Video capture setup\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Retrieve original video properties\norig_width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps <= 0 or np.isnan(fps):\n    fps = 25.0  # default fallback if FPS not available\n\n# Video writer setup\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_width, orig_height))\nif not out_writer.isOpened():\n    cap.release()\n    raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n# Helper functions for post-processing and metrics\n\ndef preprocess_frame(frame_bgr, target_w, target_h, dtype, floating_model):\n    # Resize and convert BGR to RGB as typical for TFLite SSD models\n    resized = cv2.resize(frame_bgr, (target_w, target_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(rgb, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = input_data.astype(dtype, copy=False)\n    return input_data\n\ndef to_abs_box(box, img_w, img_h):\n    # box: [ymin, xmin, ymax, xmax] normalized [0,1]\n    ymin, xmin, ymax, xmax = float(box[0]), float(box[1]), float(box[2]), float(box[3])\n    x1 = int(np.clip(xmin * img_w, 0, img_w - 1))\n    y1 = int(np.clip(ymin * img_h, 0, img_h - 1))\n    x2 = int(np.clip(xmax * img_w, 0, img_w - 1))\n    y2 = int(np.clip(ymax * img_h, 0, img_h - 1))\n    # Ensure proper ordering\n    x1, x2 = (x1, x2) if x2 >= x1 else (x2, x1)\n    y1, y2 = (y1, y2) if y2 >= y1 else (y2, y1)\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    # boxes: [x1,y1,x2,y2]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0, xB - xA + 1)\n    inter_h = max(0, yB - yA + 1)\n    inter = inter_w * inter_h\n    areaA = max(0, boxA[2] - boxA[0] + 1) * max(0, boxA[3] - boxA[1] + 1)\n    areaB = max(0, boxB[2] - boxB[0] + 1) * max(0, boxB[3] - boxB[1] + 1)\n    denom = float(areaA + areaB - inter + 1e-12)\n    return inter / denom\n\ndef nms_per_class(boxes, scores, iou_thresh=0.5):\n    # Greedy NMS for a single class\n    if len(boxes) == 0:\n        return []\n    boxes_arr = np.array(boxes, dtype=np.float32)\n    scores_arr = np.array(scores, dtype=np.float32)\n    x1 = boxes_arr[:, 0]\n    y1 = boxes_arr[:, 1]\n    x2 = boxes_arr[:, 2]\n    y2 = boxes_arr[:, 3]\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores_arr.argsort()[::-1]\n    keep = []\n\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-12)\n\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef class_color(cid):\n    # Deterministic color per class id (BGR)\n    return (int((cid * 37) % 256), int((cid * 17) % 256), int((cid * 29) % 256))\n\ndef get_label_name(cid):\n    if labels and 0 <= cid < len(labels):\n        return labels[cid]\n    return f\"class_{cid}\"\n\ndef compute_map(preds_per_class, gts_per_class, iou_threshold=0.5):\n    \"\"\"\n    Compute mAP over all classes present in gts_per_class using 11-point interpolation.\n    preds_per_class: dict[cid] -> list of dicts { 'image_id': int, 'box': [x1,y1,x2,y2], 'score': float }\n    gts_per_class:   dict[cid] -> list of dicts { 'image_id': int, 'box': [x1,y1,x2,y2] }\n    \"\"\"\n    aps = []\n    for cid in sorted(gts_per_class.keys()):\n        gts = gts_per_class.get(cid, [])\n        preds = preds_per_class.get(cid, [])\n\n        if len(gts) == 0:\n            continue\n\n        # Organize GTs by image and track \"matched\" flags\n        gts_by_img = {}\n        for gt in gts:\n            img_id = gt['image_id']\n            gts_by_img.setdefault(img_id, [])\n            gts_by_img[img_id].append({'box': gt['box'], 'matched': False})\n\n        # Sort predictions by score descending\n        preds_sorted = sorted(preds, key=lambda d: d['score'], reverse=True)\n\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n        total_gts = len(gts)\n\n        for i, p in enumerate(preds_sorted):\n            img_id = p['image_id']\n            p_box = p['box']\n            candidates = gts_by_img.get(img_id, [])\n            best_iou = 0.0\n            best_j = -1\n            for j, gt in enumerate(candidates):\n                if gt['matched']:\n                    continue\n                current_iou = iou_xyxy(p_box, gt['box'])\n                if current_iou > best_iou:\n                    best_iou = current_iou\n                    best_j = j\n            if best_iou >= iou_threshold and best_j >= 0:\n                # True positive\n                candidates[best_j]['matched'] = True\n                tp[i] = 1.0\n                fp[i] = 0.0\n            else:\n                # False positive\n                tp[i] = 0.0\n                fp[i] = 1.0\n\n        if total_gts == 0:\n            continue\n\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-12)\n        recall = cum_tp / float(total_gts)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0, 1, 11):\n            precisions_at_r = precision[recall >= r]\n            p_max = np.max(precisions_at_r) if precisions_at_r.size > 0 else 0.0\n            ap += p_max\n        ap /= 11.0\n        aps.append(ap)\n\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\n# Prepare containers for mAP computation across the video\nfrom collections import defaultdict\npreds_per_class = defaultdict(list)  # cid -> list of {image_id, box, score}\ngts_per_class = defaultdict(list)    # cid -> list of {image_id, box}\n\n# Prepare floating model flag\nfloating_model = (in_dtype == np.float32)\n\n# For mapping output indices, we need a one-time \"dry run\" or do it on first frame\nmapped = False\nboxes_idx = classes_idx = scores_idx = num_idx = None\n\n# =========================\n# Processing Loop\n# =========================\nframe_index = 0\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame(frame_bgr, in_width, in_height, in_dtype, floating_model)\n\n    # 2.3 Quantization Handling already included in preprocess_frame per floating_model\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling\n    # =========================\n    # 4.1 Get Output Tensors\n    if not mapped:\n        boxes_idx, classes_idx, scores_idx, num_idx = map_output_indices(interpreter, output_details)\n        mapped = True\n\n    # Retrieve raw outputs\n    boxes_raw = interpreter.get_tensor(output_details[boxes_idx]['index'])  # [1, N, 4]\n    boxes_raw = boxes_raw[0] if boxes_raw.ndim == 3 else boxes_raw\n\n    # Some models include num_detections; otherwise derive from outputs length\n    if num_idx is not None:\n        num_dets_raw = interpreter.get_tensor(output_details[num_idx]['index'])\n        num_dets = int(np.squeeze(num_dets_raw).astype(np.int32))\n        if num_dets <= 0 or num_dets > boxes_raw.shape[0]:\n            num_dets = boxes_raw.shape[0]\n    else:\n        num_dets = boxes_raw.shape[0]\n\n    scores_raw = interpreter.get_tensor(output_details[scores_idx]['index']) if scores_idx is not None else None\n    classes_raw = interpreter.get_tensor(output_details[classes_idx]['index']) if classes_idx is not None else None\n\n    scores_raw = scores_raw[0] if scores_raw is not None and scores_raw.ndim == 2 else scores_raw\n    classes_raw = classes_raw[0] if classes_raw is not None and classes_raw.ndim == 2 else classes_raw\n\n    if scores_raw is None:\n        # If scores not provided (unlikely for SSD), create dummy confidences\n        scores_raw = np.ones((boxes_raw.shape[0],), dtype=np.float32)\n    if classes_raw is None:\n        # If classes not provided (unlikely), assign all to a default class 0\n        classes_raw = np.zeros((boxes_raw.shape[0],), dtype=np.int32)\n\n    # Clip to num detections\n    boxes_raw = boxes_raw[:num_dets]\n    scores_raw = scores_raw[:num_dets].astype(np.float32)\n    classes_raw = classes_raw[:num_dets].astype(np.int32)\n\n    # Convert boxes to absolute pixel coordinates and clip to frame size\n    abs_boxes = [to_abs_box(b, orig_width, orig_height) for b in boxes_raw]\n    abs_scores = scores_raw.tolist()\n    abs_classes = classes_raw.tolist()\n\n    # 4.2 Interpret Results\n    # Structure detections per class\n    detections_per_class = {}\n    for i in range(len(abs_boxes)):\n        cid = int(abs_classes[i])\n        s = float(abs_scores[i])\n        box = abs_boxes[i]\n        detections_per_class.setdefault(cid, {'boxes': [], 'scores': []})\n        detections_per_class[cid]['boxes'].append(box)\n        detections_per_class[cid]['scores'].append(s)\n\n    # 4.3 Post-processing: apply confidence thresholding and NMS, clip boxes already done\n    final_detections = []  # list of dicts for drawing and GT for mAP proxy\n    for cid, data in detections_per_class.items():\n        boxes_c = data['boxes']\n        scores_c = data['scores']\n        # First filter by confidence threshold for display / GT\n        idxs_conf = [i for i, sc in enumerate(scores_c) if sc >= CONF_THRESH]\n        boxes_thr = [boxes_c[i] for i in idxs_conf]\n        scores_thr = [scores_c[i] for i in idxs_conf]\n        if len(boxes_thr) == 0:\n            continue\n        # Apply NMS per class\n        keep_idx = nms_per_class(boxes_thr, scores_thr, iou_thresh=0.5)\n        for k in keep_idx:\n            final_detections.append({\n                'class_id': cid,\n                'score': float(scores_thr[k]),\n                'box': boxes_thr[k]\n            })\n\n    # Prepare data for mAP computation (proxy: predictions vs NMS results as pseudo GT)\n    # Add ground truths (NMS results) for this frame\n    for det in final_detections:\n        gts_per_class[det['class_id']].append({\n            'image_id': frame_index,\n            'box': det['box']\n        })\n\n    # Add predictions (raw model outputs) for this frame using all proposals (score > 0)\n    for i in range(len(abs_boxes)):\n        cid = int(abs_classes[i])\n        preds_per_class[cid].append({\n            'image_id': frame_index,\n            'box': abs_boxes[i],\n            'score': float(abs_scores[i])\n        })\n\n    # Compute running mAP over seen frames\n    running_map = compute_map(preds_per_class, gts_per_class, iou_threshold=0.5)\n\n    # 4.4 Handle Output: draw rectangles and labels, overlay mAP; then write to video\n    canvas = frame_bgr.copy()\n    for det in final_detections:\n        cid = det['class_id']\n        sc = det['score']\n        x1, y1, x2, y2 = det['box']\n        color = class_color(cid)\n        cv2.rectangle(canvas, (x1, y1), (x2, y2), color, 2)\n        label_text = f\"{get_label_name(cid)}: {sc:.2f}\"\n        # Put label background for readability\n        (tw, th), bl = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(canvas, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), color, -1)\n        cv2.putText(canvas, label_text, (x1 + 2, y1 - 4),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Overlay running mAP\n    map_text = f\"mAP: {running_map:.3f}\"\n    cv2.putText(canvas, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n                0.9, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(canvas, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n                0.9, (255, 255, 255), 2, cv2.LINE_AA)\n\n    out_writer.write(canvas)\n\n    frame_index += 1\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nout_writer.release()\n\nprint(f\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Annotated output saved to: {OUTPUT_PATH}\")"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/130f3a1e",
      "latency": 153.176,
      "total_cost": 0.107011,
      "observations": [
        {
          "id": "time-04-11-54-173665_chatcmpl-D5j8ULheKFSbnrm0KTe7km4POXHpQ",
          "traceId": "130f3a1e",
          "type": "GENERATION",
          "name": "130f_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:11:54.173000+00:00",
          "endTime": "2026-02-05T02:14:27.349000+00:00",
          "completionStartTime": "2026-02-05T02:14:27.349000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.10701100000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TFLite SSD model,\ndraws bounding boxes and labels on detected objects, and saves an annotated output video.\nIt also calculates a proxy mAP (mean Average Precision) over the processed video by\ntreating Non-Maximum Suppressed (NMS) detections as pseudo ground truth and evaluating\nraw model proposals against them.\n\nPhases implemented following the provided Programming Guideline:\n- Phase 1: Setup (imports, paths, load labels, load interpreter, get model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (read video frames, preprocess)\n- Phase 3: Inference (set input tensors and invoke)\n- Phase 4: Output Interpretation & Handling Loop\n    4.1 Get output tensors\n    4.2 Interpret results (map indices to labels, structure detections)\n    4.3 Post-processing (thresholding, NMS, coordinate scaling/clipping)\n    4.4 Handle output (draw and write annotated frames, compute and overlay mAP)\n    4.5 Loop continuation\n- Phase 5: Cleanup (release video resources)\n\"\"\"\n\n# =========================\n# Phase 1: Setup\n# =========================\n# 1.1 Imports\nimport os\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESH = float('0.5')\n\n# Utility: Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels\ndef load_labels(path):\n    labels = []\n    if os.path.isfile(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name:\n                    labels.append(name)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input properties (assuming single input tensor)\nin_shape = input_details[0]['shape']  # e.g., [1, H, W, 3]\nin_height, in_width = int(in_shape[1]), int(in_shape[2])\nin_dtype = input_details[0]['dtype']\n\n# Helper: Determine which output tensor corresponds to boxes/scores/classes/num\ndef map_output_indices(interpreter, output_details):\n    \"\"\"\n    After a first inference, map output tensor indices for:\n    - boxes: shape [1, N, 4]\n    - classes: shape [1, N]\n    - scores: shape [1, N]\n    - num_detections: scalar (1) or [1]\n    \"\"\"\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n    boxes_idx = classes_idx = scores_idx = num_idx = None\n\n    # Identify boxes by last dim == 4\n    for i, od in enumerate(output_details):\n        shp = od['shape']\n        if len(shp) == 3 and shp[-1] == 4:\n            boxes_idx = i\n            break\n\n    # Identify num_detections by being scalar or having single element\n    for i, od in enumerate(output_details):\n        shp = od['shape']\n        total_elems = int(np.prod(shp))\n        if total_elems == 1:\n            num_idx = i\n            break\n\n    # Identify classes and scores among remaining [1, N]\n    # Heuristic: classes have values generally > 1, scores within [0,1]\n    cand = []\n    for i, od in enumerate(output_details):\n        if i in [boxes_idx, num_idx]:\n            continue\n        shp = od['shape']\n        if len(shp) == 2 and shp[0] == 1:\n            cand.append(i)\n\n    # If both present, decide by value ranges after a dry inference\n    if len(cand) == 2:\n        a = outputs[cand[0]].flatten()\n        b = outputs[cand[1]].flatten()\n        # Decide which looks like scores (max <= 1.0) vs classes\n        def is_scores(arr):\n            if arr.size == 0:\n                return True\n            mx = np.nanmax(arr)\n            mn = np.nanmin(arr)\n            return (mx <= 1.0001 and mn >= 0.0)\n        if is_scores(a) and not is_scores(b):\n            scores_idx, classes_idx = cand[0], cand[1]\n        elif is_scores(b) and not is_scores(a):\n            scores_idx, classes_idx = cand[1], cand[0]\n        else:\n            # Fallback: assign arbitrarily, will still work for many models\n            scores_idx, classes_idx = cand[0], cand[1]\n    elif len(cand) == 1:\n        # Some models may skip num_detections; assume the remaining is scores, and classes absent (unlikely)\n        scores_idx = cand[0]\n        classes_idx = None\n    else:\n        # Fallback, though standard SSD should have both\n        scores_idx = classes_idx = None\n\n    return boxes_idx, classes_idx, scores_idx, num_idx\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# Video capture setup\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Retrieve original video properties\norig_width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps <= 0 or np.isnan(fps):\n    fps = 25.0  # default fallback if FPS not available\n\n# Video writer setup\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_width, orig_height))\nif not out_writer.isOpened():\n    cap.release()\n    raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n# Helper functions for post-processing and metrics\n\ndef preprocess_frame(frame_bgr, target_w, target_h, dtype, floating_model):\n    # Resize and convert BGR to RGB as typical for TFLite SSD models\n    resized = cv2.resize(frame_bgr, (target_w, target_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(rgb, axis=0)\n\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = input_data.astype(dtype, copy=False)\n    return input_data\n\ndef to_abs_box(box, img_w, img_h):\n    # box: [ymin, xmin, ymax, xmax] normalized [0,1]\n    ymin, xmin, ymax, xmax = float(box[0]), float(box[1]), float(box[2]), float(box[3])\n    x1 = int(np.clip(xmin * img_w, 0, img_w - 1))\n    y1 = int(np.clip(ymin * img_h, 0, img_h - 1))\n    x2 = int(np.clip(xmax * img_w, 0, img_w - 1))\n    y2 = int(np.clip(ymax * img_h, 0, img_h - 1))\n    # Ensure proper ordering\n    x1, x2 = (x1, x2) if x2 >= x1 else (x2, x1)\n    y1, y2 = (y1, y2) if y2 >= y1 else (y2, y1)\n    return [x1, y1, x2, y2]\n\ndef iou_xyxy(boxA, boxB):\n    # boxes: [x1,y1,x2,y2]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0, xB - xA + 1)\n    inter_h = max(0, yB - yA + 1)\n    inter = inter_w * inter_h\n    areaA = max(0, boxA[2] - boxA[0] + 1) * max(0, boxA[3] - boxA[1] + 1)\n    areaB = max(0, boxB[2] - boxB[0] + 1) * max(0, boxB[3] - boxB[1] + 1)\n    denom = float(areaA + areaB - inter + 1e-12)\n    return inter / denom\n\ndef nms_per_class(boxes, scores, iou_thresh=0.5):\n    # Greedy NMS for a single class\n    if len(boxes) == 0:\n        return []\n    boxes_arr = np.array(boxes, dtype=np.float32)\n    scores_arr = np.array(scores, dtype=np.float32)\n    x1 = boxes_arr[:, 0]\n    y1 = boxes_arr[:, 1]\n    x2 = boxes_arr[:, 2]\n    y2 = boxes_arr[:, 3]\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores_arr.argsort()[::-1]\n    keep = []\n\n    while order.size > 0:\n        i = order[0]\n        keep.append(int(i))\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-12)\n\n        inds = np.where(iou <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef class_color(cid):\n    # Deterministic color per class id (BGR)\n    return (int((cid * 37) % 256), int((cid * 17) % 256), int((cid * 29) % 256))\n\ndef get_label_name(cid):\n    if labels and 0 <= cid < len(labels):\n        return labels[cid]\n    return f\"class_{cid}\"\n\ndef compute_map(preds_per_class, gts_per_class, iou_threshold=0.5):\n    \"\"\"\n    Compute mAP over all classes present in gts_per_class using 11-point interpolation.\n    preds_per_class: dict[cid] -> list of dicts { 'image_id': int, 'box': [x1,y1,x2,y2], 'score': float }\n    gts_per_class:   dict[cid] -> list of dicts { 'image_id': int, 'box': [x1,y1,x2,y2] }\n    \"\"\"\n    aps = []\n    for cid in sorted(gts_per_class.keys()):\n        gts = gts_per_class.get(cid, [])\n        preds = preds_per_class.get(cid, [])\n\n        if len(gts) == 0:\n            continue\n\n        # Organize GTs by image and track \"matched\" flags\n        gts_by_img = {}\n        for gt in gts:\n            img_id = gt['image_id']\n            gts_by_img.setdefault(img_id, [])\n            gts_by_img[img_id].append({'box': gt['box'], 'matched': False})\n\n        # Sort predictions by score descending\n        preds_sorted = sorted(preds, key=lambda d: d['score'], reverse=True)\n\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n        total_gts = len(gts)\n\n        for i, p in enumerate(preds_sorted):\n            img_id = p['image_id']\n            p_box = p['box']\n            candidates = gts_by_img.get(img_id, [])\n            best_iou = 0.0\n            best_j = -1\n            for j, gt in enumerate(candidates):\n                if gt['matched']:\n                    continue\n                current_iou = iou_xyxy(p_box, gt['box'])\n                if current_iou > best_iou:\n                    best_iou = current_iou\n                    best_j = j\n            if best_iou >= iou_threshold and best_j >= 0:\n                # True positive\n                candidates[best_j]['matched'] = True\n                tp[i] = 1.0\n                fp[i] = 0.0\n            else:\n                # False positive\n                tp[i] = 0.0\n                fp[i] = 1.0\n\n        if total_gts == 0:\n            continue\n\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-12)\n        recall = cum_tp / float(total_gts)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for r in np.linspace(0, 1, 11):\n            precisions_at_r = precision[recall >= r]\n            p_max = np.max(precisions_at_r) if precisions_at_r.size > 0 else 0.0\n            ap += p_max\n        ap /= 11.0\n        aps.append(ap)\n\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\n# Prepare containers for mAP computation across the video\nfrom collections import defaultdict\npreds_per_class = defaultdict(list)  # cid -> list of {image_id, box, score}\ngts_per_class = defaultdict(list)    # cid -> list of {image_id, box}\n\n# Prepare floating model flag\nfloating_model = (in_dtype == np.float32)\n\n# For mapping output indices, we need a one-time \"dry run\" or do it on first frame\nmapped = False\nboxes_idx = classes_idx = scores_idx = num_idx = None\n\n# =========================\n# Processing Loop\n# =========================\nframe_index = 0\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame(frame_bgr, in_width, in_height, in_dtype, floating_model)\n\n    # 2.3 Quantization Handling already included in preprocess_frame per floating_model\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling\n    # =========================\n    # 4.1 Get Output Tensors\n    if not mapped:\n        boxes_idx, classes_idx, scores_idx, num_idx = map_output_indices(interpreter, output_details)\n        mapped = True\n\n    # Retrieve raw outputs\n    boxes_raw = interpreter.get_tensor(output_details[boxes_idx]['index'])  # [1, N, 4]\n    boxes_raw = boxes_raw[0] if boxes_raw.ndim == 3 else boxes_raw\n\n    # Some models include num_detections; otherwise derive from outputs length\n    if num_idx is not None:\n        num_dets_raw = interpreter.get_tensor(output_details[num_idx]['index'])\n        num_dets = int(np.squeeze(num_dets_raw).astype(np.int32))\n        if num_dets <= 0 or num_dets > boxes_raw.shape[0]:\n            num_dets = boxes_raw.shape[0]\n    else:\n        num_dets = boxes_raw.shape[0]\n\n    scores_raw = interpreter.get_tensor(output_details[scores_idx]['index']) if scores_idx is not None else None\n    classes_raw = interpreter.get_tensor(output_details[classes_idx]['index']) if classes_idx is not None else None\n\n    scores_raw = scores_raw[0] if scores_raw is not None and scores_raw.ndim == 2 else scores_raw\n    classes_raw = classes_raw[0] if classes_raw is not None and classes_raw.ndim == 2 else classes_raw\n\n    if scores_raw is None:\n        # If scores not provided (unlikely for SSD), create dummy confidences\n        scores_raw = np.ones((boxes_raw.shape[0],), dtype=np.float32)\n    if classes_raw is None:\n        # If classes not provided (unlikely), assign all to a default class 0\n        classes_raw = np.zeros((boxes_raw.shape[0],), dtype=np.int32)\n\n    # Clip to num detections\n    boxes_raw = boxes_raw[:num_dets]\n    scores_raw = scores_raw[:num_dets].astype(np.float32)\n    classes_raw = classes_raw[:num_dets].astype(np.int32)\n\n    # Convert boxes to absolute pixel coordinates and clip to frame size\n    abs_boxes = [to_abs_box(b, orig_width, orig_height) for b in boxes_raw]\n    abs_scores = scores_raw.tolist()\n    abs_classes = classes_raw.tolist()\n\n    # 4.2 Interpret Results\n    # Structure detections per class\n    detections_per_class = {}\n    for i in range(len(abs_boxes)):\n        cid = int(abs_classes[i])\n        s = float(abs_scores[i])\n        box = abs_boxes[i]\n        detections_per_class.setdefault(cid, {'boxes': [], 'scores': []})\n        detections_per_class[cid]['boxes'].append(box)\n        detections_per_class[cid]['scores'].append(s)\n\n    # 4.3 Post-processing: apply confidence thresholding and NMS, clip boxes already done\n    final_detections = []  # list of dicts for drawing and GT for mAP proxy\n    for cid, data in detections_per_class.items():\n        boxes_c = data['boxes']\n        scores_c = data['scores']\n        # First filter by confidence threshold for display / GT\n        idxs_conf = [i for i, sc in enumerate(scores_c) if sc >= CONF_THRESH]\n        boxes_thr = [boxes_c[i] for i in idxs_conf]\n        scores_thr = [scores_c[i] for i in idxs_conf]\n        if len(boxes_thr) == 0:\n            continue\n        # Apply NMS per class\n        keep_idx = nms_per_class(boxes_thr, scores_thr, iou_thresh=0.5)\n        for k in keep_idx:\n            final_detections.append({\n                'class_id': cid,\n                'score': float(scores_thr[k]),\n                'box': boxes_thr[k]\n            })\n\n    # Prepare data for mAP computation (proxy: predictions vs NMS results as pseudo GT)\n    # Add ground truths (NMS results) for this frame\n    for det in final_detections:\n        gts_per_class[det['class_id']].append({\n            'image_id': frame_index,\n            'box': det['box']\n        })\n\n    # Add predictions (raw model outputs) for this frame using all proposals (score > 0)\n    for i in range(len(abs_boxes)):\n        cid = int(abs_classes[i])\n        preds_per_class[cid].append({\n            'image_id': frame_index,\n            'box': abs_boxes[i],\n            'score': float(abs_scores[i])\n        })\n\n    # Compute running mAP over seen frames\n    running_map = compute_map(preds_per_class, gts_per_class, iou_threshold=0.5)\n\n    # 4.4 Handle Output: draw rectangles and labels, overlay mAP; then write to video\n    canvas = frame_bgr.copy()\n    for det in final_detections:\n        cid = det['class_id']\n        sc = det['score']\n        x1, y1, x2, y2 = det['box']\n        color = class_color(cid)\n        cv2.rectangle(canvas, (x1, y1), (x2, y2), color, 2)\n        label_text = f\"{get_label_name(cid)}: {sc:.2f}\"\n        # Put label background for readability\n        (tw, th), bl = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(canvas, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), color, -1)\n        cv2.putText(canvas, label_text, (x1 + 2, y1 - 4),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Overlay running mAP\n    map_text = f\"mAP: {running_map:.3f}\"\n    cv2.putText(canvas, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n                0.9, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(canvas, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n                0.9, (255, 255, 255), 2, cv2.LINE_AA)\n\n    out_writer.write(canvas)\n\n    frame_index += 1\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nout_writer.release()\n\nprint(f\"Processing complete.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Annotated output saved to: {OUTPUT_PATH}\")\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 10672,
            "total": 12172,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.107011,
          "latency": 153176.0,
          "timeToFirstToken": 153.176,
          "updatedAt": "2026-02-05T02:14:33.460Z",
          "totalTokens": 12172,
          "costDetails": {
            "total": 0.107011
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 10672,
          "usageDetails": {
            "input": 1500,
            "output": 10672,
            "total": 12172
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:14:28.333Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:15:54.849Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:11:54.103Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "c9281b37",
      "timestamp": "2026-02-05T02:08:56.753000+00:00",
      "name": "c928_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Interpreter per guideline\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\ndef ensure_dir_exists(path):\n    directory = os.path.dirname(path)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\ndef get_output_mappings(interpreter, output_details):\n    \"\"\"\n    Attempts to identify SSD-like TFLite outputs:\n    - boxes: [1, num, 4] ymin, xmin, ymax, xmax\n    - classes: [1, num]\n    - scores: [1, num]\n    - num_detections: [1] or [1,1]\n    Returns a dict with numpy arrays for 'boxes', 'classes', 'scores', 'num'.\n    \"\"\"\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Identify boxes by last dimension being 4\n    for out in outputs:\n        arr = np.squeeze(out)\n        if arr.ndim == 2 and arr.shape[-1] == 4:\n            boxes = out\n            break\n\n    # Identify num detections by scalar or [1] or [1,1]\n    for out in outputs:\n        arr = np.squeeze(out)\n        if arr.size == 1:\n            try:\n                num = int(np.round(float(arr)))\n            except Exception:\n                num = int(arr)\n            break\n\n    # Identify scores vs classes among remaining outputs\n    for out in outputs:\n        if boxes is not None and out is boxes:\n            continue\n        arr = np.squeeze(out)\n        if arr.size == 1 and num is not None and int(np.round(float(arr))) == num:\n            # likely num already taken\n            continue\n        # Scores typically in [0,1]\n        if np.issubdtype(out.dtype, np.floating):\n            mn = float(np.min(out))\n            mx = float(np.max(out))\n            if 0.0 <= mn and mx <= 1.0:\n                scores = out\n            else:\n                # could be classes as float IDs\n                if classes is None:\n                    classes = out\n        else:\n            # integer or other => likely classes\n            if classes is None:\n                classes = out\n\n    # Fallbacks: if classes or scores are still None, try remaining\n    if scores is None or classes is None:\n        for out in outputs:\n            if out is boxes:\n                continue\n            if num is not None and np.squeeze(out).size == 1:\n                continue\n            if scores is None and np.issubdtype(out.dtype, np.floating):\n                scores = out\n            elif classes is None:\n                classes = out\n\n    return {\n        'boxes': boxes,\n        'classes': classes,\n        'scores': scores,\n        'num': num\n    }\n\ndef draw_label_with_background(img, text, left, top, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=0.5, thickness=1, color=(255,255,255), bg_color=(0,0,0)):\n    (text_w, text_h), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n    cv2.rectangle(img, (left, top - text_h - baseline), (left + text_w, top + baseline), bg_color, thickness=cv2.FILLED)\n    cv2.putText(img, text, (left, top - baseline), font, font_scale, color, thickness, lineType=cv2.LINE_AA)\n\ndef color_for_class(class_id):\n    # Deterministic color per class\n    b = (37 * (class_id + 1)) % 255\n    g = (17 * (class_id + 1)) % 255\n    r = (29 * (class_id + 1)) % 255\n    return int(b), int(g), int(r)\n\ndef get_label_name(class_id, labels):\n    # Try both 0-based and 1-based indexing robustly\n    if labels:\n        if 0 <= class_id < len(labels):\n            return labels[class_id]\n        elif 0 <= class_id - 1 < len(labels):\n            return labels[class_id - 1]\n    return f'class_{class_id}'\n\ndef compute_map_proxy(class_scores_dict):\n    \"\"\"\n    Proxy mAP calculation:\n    True mAP requires ground truth annotations. Since none are provided,\n    this function computes a proxy metric by:\n      - averaging detection scores per class across the video\n      - taking the mean over classes that had detections\n    This is NOT a true mAP, but a confidence-based proxy for demonstration.\n    \"\"\"\n    per_class_means = []\n    for scores in class_scores_dict.values():\n        if len(scores) > 0:\n            per_class_means.append(float(np.mean(scores)))\n    if not per_class_means:\n        return 0.0\n    return float(np.mean(per_class_means))\n\ndef main():\n    # 1.2 Paths/Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')\n\n    # Ensure output directory exists\n    ensure_dir_exists(output_path)\n\n    # 1.3 Load Labels\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    if not input_details:\n        raise RuntimeError(\"No input details available from the TFLite interpreter.\")\n    if not output_details:\n        raise RuntimeError(\"No output details available from the TFLite interpreter.\")\n\n    input_shape = input_details[0]['shape']\n    input_height = int(input_shape[1])\n    input_width = int(input_shape[2])\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire input data: Read a single video file from input_path\n    if not os.path.isfile(input_path):\n        raise FileNotFoundError(f\"Input video not found: {input_path}\")\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    # Prepare video writer\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # fallback\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        raise RuntimeError(f\"Failed to open video writer for output: {output_path}\")\n\n    # Stats for proxy mAP\n    class_scores_dict = {}  # class_id -> list of scores\n    total_frames = 0\n    inference_times = []\n\n    try:\n        while True:\n            ret, frame_bgr = cap.read()\n            if not ret:\n                break  # End of video\n            total_frames += 1\n\n            # 2.2 Preprocess Data to match model input shape/dtype\n            # Convert BGR to RGB, resize, add batch dimension\n            rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n            resized = cv2.resize(rgb, (input_width, input_height), interpolation=cv2.INTER_LINEAR)\n            input_data = np.expand_dims(resized, axis=0)\n\n            # 2.3 Quantization Handling\n            if floating_model:\n                input_data = (np.float32(input_data) - 127.5) / 127.5\n            else:\n                input_data = np.uint8(input_data)\n\n            # Phase 3: Inference\n            # 3.1 Set Input Tensor\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n            # 3.2 Run Inference\n            t0 = time.time()\n            interpreter.invoke()\n            t1 = time.time()\n            inference_times.append(t1 - t0)\n\n            # Phase 4: Output Interpretation & Handling\n            # 4.1 Get Output Tensors\n            out_map = get_output_mappings(interpreter, output_details)\n            boxes = out_map['boxes']\n            classes = out_map['classes']\n            scores = out_map['scores']\n            num = out_map['num']\n\n            if boxes is None or classes is None or scores is None:\n                raise RuntimeError(\"Failed to parse model outputs (boxes/classes/scores).\")\n\n            # Squeeze to expected shapes\n            boxes = np.squeeze(boxes)  # [num, 4]\n            classes = np.squeeze(classes)  # [num]\n            scores = np.squeeze(scores)  # [num]\n            if boxes.ndim == 1 and boxes.shape[0] == 4:\n                boxes = np.expand_dims(boxes, axis=0)\n            if classes.ndim == 0:\n                classes = np.expand_dims(classes, axis=0)\n            if scores.ndim == 0:\n                scores = np.expand_dims(scores, axis=0)\n\n            if num is None:\n                # Fallback: use min shape across outputs\n                det_count = min(boxes.shape[0], classes.shape[0], scores.shape[0])\n            else:\n                det_count = min(int(num), boxes.shape[0], classes.shape[0], scores.shape[0])\n\n            # 4.2 Interpret Results: map class indices to labels and prepare display\n            # 4.3 Post-processing: thresholding, coordinate scaling, clipping\n            for i in range(det_count):\n                score = float(scores[i])\n                if score < confidence_threshold:\n                    continue\n\n                box = boxes[i]  # [ymin, xmin, ymax, xmax] normalized 0..1\n                ymin = max(0, min(int(round(box[0] * frame_h)), frame_h - 1))\n                xmin = max(0, min(int(round(box[1] * frame_w)), frame_w - 1))\n                ymax = max(0, min(int(round(box[2] * frame_h)), frame_h - 1))\n                xmax = max(0, min(int(round(box[3] * frame_w)), frame_w - 1))\n\n                # Fix potential inverted boxes\n                if xmax < xmin:\n                    xmin, xmax = xmax, xmin\n                if ymax < ymin:\n                    ymin, ymax = ymax, ymin\n\n                class_id = int(round(float(classes[i])))\n                label_name = get_label_name(class_id, labels)\n                color = color_for_class(class_id)\n\n                # Draw rectangle and label\n                cv2.rectangle(frame_bgr, (xmin, ymin), (xmax, ymax), color, thickness=2)\n                label_text = f\"{label_name} {score:.2f}\"\n                draw_label_with_background(frame_bgr, label_text, xmin, max(ymin, 20), color=(255,255,255), bg_color=color)\n\n                # Update stats for proxy mAP\n                if class_id not in class_scores_dict:\n                    class_scores_dict[class_id] = []\n                class_scores_dict[class_id].append(score)\n\n            # Compute running proxy mAP and overlay\n            map_proxy = compute_map_proxy(class_scores_dict)\n            # Overlay mAP (proxy) and FPS on the frame\n            avg_infer_time = np.mean(inference_times) if len(inference_times) > 0 else 0.0\n            fps_text = f\"FPS: {1.0/avg_infer_time:.1f}\" if avg_infer_time > 0 else \"FPS: N/A\"\n            # mAP label: clarified as proxy due to lack of ground truth\n            map_text = f\"mAP (proxy): {map_proxy:.3f}\"\n            draw_label_with_background(frame_bgr, map_text, 10, 30, font_scale=0.6, thickness=2, color=(255,255,255), bg_color=(0,0,0))\n            draw_label_with_background(frame_bgr, fps_text, 10, 60, font_scale=0.6, thickness=2, color=(255,255,255), bg_color=(0,0,0))\n\n            # 4.4 Handle Output: write annotated frame to output video\n            writer.write(frame_bgr)\n\n            # 4.5 Loop Continuation: auto-continues until video ends\n\n    finally:\n        # Phase 5: Cleanup\n        cap.release()\n        writer.release()\n\n    # Final reporting\n    final_map_proxy = compute_map_proxy(class_scores_dict)\n    print(\"Processing complete.\")\n    print(f\"Output video saved to: {output_path}\")\n    print(f\"Frames processed: {total_frames}\")\n    print(f\"Average inference time per frame: {np.mean(inference_times)*1000.0:.2f} ms\" if inference_times else \"Average inference time per frame: N/A\")\n    print(f\"Calculated mAP (proxy, no ground truth provided): {final_map_proxy:.4f}\")\n    if labels:\n        print(\"First 10 labels loaded:\")\n        for i, name in enumerate(labels[:10]):\n            print(f\"  {i}: {name}\")\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Application: Object Detection via a video file\n    Target device: Raspberry Pi 4B\n    - Reads single video file from input_path\n    - Runs TFLite SSD model via ai_edge_litert Interpreter\n    - Outputs annotated video with bounding boxes, labels, and overlays proxy mAP\n    Note:\n    True mAP requires ground-truth annotations, which are not provided.\n    A confidence-based proxy mAP is computed and displayed for demonstration.\n    \"\"\"\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/c9281b37",
      "latency": 121.271,
      "total_cost": 0.078151,
      "observations": [
        {
          "id": "time-04-08-57-390580_chatcmpl-D5j5dM88TN3GU2Z1cTuYvcWF6R4t5",
          "traceId": "c9281b37",
          "type": "GENERATION",
          "name": "c928_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:08:57.390000+00:00",
          "endTime": "2026-02-05T02:10:58.661000+00:00",
          "completionStartTime": "2026-02-05T02:10:58.661000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.07815100000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Interpreter per guideline\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\ndef ensure_dir_exists(path):\n    directory = os.path.dirname(path)\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\ndef get_output_mappings(interpreter, output_details):\n    \"\"\"\n    Attempts to identify SSD-like TFLite outputs:\n    - boxes: [1, num, 4] ymin, xmin, ymax, xmax\n    - classes: [1, num]\n    - scores: [1, num]\n    - num_detections: [1] or [1,1]\n    Returns a dict with numpy arrays for 'boxes', 'classes', 'scores', 'num'.\n    \"\"\"\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Identify boxes by last dimension being 4\n    for out in outputs:\n        arr = np.squeeze(out)\n        if arr.ndim == 2 and arr.shape[-1] == 4:\n            boxes = out\n            break\n\n    # Identify num detections by scalar or [1] or [1,1]\n    for out in outputs:\n        arr = np.squeeze(out)\n        if arr.size == 1:\n            try:\n                num = int(np.round(float(arr)))\n            except Exception:\n                num = int(arr)\n            break\n\n    # Identify scores vs classes among remaining outputs\n    for out in outputs:\n        if boxes is not None and out is boxes:\n            continue\n        arr = np.squeeze(out)\n        if arr.size == 1 and num is not None and int(np.round(float(arr))) == num:\n            # likely num already taken\n            continue\n        # Scores typically in [0,1]\n        if np.issubdtype(out.dtype, np.floating):\n            mn = float(np.min(out))\n            mx = float(np.max(out))\n            if 0.0 <= mn and mx <= 1.0:\n                scores = out\n            else:\n                # could be classes as float IDs\n                if classes is None:\n                    classes = out\n        else:\n            # integer or other => likely classes\n            if classes is None:\n                classes = out\n\n    # Fallbacks: if classes or scores are still None, try remaining\n    if scores is None or classes is None:\n        for out in outputs:\n            if out is boxes:\n                continue\n            if num is not None and np.squeeze(out).size == 1:\n                continue\n            if scores is None and np.issubdtype(out.dtype, np.floating):\n                scores = out\n            elif classes is None:\n                classes = out\n\n    return {\n        'boxes': boxes,\n        'classes': classes,\n        'scores': scores,\n        'num': num\n    }\n\ndef draw_label_with_background(img, text, left, top, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=0.5, thickness=1, color=(255,255,255), bg_color=(0,0,0)):\n    (text_w, text_h), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n    cv2.rectangle(img, (left, top - text_h - baseline), (left + text_w, top + baseline), bg_color, thickness=cv2.FILLED)\n    cv2.putText(img, text, (left, top - baseline), font, font_scale, color, thickness, lineType=cv2.LINE_AA)\n\ndef color_for_class(class_id):\n    # Deterministic color per class\n    b = (37 * (class_id + 1)) % 255\n    g = (17 * (class_id + 1)) % 255\n    r = (29 * (class_id + 1)) % 255\n    return int(b), int(g), int(r)\n\ndef get_label_name(class_id, labels):\n    # Try both 0-based and 1-based indexing robustly\n    if labels:\n        if 0 <= class_id < len(labels):\n            return labels[class_id]\n        elif 0 <= class_id - 1 < len(labels):\n            return labels[class_id - 1]\n    return f'class_{class_id}'\n\ndef compute_map_proxy(class_scores_dict):\n    \"\"\"\n    Proxy mAP calculation:\n    True mAP requires ground truth annotations. Since none are provided,\n    this function computes a proxy metric by:\n      - averaging detection scores per class across the video\n      - taking the mean over classes that had detections\n    This is NOT a true mAP, but a confidence-based proxy for demonstration.\n    \"\"\"\n    per_class_means = []\n    for scores in class_scores_dict.values():\n        if len(scores) > 0:\n            per_class_means.append(float(np.mean(scores)))\n    if not per_class_means:\n        return 0.0\n    return float(np.mean(per_class_means))\n\ndef main():\n    # 1.2 Paths/Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = float('0.5')\n\n    # Ensure output directory exists\n    ensure_dir_exists(output_path)\n\n    # 1.3 Load Labels\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    if not input_details:\n        raise RuntimeError(\"No input details available from the TFLite interpreter.\")\n    if not output_details:\n        raise RuntimeError(\"No output details available from the TFLite interpreter.\")\n\n    input_shape = input_details[0]['shape']\n    input_height = int(input_shape[1])\n    input_width = int(input_shape[2])\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire input data: Read a single video file from input_path\n    if not os.path.isfile(input_path):\n        raise FileNotFoundError(f\"Input video not found: {input_path}\")\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    # Prepare video writer\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # fallback\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        raise RuntimeError(f\"Failed to open video writer for output: {output_path}\")\n\n    # Stats for proxy mAP\n    class_scores_dict = {}  # class_id -> list of scores\n    total_frames = 0\n    inference_times = []\n\n    try:\n        while True:\n            ret, frame_bgr = cap.read()\n            if not ret:\n                break  # End of video\n            total_frames += 1\n\n            # 2.2 Preprocess Data to match model input shape/dtype\n            # Convert BGR to RGB, resize, add batch dimension\n            rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n            resized = cv2.resize(rgb, (input_width, input_height), interpolation=cv2.INTER_LINEAR)\n            input_data = np.expand_dims(resized, axis=0)\n\n            # 2.3 Quantization Handling\n            if floating_model:\n                input_data = (np.float32(input_data) - 127.5) / 127.5\n            else:\n                input_data = np.uint8(input_data)\n\n            # Phase 3: Inference\n            # 3.1 Set Input Tensor\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n            # 3.2 Run Inference\n            t0 = time.time()\n            interpreter.invoke()\n            t1 = time.time()\n            inference_times.append(t1 - t0)\n\n            # Phase 4: Output Interpretation & Handling\n            # 4.1 Get Output Tensors\n            out_map = get_output_mappings(interpreter, output_details)\n            boxes = out_map['boxes']\n            classes = out_map['classes']\n            scores = out_map['scores']\n            num = out_map['num']\n\n            if boxes is None or classes is None or scores is None:\n                raise RuntimeError(\"Failed to parse model outputs (boxes/classes/scores).\")\n\n            # Squeeze to expected shapes\n            boxes = np.squeeze(boxes)  # [num, 4]\n            classes = np.squeeze(classes)  # [num]\n            scores = np.squeeze(scores)  # [num]\n            if boxes.ndim == 1 and boxes.shape[0] == 4:\n                boxes = np.expand_dims(boxes, axis=0)\n            if classes.ndim == 0:\n                classes = np.expand_dims(classes, axis=0)\n            if scores.ndim == 0:\n                scores = np.expand_dims(scores, axis=0)\n\n            if num is None:\n                # Fallback: use min shape across outputs\n                det_count = min(boxes.shape[0], classes.shape[0], scores.shape[0])\n            else:\n                det_count = min(int(num), boxes.shape[0], classes.shape[0], scores.shape[0])\n\n            # 4.2 Interpret Results: map class indices to labels and prepare display\n            # 4.3 Post-processing: thresholding, coordinate scaling, clipping\n            for i in range(det_count):\n                score = float(scores[i])\n                if score < confidence_threshold:\n                    continue\n\n                box = boxes[i]  # [ymin, xmin, ymax, xmax] normalized 0..1\n                ymin = max(0, min(int(round(box[0] * frame_h)), frame_h - 1))\n                xmin = max(0, min(int(round(box[1] * frame_w)), frame_w - 1))\n                ymax = max(0, min(int(round(box[2] * frame_h)), frame_h - 1))\n                xmax = max(0, min(int(round(box[3] * frame_w)), frame_w - 1))\n\n                # Fix potential inverted boxes\n                if xmax < xmin:\n                    xmin, xmax = xmax, xmin\n                if ymax < ymin:\n                    ymin, ymax = ymax, ymin\n\n                class_id = int(round(float(classes[i])))\n                label_name = get_label_name(class_id, labels)\n                color = color_for_class(class_id)\n\n                # Draw rectangle and label\n                cv2.rectangle(frame_bgr, (xmin, ymin), (xmax, ymax), color, thickness=2)\n                label_text = f\"{label_name} {score:.2f}\"\n                draw_label_with_background(frame_bgr, label_text, xmin, max(ymin, 20), color=(255,255,255), bg_color=color)\n\n                # Update stats for proxy mAP\n                if class_id not in class_scores_dict:\n                    class_scores_dict[class_id] = []\n                class_scores_dict[class_id].append(score)\n\n            # Compute running proxy mAP and overlay\n            map_proxy = compute_map_proxy(class_scores_dict)\n            # Overlay mAP (proxy) and FPS on the frame\n            avg_infer_time = np.mean(inference_times) if len(inference_times) > 0 else 0.0\n            fps_text = f\"FPS: {1.0/avg_infer_time:.1f}\" if avg_infer_time > 0 else \"FPS: N/A\"\n            # mAP label: clarified as proxy due to lack of ground truth\n            map_text = f\"mAP (proxy): {map_proxy:.3f}\"\n            draw_label_with_background(frame_bgr, map_text, 10, 30, font_scale=0.6, thickness=2, color=(255,255,255), bg_color=(0,0,0))\n            draw_label_with_background(frame_bgr, fps_text, 10, 60, font_scale=0.6, thickness=2, color=(255,255,255), bg_color=(0,0,0))\n\n            # 4.4 Handle Output: write annotated frame to output video\n            writer.write(frame_bgr)\n\n            # 4.5 Loop Continuation: auto-continues until video ends\n\n    finally:\n        # Phase 5: Cleanup\n        cap.release()\n        writer.release()\n\n    # Final reporting\n    final_map_proxy = compute_map_proxy(class_scores_dict)\n    print(\"Processing complete.\")\n    print(f\"Output video saved to: {output_path}\")\n    print(f\"Frames processed: {total_frames}\")\n    print(f\"Average inference time per frame: {np.mean(inference_times)*1000.0:.2f} ms\" if inference_times else \"Average inference time per frame: N/A\")\n    print(f\"Calculated mAP (proxy, no ground truth provided): {final_map_proxy:.4f}\")\n    if labels:\n        print(\"First 10 labels loaded:\")\n        for i, name in enumerate(labels[:10]):\n            print(f\"  {i}: {name}\")\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Application: Object Detection via a video file\n    Target device: Raspberry Pi 4B\n    - Reads single video file from input_path\n    - Runs TFLite SSD model via ai_edge_litert Interpreter\n    - Outputs annotated video with bounding boxes, labels, and overlays proxy mAP\n    Note:\n    True mAP requires ground-truth annotations, which are not provided.\n    A confidence-based proxy mAP is computed and displayed for demonstration.\n    \"\"\"\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 7786,
            "total": 9286,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.078151,
          "latency": 121271.0,
          "timeToFirstToken": 121.271,
          "updatedAt": "2026-02-05T02:11:04.787Z",
          "totalTokens": 9286,
          "costDetails": {
            "total": 0.078151
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 7786,
          "usageDetails": {
            "input": 1500,
            "output": 7786,
            "total": 9286
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:10:59.760Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:11:48.541Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:08:57.338Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "3ddffe40",
      "timestamp": "2026-02-05T02:06:15.976000+00:00",
      "name": "3ddf_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n# Application: Object Detection via a video file\n# Target Device: Raspberry Pi 4B\n# Description:\n#   - Loads a TFLite SSD-Mobilenet v1 model and label map.\n#   - Reads a single video file, performs object detection per frame.\n#   - Draws bounding boxes with labels on detected objects.\n#   - Computes and overlays a running mAP value (proxy; see note below) on the video.\n#   - Writes the annotated output to a video file.\n#\n# Note on mAP:\n#   Ground truth annotations are not provided for the input video. Therefore, classical mAP cannot be computed.\n#   This implementation uses a proxy: per-class AP is approximated as the mean confidence of detections for that class,\n#   and mAP is the mean of these per-class averages across all classes with at least one detection.\n\nimport os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter  # Phase 1.1: Required by guideline\n\n# -------------------------------\n# Phase 1: Setup\n# -------------------------------\n\n# Phase 1.2: Configuration Parameters (provided)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')  # Confidence threshold\n\ndef ensure_dir_for_file(file_path: str):\n    \"\"\"Ensure directory for a given file path exists.\"\"\"\n    dir_path = os.path.dirname(os.path.abspath(file_path))\n    if dir_path and not os.path.exists(dir_path):\n        os.makedirs(dir_path, exist_ok=True)\n\ndef load_labels(label_path: str):\n    \"\"\"Load labels from file into a list.\"\"\"\n    labels = []\n    try:\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line != '':\n                    labels.append(line)\n    except Exception as e:\n        print(f\"Warning: Failed to read labels from {label_path}: {e}\")\n    return labels\n\ndef find_output_indices(output_details):\n    \"\"\"\n    Identify indices for boxes, classes, scores, and num_detections tensors.\n    Typical SSD MobileNet v1 TFLite outputs:\n      - boxes: (1, N, 4)\n      - classes: (1, N)\n      - scores: (1, N)\n      - num_detections: (1)\n    \"\"\"\n    idx_boxes = idx_classes = idx_scores = idx_num = None\n    for i, det in enumerate(output_details):\n        shape = det.get('shape', None)\n        if shape is None:\n            continue\n        shape_tuple = tuple(shape)\n        # Boxes: shape like (1, N, 4)\n        if len(shape_tuple) == 3 and shape_tuple[-1] == 4:\n            idx_boxes = i\n        # Scores or Classes: (1, N)\n        elif len(shape_tuple) == 2 and shape_tuple[0] == 1:\n            # We cannot distinguish scores vs classes by shape alone; defer until we fetch data\n            # We'll still record potential candidates and resolve after first inference if needed.\n            pass\n        # Num detections: (1) or (1,1)\n        elif len(shape_tuple) in (1, 2) and np.prod(shape_tuple) == 1:\n            idx_num = i\n    # The conventional ordering for TFLite SSD is [boxes, classes, scores, num_detections].\n    # If we got boxes and num, infer rest by position if not explicitly determined later.\n    # We'll return the current best guess; will finalize after first inference if needed.\n    return idx_boxes, idx_classes, idx_scores, idx_num\n\ndef label_from_class_id(class_id: int, labels: list):\n    \"\"\"\n    Obtain label string for a class ID with robustness to indexing scheme.\n    Some models use 1-based class indices while label files might be 0-based and vice versa.\n    \"\"\"\n    name = str(class_id)\n    if labels:\n        # Try direct index\n        if 0 <= class_id < len(labels):\n            name = labels[class_id]\n        # Try 1-based to 0-based mapping\n        elif 0 <= (class_id - 1) < len(labels):\n            name = labels[class_id - 1]\n        else:\n            # Fallback to best effort within bounds\n            idx = max(0, min(class_id, len(labels) - 1))\n            name = labels[idx]\n    return name\n\ndef preprocess_frame(frame_bgr, input_details, floating_model):\n    \"\"\"\n    Phase 2.2 & 2.3: Preprocess BGR frame to model input tensor based on input_details.\n    - Resize to expected input size.\n    - Convert BGR to RGB.\n    - Adjust dtype; normalize if floating model: (x - 127.5)/127.5\n    \"\"\"\n    in_info = input_details[0]\n    in_shape = in_info['shape']  # Expected [1, height, width, 3]\n    height, width = int(in_shape[1]), int(in_shape[2])\n\n    # Resize and convert color\n    frame_resized = cv2.resize(frame_bgr, (width, height), interpolation=cv2.INTER_LINEAR)\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n\n    # Prepare input tensor\n    if floating_model:\n        input_data = (np.float32(frame_rgb) - 127.5) / 127.5\n    else:\n        # For uint8 models, keep 0..255\n        input_data = np.uint8(frame_rgb)\n\n    # Expand dims to [1, H, W, 3]\n    input_data = np.expand_dims(input_data, axis=0)\n    # Ensure dtype matches exactly\n    input_data = input_data.astype(in_info['dtype'])\n    return input_data\n\ndef scale_and_clip_box(box, frame_w, frame_h):\n    \"\"\"\n    Phase 4.3: Convert normalized ymin, xmin, ymax, xmax to pixel coords and clip to frame bounds.\n    \"\"\"\n    ymin, xmin, ymax, xmax = box\n    left = int(max(0, min(frame_w - 1, xmin * frame_w)))\n    right = int(max(0, min(frame_w - 1, xmax * frame_w)))\n    top = int(max(0, min(frame_h - 1, ymin * frame_h)))\n    bottom = int(max(0, min(frame_h - 1, ymax * frame_h)))\n    # Ensure proper ordering\n    left, right = min(left, right), max(left, right)\n    top, bottom = min(top, bottom), max(top, bottom)\n    return left, top, right, bottom\n\ndef draw_label_with_bg(img, text, origin, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=0.5, fg_color=(255,255,255), bg_color=(0,0,0), thickness=1, alpha=0.6, padding=2):\n    \"\"\"\n    Draws text with a filled rectangle background for readability.\n    \"\"\"\n    x, y = origin\n    (tw, th), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n    th_total = th + baseline\n    # Background rectangle\n    x2 = x + tw + 2 * padding\n    y2 = y + th_total + 2 * padding\n    overlay = img.copy()\n    cv2.rectangle(overlay, (x, y), (x2, y2), bg_color, thickness=-1)\n    cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)\n    # Put text\n    text_org = (x + padding, y + th + padding)\n    cv2.putText(img, text, text_org, font, font_scale, fg_color, thickness, cv2.LINE_AA)\n\ndef compute_proxy_map(class_conf_dict):\n    \"\"\"\n    Compute proxy mAP:\n      - For each class with >=1 detection, AP_class = mean(confidences of that class).\n      - mAP = mean(AP_class over classes with detections).\n    Returns mAP (float) and a dict of per-class AP.\n    \"\"\"\n    ap_values = []\n    per_class_ap = {}\n    for cls_id, confs in class_conf_dict.items():\n        if len(confs) > 0:\n            ap = float(np.mean(confs))\n            per_class_ap[cls_id] = ap\n            ap_values.append(ap)\n    if len(ap_values) == 0:\n        return 0.0, per_class_ap\n    return float(np.mean(ap_values)), per_class_ap\n\ndef main():\n    # Phase 1.3: Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Phase 1.4: Load Interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n\n    # Phase 1.5: Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    floating_model = (input_details[0]['dtype'] == np.float32)\n\n    # Determine output indices (best-effort pre-identification)\n    idx_boxes, idx_classes, idx_scores, idx_num = find_output_indices(output_details)\n\n    # Phase 2.1: Acquire Input Data (Open video file)\n    if not os.path.exists(INPUT_PATH):\n        print(f\"Error: Input video not found at {INPUT_PATH}\")\n        return\n\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: Failed to open input video: {INPUT_PATH}\")\n        return\n\n    # Prepare output writer\n    ensure_dir_for_file(OUTPUT_PATH)\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) or 0\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 0\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # default fallback\n\n    # Fallback for any invalid dimension\n    if frame_width <= 0 or frame_height <= 0:\n        # Try to read one frame to get size\n        ret_tmp, frame_tmp = cap.read()\n        if not ret_tmp:\n            print(\"Error: Unable to determine video frame size.\")\n            cap.release()\n            return\n        frame_height, frame_width = frame_tmp.shape[:2]\n        # Rewind capture\n        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n    if not writer.isOpened():\n        print(f\"Error: Failed to open output writer for {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # Data accumulators for proxy mAP\n    class_confidences = {}  # dict: class_id -> list of confidences\n    total_frames = 0\n    total_infer_time = 0.0\n\n    # -------------------------------\n    # Phase 2 & 3 & 4: Processing Loop\n    # -------------------------------\n    while True:\n        # Phase 2.1: Read next frame\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # Phase 4.5: Loop termination (end of video)\n\n        total_frames += 1\n        orig_h, orig_w = frame_bgr.shape[:2]\n\n        # Phase 2.2 & 2.3: Preprocess\n        input_data = preprocess_frame(frame_bgr, input_details, floating_model)\n\n        # Phase 3.1: Set input tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        # Phase 3.2: Inference\n        t0 = time.time()\n        interpreter.invoke()\n        infer_time = time.time() - t0\n        total_infer_time += infer_time\n\n        # Phase 4.1: Retrieve output tensors\n        # Fetch all outputs first\n        outputs = []\n        for od in output_details:\n            outputs.append(interpreter.get_tensor(od['index']))\n\n        # Resolve indices for classes and scores if unknown by analyzing shapes/dtypes\n        # Typical outputs: boxes (float32), classes (float32), scores (float32), num (float32)\n        # We'll detect by shape pattern and value ranges.\n        boxes = classes = scores = num_det = None\n\n        # Boxes\n        if idx_boxes is not None:\n            boxes = outputs[idx_boxes]\n        else:\n            for i, arr in enumerate(outputs):\n                if arr.ndim == 3 and arr.shape[-1] == 4 and arr.shape[0] == 1:\n                    boxes = arr\n                    idx_boxes = i\n                    break\n\n        # num_detections\n        if idx_num is not None:\n            num_det = outputs[idx_num]\n        else:\n            for i, arr in enumerate(outputs):\n                if np.prod(arr.shape) == 1:\n                    num_det = arr\n                    idx_num = i\n                    break\n\n        # The remaining two are classes and scores with shape (1, N)\n        candidate_indices = [i for i in range(len(outputs)) if i not in [idx_boxes, idx_num] and outputs[i].ndim == 2 and outputs[i].shape[0] == 1]\n        # Heuristic: scores are in [0,1], classes are integer-like IDs\n        for i in candidate_indices:\n            arr = outputs[i]\n            arr_flat = arr.flatten()\n            if arr.dtype == np.float32 or arr.dtype == np.float64:\n                # If values plausibly in [0,1]\n                if arr_flat.size > 0 and np.nanmin(arr_flat) >= 0.0 and np.nanmax(arr_flat) <= 1.0:\n                    scores = arr\n                else:\n                    classes = arr\n            else:\n                classes = arr\n        # If ambiguous, fall back to typical ordering: 0: boxes, 1: classes, 2: scores, 3: num\n        if classes is None or scores is None:\n            if len(outputs) >= 4:\n                boxes = boxes if boxes is not None else outputs[0]\n                classes = outputs[1]\n                scores = outputs[2]\n                num_det = num_det if num_det is not None else outputs[3]\n            else:\n                # As a last resort, try to assign by ranges\n                for i in candidate_indices:\n                    arr = outputs[i]\n                    if scores is None and (np.nanmax(arr) <= 1.0):\n                        scores = arr\n                    elif classes is None:\n                        classes = arr\n\n        # Squeeze outputs to remove batch dim\n        if boxes is not None:\n            boxes = np.squeeze(boxes, axis=0)\n        if classes is not None:\n            classes = np.squeeze(classes, axis=0)\n        if scores is not None:\n            scores = np.squeeze(scores, axis=0)\n        if num_det is not None:\n            num_val = int(np.squeeze(num_det).astype(np.int32))\n        else:\n            # If num_detections not provided, infer from boxes shape\n            num_val = boxes.shape[0] if boxes is not None else 0\n\n        # Safety checks\n        if boxes is None or classes is None or scores is None:\n            # Cannot process detections; write original frame and continue\n            writer.write(frame_bgr)\n            continue\n\n        # Phase 4.2 & 4.3: Interpret and Post-process detections\n        # Filter by confidence threshold, scale boxes to original frame size, clip, and draw\n        detections_drawn = 0\n        for i in range(num_val):\n            score = float(scores[i])\n            if score < CONF_THRESHOLD:\n                continue\n            cls_id_raw = classes[i]\n            # Convert class id to int\n            try:\n                cls_id = int(cls_id_raw)\n            except Exception:\n                cls_id = int(np.round(float(cls_id_raw)))\n\n            box = boxes[i]  # [ymin, xmin, ymax, xmax] normalized\n            left, top, right, bottom = scale_and_clip_box(box, orig_w, orig_h)\n\n            # Draw bounding box\n            cv2.rectangle(frame_bgr, (left, top), (right, bottom), (0, 255, 0), thickness=2)\n\n            # Prepare label text\n            label_text = label_from_class_id(cls_id, labels)\n            display_text = f\"{label_text}: {score:.2f}\"\n\n            # Draw label background and text\n            draw_label_with_bg(frame_bgr, display_text, (left, max(0, top - 20)), fg_color=(255,255,255), bg_color=(0,128,0))\n\n            # Accumulate confidences for proxy mAP\n            if cls_id not in class_confidences:\n                class_confidences[cls_id] = []\n            class_confidences[cls_id].append(score)\n            detections_drawn += 1\n\n        # Compute running proxy mAP and overlay\n        map_value, _ = compute_proxy_map(class_confidences)\n        map_text = f\"mAP: {map_value:.3f} | FPS(Inf): {1.0 / infer_time:.2f}\"\n        draw_label_with_bg(frame_bgr, map_text, (10, 10), fg_color=(255,255,255), bg_color=(0,0,0))\n\n        # Phase 4.4: Write annotated frame to output\n        writer.write(frame_bgr)\n\n    # -------------------------------\n    # Phase 5: Cleanup\n    # -------------------------------\n    cap.release()\n    writer.release()\n\n    # Final stats\n    if total_frames > 0:\n        avg_infer_fps = total_frames / total_infer_time if total_infer_time > 0 else 0.0\n    else:\n        avg_infer_fps = 0.0\n\n    final_map, per_class_ap = compute_proxy_map(class_confidences)\n\n    print(\"Processing complete.\")\n    print(f\"Input video: {INPUT_PATH}\")\n    print(f\"Output video: {OUTPUT_PATH}\")\n    print(f\"Frames processed: {total_frames}\")\n    print(f\"Average inference FPS: {avg_infer_fps:.2f}\")\n    print(f\"Final mAP (proxy): {final_map:.4f}\")\n    if per_class_ap:\n        # Print top few classes by AP for reference\n        sorted_items = sorted(per_class_ap.items(), key=lambda kv: kv[1], reverse=True)\n        print(\"Per-class AP (proxy) - top 10:\")\n        for cls_id, ap in sorted_items[:10]:\n            name = label_from_class_id(cls_id, labels)\n            print(f\"  {cls_id:3d} ({name}): {ap:.4f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/3ddffe40",
      "latency": 103.516,
      "total_cost": 0.074291,
      "observations": [
        {
          "id": "time-04-06-16-615760_chatcmpl-D5j328xg8OKwiRTMoLVVTXB7V1qCi",
          "traceId": "3ddffe40",
          "type": "GENERATION",
          "name": "3ddf_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:06:16.615000+00:00",
          "endTime": "2026-02-05T02:08:00.131000+00:00",
          "completionStartTime": "2026-02-05T02:08:00.131000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.07429100000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n# Application: Object Detection via a video file\n# Target Device: Raspberry Pi 4B\n# Description:\n#   - Loads a TFLite SSD-Mobilenet v1 model and label map.\n#   - Reads a single video file, performs object detection per frame.\n#   - Draws bounding boxes with labels on detected objects.\n#   - Computes and overlays a running mAP value (proxy; see note below) on the video.\n#   - Writes the annotated output to a video file.\n#\n# Note on mAP:\n#   Ground truth annotations are not provided for the input video. Therefore, classical mAP cannot be computed.\n#   This implementation uses a proxy: per-class AP is approximated as the mean confidence of detections for that class,\n#   and mAP is the mean of these per-class averages across all classes with at least one detection.\n\nimport os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter  # Phase 1.1: Required by guideline\n\n# -------------------------------\n# Phase 1: Setup\n# -------------------------------\n\n# Phase 1.2: Configuration Parameters (provided)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = float('0.5')  # Confidence threshold\n\ndef ensure_dir_for_file(file_path: str):\n    \"\"\"Ensure directory for a given file path exists.\"\"\"\n    dir_path = os.path.dirname(os.path.abspath(file_path))\n    if dir_path and not os.path.exists(dir_path):\n        os.makedirs(dir_path, exist_ok=True)\n\ndef load_labels(label_path: str):\n    \"\"\"Load labels from file into a list.\"\"\"\n    labels = []\n    try:\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line != '':\n                    labels.append(line)\n    except Exception as e:\n        print(f\"Warning: Failed to read labels from {label_path}: {e}\")\n    return labels\n\ndef find_output_indices(output_details):\n    \"\"\"\n    Identify indices for boxes, classes, scores, and num_detections tensors.\n    Typical SSD MobileNet v1 TFLite outputs:\n      - boxes: (1, N, 4)\n      - classes: (1, N)\n      - scores: (1, N)\n      - num_detections: (1)\n    \"\"\"\n    idx_boxes = idx_classes = idx_scores = idx_num = None\n    for i, det in enumerate(output_details):\n        shape = det.get('shape', None)\n        if shape is None:\n            continue\n        shape_tuple = tuple(shape)\n        # Boxes: shape like (1, N, 4)\n        if len(shape_tuple) == 3 and shape_tuple[-1] == 4:\n            idx_boxes = i\n        # Scores or Classes: (1, N)\n        elif len(shape_tuple) == 2 and shape_tuple[0] == 1:\n            # We cannot distinguish scores vs classes by shape alone; defer until we fetch data\n            # We'll still record potential candidates and resolve after first inference if needed.\n            pass\n        # Num detections: (1) or (1,1)\n        elif len(shape_tuple) in (1, 2) and np.prod(shape_tuple) == 1:\n            idx_num = i\n    # The conventional ordering for TFLite SSD is [boxes, classes, scores, num_detections].\n    # If we got boxes and num, infer rest by position if not explicitly determined later.\n    # We'll return the current best guess; will finalize after first inference if needed.\n    return idx_boxes, idx_classes, idx_scores, idx_num\n\ndef label_from_class_id(class_id: int, labels: list):\n    \"\"\"\n    Obtain label string for a class ID with robustness to indexing scheme.\n    Some models use 1-based class indices while label files might be 0-based and vice versa.\n    \"\"\"\n    name = str(class_id)\n    if labels:\n        # Try direct index\n        if 0 <= class_id < len(labels):\n            name = labels[class_id]\n        # Try 1-based to 0-based mapping\n        elif 0 <= (class_id - 1) < len(labels):\n            name = labels[class_id - 1]\n        else:\n            # Fallback to best effort within bounds\n            idx = max(0, min(class_id, len(labels) - 1))\n            name = labels[idx]\n    return name\n\ndef preprocess_frame(frame_bgr, input_details, floating_model):\n    \"\"\"\n    Phase 2.2 & 2.3: Preprocess BGR frame to model input tensor based on input_details.\n    - Resize to expected input size.\n    - Convert BGR to RGB.\n    - Adjust dtype; normalize if floating model: (x - 127.5)/127.5\n    \"\"\"\n    in_info = input_details[0]\n    in_shape = in_info['shape']  # Expected [1, height, width, 3]\n    height, width = int(in_shape[1]), int(in_shape[2])\n\n    # Resize and convert color\n    frame_resized = cv2.resize(frame_bgr, (width, height), interpolation=cv2.INTER_LINEAR)\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n\n    # Prepare input tensor\n    if floating_model:\n        input_data = (np.float32(frame_rgb) - 127.5) / 127.5\n    else:\n        # For uint8 models, keep 0..255\n        input_data = np.uint8(frame_rgb)\n\n    # Expand dims to [1, H, W, 3]\n    input_data = np.expand_dims(input_data, axis=0)\n    # Ensure dtype matches exactly\n    input_data = input_data.astype(in_info['dtype'])\n    return input_data\n\ndef scale_and_clip_box(box, frame_w, frame_h):\n    \"\"\"\n    Phase 4.3: Convert normalized ymin, xmin, ymax, xmax to pixel coords and clip to frame bounds.\n    \"\"\"\n    ymin, xmin, ymax, xmax = box\n    left = int(max(0, min(frame_w - 1, xmin * frame_w)))\n    right = int(max(0, min(frame_w - 1, xmax * frame_w)))\n    top = int(max(0, min(frame_h - 1, ymin * frame_h)))\n    bottom = int(max(0, min(frame_h - 1, ymax * frame_h)))\n    # Ensure proper ordering\n    left, right = min(left, right), max(left, right)\n    top, bottom = min(top, bottom), max(top, bottom)\n    return left, top, right, bottom\n\ndef draw_label_with_bg(img, text, origin, font=cv2.FONT_HERSHEY_SIMPLEX, font_scale=0.5, fg_color=(255,255,255), bg_color=(0,0,0), thickness=1, alpha=0.6, padding=2):\n    \"\"\"\n    Draws text with a filled rectangle background for readability.\n    \"\"\"\n    x, y = origin\n    (tw, th), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n    th_total = th + baseline\n    # Background rectangle\n    x2 = x + tw + 2 * padding\n    y2 = y + th_total + 2 * padding\n    overlay = img.copy()\n    cv2.rectangle(overlay, (x, y), (x2, y2), bg_color, thickness=-1)\n    cv2.addWeighted(overlay, alpha, img, 1 - alpha, 0, img)\n    # Put text\n    text_org = (x + padding, y + th + padding)\n    cv2.putText(img, text, text_org, font, font_scale, fg_color, thickness, cv2.LINE_AA)\n\ndef compute_proxy_map(class_conf_dict):\n    \"\"\"\n    Compute proxy mAP:\n      - For each class with >=1 detection, AP_class = mean(confidences of that class).\n      - mAP = mean(AP_class over classes with detections).\n    Returns mAP (float) and a dict of per-class AP.\n    \"\"\"\n    ap_values = []\n    per_class_ap = {}\n    for cls_id, confs in class_conf_dict.items():\n        if len(confs) > 0:\n            ap = float(np.mean(confs))\n            per_class_ap[cls_id] = ap\n            ap_values.append(ap)\n    if len(ap_values) == 0:\n        return 0.0, per_class_ap\n    return float(np.mean(ap_values)), per_class_ap\n\ndef main():\n    # Phase 1.3: Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Phase 1.4: Load Interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n\n    # Phase 1.5: Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    floating_model = (input_details[0]['dtype'] == np.float32)\n\n    # Determine output indices (best-effort pre-identification)\n    idx_boxes, idx_classes, idx_scores, idx_num = find_output_indices(output_details)\n\n    # Phase 2.1: Acquire Input Data (Open video file)\n    if not os.path.exists(INPUT_PATH):\n        print(f\"Error: Input video not found at {INPUT_PATH}\")\n        return\n\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: Failed to open input video: {INPUT_PATH}\")\n        return\n\n    # Prepare output writer\n    ensure_dir_for_file(OUTPUT_PATH)\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) or 0\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) or 0\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # default fallback\n\n    # Fallback for any invalid dimension\n    if frame_width <= 0 or frame_height <= 0:\n        # Try to read one frame to get size\n        ret_tmp, frame_tmp = cap.read()\n        if not ret_tmp:\n            print(\"Error: Unable to determine video frame size.\")\n            cap.release()\n            return\n        frame_height, frame_width = frame_tmp.shape[:2]\n        # Rewind capture\n        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n    if not writer.isOpened():\n        print(f\"Error: Failed to open output writer for {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # Data accumulators for proxy mAP\n    class_confidences = {}  # dict: class_id -> list of confidences\n    total_frames = 0\n    total_infer_time = 0.0\n\n    # -------------------------------\n    # Phase 2 & 3 & 4: Processing Loop\n    # -------------------------------\n    while True:\n        # Phase 2.1: Read next frame\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # Phase 4.5: Loop termination (end of video)\n\n        total_frames += 1\n        orig_h, orig_w = frame_bgr.shape[:2]\n\n        # Phase 2.2 & 2.3: Preprocess\n        input_data = preprocess_frame(frame_bgr, input_details, floating_model)\n\n        # Phase 3.1: Set input tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        # Phase 3.2: Inference\n        t0 = time.time()\n        interpreter.invoke()\n        infer_time = time.time() - t0\n        total_infer_time += infer_time\n\n        # Phase 4.1: Retrieve output tensors\n        # Fetch all outputs first\n        outputs = []\n        for od in output_details:\n            outputs.append(interpreter.get_tensor(od['index']))\n\n        # Resolve indices for classes and scores if unknown by analyzing shapes/dtypes\n        # Typical outputs: boxes (float32), classes (float32), scores (float32), num (float32)\n        # We'll detect by shape pattern and value ranges.\n        boxes = classes = scores = num_det = None\n\n        # Boxes\n        if idx_boxes is not None:\n            boxes = outputs[idx_boxes]\n        else:\n            for i, arr in enumerate(outputs):\n                if arr.ndim == 3 and arr.shape[-1] == 4 and arr.shape[0] == 1:\n                    boxes = arr\n                    idx_boxes = i\n                    break\n\n        # num_detections\n        if idx_num is not None:\n            num_det = outputs[idx_num]\n        else:\n            for i, arr in enumerate(outputs):\n                if np.prod(arr.shape) == 1:\n                    num_det = arr\n                    idx_num = i\n                    break\n\n        # The remaining two are classes and scores with shape (1, N)\n        candidate_indices = [i for i in range(len(outputs)) if i not in [idx_boxes, idx_num] and outputs[i].ndim == 2 and outputs[i].shape[0] == 1]\n        # Heuristic: scores are in [0,1], classes are integer-like IDs\n        for i in candidate_indices:\n            arr = outputs[i]\n            arr_flat = arr.flatten()\n            if arr.dtype == np.float32 or arr.dtype == np.float64:\n                # If values plausibly in [0,1]\n                if arr_flat.size > 0 and np.nanmin(arr_flat) >= 0.0 and np.nanmax(arr_flat) <= 1.0:\n                    scores = arr\n                else:\n                    classes = arr\n            else:\n                classes = arr\n        # If ambiguous, fall back to typical ordering: 0: boxes, 1: classes, 2: scores, 3: num\n        if classes is None or scores is None:\n            if len(outputs) >= 4:\n                boxes = boxes if boxes is not None else outputs[0]\n                classes = outputs[1]\n                scores = outputs[2]\n                num_det = num_det if num_det is not None else outputs[3]\n            else:\n                # As a last resort, try to assign by ranges\n                for i in candidate_indices:\n                    arr = outputs[i]\n                    if scores is None and (np.nanmax(arr) <= 1.0):\n                        scores = arr\n                    elif classes is None:\n                        classes = arr\n\n        # Squeeze outputs to remove batch dim\n        if boxes is not None:\n            boxes = np.squeeze(boxes, axis=0)\n        if classes is not None:\n            classes = np.squeeze(classes, axis=0)\n        if scores is not None:\n            scores = np.squeeze(scores, axis=0)\n        if num_det is not None:\n            num_val = int(np.squeeze(num_det).astype(np.int32))\n        else:\n            # If num_detections not provided, infer from boxes shape\n            num_val = boxes.shape[0] if boxes is not None else 0\n\n        # Safety checks\n        if boxes is None or classes is None or scores is None:\n            # Cannot process detections; write original frame and continue\n            writer.write(frame_bgr)\n            continue\n\n        # Phase 4.2 & 4.3: Interpret and Post-process detections\n        # Filter by confidence threshold, scale boxes to original frame size, clip, and draw\n        detections_drawn = 0\n        for i in range(num_val):\n            score = float(scores[i])\n            if score < CONF_THRESHOLD:\n                continue\n            cls_id_raw = classes[i]\n            # Convert class id to int\n            try:\n                cls_id = int(cls_id_raw)\n            except Exception:\n                cls_id = int(np.round(float(cls_id_raw)))\n\n            box = boxes[i]  # [ymin, xmin, ymax, xmax] normalized\n            left, top, right, bottom = scale_and_clip_box(box, orig_w, orig_h)\n\n            # Draw bounding box\n            cv2.rectangle(frame_bgr, (left, top), (right, bottom), (0, 255, 0), thickness=2)\n\n            # Prepare label text\n            label_text = label_from_class_id(cls_id, labels)\n            display_text = f\"{label_text}: {score:.2f}\"\n\n            # Draw label background and text\n            draw_label_with_bg(frame_bgr, display_text, (left, max(0, top - 20)), fg_color=(255,255,255), bg_color=(0,128,0))\n\n            # Accumulate confidences for proxy mAP\n            if cls_id not in class_confidences:\n                class_confidences[cls_id] = []\n            class_confidences[cls_id].append(score)\n            detections_drawn += 1\n\n        # Compute running proxy mAP and overlay\n        map_value, _ = compute_proxy_map(class_confidences)\n        map_text = f\"mAP: {map_value:.3f} | FPS(Inf): {1.0 / infer_time:.2f}\"\n        draw_label_with_bg(frame_bgr, map_text, (10, 10), fg_color=(255,255,255), bg_color=(0,0,0))\n\n        # Phase 4.4: Write annotated frame to output\n        writer.write(frame_bgr)\n\n    # -------------------------------\n    # Phase 5: Cleanup\n    # -------------------------------\n    cap.release()\n    writer.release()\n\n    # Final stats\n    if total_frames > 0:\n        avg_infer_fps = total_frames / total_infer_time if total_infer_time > 0 else 0.0\n    else:\n        avg_infer_fps = 0.0\n\n    final_map, per_class_ap = compute_proxy_map(class_confidences)\n\n    print(\"Processing complete.\")\n    print(f\"Input video: {INPUT_PATH}\")\n    print(f\"Output video: {OUTPUT_PATH}\")\n    print(f\"Frames processed: {total_frames}\")\n    print(f\"Average inference FPS: {avg_infer_fps:.2f}\")\n    print(f\"Final mAP (proxy): {final_map:.4f}\")\n    if per_class_ap:\n        # Print top few classes by AP for reference\n        sorted_items = sorted(per_class_ap.items(), key=lambda kv: kv[1], reverse=True)\n        print(\"Per-class AP (proxy) - top 10:\")\n        for cls_id, ap in sorted_items[:10]:\n            name = label_from_class_id(cls_id, labels)\n            print(f\"  {cls_id:3d} ({name}): {ap:.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 7400,
            "total": 8900,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.074291,
          "latency": 103516.0,
          "timeToFirstToken": 103.516,
          "updatedAt": "2026-02-05T02:08:05.826Z",
          "totalTokens": 8900,
          "costDetails": {
            "total": 0.074291
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 7400,
          "usageDetails": {
            "input": 1500,
            "output": 7400,
            "total": 8900
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:08:00.762Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:08:51.284Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:06:16.568Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "5e54134f",
      "timestamp": "2026-02-05T02:03:53.159000+00:00",
      "name": "5e54_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n# Application: Object Detection via a video file\n# Target Device: Raspberry Pi 4B\n#\n# This script performs object detection on a single video file using a TFLite SSD model,\n# draws bounding boxes and labels, and writes an annotated output video. It also computes\n# a running \"mAP (proxy)\" metric (mean of per-class average confidences) since no ground\n# truth annotations are provided.\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: TFLite interpreter (as requested by guideline)\nfrom ai_edge_litert.interpreter import Interpreter  # noqa: E402\n\n\ndef load_labels(label_path):\n    \"\"\"\n    Load labels from a text file; each line corresponds to a label.\n    Returns a list of strings. Handles empty lines gracefully.\n    \"\"\"\n    labels = []\n    try:\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                lbl = line.strip()\n                if lbl != '':\n                    labels.append(lbl)\n    except Exception as e:\n        print(f\"[WARN] Failed to load labels from {label_path}: {e}\")\n    return labels\n\n\ndef map_class_to_label(class_id, labels):\n    \"\"\"\n    Map class index to human-readable label.\n    If label list contains a background/??? entry as first element, use offset = 1.\n    \"\"\"\n    if labels is None or len(labels) == 0:\n        return f\"id_{int(class_id)}\"\n    offset = 1 if labels[0].strip().lower() in ('???', 'background') else 0\n    idx = int(class_id) + offset\n    if 0 <= idx < len(labels):\n        return labels[idx]\n    # Fallback when index is out of range\n    return f\"id_{int(class_id)}\"\n\n\ndef ensure_dir(path):\n    \"\"\"\n    Ensure the directory for a given file path exists.\n    \"\"\"\n    directory = os.path.dirname(os.path.abspath(path))\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n\ndef parse_ssd_outputs(interpreter, output_details):\n    \"\"\"\n    Retrieve and parse SSD-style TFLite outputs:\n    - boxes: shape [1, num, 4], normalized [ymin, xmin, ymax, xmax]\n    - classes: shape [1, num]\n    - scores: shape [1, num]\n    - num_detections: shape [1]\n    Returns (boxes, classes, scores, num) as numpy arrays (1D for classes/scores) and int for num.\n    Robust mapping to handle varied tensor names and dtypes.\n    \"\"\"\n    outputs = []\n    for od in output_details:\n        outputs.append(interpreter.get_tensor(od['index']))\n\n    boxes, classes, scores, num = None, None, None, None\n\n    # First try by name hints\n    for arr, od in zip(outputs, output_details):\n        name = str(od.get('name', '')).lower()\n        if 'box' in name and boxes is None:\n            boxes = arr\n        elif 'score' in name and scores is None:\n            scores = arr\n        elif 'class' in name and classes is None:\n            classes = arr\n        elif 'num' in name and num is None:\n            num = arr\n\n    # Fallback by shape/type if any are missing\n    if boxes is None:\n        for arr in outputs:\n            if arr.ndim == 3 and arr.shape[-1] == 4 and arr.shape[0] == 1:\n                boxes = arr\n                break\n\n    # For classes and scores (both [1, N])\n    if scores is None or classes is None:\n        candidates = [arr for arr in outputs if arr.ndim == 2 and arr.shape[0] == 1]\n        # Prefer float dtype for scores\n        for arr in candidates:\n            if scores is None and arr.dtype in (np.float32, np.float16):\n                scores = arr\n        # The other one becomes classes\n        for arr in candidates:\n            if not np.shares_memory(arr, scores) and classes is None:\n                classes = arr\n\n    if num is None:\n        # Find a scalar output\n        for arr in outputs:\n            if arr.size == 1:\n                num = arr\n                break\n\n    # Final shaping\n    if boxes is None:\n        raise RuntimeError(\"Could not find 'boxes' output from the model.\")\n    if classes is None:\n        raise RuntimeError(\"Could not find 'classes' output from the model.\")\n    if scores is None:\n        raise RuntimeError(\"Could not find 'scores' output from the model.\")\n\n    # Flatten to 1D for classes/scores; boxes -> [N,4]\n    boxes = np.squeeze(boxes, axis=0) if boxes.ndim == 3 else boxes\n    classes = np.squeeze(classes, axis=0) if classes.ndim == 2 else classes\n    scores = np.squeeze(scores, axis=0) if scores.ndim == 2 else scores\n\n    if num is None:\n        num_dets = min(len(scores), len(classes), boxes.shape[0])\n    else:\n        try:\n            num_val = int(np.squeeze(num).astype(np.int32))\n            num_dets = min(num_val, len(scores), len(classes), boxes.shape[0])\n        except Exception:\n            num_dets = min(len(scores), len(classes), boxes.shape[0])\n\n    return boxes[:num_dets], classes[:num_dets], scores[:num_dets], num_dets\n\n\ndef clip_bbox(xmin, ymin, xmax, ymax, width, height):\n    \"\"\"\n    Clip bounding box coordinates to image boundaries and convert to ints.\n    \"\"\"\n    xmin = int(max(0, min(width - 1, xmin)))\n    ymin = int(max(0, min(height - 1, ymin)))\n    xmax = int(max(0, min(width - 1, xmax)))\n    ymax = int(max(0, min(height - 1, ymax)))\n    return xmin, ymin, xmax, ymax\n\n\ndef compute_map_proxy(per_class_confidences):\n    \"\"\"\n    Compute a proxy for mean Average Precision (mAP) without ground truth:\n    For each class: AP_proxy = mean(confidences for that class).\n    mAP_proxy = mean(AP_proxy over classes that have at least one detection).\n    Returns float in [0, 1] range if confidences are in [0,1]. If no detections yet -> 0.0.\n    \"\"\"\n    if not per_class_confidences:\n        return 0.0\n    ap_values = []\n    for cid, confs in per_class_confidences.items():\n        if confs:\n            ap_values.append(float(np.mean(confs)))\n    if not ap_values:\n        return 0.0\n    return float(np.mean(ap_values))\n\n\ndef main():\n    # 1.2 Paths/Parameters from configuration\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = 0.5  # float\n\n    # Ensure output directory exists\n    ensure_dir(output_path)\n\n    # 1.3 Load labels\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    if len(input_details) < 1:\n        raise RuntimeError(\"Model has no input tensor.\")\n    input_shape = input_details[0]['shape']  # Expect [1, height, width, 3]\n    in_h, in_w = int(input_shape[1]), int(input_shape[2])\n    in_dtype = input_details[0]['dtype']\n    floating_model = (in_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop (Read a single video file)\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise FileNotFoundError(f\"Unable to open input video: {input_path}\")\n\n    src_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    src_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # fallback to 30 FPS if metadata is missing\n\n    # Prepare VideoWriter\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out_writer = cv2.VideoWriter(output_path, fourcc, fps, (src_w, src_h))\n    if not out_writer.isOpened():\n        raise RuntimeError(f\"Unable to open VideoWriter for: {output_path}\")\n\n    # Metrics containers\n    per_class_confidences = {}  # dict: class_id -> list of confidences\n    total_frames = 0\n    total_inference_time = 0.0\n    total_detections = 0\n\n    # Colors for drawing (simple palette)\n    rng = np.random.default_rng(12345)\n    color_cache = {}\n\n    def get_color_for_class(cid):\n        if cid not in color_cache:\n            # Generate a stable color for each class id\n            color_cache[cid] = tuple(int(c) for c in rng.integers(0, 255, size=3))\n        return color_cache[cid]\n\n    try:\n        while True:\n            ret, frame_bgr = cap.read()\n            if not ret:\n                break  # End of video\n            total_frames += 1\n\n            # 2.2 Preprocess to model input size and dtype\n            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n            resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n            input_data = np.expand_dims(resized, axis=0)\n\n            # 2.3 Quantization Handling: normalize for floating models\n            if floating_model:\n                input_data = (np.float32(input_data) - 127.5) / 127.5\n            else:\n                input_data = np.uint8(input_data)\n\n            # Phase 3: Inference\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n            t0 = time.time()\n            interpreter.invoke()\n            inf_time = time.time() - t0\n            total_inference_time += inf_time\n\n            # Phase 4: Output Interpretation & Handling\n            # 4.1 Get Output Tensors\n            boxes, classes, scores, num_dets = parse_ssd_outputs(interpreter, output_details)\n\n            # 4.2 Interpret Results: Scale boxes and map labels\n            # 4.3 Post-processing: thresholding and clipping\n            drawn_count = 0\n            for i in range(num_dets):\n                score = float(scores[i])\n                if score < confidence_threshold:\n                    continue\n\n                class_id = int(classes[i])\n                label_name = map_class_to_label(class_id, labels)\n\n                ymin, xmin, ymax, xmax = boxes[i]\n                # Scale normalized coords to original frame size\n                x_min = int(xmin * src_w)\n                y_min = int(ymin * src_h)\n                x_max = int(xmax * src_w)\n                y_max = int(ymax * src_h)\n                x_min, y_min, x_max, y_max = clip_bbox(x_min, y_min, x_max, y_max, src_w, src_h)\n\n                # Draw rectangle and label on BGR frame\n                color = get_color_for_class(class_id)\n                cv2.rectangle(frame_bgr, (x_min, y_min), (x_max, y_max), color, thickness=2)\n\n                label_text = f\"{label_name}: {score:.2f}\"\n                # Compute box for text label\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                font_scale = max(0.5, min(src_w, src_h) / 800.0)\n                thickness = 1\n                (text_w, text_h), baseline = cv2.getTextSize(label_text, font, font_scale, thickness)\n                # Draw filled rectangle behind text for readability\n                text_bg_xmin = x_min\n                text_bg_ymin = max(0, y_min - text_h - baseline)\n                text_bg_xmax = x_min + text_w + 2\n                text_bg_ymax = y_min\n                cv2.rectangle(frame_bgr, (text_bg_xmin, text_bg_ymin), (text_bg_xmax, text_bg_ymax), color, thickness=-1)\n                # Put the text over it\n                cv2.putText(frame_bgr, label_text, (x_min + 1, y_min - baseline - 1), font, font_scale, (255, 255, 255), thickness, cv2.LINE_AA)\n\n                # Update metrics\n                if class_id not in per_class_confidences:\n                    per_class_confidences[class_id] = []\n                per_class_confidences[class_id].append(score)\n                total_detections += 1\n                drawn_count += 1\n\n            # Compute and overlay mAP (proxy) on the frame\n            map_proxy = compute_map_proxy(per_class_confidences)\n            map_text = f\"mAP (proxy): {map_proxy:.3f} | Detections (frame): {drawn_count} | FPS: {1.0/inf_time if inf_time>0 else 0.0:.1f}\"\n            cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 255, 10), 2, cv2.LINE_AA)\n\n            # 4.4 Handle Output: write annotated frame to output video\n            out_writer.write(frame_bgr)\n\n            # 4.5 Loop continuation: continue until input video ends\n\n    finally:\n        # Phase 5: Cleanup\n        cap.release()\n        out_writer.release()\n\n    # Print summary\n    avg_fps = total_frames / total_inference_time if total_inference_time > 0 else 0.0\n    final_map_proxy = compute_map_proxy(per_class_confidences)\n    print(\"Processing complete.\")\n    print(f\"Input video: {input_path}\")\n    print(f\"Output video: {output_path}\")\n    print(f\"Frames processed: {total_frames}\")\n    print(f\"Total detections (>= {confidence_threshold}): {total_detections}\")\n    print(f\"Average FPS (inference only): {avg_fps:.2f}\")\n    print(f\"Final mAP (proxy): {final_map_proxy:.3f}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/5e54134f",
      "latency": 85.933,
      "total_cost": 0.068261,
      "observations": [
        {
          "id": "time-04-03-53-785960_chatcmpl-D5j0kYaW03aQrOxw9TYfnBGRRhXfo",
          "traceId": "5e54134f",
          "type": "GENERATION",
          "name": "5e54_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:03:53.785000+00:00",
          "endTime": "2026-02-05T02:05:19.718000+00:00",
          "completionStartTime": "2026-02-05T02:05:19.718000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.068261,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n# Application: Object Detection via a video file\n# Target Device: Raspberry Pi 4B\n#\n# This script performs object detection on a single video file using a TFLite SSD model,\n# draws bounding boxes and labels, and writes an annotated output video. It also computes\n# a running \"mAP (proxy)\" metric (mean of per-class average confidences) since no ground\n# truth annotations are provided.\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: TFLite interpreter (as requested by guideline)\nfrom ai_edge_litert.interpreter import Interpreter  # noqa: E402\n\n\ndef load_labels(label_path):\n    \"\"\"\n    Load labels from a text file; each line corresponds to a label.\n    Returns a list of strings. Handles empty lines gracefully.\n    \"\"\"\n    labels = []\n    try:\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                lbl = line.strip()\n                if lbl != '':\n                    labels.append(lbl)\n    except Exception as e:\n        print(f\"[WARN] Failed to load labels from {label_path}: {e}\")\n    return labels\n\n\ndef map_class_to_label(class_id, labels):\n    \"\"\"\n    Map class index to human-readable label.\n    If label list contains a background/??? entry as first element, use offset = 1.\n    \"\"\"\n    if labels is None or len(labels) == 0:\n        return f\"id_{int(class_id)}\"\n    offset = 1 if labels[0].strip().lower() in ('???', 'background') else 0\n    idx = int(class_id) + offset\n    if 0 <= idx < len(labels):\n        return labels[idx]\n    # Fallback when index is out of range\n    return f\"id_{int(class_id)}\"\n\n\ndef ensure_dir(path):\n    \"\"\"\n    Ensure the directory for a given file path exists.\n    \"\"\"\n    directory = os.path.dirname(os.path.abspath(path))\n    if directory and not os.path.exists(directory):\n        os.makedirs(directory, exist_ok=True)\n\n\ndef parse_ssd_outputs(interpreter, output_details):\n    \"\"\"\n    Retrieve and parse SSD-style TFLite outputs:\n    - boxes: shape [1, num, 4], normalized [ymin, xmin, ymax, xmax]\n    - classes: shape [1, num]\n    - scores: shape [1, num]\n    - num_detections: shape [1]\n    Returns (boxes, classes, scores, num) as numpy arrays (1D for classes/scores) and int for num.\n    Robust mapping to handle varied tensor names and dtypes.\n    \"\"\"\n    outputs = []\n    for od in output_details:\n        outputs.append(interpreter.get_tensor(od['index']))\n\n    boxes, classes, scores, num = None, None, None, None\n\n    # First try by name hints\n    for arr, od in zip(outputs, output_details):\n        name = str(od.get('name', '')).lower()\n        if 'box' in name and boxes is None:\n            boxes = arr\n        elif 'score' in name and scores is None:\n            scores = arr\n        elif 'class' in name and classes is None:\n            classes = arr\n        elif 'num' in name and num is None:\n            num = arr\n\n    # Fallback by shape/type if any are missing\n    if boxes is None:\n        for arr in outputs:\n            if arr.ndim == 3 and arr.shape[-1] == 4 and arr.shape[0] == 1:\n                boxes = arr\n                break\n\n    # For classes and scores (both [1, N])\n    if scores is None or classes is None:\n        candidates = [arr for arr in outputs if arr.ndim == 2 and arr.shape[0] == 1]\n        # Prefer float dtype for scores\n        for arr in candidates:\n            if scores is None and arr.dtype in (np.float32, np.float16):\n                scores = arr\n        # The other one becomes classes\n        for arr in candidates:\n            if not np.shares_memory(arr, scores) and classes is None:\n                classes = arr\n\n    if num is None:\n        # Find a scalar output\n        for arr in outputs:\n            if arr.size == 1:\n                num = arr\n                break\n\n    # Final shaping\n    if boxes is None:\n        raise RuntimeError(\"Could not find 'boxes' output from the model.\")\n    if classes is None:\n        raise RuntimeError(\"Could not find 'classes' output from the model.\")\n    if scores is None:\n        raise RuntimeError(\"Could not find 'scores' output from the model.\")\n\n    # Flatten to 1D for classes/scores; boxes -> [N,4]\n    boxes = np.squeeze(boxes, axis=0) if boxes.ndim == 3 else boxes\n    classes = np.squeeze(classes, axis=0) if classes.ndim == 2 else classes\n    scores = np.squeeze(scores, axis=0) if scores.ndim == 2 else scores\n\n    if num is None:\n        num_dets = min(len(scores), len(classes), boxes.shape[0])\n    else:\n        try:\n            num_val = int(np.squeeze(num).astype(np.int32))\n            num_dets = min(num_val, len(scores), len(classes), boxes.shape[0])\n        except Exception:\n            num_dets = min(len(scores), len(classes), boxes.shape[0])\n\n    return boxes[:num_dets], classes[:num_dets], scores[:num_dets], num_dets\n\n\ndef clip_bbox(xmin, ymin, xmax, ymax, width, height):\n    \"\"\"\n    Clip bounding box coordinates to image boundaries and convert to ints.\n    \"\"\"\n    xmin = int(max(0, min(width - 1, xmin)))\n    ymin = int(max(0, min(height - 1, ymin)))\n    xmax = int(max(0, min(width - 1, xmax)))\n    ymax = int(max(0, min(height - 1, ymax)))\n    return xmin, ymin, xmax, ymax\n\n\ndef compute_map_proxy(per_class_confidences):\n    \"\"\"\n    Compute a proxy for mean Average Precision (mAP) without ground truth:\n    For each class: AP_proxy = mean(confidences for that class).\n    mAP_proxy = mean(AP_proxy over classes that have at least one detection).\n    Returns float in [0, 1] range if confidences are in [0,1]. If no detections yet -> 0.0.\n    \"\"\"\n    if not per_class_confidences:\n        return 0.0\n    ap_values = []\n    for cid, confs in per_class_confidences.items():\n        if confs:\n            ap_values.append(float(np.mean(confs)))\n    if not ap_values:\n        return 0.0\n    return float(np.mean(ap_values))\n\n\ndef main():\n    # 1.2 Paths/Parameters from configuration\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    confidence_threshold = 0.5  # float\n\n    # Ensure output directory exists\n    ensure_dir(output_path)\n\n    # 1.3 Load labels\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    if len(input_details) < 1:\n        raise RuntimeError(\"Model has no input tensor.\")\n    input_shape = input_details[0]['shape']  # Expect [1, height, width, 3]\n    in_h, in_w = int(input_shape[1]), int(input_shape[2])\n    in_dtype = input_details[0]['dtype']\n    floating_model = (in_dtype == np.float32)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop (Read a single video file)\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise FileNotFoundError(f\"Unable to open input video: {input_path}\")\n\n    src_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    src_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # fallback to 30 FPS if metadata is missing\n\n    # Prepare VideoWriter\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out_writer = cv2.VideoWriter(output_path, fourcc, fps, (src_w, src_h))\n    if not out_writer.isOpened():\n        raise RuntimeError(f\"Unable to open VideoWriter for: {output_path}\")\n\n    # Metrics containers\n    per_class_confidences = {}  # dict: class_id -> list of confidences\n    total_frames = 0\n    total_inference_time = 0.0\n    total_detections = 0\n\n    # Colors for drawing (simple palette)\n    rng = np.random.default_rng(12345)\n    color_cache = {}\n\n    def get_color_for_class(cid):\n        if cid not in color_cache:\n            # Generate a stable color for each class id\n            color_cache[cid] = tuple(int(c) for c in rng.integers(0, 255, size=3))\n        return color_cache[cid]\n\n    try:\n        while True:\n            ret, frame_bgr = cap.read()\n            if not ret:\n                break  # End of video\n            total_frames += 1\n\n            # 2.2 Preprocess to model input size and dtype\n            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n            resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n            input_data = np.expand_dims(resized, axis=0)\n\n            # 2.3 Quantization Handling: normalize for floating models\n            if floating_model:\n                input_data = (np.float32(input_data) - 127.5) / 127.5\n            else:\n                input_data = np.uint8(input_data)\n\n            # Phase 3: Inference\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n            t0 = time.time()\n            interpreter.invoke()\n            inf_time = time.time() - t0\n            total_inference_time += inf_time\n\n            # Phase 4: Output Interpretation & Handling\n            # 4.1 Get Output Tensors\n            boxes, classes, scores, num_dets = parse_ssd_outputs(interpreter, output_details)\n\n            # 4.2 Interpret Results: Scale boxes and map labels\n            # 4.3 Post-processing: thresholding and clipping\n            drawn_count = 0\n            for i in range(num_dets):\n                score = float(scores[i])\n                if score < confidence_threshold:\n                    continue\n\n                class_id = int(classes[i])\n                label_name = map_class_to_label(class_id, labels)\n\n                ymin, xmin, ymax, xmax = boxes[i]\n                # Scale normalized coords to original frame size\n                x_min = int(xmin * src_w)\n                y_min = int(ymin * src_h)\n                x_max = int(xmax * src_w)\n                y_max = int(ymax * src_h)\n                x_min, y_min, x_max, y_max = clip_bbox(x_min, y_min, x_max, y_max, src_w, src_h)\n\n                # Draw rectangle and label on BGR frame\n                color = get_color_for_class(class_id)\n                cv2.rectangle(frame_bgr, (x_min, y_min), (x_max, y_max), color, thickness=2)\n\n                label_text = f\"{label_name}: {score:.2f}\"\n                # Compute box for text label\n                font = cv2.FONT_HERSHEY_SIMPLEX\n                font_scale = max(0.5, min(src_w, src_h) / 800.0)\n                thickness = 1\n                (text_w, text_h), baseline = cv2.getTextSize(label_text, font, font_scale, thickness)\n                # Draw filled rectangle behind text for readability\n                text_bg_xmin = x_min\n                text_bg_ymin = max(0, y_min - text_h - baseline)\n                text_bg_xmax = x_min + text_w + 2\n                text_bg_ymax = y_min\n                cv2.rectangle(frame_bgr, (text_bg_xmin, text_bg_ymin), (text_bg_xmax, text_bg_ymax), color, thickness=-1)\n                # Put the text over it\n                cv2.putText(frame_bgr, label_text, (x_min + 1, y_min - baseline - 1), font, font_scale, (255, 255, 255), thickness, cv2.LINE_AA)\n\n                # Update metrics\n                if class_id not in per_class_confidences:\n                    per_class_confidences[class_id] = []\n                per_class_confidences[class_id].append(score)\n                total_detections += 1\n                drawn_count += 1\n\n            # Compute and overlay mAP (proxy) on the frame\n            map_proxy = compute_map_proxy(per_class_confidences)\n            map_text = f\"mAP (proxy): {map_proxy:.3f} | Detections (frame): {drawn_count} | FPS: {1.0/inf_time if inf_time>0 else 0.0:.1f}\"\n            cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 255, 10), 2, cv2.LINE_AA)\n\n            # 4.4 Handle Output: write annotated frame to output video\n            out_writer.write(frame_bgr)\n\n            # 4.5 Loop continuation: continue until input video ends\n\n    finally:\n        # Phase 5: Cleanup\n        cap.release()\n        out_writer.release()\n\n    # Print summary\n    avg_fps = total_frames / total_inference_time if total_inference_time > 0 else 0.0\n    final_map_proxy = compute_map_proxy(per_class_confidences)\n    print(\"Processing complete.\")\n    print(f\"Input video: {input_path}\")\n    print(f\"Output video: {output_path}\")\n    print(f\"Frames processed: {total_frames}\")\n    print(f\"Total detections (>= {confidence_threshold}): {total_detections}\")\n    print(f\"Average FPS (inference only): {avg_fps:.2f}\")\n    print(f\"Final mAP (proxy): {final_map_proxy:.3f}\")\n\n\nif __name__ == \"__main__\":\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 6797,
            "total": 8297,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.068261,
          "latency": 85933.0,
          "timeToFirstToken": 85.933,
          "updatedAt": "2026-02-05T02:05:25.137Z",
          "totalTokens": 8297,
          "costDetails": {
            "total": 0.068261
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 6797,
          "usageDetails": {
            "input": 1500,
            "output": 6797,
            "total": 8297
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:05:20.101Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:06:10.390Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:03:53.744Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "06decec2",
      "timestamp": "2026-02-05T02:00:22.369000+00:00",
      "name": "06de_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.1 Imports: Interpreter from ai_edge_litert per guideline\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from configuration)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')  # Confidence Threshold\nPSEUDO_GT_THRESHOLD = 0.75     # Threshold to build pseudo ground-truth via high-confidence detections\nIOU_THRESHOLD = 0.5            # IoU threshold for matching during AP/mAP calculation\nNMS_IOU_THRESHOLD = 0.5        # IoU threshold for NMS when creating pseudo GT\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels\ndef load_labels(label_path):\n    labels = []\n    if os.path.exists(label_path):\n        with open(label_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    else:\n        # Fallback minimal labels (first 10 from provided useful info)\n        labels = [\n            'person','bicycle','car','motorcycle','airplane','bus','train','truck','boat','traffic light'\n        ]\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Store input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\n# Expect shape [1, height, width, 3]\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    raise RuntimeError(f'Unexpected input tensor shape: {input_shape}. Expected [1, H, W, 3].')\n\ninput_height = int(input_shape[1])\ninput_width = int(input_shape[2])\nfloating_model = (input_dtype == np.float32)\n\n# =========================\n# Utility functions\n# =========================\n\ndef bgr_to_rgb(image):\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\ndef normalize_input(img):\n    # Normalize to [-1, 1] if floating model\n    return (np.float32(img) - 127.5) / 127.5\n\ndef clip_bbox(xmin, ymin, xmax, ymax, w, h):\n    xmin = max(0, min(xmin, w - 1))\n    ymin = max(0, min(ymin, h - 1))\n    xmax = max(0, min(xmax, w - 1))\n    ymax = max(0, min(ymax, h - 1))\n    return xmin, ymin, xmax, ymax\n\ndef safe_label_name(class_id, labels_list):\n    # Try 0-based first\n    idx = int(class_id)\n    if 0 <= idx < len(labels_list):\n        return labels_list[idx]\n    # Then try 1-based (common in TF OD models)\n    idx = int(class_id) - 1\n    if 0 <= idx < len(labels_list):\n        return labels_list[idx]\n    return f'class_{int(class_id)}'\n\ndef color_for_class(class_id):\n    # Deterministic pseudo-random color for class id\n    rng = np.random.RandomState(int(class_id) + 12345)\n    color = rng.randint(0, 255, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef iou(boxA, boxB):\n    # boxes: (xmin, ymin, xmax, ymax)\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interW = max(0, xB - xA + 1)\n    interH = max(0, yB - yA + 1)\n    interArea = interW * interH\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    denom = float(boxAArea + boxBArea - interArea)\n    if denom <= 0:\n        return 0.0\n    return interArea / denom\n\ndef nms_boxes(boxes, scores, iou_thresh=0.5):\n    # Non-Maximum Suppression, returns indices of kept boxes\n    if len(boxes) == 0:\n        return []\n\n    boxes = np.array(boxes)\n    scores = np.array(scores)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        iou_vals = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n\n        inds = np.where(iou_vals <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_ap(recall, precision):\n    # Compute AP using precision envelope method\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([0.0], precision, [0.0]))\n\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # Integrate area under PR curve\n    indices = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[indices + 1] - mrec[indices]) * mpre[indices + 1])\n    return ap\n\ndef compute_map(predictions_by_class, gt_by_class, iou_thresh=0.5):\n    # predictions_by_class: {class_id: [{'frame': int, 'box':(xmin,ymin,xmax,ymax), 'score': float}, ...]}\n    # gt_by_class: {class_id: {frame_id: [ (xmin,ymin,xmax,ymax), ... ]}}\n    ap_list = []\n    for cls_id in sorted(gt_by_class.keys()):\n        # Total GT count for this class\n        frame_to_gts = gt_by_class[cls_id]\n        total_gts = sum(len(bxs) for bxs in frame_to_gts.values())\n        if total_gts == 0:\n            continue\n\n        preds = predictions_by_class.get(cls_id, [])\n        if len(preds) == 0:\n            ap_list.append(0.0)\n            continue\n\n        # Sort predictions by score desc\n        preds_sorted = sorted(preds, key=lambda d: d['score'], reverse=True)\n\n        # Prepare matched flags per frame GTs\n        matched = {f: np.zeros(len(frame_to_gts[f]), dtype=bool) for f in frame_to_gts.keys()}\n\n        tp = np.zeros(len(preds_sorted))\n        fp = np.zeros(len(preds_sorted))\n\n        for i, p in enumerate(preds_sorted):\n            f = p['frame']\n            pbox = p['box']\n            gts = frame_to_gts.get(f, [])\n            best_iou = 0.0\n            best_j = -1\n            if len(gts) > 0:\n                for j, gtbox in enumerate(gts):\n                    iouv = iou(pbox, gtbox)\n                    if iouv > best_iou:\n                        best_iou = iouv\n                        best_j = j\n\n            if best_iou >= iou_thresh and best_j >= 0 and not matched[f][best_j]:\n                tp[i] = 1\n                matched[f][best_j] = True\n            else:\n                fp[i] = 1\n\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        recall = cum_tp / (total_gts + 1e-9)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        ap = compute_ap(recall, precision)\n        ap_list.append(ap)\n\n    if len(ap_list) == 0:\n        return 0.0\n    return float(np.mean(ap_list))\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video file: {INPUT_PATH}\")\n\n# Get video properties\norig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif not fps or fps <= 0 or np.isnan(fps):\n    fps = 25.0  # Fallback FPS\n\n# Prepare video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_width, orig_height))\nif not writer.isOpened():\n    cap.release()\n    raise RuntimeError(f\"Failed to open output video file for writing: {OUTPUT_PATH}\")\n\n# Storage for mAP computation across frames\npredictions_by_class = {}  # {class_id: [ {'frame': int, 'box': (xmin,ymin,xmax,ymax), 'score': float}, ... ]}\ngt_by_class = {}           # {class_id: {frame_id: [ (xmin,ymin,xmax,ymax), ... ]}}\n\nframe_index = 0\nstart_time = time.time()\n\n# =========================\n# Main processing loop\n# =========================\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    frame_index += 1\n    frame_h, frame_w = frame.shape[:2]\n\n    # 2.2 Preprocess Data\n    resized = cv2.resize(frame, (input_width, input_height))\n    rgb = bgr_to_rgb(resized)\n    input_data = np.expand_dims(rgb, axis=0)\n\n    # 2.3 Quantization Handling\n    if floating_model:\n        input_data = normalize_input(input_data)\n    else:\n        input_data = input_data.astype(input_dtype)\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling\n    # =========================\n\n    # 4.1 Get Output Tensors\n    # Typical TFLite SSD outputs: boxes [1, N, 4], classes [1, N], scores [1, N], num_detections [1]\n    boxes = None\n    classes_out = None\n    scores = None\n    num_dets = None\n\n    # Retrieve tensors and infer which is which by shape\n    for od in output_details:\n        out = interpreter.get_tensor(od['index'])\n        shp = out.shape\n        if len(shp) == 3 and shp[-1] == 4:\n            boxes = np.squeeze(out, axis=0)\n        elif len(shp) == 2 and shp[0] == 1 and shp[1] >= 1 and out.dtype in (np.float32, np.int64, np.int32):\n            # Could be classes or scores; differentiate by dtype and value range after we read others\n            # We'll temporarily store and decide later\n            if classes_out is None:\n                classes_out = np.squeeze(out, axis=0)\n            else:\n                # Decide which is score by checking range\n                if scores is None:\n                    scores = np.squeeze(out, axis=0)\n                else:\n                    # If both already set, assign to num_dets\n                    num_dets = int(np.squeeze(out))\n        elif len(shp) == 2 and shp[0] == 1 and shp[1] == 4:\n            # Some models may return [1, 4] boxes - unlikely for SSD\n            boxes = out[0]\n        elif len(shp) == 1 and shp[0] == 1:\n            num_dets = int(np.squeeze(out))\n        else:\n            # Attempt to categorize by content\n            if shp[0] == 1 and shp[-1] == 4:\n                boxes = np.squeeze(out, axis=0)\n\n    # If ambiguous mapping, attempt to swap if necessary\n    # Ensure scores in [0,1]\n    if scores is None or np.max(scores) > 1.0 or np.min(scores) < 0.0:\n        # Try to identify which of classes_out/scores is actually scores\n        # If classes are integer-like, choose other as scores\n        cand1 = classes_out\n        # Find another 1D output if available\n        for od in output_details:\n            arr = interpreter.get_tensor(od['index'])\n            arr_s = np.squeeze(arr)\n            if arr_s.ndim == 1 and arr_s.shape[0] == cand1.shape[0] and arr_s is not cand1:\n                # Choose arr_s as scores if it looks like probabilities\n                if np.max(arr_s) <= 1.0 and np.min(arr_s) >= 0.0:\n                    scores = arr_s\n                    break\n        # If still None, fallback to zeros to avoid crash\n        if scores is None:\n            scores = np.zeros_like(classes_out, dtype=np.float32)\n\n    # Ensure classes are integer array\n    if classes_out is not None and classes_out.dtype != np.int32 and classes_out.dtype != np.int64:\n        # Round to nearest int for class ids\n        classes_int = np.rint(classes_out).astype(np.int32)\n    else:\n        classes_int = classes_out.astype(np.int32) if classes_out is not None else np.zeros_like(scores, dtype=np.int32)\n\n    # Ensure boxes exist\n    if boxes is None:\n        # Fallback: create zero boxes to avoid crash\n        boxes = np.zeros((scores.shape[0], 4), dtype=np.float32)\n\n    # If num_dets is provided, limit arrays to actual detections\n    if num_dets is not None and num_dets > 0 and num_dets <= boxes.shape[0]:\n        boxes = boxes[:num_dets]\n        classes_int = classes_int[:num_dets]\n        scores = scores[:num_dets]\n\n    # 4.2 Interpret Results: convert to pixel coords, map labels\n    detections = []  # Per-frame detections used for drawing and predictions collection\n    # boxes are expected normalized [ymin, xmin, ymax, xmax]\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score <= 0.0:\n            continue\n        # Extract bbox\n        bymin, bxmin, bymax, bxmax = boxes[i]\n        # Clip normalized coords\n        bymin = float(np.clip(bymin, 0.0, 1.0))\n        bxmin = float(np.clip(bxmin, 0.0, 1.0))\n        bymax = float(np.clip(bymax, 0.0, 1.0))\n        bxmax = float(np.clip(bxmax, 0.0, 1.0))\n        # Scale to pixel coords\n        xmin = int(bxmin * frame_w)\n        ymin = int(bymin * frame_h)\n        xmax = int(bxmax * frame_w)\n        ymax = int(bymax * frame_h)\n        xmin, ymin, xmax, ymax = clip_bbox(xmin, ymin, xmax, ymax, frame_w, frame_h)\n        if xmax <= xmin or ymax <= ymin:\n            continue\n        cls_id = int(classes_int[i])\n        label_name = safe_label_name(cls_id, labels)\n        detections.append({\n            'class_id': cls_id,\n            'label': label_name,\n            'score': score,\n            'box': (xmin, ymin, xmax, ymax)\n        })\n\n    # 4.3 Post-processing: thresholding and NMS for pseudo-GT; clip already handled\n    # Add predictions above CONF_THRESHOLD to the collection (for mAP calc)\n    for det in detections:\n        if det['score'] >= CONF_THRESHOLD:\n            cls = det['class_id']\n            predictions_by_class.setdefault(cls, []).append({\n                'frame': frame_index,\n                'box': det['box'],\n                'score': float(det['score'])\n            })\n\n    # Build pseudo ground truth per class via high-confidence detections + NMS\n    # This approximates GT for mAP computation in absence of true annotations\n    frame_gt_by_class = {}  # temporary per-frame GT\n    # Group by class\n    dets_by_class = {}\n    for det in detections:\n        if det['score'] >= PSEUDO_GT_THRESHOLD:\n            cls = det['class_id']\n            dets_by_class.setdefault(cls, {'boxes': [], 'scores': []})\n            dets_by_class[cls]['boxes'].append(det['box'])\n            dets_by_class[cls]['scores'].append(det['score'])\n    # Apply NMS and store\n    for cls, data in dets_by_class.items():\n        boxes_c = data['boxes']\n        scores_c = data['scores']\n        keep_idx = nms_boxes(boxes_c, scores_c, iou_thresh=NMS_IOU_THRESHOLD)\n        gt_boxes_kept = [boxes_c[k] for k in keep_idx]\n        if len(gt_boxes_kept) > 0:\n            # Update global GT dict\n            gt_by_class.setdefault(cls, {})\n            gt_by_class[cls].setdefault(frame_index, [])\n            gt_by_class[cls][frame_index].extend(gt_boxes_kept)\n            frame_gt_by_class[cls] = gt_boxes_kept\n\n    # 4.4 Handle Output: Draw detections and overlay running mAP\n    # Draw detections above CONF_THRESHOLD\n    for det in detections:\n        if det['score'] < CONF_THRESHOLD:\n            continue\n        xmin, ymin, xmax, ymax = det['box']\n        cls = det['class_id']\n        label = det['label']\n        score = det['score']\n        color = color_for_class(cls)\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n        label_text = f\"{label}: {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (xmin, ymin - th - 4), (xmin + tw + 2, ymin), color, -1)\n        cv2.putText(frame, label_text, (xmin + 1, ymin - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n\n    # Compute and overlay running mAP\n    running_map = compute_map(predictions_by_class, gt_by_class, iou_thresh=IOU_THRESHOLD)\n    elapsed = time.time() - start_time\n    fps_runtime = (frame_index / elapsed) if elapsed > 0 else 0.0\n    info_text = f\"mAP@IoU{IOU_THRESHOLD:.2f}: {running_map:.3f}  FPS: {fps_runtime:.1f}\"\n    cv2.putText(frame, info_text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, info_text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1, cv2.LINE_AA)\n\n    # Write frame\n    writer.write(frame)\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nwriter.release()\n\n# Final mAP over the whole video\nfinal_map = compute_map(predictions_by_class, gt_by_class, iou_thresh=IOU_THRESHOLD)\nprint(f\"Final pseudo mAP@IoU{IOU_THRESHOLD:.2f} with conf>={CONF_THRESHOLD}: {final_map:.4f}\")\nprint(f\"Output video saved to: {OUTPUT_PATH}\")"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/06decec2",
      "latency": 140.598,
      "total_cost": 0.097935,
      "observations": [
        {
          "id": "time-04-00-23-005104_chatcmpl-D5ixLjHcnUMWrLE1Hni03q4ukyqLu",
          "traceId": "06decec2",
          "type": "GENERATION",
          "name": "06de_psg_gen_attempt#1",
          "startTime": "2026-02-05T02:00:23.005000+00:00",
          "endTime": "2026-02-05T02:02:43.603000+00:00",
          "completionStartTime": "2026-02-05T02:02:43.603000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.09793500000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.1 Imports: Interpreter from ai_edge_litert per guideline\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from configuration)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = float('0.5')  # Confidence Threshold\nPSEUDO_GT_THRESHOLD = 0.75     # Threshold to build pseudo ground-truth via high-confidence detections\nIOU_THRESHOLD = 0.5            # IoU threshold for matching during AP/mAP calculation\nNMS_IOU_THRESHOLD = 0.5        # IoU threshold for NMS when creating pseudo GT\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels\ndef load_labels(label_path):\n    labels = []\n    if os.path.exists(label_path):\n        with open(label_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    else:\n        # Fallback minimal labels (first 10 from provided useful info)\n        labels = [\n            'person','bicycle','car','motorcycle','airplane','bus','train','truck','boat','traffic light'\n        ]\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Store input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\n# Expect shape [1, height, width, 3]\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    raise RuntimeError(f'Unexpected input tensor shape: {input_shape}. Expected [1, H, W, 3].')\n\ninput_height = int(input_shape[1])\ninput_width = int(input_shape[2])\nfloating_model = (input_dtype == np.float32)\n\n# =========================\n# Utility functions\n# =========================\n\ndef bgr_to_rgb(image):\n    return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\ndef normalize_input(img):\n    # Normalize to [-1, 1] if floating model\n    return (np.float32(img) - 127.5) / 127.5\n\ndef clip_bbox(xmin, ymin, xmax, ymax, w, h):\n    xmin = max(0, min(xmin, w - 1))\n    ymin = max(0, min(ymin, h - 1))\n    xmax = max(0, min(xmax, w - 1))\n    ymax = max(0, min(ymax, h - 1))\n    return xmin, ymin, xmax, ymax\n\ndef safe_label_name(class_id, labels_list):\n    # Try 0-based first\n    idx = int(class_id)\n    if 0 <= idx < len(labels_list):\n        return labels_list[idx]\n    # Then try 1-based (common in TF OD models)\n    idx = int(class_id) - 1\n    if 0 <= idx < len(labels_list):\n        return labels_list[idx]\n    return f'class_{int(class_id)}'\n\ndef color_for_class(class_id):\n    # Deterministic pseudo-random color for class id\n    rng = np.random.RandomState(int(class_id) + 12345)\n    color = rng.randint(0, 255, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef iou(boxA, boxB):\n    # boxes: (xmin, ymin, xmax, ymax)\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interW = max(0, xB - xA + 1)\n    interH = max(0, yB - yA + 1)\n    interArea = interW * interH\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    denom = float(boxAArea + boxBArea - interArea)\n    if denom <= 0:\n        return 0.0\n    return interArea / denom\n\ndef nms_boxes(boxes, scores, iou_thresh=0.5):\n    # Non-Maximum Suppression, returns indices of kept boxes\n    if len(boxes) == 0:\n        return []\n\n    boxes = np.array(boxes)\n    scores = np.array(scores)\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        i = order[0]\n        keep.append(i)\n\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        iou_vals = inter / (areas[i] + areas[order[1:]] - inter + 1e-9)\n\n        inds = np.where(iou_vals <= iou_thresh)[0]\n        order = order[inds + 1]\n    return keep\n\ndef compute_ap(recall, precision):\n    # Compute AP using precision envelope method\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([0.0], precision, [0.0]))\n\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n    # Integrate area under PR curve\n    indices = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[indices + 1] - mrec[indices]) * mpre[indices + 1])\n    return ap\n\ndef compute_map(predictions_by_class, gt_by_class, iou_thresh=0.5):\n    # predictions_by_class: {class_id: [{'frame': int, 'box':(xmin,ymin,xmax,ymax), 'score': float}, ...]}\n    # gt_by_class: {class_id: {frame_id: [ (xmin,ymin,xmax,ymax), ... ]}}\n    ap_list = []\n    for cls_id in sorted(gt_by_class.keys()):\n        # Total GT count for this class\n        frame_to_gts = gt_by_class[cls_id]\n        total_gts = sum(len(bxs) for bxs in frame_to_gts.values())\n        if total_gts == 0:\n            continue\n\n        preds = predictions_by_class.get(cls_id, [])\n        if len(preds) == 0:\n            ap_list.append(0.0)\n            continue\n\n        # Sort predictions by score desc\n        preds_sorted = sorted(preds, key=lambda d: d['score'], reverse=True)\n\n        # Prepare matched flags per frame GTs\n        matched = {f: np.zeros(len(frame_to_gts[f]), dtype=bool) for f in frame_to_gts.keys()}\n\n        tp = np.zeros(len(preds_sorted))\n        fp = np.zeros(len(preds_sorted))\n\n        for i, p in enumerate(preds_sorted):\n            f = p['frame']\n            pbox = p['box']\n            gts = frame_to_gts.get(f, [])\n            best_iou = 0.0\n            best_j = -1\n            if len(gts) > 0:\n                for j, gtbox in enumerate(gts):\n                    iouv = iou(pbox, gtbox)\n                    if iouv > best_iou:\n                        best_iou = iouv\n                        best_j = j\n\n            if best_iou >= iou_thresh and best_j >= 0 and not matched[f][best_j]:\n                tp[i] = 1\n                matched[f][best_j] = True\n            else:\n                fp[i] = 1\n\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        recall = cum_tp / (total_gts + 1e-9)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        ap = compute_ap(recall, precision)\n        ap_list.append(ap)\n\n    if len(ap_list) == 0:\n        return 0.0\n    return float(np.mean(ap_list))\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video file: {INPUT_PATH}\")\n\n# Get video properties\norig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif not fps or fps <= 0 or np.isnan(fps):\n    fps = 25.0  # Fallback FPS\n\n# Prepare video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_width, orig_height))\nif not writer.isOpened():\n    cap.release()\n    raise RuntimeError(f\"Failed to open output video file for writing: {OUTPUT_PATH}\")\n\n# Storage for mAP computation across frames\npredictions_by_class = {}  # {class_id: [ {'frame': int, 'box': (xmin,ymin,xmax,ymax), 'score': float}, ... ]}\ngt_by_class = {}           # {class_id: {frame_id: [ (xmin,ymin,xmax,ymax), ... ]}}\n\nframe_index = 0\nstart_time = time.time()\n\n# =========================\n# Main processing loop\n# =========================\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    frame_index += 1\n    frame_h, frame_w = frame.shape[:2]\n\n    # 2.2 Preprocess Data\n    resized = cv2.resize(frame, (input_width, input_height))\n    rgb = bgr_to_rgb(resized)\n    input_data = np.expand_dims(rgb, axis=0)\n\n    # 2.3 Quantization Handling\n    if floating_model:\n        input_data = normalize_input(input_data)\n    else:\n        input_data = input_data.astype(input_dtype)\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling\n    # =========================\n\n    # 4.1 Get Output Tensors\n    # Typical TFLite SSD outputs: boxes [1, N, 4], classes [1, N], scores [1, N], num_detections [1]\n    boxes = None\n    classes_out = None\n    scores = None\n    num_dets = None\n\n    # Retrieve tensors and infer which is which by shape\n    for od in output_details:\n        out = interpreter.get_tensor(od['index'])\n        shp = out.shape\n        if len(shp) == 3 and shp[-1] == 4:\n            boxes = np.squeeze(out, axis=0)\n        elif len(shp) == 2 and shp[0] == 1 and shp[1] >= 1 and out.dtype in (np.float32, np.int64, np.int32):\n            # Could be classes or scores; differentiate by dtype and value range after we read others\n            # We'll temporarily store and decide later\n            if classes_out is None:\n                classes_out = np.squeeze(out, axis=0)\n            else:\n                # Decide which is score by checking range\n                if scores is None:\n                    scores = np.squeeze(out, axis=0)\n                else:\n                    # If both already set, assign to num_dets\n                    num_dets = int(np.squeeze(out))\n        elif len(shp) == 2 and shp[0] == 1 and shp[1] == 4:\n            # Some models may return [1, 4] boxes - unlikely for SSD\n            boxes = out[0]\n        elif len(shp) == 1 and shp[0] == 1:\n            num_dets = int(np.squeeze(out))\n        else:\n            # Attempt to categorize by content\n            if shp[0] == 1 and shp[-1] == 4:\n                boxes = np.squeeze(out, axis=0)\n\n    # If ambiguous mapping, attempt to swap if necessary\n    # Ensure scores in [0,1]\n    if scores is None or np.max(scores) > 1.0 or np.min(scores) < 0.0:\n        # Try to identify which of classes_out/scores is actually scores\n        # If classes are integer-like, choose other as scores\n        cand1 = classes_out\n        # Find another 1D output if available\n        for od in output_details:\n            arr = interpreter.get_tensor(od['index'])\n            arr_s = np.squeeze(arr)\n            if arr_s.ndim == 1 and arr_s.shape[0] == cand1.shape[0] and arr_s is not cand1:\n                # Choose arr_s as scores if it looks like probabilities\n                if np.max(arr_s) <= 1.0 and np.min(arr_s) >= 0.0:\n                    scores = arr_s\n                    break\n        # If still None, fallback to zeros to avoid crash\n        if scores is None:\n            scores = np.zeros_like(classes_out, dtype=np.float32)\n\n    # Ensure classes are integer array\n    if classes_out is not None and classes_out.dtype != np.int32 and classes_out.dtype != np.int64:\n        # Round to nearest int for class ids\n        classes_int = np.rint(classes_out).astype(np.int32)\n    else:\n        classes_int = classes_out.astype(np.int32) if classes_out is not None else np.zeros_like(scores, dtype=np.int32)\n\n    # Ensure boxes exist\n    if boxes is None:\n        # Fallback: create zero boxes to avoid crash\n        boxes = np.zeros((scores.shape[0], 4), dtype=np.float32)\n\n    # If num_dets is provided, limit arrays to actual detections\n    if num_dets is not None and num_dets > 0 and num_dets <= boxes.shape[0]:\n        boxes = boxes[:num_dets]\n        classes_int = classes_int[:num_dets]\n        scores = scores[:num_dets]\n\n    # 4.2 Interpret Results: convert to pixel coords, map labels\n    detections = []  # Per-frame detections used for drawing and predictions collection\n    # boxes are expected normalized [ymin, xmin, ymax, xmax]\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score <= 0.0:\n            continue\n        # Extract bbox\n        bymin, bxmin, bymax, bxmax = boxes[i]\n        # Clip normalized coords\n        bymin = float(np.clip(bymin, 0.0, 1.0))\n        bxmin = float(np.clip(bxmin, 0.0, 1.0))\n        bymax = float(np.clip(bymax, 0.0, 1.0))\n        bxmax = float(np.clip(bxmax, 0.0, 1.0))\n        # Scale to pixel coords\n        xmin = int(bxmin * frame_w)\n        ymin = int(bymin * frame_h)\n        xmax = int(bxmax * frame_w)\n        ymax = int(bymax * frame_h)\n        xmin, ymin, xmax, ymax = clip_bbox(xmin, ymin, xmax, ymax, frame_w, frame_h)\n        if xmax <= xmin or ymax <= ymin:\n            continue\n        cls_id = int(classes_int[i])\n        label_name = safe_label_name(cls_id, labels)\n        detections.append({\n            'class_id': cls_id,\n            'label': label_name,\n            'score': score,\n            'box': (xmin, ymin, xmax, ymax)\n        })\n\n    # 4.3 Post-processing: thresholding and NMS for pseudo-GT; clip already handled\n    # Add predictions above CONF_THRESHOLD to the collection (for mAP calc)\n    for det in detections:\n        if det['score'] >= CONF_THRESHOLD:\n            cls = det['class_id']\n            predictions_by_class.setdefault(cls, []).append({\n                'frame': frame_index,\n                'box': det['box'],\n                'score': float(det['score'])\n            })\n\n    # Build pseudo ground truth per class via high-confidence detections + NMS\n    # This approximates GT for mAP computation in absence of true annotations\n    frame_gt_by_class = {}  # temporary per-frame GT\n    # Group by class\n    dets_by_class = {}\n    for det in detections:\n        if det['score'] >= PSEUDO_GT_THRESHOLD:\n            cls = det['class_id']\n            dets_by_class.setdefault(cls, {'boxes': [], 'scores': []})\n            dets_by_class[cls]['boxes'].append(det['box'])\n            dets_by_class[cls]['scores'].append(det['score'])\n    # Apply NMS and store\n    for cls, data in dets_by_class.items():\n        boxes_c = data['boxes']\n        scores_c = data['scores']\n        keep_idx = nms_boxes(boxes_c, scores_c, iou_thresh=NMS_IOU_THRESHOLD)\n        gt_boxes_kept = [boxes_c[k] for k in keep_idx]\n        if len(gt_boxes_kept) > 0:\n            # Update global GT dict\n            gt_by_class.setdefault(cls, {})\n            gt_by_class[cls].setdefault(frame_index, [])\n            gt_by_class[cls][frame_index].extend(gt_boxes_kept)\n            frame_gt_by_class[cls] = gt_boxes_kept\n\n    # 4.4 Handle Output: Draw detections and overlay running mAP\n    # Draw detections above CONF_THRESHOLD\n    for det in detections:\n        if det['score'] < CONF_THRESHOLD:\n            continue\n        xmin, ymin, xmax, ymax = det['box']\n        cls = det['class_id']\n        label = det['label']\n        score = det['score']\n        color = color_for_class(cls)\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n        label_text = f\"{label}: {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (xmin, ymin - th - 4), (xmin + tw + 2, ymin), color, -1)\n        cv2.putText(frame, label_text, (xmin + 1, ymin - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n\n    # Compute and overlay running mAP\n    running_map = compute_map(predictions_by_class, gt_by_class, iou_thresh=IOU_THRESHOLD)\n    elapsed = time.time() - start_time\n    fps_runtime = (frame_index / elapsed) if elapsed > 0 else 0.0\n    info_text = f\"mAP@IoU{IOU_THRESHOLD:.2f}: {running_map:.3f}  FPS: {fps_runtime:.1f}\"\n    cv2.putText(frame, info_text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 3, cv2.LINE_AA)\n    cv2.putText(frame, info_text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 1, cv2.LINE_AA)\n\n    # Write frame\n    writer.write(frame)\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nwriter.release()\n\n# Final mAP over the whole video\nfinal_map = compute_map(predictions_by_class, gt_by_class, iou_thresh=IOU_THRESHOLD)\nprint(f\"Final pseudo mAP@IoU{IOU_THRESHOLD:.2f} with conf>={CONF_THRESHOLD}: {final_map:.4f}\")\nprint(f\"Output video saved to: {OUTPUT_PATH}\")\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 9606,
            "total": 11106,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.097935,
          "latency": 140598.0,
          "timeToFirstToken": 140.598,
          "updatedAt": "2026-02-05T02:02:49.268Z",
          "totalTokens": 11106,
          "costDetails": {
            "total": 0.097935
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 9606,
          "usageDetails": {
            "input": 1500,
            "output": 9606,
            "total": 11106
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T02:02:44.171Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:03:47.663Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T02:00:22.957Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "34f1cb61",
      "timestamp": "2026-02-05T01:56:24.603000+00:00",
      "name": "34f1_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a given video file using a TFLite SSD model\nvia ai_edge_litert Interpreter. It draws bounding boxes and labels on detected objects,\nwrites an annotated output video, and computes a running proxy mAP (mean Average Precision)\nover the processed frames. Due to the absence of ground-truth annotations, the mAP computed\nhere is a self-consistency proxy:\n- For each frame and each class, only the highest-confidence detection is treated as a TP.\n- Other detections for that class in the same frame are treated as FP.\n- AP is calculated per class from the precision-recall curve formed by sorting detections by confidence.\n- mAP is the mean of AP over classes that had at least one TP during processing.\n\nNote: This proxy mAP is not a substitute for true mAP computed against ground-truth data.\n\"\"\"\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.1 Imports (standard libs only as required)\nimport os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter  # literal import as required\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD  = 0.5\nMETRIC_MIN_SCORE = 0.01  # minimal score to consider a detection in proxy mAP calculation\n\n# Utility: Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n\ndef load_labels(label_path):\n    \"\"\"\n    1.3 Load Labels (if provided and relevant)\n    Reads the label file into a list, stripping whitespace and empty lines.\n    \"\"\"\n    labels = []\n    if label_path and os.path.exists(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line != '':\n                    labels.append(line)\n    return labels\n\n\ndef get_label_for_class_id(labels, class_id):\n    \"\"\"\n    Map class_id to a human-readable label using the labels list.\n    Handles potential off-by-one indexing differences between model outputs and label file.\n    \"\"\"\n    ci = int(class_id)\n    # Direct mapping\n    if 0 <= ci < len(labels):\n        return labels[ci]\n    # Fallback: if outputs are 1-based but labels are 0-based\n    if 0 <= (ci - 1) < len(labels):\n        return labels[ci - 1]\n    # As a last resort, return generic identifier\n    return f'id_{ci}'\n\n\ndef color_for_class_id(class_id):\n    \"\"\"\n    Generate a pseudo-random but deterministic color for a given class id.\n    \"\"\"\n    ci = int(class_id)\n    np.random.seed(ci)\n    color = tuple(int(x) for x in np.random.randint(0, 255, size=3))\n    return color\n\n\ndef compute_ap_from_prec_recall(precisions, recalls):\n    \"\"\"\n    Compute Average Precision (AP) using the typical interpolation method:\n    - Ensure precision is a non-increasing function by setting p[i] = max(p[i], p[i+1], ...)\n    - Sum over recall deltas weighted by precision at those points.\n    \"\"\"\n    if len(precisions) == 0 or len(recalls) == 0:\n        return 0.0\n    # Append boundary points\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n    # Make precision non-increasing\n    for i in range(len(mpre) - 2, -1, -1):\n        mpre[i] = max(mpre[i], mpre[i + 1])\n    # Integrate area under curve where recall changes\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = float(np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1]))\n    return ap\n\n\ndef compute_proxy_map(preds_per_class):\n    \"\"\"\n    Compute proxy mAP (no ground truth):\n    - For each class, we have a list of detections with fields: frame, score, is_tp (True if it's the\n      top detection for that class in that frame, False otherwise).\n    - Sort detections by score descending, then compute precision-recall based on TP/FP flags.\n    - AP is computed per class; mAP is the mean AP over classes that had at least one TP.\n    \"\"\"\n    ap_list = []\n    for cls_id, dets in preds_per_class.items():\n        if not dets:\n            continue\n        # Sort detections by score descending\n        dets_sorted = sorted(dets, key=lambda d: d['score'], reverse=True)\n        # Determine number of positives (unique frames that contributed a TP)\n        npos = len({d['frame'] for d in dets_sorted if d['is_tp']})\n        if npos == 0:\n            # No positives for this class -> skip\n            continue\n        # Build TP/FP arrays\n        tp = np.array([1 if d['is_tp'] else 0 for d in dets_sorted], dtype=np.float32)\n        fp = 1.0 - tp\n        tp_cum = np.cumsum(tp)\n        fp_cum = np.cumsum(fp)\n        precisions = tp_cum / np.maximum(tp_cum + fp_cum, 1e-12)\n        recalls = tp_cum / float(npos)\n        ap = compute_ap_from_prec_recall(precisions, recalls)\n        ap_list.append(ap)\n    if len(ap_list) == 0:\n        return 0.0, 0  # mAP, number of classes contributing\n    mAP = float(np.mean(ap_list))\n    return mAP, len(ap_list)\n\n\ndef main():\n    # 1.3 Load labels (relevant for visualization)\n    labels = load_labels(LABEL_PATH)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Extract input shape and properties\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']  # e.g., [1, H, W, 3]\n    in_h, in_w = int(input_shape[1]), int(input_shape[2])\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n\n    # =========================\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # =========================\n\n    # 2.1 Acquire Input Data: Read a single video file from the given input_path\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: Cannot open input video: {INPUT_PATH}\")\n        raise SystemExit(1)\n\n    # Prepare output video writer\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # default fallback\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n    if not writer.isOpened():\n        print(f\"Error: Cannot open output video for writing: {OUTPUT_PATH}\")\n        cap.release()\n        raise SystemExit(1)\n\n    # Aggregators for proxy mAP calculation\n    # preds_per_class: dict {class_id: [{'frame': frame_idx, 'score': float, 'is_tp': bool}, ...]}\n    preds_per_class = {}\n\n    frame_idx = 0\n    t_start = time.time()\n\n    # 2.4 Loop Control: Iterate through all frames in the single input video\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break  # end of video\n\n        # 2.2 Preprocess Data:\n        # - Resize to model input dimensions\n        # - Convert BGR (OpenCV) to RGB if needed by model\n        # - Expand dims to [1, H, W, 3]\n        # - Cast to required dtype and normalize if floating model\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n        input_data = np.expand_dims(resized, axis=0)\n\n        # Ensure dtype\n        if floating_model:\n            # 2.3 Quantization Handling: normalize to [-1, 1] for floating models\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n        else:\n            # uint8 quantized model expects 0..255\n            input_data = np.uint8(input_data)\n\n        # =========================\n        # Phase 3: Inference (Run per preprocessed input)\n        # =========================\n\n        # 3.1 Set Input Tensor(s)\n        interpreter.set_tensor(input_index, input_data)\n\n        # 3.2 Run Inference\n        interpreter.invoke()\n\n        # =========================\n        # Phase 4: Output Interpretation & Handling Loop\n        # =========================\n\n        # 4.1 Get Output Tensor(s)\n        # Try to identify boxes, classes, scores, and num detections\n        boxes = None\n        classes = None\n        scores = None\n        num = None\n\n        # Retrieve all output tensors\n        out_tensors = []\n        for od in output_details:\n            out_tensors.append(interpreter.get_tensor(od['index']))\n\n        # Identify tensors by shape and value ranges\n        for out in out_tensors:\n            arr = np.squeeze(out)\n            if arr.ndim == 2 and arr.shape[1] == 4:\n                # Shape [N, 4] or [1, N, 4]; normalize to [N, 4]\n                boxes = arr\n            elif arr.ndim == 1 and arr.size > 4:\n                # Could be classes or scores; distinguish by value range\n                maxv = float(np.max(arr)) if arr.size > 0 else 0.0\n                minv = float(np.min(arr)) if arr.size > 0 else 0.0\n                if maxv <= 1.0001 and minv >= 0.0:\n                    scores = arr\n                else:\n                    classes = arr\n            elif arr.ndim == 0 or (arr.ndim == 1 and arr.size == 1):\n                num = int(np.round(float(arr)))\n\n        # Fallback for common TFLite SSD order if above heuristic fails\n        if boxes is None or classes is None or scores is None:\n            # Try the conventional ordering: [boxes, classes, scores, num]\n            try:\n                b = np.squeeze(out_tensors[0])\n                c = np.squeeze(out_tensors[1])\n                s = np.squeeze(out_tensors[2])\n                n = int(np.round(float(np.squeeze(out_tensors[3]))))\n                if b.ndim == 2 and b.shape[1] == 4:\n                    boxes = b\n                if c.ndim == 1:\n                    classes = c\n                if s.ndim == 1:\n                    scores = s\n                num = n\n            except Exception:\n                pass\n\n        # Validate outputs\n        if boxes is None or classes is None or scores is None:\n            # If outputs are still not recognized, skip frame gracefully\n            cv2.putText(frame, \"Output parsing error\", (10, 30),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n            writer.write(frame)\n            frame_idx += 1\n            continue\n\n        # Determine number of detections\n        if num is None:\n            num = min(len(scores), len(classes), len(boxes))\n        else:\n            num = min(num, len(scores), len(classes), len(boxes))\n\n        # 4.2 Interpret Results:\n        # Convert normalized box coordinates to pixel coordinates and map class IDs to labels\n        h, w = frame.shape[0], frame.shape[1]\n        detections_for_frame = []\n        for i in range(num):\n            score = float(scores[i])\n            cls_id = int(classes[i])\n\n            # Ignore extremely low scores for speed in both drawing and metric\n            if score < METRIC_MIN_SCORE:\n                continue\n\n            # TFLite SSD boxes are in [ymin, xmin, ymax, xmax], normalized [0,1]\n            ymin, xmin, ymax, xmax = boxes[i]\n            # 4.3 Post-processing: clip coordinates to valid ranges\n            ymin = max(0.0, min(1.0, float(ymin)))\n            xmin = max(0.0, min(1.0, float(xmin)))\n            ymax = max(0.0, min(1.0, float(ymax)))\n            xmax = max(0.0, min(1.0, float(xmax)))\n\n            # Scale to pixel coordinates\n            x1 = int(round(xmin * w))\n            y1 = int(round(ymin * h))\n            x2 = int(round(xmax * w))\n            y2 = int(round(ymax * h))\n\n            # Ensure proper ordering and clipping\n            x1 = max(0, min(w - 1, x1))\n            y1 = max(0, min(h - 1, y1))\n            x2 = max(0, min(w - 1, x2))\n            y2 = max(0, min(h - 1, y2))\n            if x2 <= x1 or y2 <= y1:\n                continue  # discard degenerate boxes\n\n            label_text = get_label_for_class_id(labels, cls_id) if labels else f'class_{cls_id}'\n            detections_for_frame.append({\n                'class_id': cls_id,\n                'score': score,\n                'bbox': (x1, y1, x2, y2),\n                'label': label_text\n            })\n\n        # Build per-class organization for the current frame (for proxy mAP)\n        per_class_scores = {}\n        for det in detections_for_frame:\n            cid = det['class_id']\n            per_class_scores.setdefault(cid, [])\n            per_class_scores[cid].append(det['score'])\n\n        # For each class in this frame, identify the top score as TP; others as FP\n        used_tp_for_frame_class = set()\n        for det in detections_for_frame:\n            cid = det['class_id']\n            score = det['score']\n            # Determine top score for this class in this frame\n            top_score = max(per_class_scores[cid]) if len(per_class_scores[cid]) > 0 else score\n            is_tp = False\n            if (frame_idx, cid) not in used_tp_for_frame_class and abs(score - top_score) < 1e-12:\n                # First occurrence of top score for this (frame, class) pair -> TP\n                is_tp = True\n                used_tp_for_frame_class.add((frame_idx, cid))\n            preds_per_class.setdefault(cid, [])\n            preds_per_class[cid].append({\n                'frame': frame_idx,\n                'score': score,\n                'is_tp': is_tp\n            })\n\n        # 4.4 Handle Output: Draw detections and overlay mAP, then write frame to output video\n        draw_count = 0\n        for det in detections_for_frame:\n            if det['score'] < CONFIDENCE_THRESHOLD:\n                continue\n            x1, y1, x2, y2 = det['bbox']\n            color = color_for_class_id(det['class_id'])\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            caption = f\"{det['label']}: {det['score']:.2f}\"\n            # Put text with background for readability\n            (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame, (x1, max(0, y1 - th - 6)), (x1 + tw + 2, y1), color, -1)\n            cv2.putText(frame, caption, (x1 + 1, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n                        (255, 255, 255), 1, cv2.LINE_AA)\n            draw_count += 1\n\n        # Compute running proxy mAP and overlay\n        mAP_value, contributing_classes = compute_proxy_map(preds_per_class)\n        info_lines = [\n            f\"Detections (>= {CONFIDENCE_THRESHOLD:.2f}): {draw_count}\",\n            f\"Proxy mAP (no GT): {mAP_value:.3f} over {contributing_classes} classes\",\n            f\"Model Input: {in_w}x{in_h}, dtype: {'float32' if floating_model else 'uint8'}\"\n        ]\n        # Draw info box\n        padding = 5\n        line_h = 18\n        box_w = max(cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0][0] for line in info_lines) + 2 * padding\n        box_h = line_h * len(info_lines) + 2 * padding\n        cv2.rectangle(frame, (5, 5), (5 + box_w, 5 + box_h), (0, 0, 0), -1)\n        for idx, line in enumerate(info_lines):\n            cv2.putText(frame, line, (10, 10 + (idx + 1) * line_h - 5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n\n        # Write annotated frame to output\n        writer.write(frame)\n        frame_idx += 1\n\n    # =========================\n    # Phase 5: Cleanup\n    # =========================\n    cap.release()\n    writer.release()\n\n    elapsed = time.time() - t_start\n    final_mAP, final_classes = compute_proxy_map(preds_per_class)\n    print(\"Processing complete.\")\n    print(f\"Total frames processed: {frame_idx}\")\n    print(f\"Elapsed time (s): {elapsed:.2f}\")\n    print(f\"Proxy mAP (no ground truth): {final_mAP:.4f} over {final_classes} classes\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n\n\nif __name__ == '__main__':\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/34f1cb61",
      "latency": 174.868,
      "total_cost": 0.100721,
      "observations": [
        {
          "id": "time-03-56-25-314518_chatcmpl-D5itVJ4fht9Vj0LEnXzpyGQGdcW07",
          "traceId": "34f1cb61",
          "type": "GENERATION",
          "name": "34f1_psg_gen_attempt#1",
          "startTime": "2026-02-05T01:56:25.314000+00:00",
          "endTime": "2026-02-05T01:59:20.182000+00:00",
          "completionStartTime": "2026-02-05T01:59:20.182000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.100721,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a given video file using a TFLite SSD model\nvia ai_edge_litert Interpreter. It draws bounding boxes and labels on detected objects,\nwrites an annotated output video, and computes a running proxy mAP (mean Average Precision)\nover the processed frames. Due to the absence of ground-truth annotations, the mAP computed\nhere is a self-consistency proxy:\n- For each frame and each class, only the highest-confidence detection is treated as a TP.\n- Other detections for that class in the same frame are treated as FP.\n- AP is calculated per class from the precision-recall curve formed by sorting detections by confidence.\n- mAP is the mean of AP over classes that had at least one TP during processing.\n\nNote: This proxy mAP is not a substitute for true mAP computed against ground-truth data.\n\"\"\"\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.1 Imports (standard libs only as required)\nimport os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter  # literal import as required\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONFIDENCE_THRESHOLD = float('0.5')  # drawing threshold for visualization\nMETRIC_MIN_SCORE = 0.01  # minimal score to consider a detection in proxy mAP calculation\n\n# Utility: Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n\ndef load_labels(label_path):\n    \"\"\"\n    1.3 Load Labels (if provided and relevant)\n    Reads the label file into a list, stripping whitespace and empty lines.\n    \"\"\"\n    labels = []\n    if label_path and os.path.exists(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line != '':\n                    labels.append(line)\n    return labels\n\n\ndef get_label_for_class_id(labels, class_id):\n    \"\"\"\n    Map class_id to a human-readable label using the labels list.\n    Handles potential off-by-one indexing differences between model outputs and label file.\n    \"\"\"\n    ci = int(class_id)\n    # Direct mapping\n    if 0 <= ci < len(labels):\n        return labels[ci]\n    # Fallback: if outputs are 1-based but labels are 0-based\n    if 0 <= (ci - 1) < len(labels):\n        return labels[ci - 1]\n    # As a last resort, return generic identifier\n    return f'id_{ci}'\n\n\ndef color_for_class_id(class_id):\n    \"\"\"\n    Generate a pseudo-random but deterministic color for a given class id.\n    \"\"\"\n    ci = int(class_id)\n    np.random.seed(ci)\n    color = tuple(int(x) for x in np.random.randint(0, 255, size=3))\n    return color\n\n\ndef compute_ap_from_prec_recall(precisions, recalls):\n    \"\"\"\n    Compute Average Precision (AP) using the typical interpolation method:\n    - Ensure precision is a non-increasing function by setting p[i] = max(p[i], p[i+1], ...)\n    - Sum over recall deltas weighted by precision at those points.\n    \"\"\"\n    if len(precisions) == 0 or len(recalls) == 0:\n        return 0.0\n    # Append boundary points\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n    # Make precision non-increasing\n    for i in range(len(mpre) - 2, -1, -1):\n        mpre[i] = max(mpre[i], mpre[i + 1])\n    # Integrate area under curve where recall changes\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = float(np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1]))\n    return ap\n\n\ndef compute_proxy_map(preds_per_class):\n    \"\"\"\n    Compute proxy mAP (no ground truth):\n    - For each class, we have a list of detections with fields: frame, score, is_tp (True if it's the\n      top detection for that class in that frame, False otherwise).\n    - Sort detections by score descending, then compute precision-recall based on TP/FP flags.\n    - AP is computed per class; mAP is the mean AP over classes that had at least one TP.\n    \"\"\"\n    ap_list = []\n    for cls_id, dets in preds_per_class.items():\n        if not dets:\n            continue\n        # Sort detections by score descending\n        dets_sorted = sorted(dets, key=lambda d: d['score'], reverse=True)\n        # Determine number of positives (unique frames that contributed a TP)\n        npos = len({d['frame'] for d in dets_sorted if d['is_tp']})\n        if npos == 0:\n            # No positives for this class -> skip\n            continue\n        # Build TP/FP arrays\n        tp = np.array([1 if d['is_tp'] else 0 for d in dets_sorted], dtype=np.float32)\n        fp = 1.0 - tp\n        tp_cum = np.cumsum(tp)\n        fp_cum = np.cumsum(fp)\n        precisions = tp_cum / np.maximum(tp_cum + fp_cum, 1e-12)\n        recalls = tp_cum / float(npos)\n        ap = compute_ap_from_prec_recall(precisions, recalls)\n        ap_list.append(ap)\n    if len(ap_list) == 0:\n        return 0.0, 0  # mAP, number of classes contributing\n    mAP = float(np.mean(ap_list))\n    return mAP, len(ap_list)\n\n\ndef main():\n    # 1.3 Load labels (relevant for visualization)\n    labels = load_labels(LABEL_PATH)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=MODEL_PATH)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Extract input shape and properties\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']  # e.g., [1, H, W, 3]\n    in_h, in_w = int(input_shape[1]), int(input_shape[2])\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n\n    # =========================\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # =========================\n\n    # 2.1 Acquire Input Data: Read a single video file from the given input_path\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: Cannot open input video: {INPUT_PATH}\")\n        raise SystemExit(1)\n\n    # Prepare output video writer\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # default fallback\n    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_width, frame_height))\n    if not writer.isOpened():\n        print(f\"Error: Cannot open output video for writing: {OUTPUT_PATH}\")\n        cap.release()\n        raise SystemExit(1)\n\n    # Aggregators for proxy mAP calculation\n    # preds_per_class: dict {class_id: [{'frame': frame_idx, 'score': float, 'is_tp': bool}, ...]}\n    preds_per_class = {}\n\n    frame_idx = 0\n    t_start = time.time()\n\n    # 2.4 Loop Control: Iterate through all frames in the single input video\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break  # end of video\n\n        # 2.2 Preprocess Data:\n        # - Resize to model input dimensions\n        # - Convert BGR (OpenCV) to RGB if needed by model\n        # - Expand dims to [1, H, W, 3]\n        # - Cast to required dtype and normalize if floating model\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n        input_data = np.expand_dims(resized, axis=0)\n\n        # Ensure dtype\n        if floating_model:\n            # 2.3 Quantization Handling: normalize to [-1, 1] for floating models\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n        else:\n            # uint8 quantized model expects 0..255\n            input_data = np.uint8(input_data)\n\n        # =========================\n        # Phase 3: Inference (Run per preprocessed input)\n        # =========================\n\n        # 3.1 Set Input Tensor(s)\n        interpreter.set_tensor(input_index, input_data)\n\n        # 3.2 Run Inference\n        interpreter.invoke()\n\n        # =========================\n        # Phase 4: Output Interpretation & Handling Loop\n        # =========================\n\n        # 4.1 Get Output Tensor(s)\n        # Try to identify boxes, classes, scores, and num detections\n        boxes = None\n        classes = None\n        scores = None\n        num = None\n\n        # Retrieve all output tensors\n        out_tensors = []\n        for od in output_details:\n            out_tensors.append(interpreter.get_tensor(od['index']))\n\n        # Identify tensors by shape and value ranges\n        for out in out_tensors:\n            arr = np.squeeze(out)\n            if arr.ndim == 2 and arr.shape[1] == 4:\n                # Shape [N, 4] or [1, N, 4]; normalize to [N, 4]\n                boxes = arr\n            elif arr.ndim == 1 and arr.size > 4:\n                # Could be classes or scores; distinguish by value range\n                maxv = float(np.max(arr)) if arr.size > 0 else 0.0\n                minv = float(np.min(arr)) if arr.size > 0 else 0.0\n                if maxv <= 1.0001 and minv >= 0.0:\n                    scores = arr\n                else:\n                    classes = arr\n            elif arr.ndim == 0 or (arr.ndim == 1 and arr.size == 1):\n                num = int(np.round(float(arr)))\n\n        # Fallback for common TFLite SSD order if above heuristic fails\n        if boxes is None or classes is None or scores is None:\n            # Try the conventional ordering: [boxes, classes, scores, num]\n            try:\n                b = np.squeeze(out_tensors[0])\n                c = np.squeeze(out_tensors[1])\n                s = np.squeeze(out_tensors[2])\n                n = int(np.round(float(np.squeeze(out_tensors[3]))))\n                if b.ndim == 2 and b.shape[1] == 4:\n                    boxes = b\n                if c.ndim == 1:\n                    classes = c\n                if s.ndim == 1:\n                    scores = s\n                num = n\n            except Exception:\n                pass\n\n        # Validate outputs\n        if boxes is None or classes is None or scores is None:\n            # If outputs are still not recognized, skip frame gracefully\n            cv2.putText(frame, \"Output parsing error\", (10, 30),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n            writer.write(frame)\n            frame_idx += 1\n            continue\n\n        # Determine number of detections\n        if num is None:\n            num = min(len(scores), len(classes), len(boxes))\n        else:\n            num = min(num, len(scores), len(classes), len(boxes))\n\n        # 4.2 Interpret Results:\n        # Convert normalized box coordinates to pixel coordinates and map class IDs to labels\n        h, w = frame.shape[0], frame.shape[1]\n        detections_for_frame = []\n        for i in range(num):\n            score = float(scores[i])\n            cls_id = int(classes[i])\n\n            # Ignore extremely low scores for speed in both drawing and metric\n            if score < METRIC_MIN_SCORE:\n                continue\n\n            # TFLite SSD boxes are in [ymin, xmin, ymax, xmax], normalized [0,1]\n            ymin, xmin, ymax, xmax = boxes[i]\n            # 4.3 Post-processing: clip coordinates to valid ranges\n            ymin = max(0.0, min(1.0, float(ymin)))\n            xmin = max(0.0, min(1.0, float(xmin)))\n            ymax = max(0.0, min(1.0, float(ymax)))\n            xmax = max(0.0, min(1.0, float(xmax)))\n\n            # Scale to pixel coordinates\n            x1 = int(round(xmin * w))\n            y1 = int(round(ymin * h))\n            x2 = int(round(xmax * w))\n            y2 = int(round(ymax * h))\n\n            # Ensure proper ordering and clipping\n            x1 = max(0, min(w - 1, x1))\n            y1 = max(0, min(h - 1, y1))\n            x2 = max(0, min(w - 1, x2))\n            y2 = max(0, min(h - 1, y2))\n            if x2 <= x1 or y2 <= y1:\n                continue  # discard degenerate boxes\n\n            label_text = get_label_for_class_id(labels, cls_id) if labels else f'class_{cls_id}'\n            detections_for_frame.append({\n                'class_id': cls_id,\n                'score': score,\n                'bbox': (x1, y1, x2, y2),\n                'label': label_text\n            })\n\n        # Build per-class organization for the current frame (for proxy mAP)\n        per_class_scores = {}\n        for det in detections_for_frame:\n            cid = det['class_id']\n            per_class_scores.setdefault(cid, [])\n            per_class_scores[cid].append(det['score'])\n\n        # For each class in this frame, identify the top score as TP; others as FP\n        used_tp_for_frame_class = set()\n        for det in detections_for_frame:\n            cid = det['class_id']\n            score = det['score']\n            # Determine top score for this class in this frame\n            top_score = max(per_class_scores[cid]) if len(per_class_scores[cid]) > 0 else score\n            is_tp = False\n            if (frame_idx, cid) not in used_tp_for_frame_class and abs(score - top_score) < 1e-12:\n                # First occurrence of top score for this (frame, class) pair -> TP\n                is_tp = True\n                used_tp_for_frame_class.add((frame_idx, cid))\n            preds_per_class.setdefault(cid, [])\n            preds_per_class[cid].append({\n                'frame': frame_idx,\n                'score': score,\n                'is_tp': is_tp\n            })\n\n        # 4.4 Handle Output: Draw detections and overlay mAP, then write frame to output video\n        draw_count = 0\n        for det in detections_for_frame:\n            if det['score'] < CONFIDENCE_THRESHOLD:\n                continue\n            x1, y1, x2, y2 = det['bbox']\n            color = color_for_class_id(det['class_id'])\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            caption = f\"{det['label']}: {det['score']:.2f}\"\n            # Put text with background for readability\n            (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame, (x1, max(0, y1 - th - 6)), (x1 + tw + 2, y1), color, -1)\n            cv2.putText(frame, caption, (x1 + 1, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n                        (255, 255, 255), 1, cv2.LINE_AA)\n            draw_count += 1\n\n        # Compute running proxy mAP and overlay\n        mAP_value, contributing_classes = compute_proxy_map(preds_per_class)\n        info_lines = [\n            f\"Detections (>= {CONFIDENCE_THRESHOLD:.2f}): {draw_count}\",\n            f\"Proxy mAP (no GT): {mAP_value:.3f} over {contributing_classes} classes\",\n            f\"Model Input: {in_w}x{in_h}, dtype: {'float32' if floating_model else 'uint8'}\"\n        ]\n        # Draw info box\n        padding = 5\n        line_h = 18\n        box_w = max(cv2.getTextSize(line, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0][0] for line in info_lines) + 2 * padding\n        box_h = line_h * len(info_lines) + 2 * padding\n        cv2.rectangle(frame, (5, 5), (5 + box_w, 5 + box_h), (0, 0, 0), -1)\n        for idx, line in enumerate(info_lines):\n            cv2.putText(frame, line, (10, 10 + (idx + 1) * line_h - 5),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1, cv2.LINE_AA)\n\n        # Write annotated frame to output\n        writer.write(frame)\n        frame_idx += 1\n\n    # =========================\n    # Phase 5: Cleanup\n    # =========================\n    cap.release()\n    writer.release()\n\n    elapsed = time.time() - t_start\n    final_mAP, final_classes = compute_proxy_map(preds_per_class)\n    print(\"Processing complete.\")\n    print(f\"Total frames processed: {frame_idx}\")\n    print(f\"Elapsed time (s): {elapsed:.2f}\")\n    print(f\"Proxy mAP (no ground truth): {final_mAP:.4f} over {final_classes} classes\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n\n\nif __name__ == '__main__':\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 10043,
            "total": 11543,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.100721,
          "latency": 174868.0,
          "timeToFirstToken": 174.868,
          "updatedAt": "2026-02-05T01:59:25.698Z",
          "totalTokens": 11543,
          "costDetails": {
            "total": 0.100721
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 10043,
          "usageDetails": {
            "input": 1500,
            "output": 10043,
            "total": 11543
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T01:59:20.634Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T02:00:16.886Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T01:56:25.189Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "aed67aaf",
      "timestamp": "2026-02-05T01:53:10.869000+00:00",
      "name": "aed6_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video file using a TFLite SSD MobileNet v1 model.\nIt writes an output video with bounding boxes and labels overlaid, and overlays a running \"mAP\" estimate\n(computed without ground-truth as a self-consistency proxy; see comments in Phase 4.2 for details).\n\nPhases implemented according to the provided Programming Guidelines:\n- Phase 1: Setup\n- Phase 2: Input Acquisition & Preprocessing Loop (explicit)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop, including:\n  - 4.2: Interpret Results (explicit)\n  - 4.3: Post-processing (explicit)\n- Phase 5: Cleanup\n\nDependencies:\n- Standard libraries: os, time, numpy, cv2 (video/image processing)\n- TFLite runtime: ai_edge_litert.interpreter.Interpreter\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nmodel_path  = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path  = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path  = \"data/object_detection/sheeps.mp4\"\noutput_path  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = 0.5  # For drawing/display\n\n# 1.3 Load Labels (if provided and relevant)\ndef load_labels(path):\n    labels = []\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(label_path)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract primary input tensor info\nin_idx = input_details[0]['index']\nin_shape = input_details[0]['shape']\n# Expect shape [1, height, width, 3]\nif len(in_shape) != 4 or in_shape[0] != 1 or in_shape[-1] != 3:\n    raise ValueError(\"Unexpected model input shape: {}\".format(in_shape))\nin_height, in_width = int(in_shape[1]), int(in_shape[2])\nin_dtype = input_details[0]['dtype']\nfloating_model = (in_dtype == np.float32)\n\n# Helper to get consistent color per class id\ndef class_color(cid):\n    # Generate a color based on the class id for consistency.\n    np.random.seed(cid + 12345)\n    color = tuple(int(x) for x in np.random.randint(0, 255, size=3))\n    # OpenCV uses BGR\n    return (color[0], color[1], color[2])\n\n# Directory prep for output\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# Input Method: Read a single video file from the given input_path\n# =========================\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Failed to open input video: {}\".format(input_path))\n\n# Read input video properties\nfps = cap.get(cv2.CAP_PROP_FPS)\nif not fps or np.isnan(fps) or fps <= 0:\n    fps = 25.0  # Fallback if FPS is unavailable\nframe_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Setup VideoWriter for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\nif not writer.isOpened():\n    cap.release()\n    raise RuntimeError(\"Failed to open output video for writing: {}\".format(output_path))\n\n# Structures for running \"mAP\" proxy computation (no ground-truth available)\n# We compute a self-consistency proxy AP per class:\n# - For each frame and class, the highest-scoring detection is treated as TP candidate (1 per frame),\n#   and all other detections for that class in that frame are treated as FP.\n# - Aggregate across frames and compute AP as area under precision-recall curve over scores.\nper_class_scores_flags = {}  # dict: class_id -> list of tuples (score, is_tp)\nframes_processed = 0\n\ndef compute_ap_from_scores_flags(sf_list):\n    \"\"\"\n    Compute AP from list of (score, is_tp) entries for a single class, across frames processed so far.\n    Uses a standard precision-recall area with monotonically decreasing precision envelope.\n    If there are no positives (no frames with detections), returns None to indicate \"skip in mAP\".\n    \"\"\"\n    if not sf_list:\n        return None\n\n    # Count positives (is_tp True)\n    num_positives = sum(1 for _, is_tp in sf_list if is_tp)\n    if num_positives == 0:\n        return None\n\n    # Sort by score descending\n    sf_sorted = sorted(sf_list, key=lambda x: x[0], reverse=True)\n\n    tp_cum = 0\n    fp_cum = 0\n    precisions = []\n    recalls = []\n    for score, is_tp in sf_sorted:\n        if is_tp:\n            tp_cum += 1\n        else:\n            fp_cum += 1\n        precision = tp_cum / (tp_cum + fp_cum)\n        recall = tp_cum / num_positives\n        precisions.append(precision)\n        recalls.append(recall)\n\n    # Compute AP via precision envelope integration (VOC 2010 style)\n    precisions = np.array(precisions, dtype=np.float32)\n    recalls = np.array(recalls, dtype=np.float32)\n\n    # Monotonic precision envelope\n    for i in range(len(precisions) - 2, -1, -1):\n        if precisions[i] < precisions[i + 1]:\n            precisions[i] = precisions[i + 1]\n\n    # Identify points where recall changes\n    # We need to include the starting (0, p0) and ending points; handle accordingly\n    unique_recalls, indices = np.unique(recalls, return_index=True)\n    # Sort indices to have increasing recall\n    order = np.argsort(unique_recalls)\n    unique_recalls = unique_recalls[order]\n    indices = indices[order]\n\n    # For integration, we can approximate as sum over recall diffs with corresponding precision\n    ap = 0.0\n    prev_recall = 0.0\n    for r, idx in zip(unique_recalls, indices):\n        ap += precisions[idx] * (r - prev_recall)\n        prev_recall = r\n\n    return float(ap)\n\ndef compute_running_map(per_class_dict):\n    aps = []\n    for cid, sf_list in per_class_dict.items():\n        ap = compute_ap_from_scores_flags(sf_list)\n        if ap is not None:\n            aps.append(ap)\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\n# Main processing loop\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break  # End of video\n\n    frames_processed += 1\n\n    # 2.2 Preprocess data to match model input\n    # Convert BGR to RGB, resize to model input size, add batch dimension\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (in_width, in_height), interpolation=cv2.INTER_LINEAR)\n    input_data = np.expand_dims(resized, axis=0)\n\n    # Convert dtype and normalize if floating model\n    if floating_model:\n        # Normalize to [-1, 1] as typical for SSD MobileNet v1\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # Ensure dtype matches (usually uint8)\n        input_data = input_data.astype(in_dtype, copy=False)\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    interpreter.set_tensor(in_idx, input_data)\n    interpreter.invoke()\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling Loop\n    # =========================\n\n    # 4.1 Get Output Tensors\n    # Typical SSD TFLite outputs:\n    # - boxes: [1, N, 4] (ymin, xmin, ymax, xmax) normalized to 0..1\n    # - classes: [1, N] (float indices)\n    # - scores: [1, N] (0..1)\n    # - num_detections: [1]\n    boxes = None\n    classes = None\n    scores = None\n    num_detections = None\n\n    for od in output_details:\n        out = interpreter.get_tensor(od['index'])\n        # Identify tensors by shape and value ranges\n        if isinstance(out, np.ndarray):\n            if out.ndim == 3 and out.shape[0] == 1 and out.shape[2] == 4:\n                boxes = out[0]\n            elif out.ndim == 2 and out.shape[0] == 1:\n                arr = out[0]\n                if arr.dtype.kind == 'f' and np.max(arr) <= 1.0 and np.min(arr) >= 0.0:\n                    # Likely scores\n                    scores = arr\n                else:\n                    # Likely classes (float indices)\n                    classes = arr\n            elif out.size == 1:\n                # num_detections\n                num_detections = int(np.round(float(out.flatten()[0])))\n\n    # Fallbacks in case num_detections not provided\n    max_detections = None\n    if boxes is not None:\n        max_detections = boxes.shape[0]\n    if scores is not None:\n        max_detections = scores.shape[0] if max_detections is None else min(max_detections, scores.shape[0])\n    if classes is not None:\n        max_detections = classes.shape[0] if max_detections is None else min(max_detections, classes.shape[0])\n    if num_detections is None:\n        num_detections = max_detections if max_detections is not None else 0\n    num_detections = int(num_detections)\n\n    # Defensive checks\n    if boxes is None or scores is None or classes is None:\n        # If model outputs aren't recognized, skip frame safely\n        cv2.putText(frame, \"Model output parsing error\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n        writer.write(frame)\n        continue\n\n    # Ensure shapes align\n    num_detections = min(num_detections, boxes.shape[0], scores.shape[0], classes.shape[0])\n\n    # 4.2 Interpret Results\n    # - Map class indices to human-readable labels if available.\n    # - Prepare list of detections for drawing and for mAP proxy accumulation.\n    dets_for_frame = []  # list of dicts: {'cid': int, 'score': float, 'box': (x1,y1,x2,y2), 'label': str}\n\n    for i in range(num_detections):\n        score = float(scores[i])\n        # Class id: handle both 0-based and 1-based indexing defensively\n        raw_cid = int(classes[i])\n        if raw_cid < 0:\n            continue\n        cid = raw_cid\n        if labels and (cid >= len(labels)) and (cid - 1) >= 0 and (cid - 1) < len(labels):\n            # If class id looks 1-based, shift to 0-based for label lookup\n            cid = cid - 1\n\n        label_name = None\n        if labels and (0 <= cid < len(labels)):\n            label_name = labels[cid]\n        else:\n            label_name = f\"id_{raw_cid}\"\n\n        # 4.3 Post-processing: scale and clip boxes to frame boundaries\n        # boxes[i] expected [ymin, xmin, ymax, xmax] normalized to 0..1\n        ymin, xmin, ymax, xmax = boxes[i]\n        # Clip normalized range\n        ymin = float(np.clip(ymin, 0.0, 1.0))\n        xmin = float(np.clip(xmin, 0.0, 1.0))\n        ymax = float(np.clip(ymax, 0.0, 1.0))\n        xmax = float(np.clip(xmax, 0.0, 1.0))\n        # Scale to frame size\n        x1 = int(xmin * frame_w)\n        y1 = int(ymin * frame_h)\n        x2 = int(xmax * frame_w)\n        y2 = int(ymax * frame_h)\n        # Clip to frame bounds\n        x1 = max(0, min(x1, frame_w - 1))\n        y1 = max(0, min(y1, frame_h - 1))\n        x2 = max(0, min(x2, frame_w - 1))\n        y2 = max(0, min(y2, frame_h - 1))\n\n        # Store detection entry\n        dets_for_frame.append({\n            'cid': cid,\n            'raw_cid': raw_cid,\n            'score': score,\n            'box': (x1, y1, x2, y2),\n            'label': label_name\n        })\n\n    # Aggregate detections for mAP proxy computation:\n    # For each class in current frame, mark the highest-score detection as TP; others as FP.\n    if dets_for_frame:\n        # Group by class\n        by_class = {}\n        for d in dets_for_frame:\n            by_class.setdefault(d['cid'], []).append(d)\n        for cid, det_list in by_class.items():\n            # Determine the top-scoring detection\n            top_idx = int(np.argmax([d['score'] for d in det_list]))\n            for idx, d in enumerate(det_list):\n                is_tp = (idx == top_idx)\n                per_class_scores_flags.setdefault(cid, []).append((d['score'], is_tp))\n\n    # Compute running mAP (proxy) across processed frames\n    running_map = compute_running_map(per_class_scores_flags)\n\n    # 4.4 Handle Output: Draw detections and overlay mAP\n    # Draw thresholded detections\n    for d in dets_for_frame:\n        if d['score'] < CONF_THRESHOLD:\n            continue  # Only draw those above threshold\n        x1, y1, x2, y2 = d['box']\n        color = class_color(d['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        label_text = f\"{d['label']}: {d['score']:.2f}\"\n        # Put label background for readability\n        (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        th_box = th + baseline + 4\n        cv2.rectangle(frame, (x1, max(0, y1 - th_box)), (x1 + tw + 4, y1), color, thickness=-1)\n        cv2.putText(frame, label_text, (x1 + 2, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n    # Overlay running mAP (proxy) at top-left\n    map_text = f\"mAP (proxy): {running_map * 100:.2f}%\"\n    (tw, th), baseline = cv2.getTextSize(map_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)\n    cv2.rectangle(frame, (8, 8), (8 + tw + 8, 8 + th + baseline + 8), (0, 0, 0), thickness=-1)\n    cv2.putText(frame, map_text, (12, 12 + th), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n\n    # Write frame to output video\n    writer.write(frame)\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nwriter.release()\n\n# Print final mAP proxy\nfinal_map = compute_running_map(per_class_scores_flags)\nprint(\"Processing completed.\")\nprint(f\"Frames processed: {frames_processed}\")\nprint(f\"Final mAP (proxy, self-consistency without ground-truth): {final_map:.4f} ({final_map*100:.2f}%)\")\nprint(f\"Output saved to: {output_path}\")"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/aed67aaf",
      "latency": 130.611,
      "total_cost": 0.085511,
      "observations": [
        {
          "id": "time-03-53-11-527701_chatcmpl-D5iqNmLh0RIbPlPXvhPgdKJViqSEG",
          "traceId": "aed67aaf",
          "type": "GENERATION",
          "name": "aed6_psg_gen_attempt#1",
          "startTime": "2026-02-05T01:53:11.527000+00:00",
          "endTime": "2026-02-05T01:55:22.138000+00:00",
          "completionStartTime": "2026-02-05T01:55:22.138000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.085511,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video file using a TFLite SSD MobileNet v1 model.\nIt writes an output video with bounding boxes and labels overlaid, and overlays a running \"mAP\" estimate\n(computed without ground-truth as a self-consistency proxy; see comments in Phase 4.2 for details).\n\nPhases implemented according to the provided Programming Guidelines:\n- Phase 1: Setup\n- Phase 2: Input Acquisition & Preprocessing Loop (explicit)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop, including:\n  - 4.2: Interpret Results (explicit)\n  - 4.3: Post-processing (explicit)\n- Phase 5: Cleanup\n\nDependencies:\n- Standard libraries: os, time, numpy, cv2 (video/image processing)\n- TFLite runtime: ai_edge_litert.interpreter.Interpreter\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = 0.5  # For drawing/display\n\n# 1.3 Load Labels (if provided and relevant)\ndef load_labels(path):\n    labels = []\n    if path and os.path.exists(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(label_path)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract primary input tensor info\nin_idx = input_details[0]['index']\nin_shape = input_details[0]['shape']\n# Expect shape [1, height, width, 3]\nif len(in_shape) != 4 or in_shape[0] != 1 or in_shape[-1] != 3:\n    raise ValueError(\"Unexpected model input shape: {}\".format(in_shape))\nin_height, in_width = int(in_shape[1]), int(in_shape[2])\nin_dtype = input_details[0]['dtype']\nfloating_model = (in_dtype == np.float32)\n\n# Helper to get consistent color per class id\ndef class_color(cid):\n    # Generate a color based on the class id for consistency.\n    np.random.seed(cid + 12345)\n    color = tuple(int(x) for x in np.random.randint(0, 255, size=3))\n    # OpenCV uses BGR\n    return (color[0], color[1], color[2])\n\n# Directory prep for output\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# Input Method: Read a single video file from the given input_path\n# =========================\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise RuntimeError(\"Failed to open input video: {}\".format(input_path))\n\n# Read input video properties\nfps = cap.get(cv2.CAP_PROP_FPS)\nif not fps or np.isnan(fps) or fps <= 0:\n    fps = 25.0  # Fallback if FPS is unavailable\nframe_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Setup VideoWriter for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\nif not writer.isOpened():\n    cap.release()\n    raise RuntimeError(\"Failed to open output video for writing: {}\".format(output_path))\n\n# Structures for running \"mAP\" proxy computation (no ground-truth available)\n# We compute a self-consistency proxy AP per class:\n# - For each frame and class, the highest-scoring detection is treated as TP candidate (1 per frame),\n#   and all other detections for that class in that frame are treated as FP.\n# - Aggregate across frames and compute AP as area under precision-recall curve over scores.\nper_class_scores_flags = {}  # dict: class_id -> list of tuples (score, is_tp)\nframes_processed = 0\n\ndef compute_ap_from_scores_flags(sf_list):\n    \"\"\"\n    Compute AP from list of (score, is_tp) entries for a single class, across frames processed so far.\n    Uses a standard precision-recall area with monotonically decreasing precision envelope.\n    If there are no positives (no frames with detections), returns None to indicate \"skip in mAP\".\n    \"\"\"\n    if not sf_list:\n        return None\n\n    # Count positives (is_tp True)\n    num_positives = sum(1 for _, is_tp in sf_list if is_tp)\n    if num_positives == 0:\n        return None\n\n    # Sort by score descending\n    sf_sorted = sorted(sf_list, key=lambda x: x[0], reverse=True)\n\n    tp_cum = 0\n    fp_cum = 0\n    precisions = []\n    recalls = []\n    for score, is_tp in sf_sorted:\n        if is_tp:\n            tp_cum += 1\n        else:\n            fp_cum += 1\n        precision = tp_cum / (tp_cum + fp_cum)\n        recall = tp_cum / num_positives\n        precisions.append(precision)\n        recalls.append(recall)\n\n    # Compute AP via precision envelope integration (VOC 2010 style)\n    precisions = np.array(precisions, dtype=np.float32)\n    recalls = np.array(recalls, dtype=np.float32)\n\n    # Monotonic precision envelope\n    for i in range(len(precisions) - 2, -1, -1):\n        if precisions[i] < precisions[i + 1]:\n            precisions[i] = precisions[i + 1]\n\n    # Identify points where recall changes\n    # We need to include the starting (0, p0) and ending points; handle accordingly\n    unique_recalls, indices = np.unique(recalls, return_index=True)\n    # Sort indices to have increasing recall\n    order = np.argsort(unique_recalls)\n    unique_recalls = unique_recalls[order]\n    indices = indices[order]\n\n    # For integration, we can approximate as sum over recall diffs with corresponding precision\n    ap = 0.0\n    prev_recall = 0.0\n    for r, idx in zip(unique_recalls, indices):\n        ap += precisions[idx] * (r - prev_recall)\n        prev_recall = r\n\n    return float(ap)\n\ndef compute_running_map(per_class_dict):\n    aps = []\n    for cid, sf_list in per_class_dict.items():\n        ap = compute_ap_from_scores_flags(sf_list)\n        if ap is not None:\n            aps.append(ap)\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\n# Main processing loop\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break  # End of video\n\n    frames_processed += 1\n\n    # 2.2 Preprocess data to match model input\n    # Convert BGR to RGB, resize to model input size, add batch dimension\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (in_width, in_height), interpolation=cv2.INTER_LINEAR)\n    input_data = np.expand_dims(resized, axis=0)\n\n    # Convert dtype and normalize if floating model\n    if floating_model:\n        # Normalize to [-1, 1] as typical for SSD MobileNet v1\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # Ensure dtype matches (usually uint8)\n        input_data = input_data.astype(in_dtype, copy=False)\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    interpreter.set_tensor(in_idx, input_data)\n    interpreter.invoke()\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling Loop\n    # =========================\n\n    # 4.1 Get Output Tensors\n    # Typical SSD TFLite outputs:\n    # - boxes: [1, N, 4] (ymin, xmin, ymax, xmax) normalized to 0..1\n    # - classes: [1, N] (float indices)\n    # - scores: [1, N] (0..1)\n    # - num_detections: [1]\n    boxes = None\n    classes = None\n    scores = None\n    num_detections = None\n\n    for od in output_details:\n        out = interpreter.get_tensor(od['index'])\n        # Identify tensors by shape and value ranges\n        if isinstance(out, np.ndarray):\n            if out.ndim == 3 and out.shape[0] == 1 and out.shape[2] == 4:\n                boxes = out[0]\n            elif out.ndim == 2 and out.shape[0] == 1:\n                arr = out[0]\n                if arr.dtype.kind == 'f' and np.max(arr) <= 1.0 and np.min(arr) >= 0.0:\n                    # Likely scores\n                    scores = arr\n                else:\n                    # Likely classes (float indices)\n                    classes = arr\n            elif out.size == 1:\n                # num_detections\n                num_detections = int(np.round(float(out.flatten()[0])))\n\n    # Fallbacks in case num_detections not provided\n    max_detections = None\n    if boxes is not None:\n        max_detections = boxes.shape[0]\n    if scores is not None:\n        max_detections = scores.shape[0] if max_detections is None else min(max_detections, scores.shape[0])\n    if classes is not None:\n        max_detections = classes.shape[0] if max_detections is None else min(max_detections, classes.shape[0])\n    if num_detections is None:\n        num_detections = max_detections if max_detections is not None else 0\n    num_detections = int(num_detections)\n\n    # Defensive checks\n    if boxes is None or scores is None or classes is None:\n        # If model outputs aren't recognized, skip frame safely\n        cv2.putText(frame, \"Model output parsing error\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n        writer.write(frame)\n        continue\n\n    # Ensure shapes align\n    num_detections = min(num_detections, boxes.shape[0], scores.shape[0], classes.shape[0])\n\n    # 4.2 Interpret Results\n    # - Map class indices to human-readable labels if available.\n    # - Prepare list of detections for drawing and for mAP proxy accumulation.\n    dets_for_frame = []  # list of dicts: {'cid': int, 'score': float, 'box': (x1,y1,x2,y2), 'label': str}\n\n    for i in range(num_detections):\n        score = float(scores[i])\n        # Class id: handle both 0-based and 1-based indexing defensively\n        raw_cid = int(classes[i])\n        if raw_cid < 0:\n            continue\n        cid = raw_cid\n        if labels and (cid >= len(labels)) and (cid - 1) >= 0 and (cid - 1) < len(labels):\n            # If class id looks 1-based, shift to 0-based for label lookup\n            cid = cid - 1\n\n        label_name = None\n        if labels and (0 <= cid < len(labels)):\n            label_name = labels[cid]\n        else:\n            label_name = f\"id_{raw_cid}\"\n\n        # 4.3 Post-processing: scale and clip boxes to frame boundaries\n        # boxes[i] expected [ymin, xmin, ymax, xmax] normalized to 0..1\n        ymin, xmin, ymax, xmax = boxes[i]\n        # Clip normalized range\n        ymin = float(np.clip(ymin, 0.0, 1.0))\n        xmin = float(np.clip(xmin, 0.0, 1.0))\n        ymax = float(np.clip(ymax, 0.0, 1.0))\n        xmax = float(np.clip(xmax, 0.0, 1.0))\n        # Scale to frame size\n        x1 = int(xmin * frame_w)\n        y1 = int(ymin * frame_h)\n        x2 = int(xmax * frame_w)\n        y2 = int(ymax * frame_h)\n        # Clip to frame bounds\n        x1 = max(0, min(x1, frame_w - 1))\n        y1 = max(0, min(y1, frame_h - 1))\n        x2 = max(0, min(x2, frame_w - 1))\n        y2 = max(0, min(y2, frame_h - 1))\n\n        # Store detection entry\n        dets_for_frame.append({\n            'cid': cid,\n            'raw_cid': raw_cid,\n            'score': score,\n            'box': (x1, y1, x2, y2),\n            'label': label_name\n        })\n\n    # Aggregate detections for mAP proxy computation:\n    # For each class in current frame, mark the highest-score detection as TP; others as FP.\n    if dets_for_frame:\n        # Group by class\n        by_class = {}\n        for d in dets_for_frame:\n            by_class.setdefault(d['cid'], []).append(d)\n        for cid, det_list in by_class.items():\n            # Determine the top-scoring detection\n            top_idx = int(np.argmax([d['score'] for d in det_list]))\n            for idx, d in enumerate(det_list):\n                is_tp = (idx == top_idx)\n                per_class_scores_flags.setdefault(cid, []).append((d['score'], is_tp))\n\n    # Compute running mAP (proxy) across processed frames\n    running_map = compute_running_map(per_class_scores_flags)\n\n    # 4.4 Handle Output: Draw detections and overlay mAP\n    # Draw thresholded detections\n    for d in dets_for_frame:\n        if d['score'] < CONF_THRESHOLD:\n            continue  # Only draw those above threshold\n        x1, y1, x2, y2 = d['box']\n        color = class_color(d['cid'])\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        label_text = f\"{d['label']}: {d['score']:.2f}\"\n        # Put label background for readability\n        (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        th_box = th + baseline + 4\n        cv2.rectangle(frame, (x1, max(0, y1 - th_box)), (x1 + tw + 4, y1), color, thickness=-1)\n        cv2.putText(frame, label_text, (x1 + 2, y1 - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n    # Overlay running mAP (proxy) at top-left\n    map_text = f\"mAP (proxy): {running_map * 100:.2f}%\"\n    (tw, th), baseline = cv2.getTextSize(map_text, cv2.FONT_HERSHEY_SIMPLEX, 0.8, 2)\n    cv2.rectangle(frame, (8, 8), (8 + tw + 8, 8 + th + baseline + 8), (0, 0, 0), thickness=-1)\n    cv2.putText(frame, map_text, (12, 12 + th), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)\n\n    # Write frame to output video\n    writer.write(frame)\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nwriter.release()\n\n# Print final mAP proxy\nfinal_map = compute_running_map(per_class_scores_flags)\nprint(\"Processing completed.\")\nprint(f\"Frames processed: {frames_processed}\")\nprint(f\"Final mAP (proxy, self-consistency without ground-truth): {final_map:.4f} ({final_map*100:.2f}%)\")\nprint(f\"Output saved to: {output_path}\")\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 8522,
            "total": 10022,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.085511,
          "latency": 130611.0,
          "timeToFirstToken": 130.611,
          "updatedAt": "2026-02-05T01:55:27.728Z",
          "totalTokens": 10022,
          "costDetails": {
            "total": 0.085511
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 8522,
          "usageDetails": {
            "input": 1500,
            "output": 8522,
            "total": 10022
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T01:55:22.675Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T01:56:19.704Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T01:53:11.461Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "7fd470c2",
      "timestamp": "2026-02-05T01:50:31.961000+00:00",
      "name": "7fd4_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n# Application: Object Detection via a video file\n# Deployment target: Raspberry Pi 4B\n#\n# This script follows the provided TinyML Programming Guideline phases.\n# It performs TFLite inference using ai_edge_litert on a video file input,\n# draws detection rectangles with labels on each frame, writes the annotated\n# video to the specified output path, and overlays a computed mAP value.\n# Note: Without provided ground-truth annotations, mAP is not computable;\n# this script reports mAP as 0.0 (no ground truth) and overlays it.\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# -----------------------------\n# Phase 1: Setup\n# -----------------------------\n# 1.1 Imports (Interpreter)\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from configuration)\nmodel_path  = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path  = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path  = \"data/object_detection/sheeps.mp4\"\noutput_path  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')  # Confidence Threshold: '0.5'\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n\ndef load_labels(path):\n    \"\"\"\n    Phase 1.3 Load Labels (conditional)\n    Reads label file into a list of strings.\n    \"\"\"\n    labels = []\n    try:\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name:\n                    labels.append(name)\n    except FileNotFoundError:\n        # If labels are missing, we still proceed, mapping unknown labels by id.\n        labels = []\n    return labels\n\n\ndef prepare_interpreter(model_path_):\n    \"\"\"\n    Phase 1.4 Load Interpreter and allocate tensors\n    Phase 1.5 Get Model Details\n    Returns interpreter, input_details, output_details.\n    \"\"\"\n    interpreter_ = Interpreter(model_path=model_path_)\n    interpreter_.allocate_tensors()\n    input_details_ = interpreter_.get_input_details()\n    output_details_ = interpreter_.get_output_details()\n    return interpreter_, input_details_, output_details_\n\n\ndef preprocess_frame_bgr_to_input(frame_bgr, input_shape, floating_model):\n    \"\"\"\n    Phase 2.2 Preprocess Data\n    - Convert BGR to RGB\n    - Resize to model input size\n    - Expand dims to [1, H, W, C]\n    - Normalize if floating model\n    \"\"\"\n    _, height, width, channels = input_shape  # NHWC\n    # Convert BGR -> RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # Resize to model input size\n    resized = cv2.resize(frame_rgb, (width, height))\n    # Ensure dtype\n    if floating_model:\n        input_data = np.asarray(resized, dtype=np.float32)\n        input_data = (input_data - 127.5) / 127.5  # normalize to [-1, 1]\n    else:\n        input_data = np.asarray(resized, dtype=np.uint8)\n    # Add batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n\ndef clip_bbox(ymin, xmin, ymax, xmax):\n    \"\"\"\n    Phase 4.3 Post-processing helper: Clip bounding boxes to [0,1]\n    \"\"\"\n    ymin = min(max(ymin, 0.0), 1.0)\n    xmin = min(max(xmin, 0.0), 1.0)\n    ymax = min(max(ymax, 0.0), 1.0)\n    xmax = min(max(xmax, 0.0), 1.0)\n    return ymin, xmin, ymax, xmax\n\n\ndef get_color_for_id(id_int):\n    \"\"\"\n    Deterministic color generation for a given integer id.\n    \"\"\"\n    # Simple hash-based color\n    r = (37 * (id_int + 1)) % 255\n    g = (17 * (id_int + 7)) % 255\n    b = (29 * (id_int + 13)) % 255\n    return int(b), int(g), int(r)\n\n\ndef map_outputs_first_time(output_arrays):\n    \"\"\"\n    Determine indices for boxes, classes, scores, and num_detections arrays\n    by inspecting output shapes and value ranges.\n\n    Expected arrays:\n      - boxes: [1, N, 4]\n      - classes: [1, N]\n      - scores: [1, N]\n      - num_detections: [1]\n    \"\"\"\n    idx_boxes = None\n    idx_classes = None\n    idx_scores = None\n    idx_num = None\n\n    # Identify boxes and num_detections by shapes\n    for i, arr in enumerate(output_arrays):\n        if arr.ndim == 3 and arr.shape[-1] == 4:\n            idx_boxes = i\n        elif arr.size == 1:\n            idx_num = i\n\n    # Identify classes vs scores among 2D outputs\n    candidate_1d2 = [i for i, arr in enumerate(output_arrays) if arr.ndim == 2]\n    # Heuristic: scores are in [0,1], classes typically > 1\n    if len(candidate_1d2) == 2:\n        a_idx, b_idx = candidate_1d2\n        a = output_arrays[a_idx]\n        b = output_arrays[b_idx]\n        a_min, a_max = float(np.min(a)), float(np.max(a))\n        b_min, b_max = float(np.min(b)), float(np.max(b))\n        # If a is likely scores (0..1)\n        if 0.0 <= a_min and a_max <= 1.0:\n            idx_scores = a_idx\n            idx_classes = b_idx\n        # If b is likely scores (0..1)\n        elif 0.0 <= b_min and b_max <= 1.0:\n            idx_scores = b_idx\n            idx_classes = a_idx\n        else:\n            # Fallback: assume order [boxes, classes, scores, num]\n            # Find one with larger range as classes\n            # We'll assign arbitrarily if not sure.\n            idx_classes = a_idx\n            idx_scores = b_idx\n\n    return idx_boxes, idx_classes, idx_scores, idx_num\n\n\ndef draw_detections_on_frame(frame_bgr, boxes, classes, scores, num_det, labels, conf_thr):\n    \"\"\"\n    Phase 4.2 Interpretation & Phase 4.3 Post-processing\n    - Threshold by confidence\n    - Scale boxes to frame size\n    - Clip bounding boxes to valid ranges\n    - Draw rectangles and label texts\n    \"\"\"\n    h, w = frame_bgr.shape[:2]\n    num = int(num_det)\n    for i in range(num):\n        score = float(scores[0, i]) if scores.ndim == 2 else float(scores[i])\n        if score < conf_thr:\n            continue\n        class_id_raw = int(classes[0, i]) if classes.ndim == 2 else int(classes[i])\n        # Map label\n        if 0 <= class_id_raw < len(labels):\n            label_name = labels[class_id_raw]\n        else:\n            label_name = f\"id_{class_id_raw}\"\n        # Get box and clip\n        if boxes.ndim == 3:\n            ymin, xmin, ymax, xmax = boxes[0, i]\n        else:\n            ymin, xmin, ymax, xmax = boxes[i]\n        ymin, xmin, ymax, xmax = clip_bbox(float(ymin), float(xmin), float(ymax), float(xmax))\n        # Scale to pixel coords\n        x1 = int(xmin * w)\n        y1 = int(ymin * h)\n        x2 = int(xmax * w)\n        y2 = int(ymax * h)\n        # Ensure valid rectangle\n        x1 = max(0, min(x1, w - 1))\n        y1 = max(0, min(y1, h - 1))\n        x2 = max(0, min(x2, w - 1))\n        y2 = max(0, min(y2, h - 1))\n        # Draw rectangle\n        color = get_color_for_id(class_id_raw)\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        # Draw label text with background\n        caption = f\"{label_name}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        # Background rectangle\n        box_x1 = x1\n        box_y1 = max(0, y1 - th - baseline - 4)\n        box_x2 = min(w - 1, x1 + tw + 4)\n        box_y2 = y1\n        cv2.rectangle(frame_bgr, (box_x1, box_y1), (box_x2, box_y2), color, thickness=-1)\n        # Put text\n        cv2.putText(frame_bgr, caption, (x1 + 2, y1 - baseline - 2),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), thickness=1, lineType=cv2.LINE_AA)\n    return frame_bgr\n\n\ndef overlay_info(frame_bgr, text, at_top=True):\n    \"\"\"\n    Draws informational text onto the frame at top-left or bottom-left.\n    \"\"\"\n    h, w = frame_bgr.shape[:2]\n    org_y = 20 if at_top else (h - 10)\n    cv2.putText(frame_bgr, text, (10, org_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (10, 240, 10), 2, cv2.LINE_AA)\n\n\ndef main():\n    # Load labels (Phase 1.3)\n    labels = load_labels(label_path)\n\n    # Load Interpreter and get model details (Phase 1.4 & 1.5)\n    interpreter, input_details, output_details = prepare_interpreter(model_path)\n\n    # Extract input tensor details\n    input_shape = input_details[0]['shape']  # [1, height, width, channels]\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n\n    # -----------------------------\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # -----------------------------\n    # 2.1 Acquire input data: Read a single video file from input_path\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    # Prepare output video writer\n    in_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 25.0  # fallback if fps is not available\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (in_width, in_height))\n    if not out.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {output_path}\")\n\n    # Pre-calculate a \"mAP\" value; without GT it is 0.0\n    # This is a computed constant noted in the overlay.\n    computed_map = 0.0\n    map_note = f\"mAP (no GT): {computed_map:.3f}\"\n\n    # Variables for performance statistics\n    frame_count = 0\n    t_inference_total = 0.0\n\n    # Output mapping indices (detected after first inference)\n    out_idx_boxes = None\n    out_idx_classes = None\n    out_idx_scores = None\n    out_idx_num = None\n\n    # 2.4 Loop control: process all frames from the video file\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # 2.2 Preprocess frame\n        input_data = preprocess_frame_bgr_to_input(frame_bgr, input_shape, floating_model)\n\n        # 2.3 Quantization handling already applied in preprocess based on floating_model\n\n        # -----------------------------\n        # Phase 3: Inference\n        # -----------------------------\n        # 3.1 Set input tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        # 3.2 Run inference\n        t0 = time.time()\n        interpreter.invoke()\n        t1 = time.time()\n        t_inference_total += (t1 - t0)\n\n        # -----------------------------\n        # Phase 4: Output Interpretation & Handling Loop\n        # -----------------------------\n        # 4.1 Get output tensors\n        output_arrays = [interpreter.get_tensor(od['index']) for od in output_details]\n\n        # Determine output indices on first frame\n        if (out_idx_boxes is None) or (out_idx_classes is None) or (out_idx_scores is None) or (out_idx_num is None):\n            out_idx_boxes, out_idx_classes, out_idx_scores, out_idx_num = map_outputs_first_time(output_arrays)\n            # Safety checks\n            if out_idx_boxes is None or out_idx_scores is None or out_idx_classes is None or out_idx_num is None:\n                # Attempt fallback to common order [boxes, classes, scores, num_detections]\n                if len(output_arrays) >= 4:\n                    out_idx_boxes = out_idx_boxes if out_idx_boxes is not None else 0\n                    out_idx_classes = out_idx_classes if out_idx_classes is not None else 1\n                    out_idx_scores = out_idx_scores if out_idx_scores is not None else 2\n                    out_idx_num = out_idx_num if out_idx_num is not None else 3\n                else:\n                    raise RuntimeError(\"Unable to map model outputs to boxes/classes/scores/num_detections.\")\n\n        # 4.2 Interpret Results\n        boxes = output_arrays[out_idx_boxes]\n        classes = output_arrays[out_idx_classes]\n        scores = output_arrays[out_idx_scores]\n        num_detections = output_arrays[out_idx_num]\n        # Ensure correct dtypes\n        if classes.dtype != np.int32 and classes.dtype != np.int64:\n            # SSD TFLite often returns float class indices; cast to int after usage.\n            pass  # We'll cast when indexing\n        # num_detections may be float or int\n        num_det_val = int(num_detections[0]) if num_detections.size == 1 else int(num_detections.flatten()[0])\n\n        # 4.3 Post-processing (confidence thresholding, scaling, clipping are handled in helper)\n        frame_bgr = draw_detections_on_frame(\n            frame_bgr=frame_bgr,\n            boxes=boxes,\n            classes=classes.astype(np.int32) if classes.dtype != np.int32 else classes,\n            scores=scores,\n            num_det=num_det_val,\n            labels=labels,\n            conf_thr=CONF_THRESHOLD\n        )\n\n        # 4.4 Handle Output: overlay mAP and FPS info, then write to file\n        # Compute instantaneous FPS for display\n        inf_ms = (t1 - t0) * 1000.0\n        fps_inst = 1000.0 / inf_ms if inf_ms > 0 else 0.0\n        overlay_info(frame_bgr, f\"{map_note}\", at_top=True)\n        overlay_info(frame_bgr, f\"Inference: {inf_ms:.1f} ms ({fps_inst:.1f} FPS)\", at_top=False)\n        out.write(frame_bgr)\n\n        # 4.5 Loop continuation handled by while True; break when frames end\n\n    # -----------------------------\n    # Phase 5: Cleanup\n    # -----------------------------\n    cap.release()\n    out.release()\n\n    # Summary logging\n    if frame_count > 0:\n        avg_inf_ms = (t_inference_total / frame_count) * 1000.0\n        print(f\"Processed {frame_count} frames.\")\n        print(f\"Average inference time: {avg_inf_ms:.2f} ms per frame.\")\n        print(f\"Output video saved to: {output_path}\")\n        print(f\"Reported mAP (no ground truth provided): {computed_map:.3f}\")\n    else:\n        print(\"No frames were processed. Please check the input video file.\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/7fd470c2",
      "latency": 102.545,
      "total_cost": 0.075221,
      "observations": [
        {
          "id": "time-03-50-32-635415_chatcmpl-D5inoF8cxMvgmqGpSe6eTQtY4voJZ",
          "traceId": "7fd470c2",
          "type": "GENERATION",
          "name": "7fd4_psg_gen_attempt#1",
          "startTime": "2026-02-05T01:50:32.635000+00:00",
          "endTime": "2026-02-05T01:52:15.180000+00:00",
          "completionStartTime": "2026-02-05T01:52:15.180000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.07522100000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n# Application: Object Detection via a video file\n# Deployment target: Raspberry Pi 4B\n#\n# This script follows the provided TinyML Programming Guideline phases.\n# It performs TFLite inference using ai_edge_litert on a video file input,\n# draws detection rectangles with labels on each frame, writes the annotated\n# video to the specified output path, and overlays a computed mAP value.\n# Note: Without provided ground-truth annotations, mAP is not computable;\n# this script reports mAP as 0.0 (no ground truth) and overlays it.\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# -----------------------------\n# Phase 1: Setup\n# -----------------------------\n# 1.1 Imports (Interpreter)\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from configuration)\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = float('0.5')  # Confidence Threshold: '0.5'\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n\ndef load_labels(path):\n    \"\"\"\n    Phase 1.3 Load Labels (conditional)\n    Reads label file into a list of strings.\n    \"\"\"\n    labels = []\n    try:\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name:\n                    labels.append(name)\n    except FileNotFoundError:\n        # If labels are missing, we still proceed, mapping unknown labels by id.\n        labels = []\n    return labels\n\n\ndef prepare_interpreter(model_path_):\n    \"\"\"\n    Phase 1.4 Load Interpreter and allocate tensors\n    Phase 1.5 Get Model Details\n    Returns interpreter, input_details, output_details.\n    \"\"\"\n    interpreter_ = Interpreter(model_path=model_path_)\n    interpreter_.allocate_tensors()\n    input_details_ = interpreter_.get_input_details()\n    output_details_ = interpreter_.get_output_details()\n    return interpreter_, input_details_, output_details_\n\n\ndef preprocess_frame_bgr_to_input(frame_bgr, input_shape, floating_model):\n    \"\"\"\n    Phase 2.2 Preprocess Data\n    - Convert BGR to RGB\n    - Resize to model input size\n    - Expand dims to [1, H, W, C]\n    - Normalize if floating model\n    \"\"\"\n    _, height, width, channels = input_shape  # NHWC\n    # Convert BGR -> RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # Resize to model input size\n    resized = cv2.resize(frame_rgb, (width, height))\n    # Ensure dtype\n    if floating_model:\n        input_data = np.asarray(resized, dtype=np.float32)\n        input_data = (input_data - 127.5) / 127.5  # normalize to [-1, 1]\n    else:\n        input_data = np.asarray(resized, dtype=np.uint8)\n    # Add batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n\ndef clip_bbox(ymin, xmin, ymax, xmax):\n    \"\"\"\n    Phase 4.3 Post-processing helper: Clip bounding boxes to [0,1]\n    \"\"\"\n    ymin = min(max(ymin, 0.0), 1.0)\n    xmin = min(max(xmin, 0.0), 1.0)\n    ymax = min(max(ymax, 0.0), 1.0)\n    xmax = min(max(xmax, 0.0), 1.0)\n    return ymin, xmin, ymax, xmax\n\n\ndef get_color_for_id(id_int):\n    \"\"\"\n    Deterministic color generation for a given integer id.\n    \"\"\"\n    # Simple hash-based color\n    r = (37 * (id_int + 1)) % 255\n    g = (17 * (id_int + 7)) % 255\n    b = (29 * (id_int + 13)) % 255\n    return int(b), int(g), int(r)\n\n\ndef map_outputs_first_time(output_arrays):\n    \"\"\"\n    Determine indices for boxes, classes, scores, and num_detections arrays\n    by inspecting output shapes and value ranges.\n\n    Expected arrays:\n      - boxes: [1, N, 4]\n      - classes: [1, N]\n      - scores: [1, N]\n      - num_detections: [1]\n    \"\"\"\n    idx_boxes = None\n    idx_classes = None\n    idx_scores = None\n    idx_num = None\n\n    # Identify boxes and num_detections by shapes\n    for i, arr in enumerate(output_arrays):\n        if arr.ndim == 3 and arr.shape[-1] == 4:\n            idx_boxes = i\n        elif arr.size == 1:\n            idx_num = i\n\n    # Identify classes vs scores among 2D outputs\n    candidate_1d2 = [i for i, arr in enumerate(output_arrays) if arr.ndim == 2]\n    # Heuristic: scores are in [0,1], classes typically > 1\n    if len(candidate_1d2) == 2:\n        a_idx, b_idx = candidate_1d2\n        a = output_arrays[a_idx]\n        b = output_arrays[b_idx]\n        a_min, a_max = float(np.min(a)), float(np.max(a))\n        b_min, b_max = float(np.min(b)), float(np.max(b))\n        # If a is likely scores (0..1)\n        if 0.0 <= a_min and a_max <= 1.0:\n            idx_scores = a_idx\n            idx_classes = b_idx\n        # If b is likely scores (0..1)\n        elif 0.0 <= b_min and b_max <= 1.0:\n            idx_scores = b_idx\n            idx_classes = a_idx\n        else:\n            # Fallback: assume order [boxes, classes, scores, num]\n            # Find one with larger range as classes\n            # We'll assign arbitrarily if not sure.\n            idx_classes = a_idx\n            idx_scores = b_idx\n\n    return idx_boxes, idx_classes, idx_scores, idx_num\n\n\ndef draw_detections_on_frame(frame_bgr, boxes, classes, scores, num_det, labels, conf_thr):\n    \"\"\"\n    Phase 4.2 Interpretation & Phase 4.3 Post-processing\n    - Threshold by confidence\n    - Scale boxes to frame size\n    - Clip bounding boxes to valid ranges\n    - Draw rectangles and label texts\n    \"\"\"\n    h, w = frame_bgr.shape[:2]\n    num = int(num_det)\n    for i in range(num):\n        score = float(scores[0, i]) if scores.ndim == 2 else float(scores[i])\n        if score < conf_thr:\n            continue\n        class_id_raw = int(classes[0, i]) if classes.ndim == 2 else int(classes[i])\n        # Map label\n        if 0 <= class_id_raw < len(labels):\n            label_name = labels[class_id_raw]\n        else:\n            label_name = f\"id_{class_id_raw}\"\n        # Get box and clip\n        if boxes.ndim == 3:\n            ymin, xmin, ymax, xmax = boxes[0, i]\n        else:\n            ymin, xmin, ymax, xmax = boxes[i]\n        ymin, xmin, ymax, xmax = clip_bbox(float(ymin), float(xmin), float(ymax), float(xmax))\n        # Scale to pixel coords\n        x1 = int(xmin * w)\n        y1 = int(ymin * h)\n        x2 = int(xmax * w)\n        y2 = int(ymax * h)\n        # Ensure valid rectangle\n        x1 = max(0, min(x1, w - 1))\n        y1 = max(0, min(y1, h - 1))\n        x2 = max(0, min(x2, w - 1))\n        y2 = max(0, min(y2, h - 1))\n        # Draw rectangle\n        color = get_color_for_id(class_id_raw)\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        # Draw label text with background\n        caption = f\"{label_name}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        # Background rectangle\n        box_x1 = x1\n        box_y1 = max(0, y1 - th - baseline - 4)\n        box_x2 = min(w - 1, x1 + tw + 4)\n        box_y2 = y1\n        cv2.rectangle(frame_bgr, (box_x1, box_y1), (box_x2, box_y2), color, thickness=-1)\n        # Put text\n        cv2.putText(frame_bgr, caption, (x1 + 2, y1 - baseline - 2),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), thickness=1, lineType=cv2.LINE_AA)\n    return frame_bgr\n\n\ndef overlay_info(frame_bgr, text, at_top=True):\n    \"\"\"\n    Draws informational text onto the frame at top-left or bottom-left.\n    \"\"\"\n    h, w = frame_bgr.shape[:2]\n    org_y = 20 if at_top else (h - 10)\n    cv2.putText(frame_bgr, text, (10, org_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (10, 240, 10), 2, cv2.LINE_AA)\n\n\ndef main():\n    # Load labels (Phase 1.3)\n    labels = load_labels(label_path)\n\n    # Load Interpreter and get model details (Phase 1.4 & 1.5)\n    interpreter, input_details, output_details = prepare_interpreter(model_path)\n\n    # Extract input tensor details\n    input_shape = input_details[0]['shape']  # [1, height, width, channels]\n    input_dtype = input_details[0]['dtype']\n    floating_model = (input_dtype == np.float32)\n\n    # -----------------------------\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # -----------------------------\n    # 2.1 Acquire input data: Read a single video file from input_path\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    # Prepare output video writer\n    in_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 25.0  # fallback if fps is not available\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (in_width, in_height))\n    if not out.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {output_path}\")\n\n    # Pre-calculate a \"mAP\" value; without GT it is 0.0\n    # This is a computed constant noted in the overlay.\n    computed_map = 0.0\n    map_note = f\"mAP (no GT): {computed_map:.3f}\"\n\n    # Variables for performance statistics\n    frame_count = 0\n    t_inference_total = 0.0\n\n    # Output mapping indices (detected after first inference)\n    out_idx_boxes = None\n    out_idx_classes = None\n    out_idx_scores = None\n    out_idx_num = None\n\n    # 2.4 Loop control: process all frames from the video file\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # 2.2 Preprocess frame\n        input_data = preprocess_frame_bgr_to_input(frame_bgr, input_shape, floating_model)\n\n        # 2.3 Quantization handling already applied in preprocess based on floating_model\n\n        # -----------------------------\n        # Phase 3: Inference\n        # -----------------------------\n        # 3.1 Set input tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        # 3.2 Run inference\n        t0 = time.time()\n        interpreter.invoke()\n        t1 = time.time()\n        t_inference_total += (t1 - t0)\n\n        # -----------------------------\n        # Phase 4: Output Interpretation & Handling Loop\n        # -----------------------------\n        # 4.1 Get output tensors\n        output_arrays = [interpreter.get_tensor(od['index']) for od in output_details]\n\n        # Determine output indices on first frame\n        if (out_idx_boxes is None) or (out_idx_classes is None) or (out_idx_scores is None) or (out_idx_num is None):\n            out_idx_boxes, out_idx_classes, out_idx_scores, out_idx_num = map_outputs_first_time(output_arrays)\n            # Safety checks\n            if out_idx_boxes is None or out_idx_scores is None or out_idx_classes is None or out_idx_num is None:\n                # Attempt fallback to common order [boxes, classes, scores, num_detections]\n                if len(output_arrays) >= 4:\n                    out_idx_boxes = out_idx_boxes if out_idx_boxes is not None else 0\n                    out_idx_classes = out_idx_classes if out_idx_classes is not None else 1\n                    out_idx_scores = out_idx_scores if out_idx_scores is not None else 2\n                    out_idx_num = out_idx_num if out_idx_num is not None else 3\n                else:\n                    raise RuntimeError(\"Unable to map model outputs to boxes/classes/scores/num_detections.\")\n\n        # 4.2 Interpret Results\n        boxes = output_arrays[out_idx_boxes]\n        classes = output_arrays[out_idx_classes]\n        scores = output_arrays[out_idx_scores]\n        num_detections = output_arrays[out_idx_num]\n        # Ensure correct dtypes\n        if classes.dtype != np.int32 and classes.dtype != np.int64:\n            # SSD TFLite often returns float class indices; cast to int after usage.\n            pass  # We'll cast when indexing\n        # num_detections may be float or int\n        num_det_val = int(num_detections[0]) if num_detections.size == 1 else int(num_detections.flatten()[0])\n\n        # 4.3 Post-processing (confidence thresholding, scaling, clipping are handled in helper)\n        frame_bgr = draw_detections_on_frame(\n            frame_bgr=frame_bgr,\n            boxes=boxes,\n            classes=classes.astype(np.int32) if classes.dtype != np.int32 else classes,\n            scores=scores,\n            num_det=num_det_val,\n            labels=labels,\n            conf_thr=CONF_THRESHOLD\n        )\n\n        # 4.4 Handle Output: overlay mAP and FPS info, then write to file\n        # Compute instantaneous FPS for display\n        inf_ms = (t1 - t0) * 1000.0\n        fps_inst = 1000.0 / inf_ms if inf_ms > 0 else 0.0\n        overlay_info(frame_bgr, f\"{map_note}\", at_top=True)\n        overlay_info(frame_bgr, f\"Inference: {inf_ms:.1f} ms ({fps_inst:.1f} FPS)\", at_top=False)\n        out.write(frame_bgr)\n\n        # 4.5 Loop continuation handled by while True; break when frames end\n\n    # -----------------------------\n    # Phase 5: Cleanup\n    # -----------------------------\n    cap.release()\n    out.release()\n\n    # Summary logging\n    if frame_count > 0:\n        avg_inf_ms = (t_inference_total / frame_count) * 1000.0\n        print(f\"Processed {frame_count} frames.\")\n        print(f\"Average inference time: {avg_inf_ms:.2f} ms per frame.\")\n        print(f\"Output video saved to: {output_path}\")\n        print(f\"Reported mAP (no ground truth provided): {computed_map:.3f}\")\n    else:\n        print(\"No frames were processed. Please check the input video file.\")\n\n\nif __name__ == \"__main__\":\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 7493,
            "total": 8993,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.075221,
          "latency": 102545.0,
          "timeToFirstToken": 102.545,
          "updatedAt": "2026-02-05T01:52:20.837Z",
          "totalTokens": 8993,
          "costDetails": {
            "total": 0.075221
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 7493,
          "usageDetails": {
            "input": 1500,
            "output": 7493,
            "total": 8993
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T01:52:15.794Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T01:53:05.485Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T01:50:32.543Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "2c2a2293",
      "timestamp": "2026-02-05T01:47:26.189000+00:00",
      "name": "2c2a_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection using a TFLite SSD model on a single input video file,\ndraws bounding boxes and labels on detected objects, writes the annotated video to an output path,\nand computes a proxy mAP (mean Average Precision) based on temporal consistency between consecutive frames.\n\nProgramming Guideline Phases are explicitly followed and annotated in the code.\n\"\"\"\n\n# =========================\n# Phase 1: Setup\n# =========================\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# 1.1 Imports: Import interpreter literally by the specified path\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = 0.5  # Confidence Threshold\n\n# Utility: Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (Conditional)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if len(name) > 0:\n                    labels.append(name)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Assuming single input tensor for SSD models\ninput_shape = input_details[0]['shape']\ninput_height = int(input_shape[1])\ninput_width = int(input_shape[2])\ninput_dtype = input_details[0]['dtype']\n\n# Helper flags\nfloating_model = (input_dtype == np.float32)\n\n# ======================================================================\n# Helper functions specific to detection interpretation and postprocessing\n# ======================================================================\n\ndef iou(box_a, box_b):\n    \"\"\"\n    Compute IoU between two boxes in absolute pixel coordinates.\n    Boxes format: [x_min, y_min, x_max, y_max]\n    \"\"\"\n    xa1, ya1, xa2, ya2 = box_a\n    xb1, yb1, xb2, yb2 = box_b\n\n    inter_x1 = max(xa1, xb1)\n    inter_y1 = max(ya1, yb1)\n    inter_x2 = min(xa2, xb2)\n    inter_y2 = min(ya2, yb2)\n\n    inter_w = max(0.0, inter_x2 - inter_x1)\n    inter_h = max(0.0, inter_y2 - inter_y1)\n    inter_area = inter_w * inter_h\n\n    area_a = max(0.0, xa2 - xa1) * max(0.0, ya2 - ya1)\n    area_b = max(0.0, xb2 - xb1) * max(0.0, yb2 - yb1)\n\n    union = area_a + area_b - inter_area\n    if union <= 0:\n        return 0.0\n    return inter_area / union\n\ndef map_class_to_label(cls_id, labels_list):\n    \"\"\"\n    Map class ID (float/int from TFLite output) to label string.\n    Handles common off-by-one indexing differences by trying direct and -1 index.\n    \"\"\"\n    cid = int(cls_id)\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return f\"id {cid}\"\n\ndef extract_detections_from_outputs(outputs, frame_w, frame_h, conf_thresh):\n    \"\"\"\n    Phase 4.2 + 4.3 support: Interpret and post-process raw model outputs.\n\n    outputs: list of numpy arrays from interpreter.get_tensor for each output_detail\n    Returns a list of detections with fields:\n      - bbox: [x_min, y_min, x_max, y_max] ints clipped to image\n      - score: float\n      - class_id: int\n      - label: str (if available)\n    \"\"\"\n    # Identify standard SSD outputs: boxes, classes, scores, num_detections\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Assign by inspecting shapes and value ranges to be robust\n    for arr in outputs:\n        arr_np = np.array(arr)\n        if arr_np.size == 1:\n            num_det = int(np.squeeze(arr_np))\n        elif arr_np.ndim >= 2 and arr_np.shape[-1] == 4:\n            boxes = arr_np\n        else:\n            # scores vs classes: both [1, N]; scores are [0,1], classes likely > 1.0\n            max_val = np.max(arr_np)\n            min_val = np.min(arr_np)\n            if max_val <= 1.0 and min_val >= 0.0:\n                scores = arr_np\n            else:\n                classes = arr_np\n\n    # Fallback handling in case some is None; typical TFLite SSD is [1,10,4], [1,10], [1,10], [1]\n    if boxes is None or classes is None or scores is None:\n        # Try another heuristic: order by output_details name if present\n        # However, we stick to the guideline strictly; raise clear error if not found\n        raise RuntimeError(\"Failed to interpret model outputs (boxes/classes/scores).\")\n\n    boxes = np.squeeze(boxes)\n    classes = np.squeeze(classes)\n    scores = np.squeeze(scores)\n\n    # num_det may be absent in some models; infer from scores length\n    if num_det is None:\n        num_det = scores.shape[0] if scores.ndim == 1 else scores.shape[1] if scores.ndim == 2 else len(scores)\n\n    detections = []\n    for i in range(int(num_det)):\n        score = float(scores[i])\n        if score < conf_thresh:\n            continue\n\n        # TFLite SSD boxes are in normalized coordinates [ymin, xmin, ymax, xmax]\n        y_min, x_min, y_max, x_max = boxes[i].tolist()\n\n        x_min_abs = int(max(0, min(frame_w - 1, x_min * frame_w)))\n        y_min_abs = int(max(0, min(frame_h - 1, y_min * frame_h)))\n        x_max_abs = int(max(0, min(frame_w - 1, x_max * frame_w)))\n        y_max_abs = int(max(0, min(frame_h - 1, y_max * frame_h)))\n\n        # Ensure proper box ordering and non-zero area\n        x1 = min(x_min_abs, x_max_abs)\n        y1 = min(y_min_abs, y_max_abs)\n        x2 = max(x_min_abs, x_max_abs)\n        y2 = max(y_min_abs, y_max_abs)\n\n        # Discard degenerate boxes\n        if x2 <= x1 or y2 <= y1:\n            continue\n\n        cls_id = int(classes[i])\n        label = map_class_to_label(cls_id, labels) if len(labels) > 0 else str(cls_id)\n\n        detections.append({\n            'bbox': [x1, y1, x2, y2],\n            'score': score,\n            'class_id': cls_id,\n            'label': label\n        })\n    return detections\n\ndef compute_ap(scores, is_tp, total_positives):\n    \"\"\"\n    Compute Average Precision (AP) using precision envelope integration.\n    scores: list of floats\n    is_tp: list of bools (True for TP, False for FP)\n    total_positives: int (number of true positives in ground-truth sense for this proxy metric)\n    \"\"\"\n    if total_positives <= 0 or len(scores) == 0:\n        return 0.0\n\n    # Sort by score descending\n    order = np.argsort(-np.array(scores))\n    tp_sorted = np.array(is_tp, dtype=np.int32)[order]\n    fp_sorted = 1 - tp_sorted\n\n    cum_tp = np.cumsum(tp_sorted)\n    cum_fp = np.cumsum(fp_sorted)\n\n    precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-12)\n    recall = cum_tp / float(total_positives)\n\n    # Make precision monotonically decreasing (precision envelope)\n    # and integrate over recall changes\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([0.0], precision, [0.0]))\n\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n\n    # Compute area under PR curve by summing Delta recall * precision\n    ap = 0.0\n    for i in range(1, mrec.size):\n        delta_rec = mrec[i] - mrec[i - 1]\n        ap += delta_rec * mpre[i]\n    return float(ap)\n\ndef compute_proxy_map(eval_store):\n    \"\"\"\n    Compute proxy mAP across classes from the running eval_store.\n    eval_store structure:\n      {\n        class_id: {\n            'scores': [float, ...], # Detection confidences (thresholded detections)\n            'is_tp': [bool, ...],   # Temporal consistency TP flags vs previous frame\n            'tp_total': int         # Total positives for proxy evaluation (sum of is_tp)\n        }, ...\n      }\n    Returns mAP (float) across classes with tp_total > 0.\n    \"\"\"\n    aps = []\n    for cls_id, data in eval_store.items():\n        tp_total = int(data.get('tp_total', 0))\n        if tp_total <= 0:\n            continue\n        ap = compute_ap(data['scores'], data['is_tp'], tp_total)\n        aps.append(ap)\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\ndef draw_detections_on_frame(frame, detections, map_value):\n    \"\"\"\n    Draw bounding boxes and labels on the frame, and overlay current proxy mAP.\n    \"\"\"\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        score = det['score']\n        label = det['label']\n\n        # Box\n        color = (0, 255, 0)  # Green\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness=2)\n\n        # Label text\n        text = f\"{label}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - baseline), (x1 + tw, y1), color, thickness=-1)\n        cv2.putText(frame, text, (x1, y1 - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), thickness=1, lineType=cv2.LINE_AA)\n\n    # Overlay proxy mAP at top-left\n    map_text = f\"mAP: {map_value:.3f}\"\n    cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (50, 50, 255), thickness=2, lineType=cv2.LINE_AA)\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# 2.1 Acquire Input Data: Read a single video file from the given input_path\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video not found: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\norig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0:\n    fps = 30.0  # reasonable default if not present\n\n# Prepare VideoWriter\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_width, orig_height))\nif not writer.isOpened():\n    raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n# Structures for proxy mAP evaluation across frames\n# eval_store[class_id] = {'scores': [], 'is_tp': [], 'tp_total': 0}\neval_store = {}\nprev_dets_by_class = {}  # Previous frame detections grouped by class_id: list of bboxes\n\nframe_index = 0\nt_start = time.time()\n\ntry:\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # End of video\n\n        frame_index += 1\n        frame_h, frame_w = frame_bgr.shape[:2]\n\n        # 2.2 Preprocess Data:\n        # Convert BGR to RGB as most TFLite SSD models expect RGB input\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        resized = cv2.resize(frame_rgb, (input_width, input_height))\n        input_data = np.expand_dims(resized, axis=0)\n\n        # 2.3 Quantization Handling\n        if floating_model:\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n        else:\n            # Ensure dtype matches model input type\n            input_data = np.asarray(input_data, dtype=input_dtype)\n\n        # =========================\n        # Phase 3: Inference\n        # =========================\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        interpreter.invoke()\n\n        # =========================\n        # Phase 4: Output Interpretation & Handling\n        # =========================\n\n        # 4.1 Get Output Tensor(s)\n        raw_outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n        # 4.2 Interpret Results\n        detections = extract_detections_from_outputs(raw_outputs, frame_w, frame_h, CONF_THRESHOLD)\n\n        # 4.3 Post-processing:\n        # - Confidence thresholding and coordinate scaling already applied in extract_detections_from_outputs\n        # - Perform simple greedy matching against previous frame by class to build proxy TP/FP for mAP\n\n        # Group current detections by class\n        current_by_class = {}\n        for det in detections:\n            cid = det['class_id']\n            current_by_class.setdefault(cid, []).append(det)\n\n        # Initialize eval_store entries\n        for cid in current_by_class.keys():\n            if cid not in eval_store:\n                eval_store[cid] = {'scores': [], 'is_tp': [], 'tp_total': 0}\n        for cid in prev_dets_by_class.keys():\n            if cid not in eval_store:\n                eval_store[cid] = {'scores': [], 'is_tp': [], 'tp_total': 0}\n\n        # For each class, match current detections to previous detections via IoU >= 0.5\n        new_prev_dets_by_class = {}\n        for cid, curr_list in current_by_class.items():\n            prev_list = prev_dets_by_class.get(cid, [])\n            prev_used = [False] * len(prev_list)\n\n            # For future prev tracking for next frame\n            new_prev_dets_by_class[cid] = [det['bbox'] for det in curr_list]\n\n            for det in curr_list:\n                bbox = det['bbox']\n                score = det['score']\n\n                matched = False\n                # Greedy match: find best IoU with unused previous boxes\n                best_iou = 0.0\n                best_j = -1\n                for j, prev_bbox in enumerate(prev_list):\n                    if prev_used[j]:\n                        continue\n                    ov = iou(bbox, prev_bbox)\n                    if ov >= 0.5 and ov > best_iou:\n                        best_iou = ov\n                        best_j = j\n\n                if best_j >= 0:\n                    prev_used[best_j] = True\n                    matched = True\n\n                # Update evaluation store\n                eval_store[cid]['scores'].append(score)\n                eval_store[cid]['is_tp'].append(bool(matched))\n                # Count positives as matched pairs (proxy positives)\n                if matched:\n                    eval_store[cid]['tp_total'] += 1\n\n        # Also carry forward classes that had only previous detections but none current\n        for cid, prev_list in prev_dets_by_class.items():\n            if cid not in new_prev_dets_by_class:\n                new_prev_dets_by_class[cid] = []\n\n        # Update prev for next iteration\n        prev_dets_by_class = new_prev_dets_by_class\n\n        # Compute running proxy mAP\n        running_map = compute_proxy_map(eval_store)\n\n        # 4.4 Handle Output: draw and write frame\n        output_frame = frame_bgr.copy()\n        draw_detections_on_frame(output_frame, detections, running_map)\n        writer.write(output_frame)\n\n        # 4.5 Loop Continuation: continue until video ends; no external break\n\nfinally:\n    # =========================\n    # Phase 5: Cleanup\n    # =========================\n    cap.release()\n    writer.release()\n    t_elapsed = time.time() - t_start\n\n# Print summary and final mAP\nfinal_map = compute_proxy_map(eval_store)\nprint(\"Processing finished.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}\")\nprint(f\"Elapsed time: {t_elapsed:.2f} s, FPS (effective): {frame_index / max(t_elapsed, 1e-9):.2f}\")\nprint(f\"Calculated mAP (proxy, temporal consistency): {final_map:.4f}\")"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/2c2a2293",
      "latency": 125.893,
      "total_cost": 0.098171,
      "observations": [
        {
          "id": "time-03-47-26-850401_chatcmpl-D5ikpNMiLXAJXPpZ6XhtZ2Y1tzNCT",
          "traceId": "2c2a2293",
          "type": "GENERATION",
          "name": "2c2a_psg_gen_attempt#1",
          "startTime": "2026-02-05T01:47:26.850000+00:00",
          "endTime": "2026-02-05T01:49:32.743000+00:00",
          "completionStartTime": "2026-02-05T01:49:32.743000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.09817100000000001,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script performs object detection using a TFLite SSD model on a single input video file,\ndraws bounding boxes and labels on detected objects, writes the annotated video to an output path,\nand computes a proxy mAP (mean Average Precision) based on temporal consistency between consecutive frames.\n\nProgramming Guideline Phases are explicitly followed and annotated in the code.\n\"\"\"\n\n# =========================\n# Phase 1: Setup\n# =========================\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# 1.1 Imports: Import interpreter literally by the specified path\nfrom ai_edge_litert.interpreter import Interpreter\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = 0.5  # Confidence Threshold\n\n# Utility: Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (Conditional)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if len(name) > 0:\n                    labels.append(name)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Assuming single input tensor for SSD models\ninput_shape = input_details[0]['shape']\ninput_height = int(input_shape[1])\ninput_width = int(input_shape[2])\ninput_dtype = input_details[0]['dtype']\n\n# Helper flags\nfloating_model = (input_dtype == np.float32)\n\n# ======================================================================\n# Helper functions specific to detection interpretation and postprocessing\n# ======================================================================\n\ndef iou(box_a, box_b):\n    \"\"\"\n    Compute IoU between two boxes in absolute pixel coordinates.\n    Boxes format: [x_min, y_min, x_max, y_max]\n    \"\"\"\n    xa1, ya1, xa2, ya2 = box_a\n    xb1, yb1, xb2, yb2 = box_b\n\n    inter_x1 = max(xa1, xb1)\n    inter_y1 = max(ya1, yb1)\n    inter_x2 = min(xa2, xb2)\n    inter_y2 = min(ya2, yb2)\n\n    inter_w = max(0.0, inter_x2 - inter_x1)\n    inter_h = max(0.0, inter_y2 - inter_y1)\n    inter_area = inter_w * inter_h\n\n    area_a = max(0.0, xa2 - xa1) * max(0.0, ya2 - ya1)\n    area_b = max(0.0, xb2 - xb1) * max(0.0, yb2 - yb1)\n\n    union = area_a + area_b - inter_area\n    if union <= 0:\n        return 0.0\n    return inter_area / union\n\ndef map_class_to_label(cls_id, labels_list):\n    \"\"\"\n    Map class ID (float/int from TFLite output) to label string.\n    Handles common off-by-one indexing differences by trying direct and -1 index.\n    \"\"\"\n    cid = int(cls_id)\n    if 0 <= cid < len(labels_list):\n        return labels_list[cid]\n    if 0 <= (cid - 1) < len(labels_list):\n        return labels_list[cid - 1]\n    return f\"id {cid}\"\n\ndef extract_detections_from_outputs(outputs, frame_w, frame_h, conf_thresh):\n    \"\"\"\n    Phase 4.2 + 4.3 support: Interpret and post-process raw model outputs.\n\n    outputs: list of numpy arrays from interpreter.get_tensor for each output_detail\n    Returns a list of detections with fields:\n      - bbox: [x_min, y_min, x_max, y_max] ints clipped to image\n      - score: float\n      - class_id: int\n      - label: str (if available)\n    \"\"\"\n    # Identify standard SSD outputs: boxes, classes, scores, num_detections\n    boxes = None\n    classes = None\n    scores = None\n    num_det = None\n\n    # Assign by inspecting shapes and value ranges to be robust\n    for arr in outputs:\n        arr_np = np.array(arr)\n        if arr_np.size == 1:\n            num_det = int(np.squeeze(arr_np))\n        elif arr_np.ndim >= 2 and arr_np.shape[-1] == 4:\n            boxes = arr_np\n        else:\n            # scores vs classes: both [1, N]; scores are [0,1], classes likely > 1.0\n            max_val = np.max(arr_np)\n            min_val = np.min(arr_np)\n            if max_val <= 1.0 and min_val >= 0.0:\n                scores = arr_np\n            else:\n                classes = arr_np\n\n    # Fallback handling in case some is None; typical TFLite SSD is [1,10,4], [1,10], [1,10], [1]\n    if boxes is None or classes is None or scores is None:\n        # Try another heuristic: order by output_details name if present\n        # However, we stick to the guideline strictly; raise clear error if not found\n        raise RuntimeError(\"Failed to interpret model outputs (boxes/classes/scores).\")\n\n    boxes = np.squeeze(boxes)\n    classes = np.squeeze(classes)\n    scores = np.squeeze(scores)\n\n    # num_det may be absent in some models; infer from scores length\n    if num_det is None:\n        num_det = scores.shape[0] if scores.ndim == 1 else scores.shape[1] if scores.ndim == 2 else len(scores)\n\n    detections = []\n    for i in range(int(num_det)):\n        score = float(scores[i])\n        if score < conf_thresh:\n            continue\n\n        # TFLite SSD boxes are in normalized coordinates [ymin, xmin, ymax, xmax]\n        y_min, x_min, y_max, x_max = boxes[i].tolist()\n\n        x_min_abs = int(max(0, min(frame_w - 1, x_min * frame_w)))\n        y_min_abs = int(max(0, min(frame_h - 1, y_min * frame_h)))\n        x_max_abs = int(max(0, min(frame_w - 1, x_max * frame_w)))\n        y_max_abs = int(max(0, min(frame_h - 1, y_max * frame_h)))\n\n        # Ensure proper box ordering and non-zero area\n        x1 = min(x_min_abs, x_max_abs)\n        y1 = min(y_min_abs, y_max_abs)\n        x2 = max(x_min_abs, x_max_abs)\n        y2 = max(y_min_abs, y_max_abs)\n\n        # Discard degenerate boxes\n        if x2 <= x1 or y2 <= y1:\n            continue\n\n        cls_id = int(classes[i])\n        label = map_class_to_label(cls_id, labels) if len(labels) > 0 else str(cls_id)\n\n        detections.append({\n            'bbox': [x1, y1, x2, y2],\n            'score': score,\n            'class_id': cls_id,\n            'label': label\n        })\n    return detections\n\ndef compute_ap(scores, is_tp, total_positives):\n    \"\"\"\n    Compute Average Precision (AP) using precision envelope integration.\n    scores: list of floats\n    is_tp: list of bools (True for TP, False for FP)\n    total_positives: int (number of true positives in ground-truth sense for this proxy metric)\n    \"\"\"\n    if total_positives <= 0 or len(scores) == 0:\n        return 0.0\n\n    # Sort by score descending\n    order = np.argsort(-np.array(scores))\n    tp_sorted = np.array(is_tp, dtype=np.int32)[order]\n    fp_sorted = 1 - tp_sorted\n\n    cum_tp = np.cumsum(tp_sorted)\n    cum_fp = np.cumsum(fp_sorted)\n\n    precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-12)\n    recall = cum_tp / float(total_positives)\n\n    # Make precision monotonically decreasing (precision envelope)\n    # and integrate over recall changes\n    mrec = np.concatenate(([0.0], recall, [1.0]))\n    mpre = np.concatenate(([0.0], precision, [0.0]))\n\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n\n    # Compute area under PR curve by summing Delta recall * precision\n    ap = 0.0\n    for i in range(1, mrec.size):\n        delta_rec = mrec[i] - mrec[i - 1]\n        ap += delta_rec * mpre[i]\n    return float(ap)\n\ndef compute_proxy_map(eval_store):\n    \"\"\"\n    Compute proxy mAP across classes from the running eval_store.\n    eval_store structure:\n      {\n        class_id: {\n            'scores': [float, ...], # Detection confidences (thresholded detections)\n            'is_tp': [bool, ...],   # Temporal consistency TP flags vs previous frame\n            'tp_total': int         # Total positives for proxy evaluation (sum of is_tp)\n        }, ...\n      }\n    Returns mAP (float) across classes with tp_total > 0.\n    \"\"\"\n    aps = []\n    for cls_id, data in eval_store.items():\n        tp_total = int(data.get('tp_total', 0))\n        if tp_total <= 0:\n            continue\n        ap = compute_ap(data['scores'], data['is_tp'], tp_total)\n        aps.append(ap)\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\ndef draw_detections_on_frame(frame, detections, map_value):\n    \"\"\"\n    Draw bounding boxes and labels on the frame, and overlay current proxy mAP.\n    \"\"\"\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        score = det['score']\n        label = det['label']\n\n        # Box\n        color = (0, 255, 0)  # Green\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, thickness=2)\n\n        # Label text\n        text = f\"{label}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - baseline), (x1 + tw, y1), color, thickness=-1)\n        cv2.putText(frame, text, (x1, y1 - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), thickness=1, lineType=cv2.LINE_AA)\n\n    # Overlay proxy mAP at top-left\n    map_text = f\"mAP: {map_value:.3f}\"\n    cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (50, 50, 255), thickness=2, lineType=cv2.LINE_AA)\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# 2.1 Acquire Input Data: Read a single video file from the given input_path\nif not os.path.isfile(INPUT_PATH):\n    raise FileNotFoundError(f\"Input video not found: {INPUT_PATH}\")\n\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\norig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0:\n    fps = 30.0  # reasonable default if not present\n\n# Prepare VideoWriter\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_width, orig_height))\nif not writer.isOpened():\n    raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n# Structures for proxy mAP evaluation across frames\n# eval_store[class_id] = {'scores': [], 'is_tp': [], 'tp_total': 0}\neval_store = {}\nprev_dets_by_class = {}  # Previous frame detections grouped by class_id: list of bboxes\n\nframe_index = 0\nt_start = time.time()\n\ntry:\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # End of video\n\n        frame_index += 1\n        frame_h, frame_w = frame_bgr.shape[:2]\n\n        # 2.2 Preprocess Data:\n        # Convert BGR to RGB as most TFLite SSD models expect RGB input\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        resized = cv2.resize(frame_rgb, (input_width, input_height))\n        input_data = np.expand_dims(resized, axis=0)\n\n        # 2.3 Quantization Handling\n        if floating_model:\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n        else:\n            # Ensure dtype matches model input type\n            input_data = np.asarray(input_data, dtype=input_dtype)\n\n        # =========================\n        # Phase 3: Inference\n        # =========================\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        interpreter.invoke()\n\n        # =========================\n        # Phase 4: Output Interpretation & Handling\n        # =========================\n\n        # 4.1 Get Output Tensor(s)\n        raw_outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n        # 4.2 Interpret Results\n        detections = extract_detections_from_outputs(raw_outputs, frame_w, frame_h, CONF_THRESHOLD)\n\n        # 4.3 Post-processing:\n        # - Confidence thresholding and coordinate scaling already applied in extract_detections_from_outputs\n        # - Perform simple greedy matching against previous frame by class to build proxy TP/FP for mAP\n\n        # Group current detections by class\n        current_by_class = {}\n        for det in detections:\n            cid = det['class_id']\n            current_by_class.setdefault(cid, []).append(det)\n\n        # Initialize eval_store entries\n        for cid in current_by_class.keys():\n            if cid not in eval_store:\n                eval_store[cid] = {'scores': [], 'is_tp': [], 'tp_total': 0}\n        for cid in prev_dets_by_class.keys():\n            if cid not in eval_store:\n                eval_store[cid] = {'scores': [], 'is_tp': [], 'tp_total': 0}\n\n        # For each class, match current detections to previous detections via IoU >= 0.5\n        new_prev_dets_by_class = {}\n        for cid, curr_list in current_by_class.items():\n            prev_list = prev_dets_by_class.get(cid, [])\n            prev_used = [False] * len(prev_list)\n\n            # For future prev tracking for next frame\n            new_prev_dets_by_class[cid] = [det['bbox'] for det in curr_list]\n\n            for det in curr_list:\n                bbox = det['bbox']\n                score = det['score']\n\n                matched = False\n                # Greedy match: find best IoU with unused previous boxes\n                best_iou = 0.0\n                best_j = -1\n                for j, prev_bbox in enumerate(prev_list):\n                    if prev_used[j]:\n                        continue\n                    ov = iou(bbox, prev_bbox)\n                    if ov >= 0.5 and ov > best_iou:\n                        best_iou = ov\n                        best_j = j\n\n                if best_j >= 0:\n                    prev_used[best_j] = True\n                    matched = True\n\n                # Update evaluation store\n                eval_store[cid]['scores'].append(score)\n                eval_store[cid]['is_tp'].append(bool(matched))\n                # Count positives as matched pairs (proxy positives)\n                if matched:\n                    eval_store[cid]['tp_total'] += 1\n\n        # Also carry forward classes that had only previous detections but none current\n        for cid, prev_list in prev_dets_by_class.items():\n            if cid not in new_prev_dets_by_class:\n                new_prev_dets_by_class[cid] = []\n\n        # Update prev for next iteration\n        prev_dets_by_class = new_prev_dets_by_class\n\n        # Compute running proxy mAP\n        running_map = compute_proxy_map(eval_store)\n\n        # 4.4 Handle Output: draw and write frame\n        output_frame = frame_bgr.copy()\n        draw_detections_on_frame(output_frame, detections, running_map)\n        writer.write(output_frame)\n\n        # 4.5 Loop Continuation: continue until video ends; no external break\n\nfinally:\n    # =========================\n    # Phase 5: Cleanup\n    # =========================\n    cap.release()\n    writer.release()\n    t_elapsed = time.time() - t_start\n\n# Print summary and final mAP\nfinal_map = compute_proxy_map(eval_store)\nprint(\"Processing finished.\")\nprint(f\"Input video: {INPUT_PATH}\")\nprint(f\"Output video: {OUTPUT_PATH}\")\nprint(f\"Frames processed: {frame_index}\")\nprint(f\"Elapsed time: {t_elapsed:.2f} s, FPS (effective): {frame_index / max(t_elapsed, 1e-9):.2f}\")\nprint(f\"Calculated mAP (proxy, temporal consistency): {final_map:.4f}\")\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 9788,
            "total": 11288,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.098171,
          "latency": 125893.0,
          "timeToFirstToken": 125.893,
          "updatedAt": "2026-02-05T01:49:38.032Z",
          "totalTokens": 11288,
          "costDetails": {
            "total": 0.098171
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 9788,
          "usageDetails": {
            "input": 1500,
            "output": 9788,
            "total": 11288
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T01:49:32.995Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T01:50:27.198Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T01:47:27.009Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "f41d8291",
      "timestamp": "2026-02-05T01:43:18.313000+00:00",
      "name": "f41d_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nApplication: Object Detection via a video file\nDescription:\n- Reads a single video file from the given input_path.\n- Runs SSD MobileNet v1 TFLite model inference using ai_edge_litert Interpreter.\n- Outputs a video file with rectangles drawn on detected objects, labels, and an on-frame\n  running mAP (proxy) metric. Also prints a final proxy mAP summary.\n\nPhases implemented per Programming Guidelines:\n- Phase 1: Setup (imports, paths, labels, interpreter, I/O details)\n- Phase 2: Input Acquisition & Preprocessing Loop (explicitly implemented)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling (explicit 4.2 & 4.3 implemented)\n- Phase 5: Cleanup\n\nNote:\n- Only standard libraries plus numpy and cv2 are used for video/image processing.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Import Interpreter exactly as specified\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef load_labels(label_file_path):\n    \"\"\"\n    Load labels from a text file into a list, stripping whitespace and skipping empty lines.\n    \"\"\"\n    labels = []\n    if os.path.isfile(label_file_path):\n        with open(label_file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line != '':\n                    labels.append(line)\n    return labels\n\ndef create_video_writer(out_path, fps, frame_w, frame_h):\n    \"\"\"\n    Create a cv2.VideoWriter ensuring parent directories exist.\n    \"\"\"\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # mp4 codec\n    writer = cv2.VideoWriter(out_path, fourcc, float(fps), (int(frame_w), int(frame_h)))\n    return writer\n\ndef color_for_class(class_id):\n    \"\"\"\n    Deterministic BGR color for a class id.\n    \"\"\"\n    rng = np.random.RandomState(int(class_id) * 2654435761 % (2**32))\n    color = tuple(int(c) for c in rng.randint(0, 255, size=3))\n    return color\n\ndef clip_bbox_to_frame(b, width, height):\n    \"\"\"\n    Clip bounding box to image boundaries; input b=(ymin,xmin,ymax,xmax) in absolute pixels.\n    Returns ints: (ymin,xmin,ymax,xmax).\n    \"\"\"\n    ymin, xmin, ymax, xmax = b\n    ymin = max(0, min(int(round(ymin)), height - 1))\n    xmin = max(0, min(int(round(xmin)), width - 1))\n    ymax = max(0, min(int(round(ymax)), height - 1))\n    xmax = max(0, min(int(round(xmax)), width - 1))\n    # Ensure proper ordering after clipping\n    ymin, ymax = min(ymin, ymax), max(ymin, ymax)\n    xmin, xmax = min(xmin, xmax), max(xmin, xmax)\n    return ymin, xmin, ymax, xmax\n\ndef squeeze_out(arr):\n    \"\"\"\n    Remove leading batch dims of size 1 and trailing singleton dims where appropriate.\n    Keeps 2D for boxes and 1D for classes/scores.\n    \"\"\"\n    a = arr\n    # Remove leading batch dimension if 1\n    while a.ndim > 0 and a.shape[0] == 1 and a.ndim > 1:\n        a = a[0]\n    # If still has shape like [1, N], squeeze to [N]\n    if a.ndim == 2 and a.shape[0] == 1:\n        a = a[0]\n    return a\n\ndef parse_detection_outputs(output_details, output_arrays):\n    \"\"\"\n    Parse outputs from SSD-style TFLite detection model.\n\n    Returns:\n      boxes: np.ndarray of shape [N,4] (ymin, xmin, ymax, xmax)\n      classes: np.ndarray of shape [N] (float/int class ids)\n      scores: np.ndarray of shape [N] (float confidences in [0,1])\n      num: int, number of valid detections suggested by the model\n    \"\"\"\n    # Squeeze and collect\n    arrays = [squeeze_out(a) for a in output_arrays]\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # First, try to use names if present\n    names = []\n    try:\n        names = [od.get('name', '') for od in output_details]\n    except Exception:\n        names = [''] * len(output_details)\n\n    # Map by typical names when available\n    for a, n in zip(arrays, names):\n        n_low = (n or '').lower()\n        if 'box' in n_low or 'bbox' in n_low or (a.ndim == 2 and a.shape[-1] == 4):\n            if a.ndim == 2 and a.shape[-1] == 4:\n                boxes = a\n        if ('class' in n_low or 'label' in n_low) and a.ndim == 1:\n            classes = a\n        if ('score' in n_low or 'confidence' in n_low) and a.ndim == 1:\n            scores = a\n        if 'num' in n_low and a.size == 1:\n            try:\n                num = int(round(float(np.squeeze(a))))\n            except Exception:\n                pass\n\n    # Heuristic fallback if any missing\n    if boxes is None:\n        for a in arrays:\n            if a.ndim == 2 and a.shape[-1] == 4:\n                boxes = a\n                break\n\n    one_d = [a for a in arrays if a.ndim == 1 and a.size > 0]\n    if scores is None:\n        # scores expected to be in [0,1]\n        cand = None\n        best_range_fit = -1\n        for a in one_d:\n            amin, amax = float(np.min(a)), float(np.max(a))\n            # Score arrays should largely be within [0,1]\n            comp = -abs(0.0 - amin) - abs(1.0 - max(min(amax, 1.0), 0.0))\n            if comp > best_range_fit:\n                best_range_fit = comp\n                cand = a\n        scores = cand\n\n    if classes is None and len(one_d) > 0:\n        # Choose the 1-D array different from scores with more integer-like values\n        cand_list = [a for a in one_d if a is not scores]\n        if len(cand_list) == 1:\n            classes = cand_list[0]\n        elif len(cand_list) > 1:\n            best_int_like = -1.0\n            best = None\n            for a in cand_list:\n                frac = np.mod(a, 1.0)\n                int_like = float(np.mean(np.abs(frac) < 1e-3))\n                if int_like > best_int_like:\n                    best_int_like = int_like\n                    best = a\n            classes = best\n\n    if num is None:\n        # Infer num from the smallest consistent length\n        lengths = []\n        if boxes is not None:\n            lengths.append(int(boxes.shape[0]))\n        if classes is not None:\n            lengths.append(int(classes.shape[0]))\n        if scores is not None:\n            lengths.append(int(scores.shape[0]))\n        num = int(min(lengths)) if len(lengths) > 0 else 0\n\n    # Ensure proper shapes and consistent lengths\n    if boxes is not None and boxes.ndim != 2:\n        boxes = np.reshape(boxes, (-1, 4)) if boxes.size % 4 == 0 else None\n\n    # Final safety trims based on computed num\n    def safe_slice_1d(a, n):\n        if a is None:\n            return None\n        return a[:min(n, a.shape[0])] if a.ndim == 1 else a\n\n    def safe_slice_2d(a, n):\n        if a is None:\n            return None\n        return a[:min(n, a.shape[0]), :] if a.ndim == 2 else a\n\n    boxes = safe_slice_2d(boxes, num)\n    classes = safe_slice_1d(classes, num)\n    scores = safe_slice_1d(scores, num)\n\n    # Recompute n as the minimum available length to prevent OOB\n    lengths = []\n    if boxes is not None and boxes.ndim == 2:\n        lengths.append(int(boxes.shape[0]))\n    if classes is not None and classes.ndim == 1:\n        lengths.append(int(classes.shape[0]))\n    if scores is not None and scores.ndim == 1:\n        lengths.append(int(scores.shape[0]))\n    n_final = int(min(lengths)) if len(lengths) > 0 else 0\n\n    return boxes, classes, scores, n_final\n\ndef main():\n    # 1.2 Paths/Parameters (provided)\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    CONF_THRESHOLD = float('0.5')\n\n    # 1.3 Load Labels\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Single input expected: [1, H, W, C]\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n    in_h, in_w, in_c = int(input_shape[1]), int(input_shape[2]), int(input_shape[3])\n\n    # Quantization params if present\n    input_scale, input_zero_point = (1.0, 0)\n    try:\n        q = input_details[0].get('quantization', (1.0, 0))\n        if isinstance(q, (tuple, list)) and len(q) == 2:\n            input_scale, input_zero_point = q\n            input_scale = 1.0 if input_scale is None else input_scale\n            input_zero_point = 0 if input_zero_point is None else input_zero_point\n    except Exception:\n        pass\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: open the specific video file\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f'ERROR: Failed to open input video: {input_path}')\n        return\n\n    # Input video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    fps = fps if fps and not np.isnan(fps) and fps > 0 else 25.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) > 0 else -1\n\n    # Create output writer\n    writer = create_video_writer(output_path, fps, frame_w, frame_h)\n\n    # Stats for mAP (proxy): per-class list of confidences above threshold\n    class_confidences = {}\n\n    frame_idx = 0\n    last_print_time = time.time()\n\n    print('Starting inference on video...')\n    print(f'Input: {input_path}')\n    print(f'Output: {output_path}')\n    print(f'Model: {model_path}')\n    print(f'Labels: {label_path} (loaded {len(labels)} labels)')\n    print(f'Confidence threshold: {CONF_THRESHOLD:.2f}')\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break  # End of video\n            frame_idx += 1\n\n            original_frame = frame  # will annotate directly\n\n            # 2.2 Preprocess Data: resize to model input, convert BGR->RGB\n            resized = cv2.resize(original_frame, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n            rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n            # 2.3 Quantization Handling\n            if input_dtype == np.float32:\n                # Normalize to [-1, 1] as common for SSD MobileNet\n                input_data = (np.float32(rgb) - 127.5) / 127.5\n            elif input_dtype == np.uint8:\n                input_data = np.uint8(rgb)\n            elif input_dtype == np.int8:\n                # Quantize from normalized float using scale/zero-point when provided\n                float_in = (np.float32(rgb) - 127.5) / 127.5\n                scale = input_scale if input_scale not in (None, 0) else 1.0\n                zero = input_zero_point if input_zero_point is not None else 0\n                q = np.round(float_in / scale + zero)\n                q = np.clip(q, -128, 127)\n                input_data = q.astype(np.int8)\n            else:\n                input_data = rgb.astype(input_dtype)\n\n            # Add batch dimension [1, H, W, C]\n            input_data = np.expand_dims(input_data, axis=0)\n\n            # Phase 3: Inference\n            interpreter.set_tensor(input_index, input_data)\n            t0 = time.time()\n            interpreter.invoke()\n            infer_time_ms = (time.time() - t0) * 1000.0\n\n            # Phase 4: Output Interpretation & Handling\n            # 4.1 Get Output Tensor(s)\n            output_arrays = []\n            for od in output_details:\n                out = interpreter.get_tensor(od['index'])\n                output_arrays.append(out)\n\n            # 4.2 Interpret Results: parse SSD outputs (boxes/classes/scores/num)\n            boxes, classes, scores, num = parse_detection_outputs(output_details, output_arrays)\n\n            # 4.3 Post-processing: thresholding, coordinate scaling, clipping\n            kept = []\n            if boxes is not None and classes is not None and scores is not None and num > 0:\n                # Decide if boxes are normalized or absolute by checking range\n                # If typical SSD output: normalized [0,1]. If max coord > 2, treat as absolute.\n                max_box_val = float(np.max(np.abs(boxes))) if boxes.size > 0 else 0.0\n                boxes_are_normalized = not (max_box_val > 2.0)\n\n                N = int(min(num, boxes.shape[0], classes.shape[0], scores.shape[0]))\n                for i in range(N):\n                    score = float(scores[i])\n                    if score < CONF_THRESHOLD:\n                        continue\n\n                    # Class id (may be float from model); round then int\n                    cls_id_raw = int(round(float(classes[i])))\n\n                    y_min, x_min, y_max, x_max = boxes[i]\n                    if boxes_are_normalized:\n                        y_min *= frame_h\n                        y_max *= frame_h\n                        x_min *= frame_w\n                        x_max *= frame_w\n\n                    y_min, x_min, y_max, x_max = clip_bbox_to_frame((y_min, x_min, y_max, x_max), frame_w, frame_h)\n                    kept.append((cls_id_raw, score, (x_min, y_min, x_max, y_max)))\n\n                    # Accumulate for proxy mAP\n                    class_confidences.setdefault(cls_id_raw, []).append(score)\n\n            # Draw detections with labels\n            for (cls_id, score, (x_min, y_min, x_max, y_max)) in kept:\n                color = color_for_class(cls_id)\n                cv2.rectangle(original_frame, (x_min, y_min), (x_max, y_max), color, 2)\n\n                # Resolve label name\n                if 0 <= cls_id < len(labels):\n                    label_name = labels[cls_id]\n                elif 0 <= (cls_id - 1) < len(labels):\n                    # Some SSD label maps are 1-based (background at 0)\n                    label_name = labels[cls_id - 1]\n                else:\n                    label_name = f'id_{cls_id}'\n\n                caption = f'{label_name}: {score:.2f}'\n                (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n                y_text_top = max(0, y_min - th - bl - 4)\n                cv2.rectangle(original_frame, (x_min, y_text_top), (x_min + tw + 6, y_min), color, thickness=-1)\n                cv2.putText(original_frame, caption, (x_min + 3, y_min - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n            # Compute running mAP (proxy): mean of per-class mean confidences\n            ap_vals = []\n            for _, conf_list in class_confidences.items():\n                if len(conf_list) > 0:\n                    ap_vals.append(float(np.mean(conf_list)))\n            mAP_proxy = float(np.mean(ap_vals)) if len(ap_vals) > 0 else 0.0\n\n            # 4.4 Handle Output: overlay info and write to output video\n            info1 = f'Frame: {frame_idx}/{total_frames if total_frames > 0 else \"?\"}  Inference: {infer_time_ms:.1f} ms'\n            info2 = f'mAP (proxy): {mAP_proxy:.3f}  Detections: {len(kept)}  Thresh: {CONF_THRESHOLD:.2f}'\n            cv2.putText(original_frame, info1, (8, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n            cv2.putText(original_frame, info2, (8, 48), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n\n            writer.write(original_frame)\n\n            # 4.5 Progress logging (periodic)\n            now = time.time()\n            if now - last_print_time > 2.0:\n                print(f'Processed frame {frame_idx} | {len(kept)} detections | mAP (proxy): {mAP_proxy:.3f}')\n                last_print_time = now\n\n    finally:\n        # Phase 5: Cleanup\n        cap.release()\n        writer.release()\n\n    # Final report\n    final_ap_vals = []\n    for _, conf_list in class_confidences.items():\n        if len(conf_list) > 0:\n            final_ap_vals.append(float(np.mean(conf_list)))\n    final_map_proxy = float(np.mean(final_ap_vals)) if len(final_ap_vals) > 0 else 0.0\n\n    print('Inference complete.')\n    print(f'Approximate mAP (proxy, mean per-class confidence over kept detections): {final_map_proxy:.4f}')\n    print(f'Annotated video saved to: {output_path}')\n\n\nif __name__ == '__main__':\n    main()\n\n# -------------------------------------------------------------------------\n# REPORT OF THE FIX OF THE LAST ERROR:\n# The previous script crashed with IndexError when iterating detections:\n#   for i in range(num): cls_id_raw = int(round(float(classes[i])))\n# because 'num' exceeded the actual length of the 'classes' array.\n#\n# Fixes implemented:\n# 1) Robust output parsing (parse_detection_outputs) that:\n#    - Squeezes shapes, identifies boxes/classes/scores using names and heuristics.\n#    - Computes 'num' and then re-slices arrays safely.\n#    - Returns the final N as the minimum consistent length across outputs.\n# 2) In post-processing, iterate using N = min(num, len(boxes), len(classes), len(scores))\n#    to prevent out-of-bounds indexing.\n# These changes ensure safe indexing and prevent the IndexError.\n# -------------------------------------------------------------------------"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/f41d8291",
      "latency": 188.978,
      "total_cost": 0.1477735,
      "observations": [
        {
          "id": "time-03-43-19-002293_chatcmpl-D5igp5ALVLwlsiyxHNXohyabemN7I",
          "traceId": "f41d8291",
          "type": "GENERATION",
          "name": "f41d_psg_gen_attempt#1",
          "startTime": "2026-02-05T01:43:19.002000+00:00",
          "endTime": "2026-02-05T01:45:04.225000+00:00",
          "completionStartTime": "2026-02-05T01:45:04.225000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.074981,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script loads a TFLite SSD model and performs object detection on a single\ninput video file. It writes an output video with rectangles and labels drawn\nfor detected objects and overlays a running \"mAP (proxy)\" metric computed as\nthe mean of average per-class confidences of detections above threshold.\n\nPhases implemented per Programming Guidelines:\n- Phase 1: Setup (imports, paths, labels, interpreter, I/O details)\n- Phase 2: Input Acquisition & Preprocessing Loop\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling (explicit 4.2 & 4.3 implemented)\n- Phase 5: Cleanup\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Import TFLite interpreter per guideline\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef load_labels(label_file_path):\n    \"\"\"\n    Load labels from a text file into a list.\n    Ignores empty lines and trims whitespace.\n    \"\"\"\n    labels = []\n    if os.path.isfile(label_file_path):\n        with open(label_file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line != '':\n                    labels.append(line)\n    return labels\n\ndef create_video_writer(out_path, fps, frame_w, frame_h):\n    \"\"\"\n    Create a cv2.VideoWriter ensuring parent directories exist.\n    \"\"\"\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n    # Use mp4v codec for mp4 output; widely supported on Raspberry Pi\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(out_path, fourcc, fps, (frame_w, frame_h))\n    return writer\n\ndef color_for_class(class_id):\n    \"\"\"\n    Deterministic color for a class id.\n    \"\"\"\n    # Simple hashing to map class id to BGR color\n    np.random.seed(int(class_id) * 97 + 23)\n    color = tuple(int(c) for c in np.random.randint(0, 255, size=3))\n    return color\n\ndef clip_bbox(b, width, height):\n    \"\"\"\n    Clip bounding box to image boundaries.\n    b: (ymin, xmin, ymax, xmax) in absolute pixel coords\n    \"\"\"\n    ymin, xmin, ymax, xmax = b\n    ymin = max(0, min(int(round(ymin)), height - 1))\n    xmin = max(0, min(int(round(xmin)), width - 1))\n    ymax = max(0, min(int(round(ymax)), height - 1))\n    xmax = max(0, min(int(round(xmax)), width - 1))\n    return ymin, xmin, ymax, xmax\n\ndef interpret_outputs(output_tensors):\n    \"\"\"\n    Try to parse standard SSD MobileNet TFLite outputs into:\n    boxes [N,4], classes [N], scores [N], num [scalar or N]\n    This handles common variants by inspecting tensor shapes and values.\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    for arr in output_tensors:\n        a = arr\n        # Remove batch dim if present and batch=1\n        if a.ndim == 3 and a.shape[0] == 1:\n            a = a[0]\n        if a.ndim == 2 and a.shape[-1] == 4:\n            # Candidate for boxes\n            boxes = a\n        elif a.ndim == 2 and a.shape[0] == 4 and a.shape[1] != 4:\n            # Some models may transpose; handle cautiously (rare)\n            # Not expected; ignore\n            pass\n        elif a.ndim == 1 and a.size == 1:\n            # num_detections\n            num = int(round(float(a[0])))\n        elif a.ndim == 2:\n            # Could be [1, N] already handled; now maybe [N]\n            pass\n        elif a.ndim == 1:\n            # Could be classes or scores\n            pass\n\n    # If still ambiguous, specifically search by heuristics\n    # Collect flattened variants as [N] for classes/scores and [N,4] for boxes\n    flats = []\n    for arr in output_tensors:\n        a = arr\n        if a.ndim == 3 and a.shape[0] == 1:\n            a = a[0]\n        if a.ndim == 2 and a.shape[0] == 1:\n            a = a[0]\n        flats.append(a)\n\n    # Assign by heuristics\n    for a in flats:\n        if a.ndim == 2 and a.shape[-1] == 4:\n            boxes = a\n    # For 1-D arrays of same length (N), decide class vs score\n    cands = [a for a in flats if a.ndim == 1]\n    if len(cands) >= 2:\n        # Pick scores as the one in [0,1] range, classes as integer-like\n        for a in cands:\n            if a.size == 0:\n                continue\n            maxv, minv = float(np.max(a)), float(np.min(a))\n            # Scores typically 0..1\n            if 0.0 <= minv and maxv <= 1.0:\n                scores = a\n        for a in cands:\n            if a is scores:\n                continue\n            # Classes: integer-like values; may be float dtype\n            if a.size > 0:\n                frac = np.mod(a, 1.0)\n                if np.mean(frac == 0) > 0.9 or np.mean(np.abs(frac) < 1e-3) > 0.9:\n                    classes = a\n        # Fallback if not found\n        if classes is None and len(cands) >= 2:\n            # Choose the one with larger max as classes\n            cands2 = [a for a in cands if a is not scores]\n            if len(cands2) > 0:\n                classes = max(cands2, key=lambda x: float(np.max(x)))\n    elif len(cands) == 1:\n        # If only one is present, assume scores; classes may be embedded differently (unlikely)\n        a = cands[0]\n        maxv, minv = float(np.max(a)), float(np.min(a))\n        if 0.0 <= minv and maxv <= 1.0:\n            scores = a\n        else:\n            classes = a\n\n    # If num not provided, infer from available arrays\n    if num is None:\n        lengths = []\n        for a in [scores, classes]:\n            if a is not None:\n                lengths.append(int(a.shape[0]))\n        if boxes is not None:\n            lengths.append(int(boxes.shape[0]))\n        num = min(lengths) if lengths else 0\n\n    # Slice to num\n    if boxes is not None and boxes.shape[0] >= num:\n        boxes = boxes[:num]\n    if classes is not None and classes.shape[0] >= num:\n        classes = classes[:num]\n    if scores is not None and scores.shape[0] >= num:\n        scores = scores[:num]\n\n    return boxes, classes, scores, num\n\ndef main():\n    # 1.2 Paths/Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    CONF_THRESHOLD = float('0.5')\n\n    # 1.3 Load Labels (conditional and relevant)\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details (input/output tensor specs)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Assume single input tensor for SSD model\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n    # Expected input shape [1, H, W, C]\n    in_h, in_w, in_c = int(input_shape[1]), int(input_shape[2]), int(input_shape[3])\n    # Quantization parameters (if any)\n    input_scale, input_zero_point = (1.0, 0)\n    if 'quantization' in input_details[0] and input_details[0]['quantization'] is not None:\n        input_scale, input_zero_point = input_details[0]['quantization']\n        if input_scale is None:\n            input_scale = 1.0\n        if input_zero_point is None:\n            input_zero_point = 0\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: open the specific video file\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f'ERROR: Failed to open input video: {input_path}')\n        return\n\n    # Get input video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 25.0  # fallback\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Prepare output writer\n    writer = create_video_writer(output_path, fps, frame_w, frame_h)\n\n    # Stats for mAP (proxy)\n    # Accumulate class_id -> list of detection confidences (kept after thresholding)\n    class_confidences = {}\n\n    frame_idx = 0\n    last_print_time = time.time()\n\n    print('Starting inference on video...')\n    print(f'Input: {input_path}')\n    print(f'Output: {output_path}')\n    print(f'Model: {model_path}')\n    print(f'Labels: {label_path} (loaded {len(labels)} labels)')\n    print(f'Confidence threshold: {CONF_THRESHOLD}')\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break  # End of video\n\n            frame_idx += 1\n            original_frame = frame.copy()\n            # 2.2 Preprocess Data: resize and convert color to match model input\n            resized = cv2.resize(frame, (in_w, in_h))\n            # Convert BGR (OpenCV) to RGB as many models expect RGB\n            resized_rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n            # 2.3 Quantization Handling and dtype preparation\n            if input_dtype == np.float32:\n                # Normalize to [-1,1]\n                input_data = (np.float32(resized_rgb) - 127.5) / 127.5\n            elif input_dtype == np.uint8:\n                # Feed as uint8; many quantized SSD models expect [0,255] uint8\n                input_data = np.uint8(resized_rgb)\n            elif input_dtype == np.int8:\n                # Use quantization parameters to quantize normalized float input\n                float_in = (np.float32(resized_rgb) - 127.5) / 127.5\n                if input_scale == 0:\n                    input_scale = 1.0\n                q = np.round(float_in / input_scale + input_zero_point)\n                q = np.clip(q, -128, 127)\n                input_data = q.astype(np.int8)\n            else:\n                # Fallback: cast resized RGB to required dtype directly\n                input_data = resized_rgb.astype(input_dtype)\n\n            # Add batch dimension [1, H, W, C]\n            input_data = np.expand_dims(input_data, axis=0)\n\n            # Phase 3: Inference\n            # 3.1 Set Input Tensor(s)\n            interpreter.set_tensor(input_index, input_data)\n            # 3.2 Run Inference\n            t0 = time.time()\n            interpreter.invoke()\n            infer_time = (time.time() - t0) * 1000.0  # ms\n\n            # Phase 4: Output Interpretation & Handling\n            # 4.1 Get Output Tensors\n            output_tensors = []\n            for od in output_details:\n                out_data = interpreter.get_tensor(od['index'])\n                output_tensors.append(out_data)\n\n            # 4.2 Interpret Results\n            boxes, classes, scores, num = interpret_outputs(output_tensors)\n            if boxes is None or scores is None or classes is None:\n                # Unable to parse outputs; write original frame\n                writer.write(original_frame)\n                continue\n\n            # 4.3 Post-processing: thresholding, coordinate scaling, clipping\n            kept = []\n            for i in range(num):\n                score = float(scores[i])\n                if score < CONF_THRESHOLD:\n                    continue\n                cls_id_raw = int(round(float(classes[i])))\n                # Many TFLite SSD models produce boxes in normalized ymin, xmin, ymax, xmax\n                ymin, xmin, ymax, xmax = boxes[i]\n                # Scale to original frame size\n                ymin = ymin * frame_h\n                ymax = ymax * frame_h\n                xmin = xmin * frame_w\n                xmax = xmax * frame_w\n                ymin, xmin, ymax, xmax = clip_bbox((ymin, xmin, ymax, xmax), frame_w, frame_h)\n\n                kept.append((cls_id_raw, score, (xmin, ymin, xmax, ymax)))\n\n                # Accumulate for mAP (proxy)\n                class_confidences.setdefault(cls_id_raw, []).append(score)\n\n            # Draw detections\n            for (cls_id, score, (xmin, ymin, xmax, ymax)) in kept:\n                color = color_for_class(cls_id)\n                cv2.rectangle(original_frame, (xmin, ymin), (xmax, ymax), color, 2)\n                # Label name resolution with robust fallback\n                label_name = None\n                if 0 <= cls_id < len(labels):\n                    label_name = labels[cls_id]\n                elif 0 <= cls_id - 1 < len(labels):\n                    # Some label maps include a background at index 0\n                    label_name = labels[cls_id - 1]\n                else:\n                    label_name = f'id_{cls_id}'\n\n                caption = f'{label_name}: {score:.2f}'\n                # Put a filled rectangle for text background\n                (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n                cv2.rectangle(original_frame, (xmin, max(0, ymin - th - baseline - 4)),\n                              (xmin + tw + 6, ymin), color, thickness=-1)\n                cv2.putText(original_frame, caption, (xmin + 3, ymin - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n            # Compute running mAP (proxy): mean over classes of the mean confidence of kept detections\n            ap_values = []\n            for cls_id, conf_list in class_confidences.items():\n                if len(conf_list) > 0:\n                    ap_values.append(float(np.mean(conf_list)))\n            mAP_proxy = float(np.mean(ap_values)) if len(ap_values) > 0 else 0.0\n\n            # Overlay runtime info and mAP proxy\n            info_text_1 = f'Frame: {frame_idx}/{total_frames if total_frames>0 else \"?\"}  Inference: {infer_time:.1f} ms'\n            info_text_2 = f'mAP (proxy): {mAP_proxy:.3f}  Detections: {len(kept)}  Thresh: {CONF_THRESHOLD:.2f}'\n            cv2.putText(original_frame, info_text_1, (8, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n            cv2.putText(original_frame, info_text_2, (8, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n\n            # 4.4 Handle Output: write annotated frame to output video\n            writer.write(original_frame)\n\n            # 4.5 Optional progress print (not too frequent)\n            now = time.time()\n            if now - last_print_time > 2.0:\n                print(f'Processed frame {frame_idx} | {len(kept)} detections | mAP (proxy): {mAP_proxy:.3f}')\n                last_print_time = now\n\n    finally:\n        # Phase 5: Cleanup\n        cap.release()\n        writer.release()\n\n    # Final report\n    final_ap_values = []\n    for cls_id, conf_list in class_confidences.items():\n        if len(conf_list) > 0:\n            final_ap_values.append(float(np.mean(conf_list)))\n    final_map_proxy = float(np.mean(final_ap_values)) if len(final_ap_values) > 0 else 0.0\n\n    print('Inference complete.')\n    print(f'Approximate mAP (proxy, mean per-class confidence over kept detections): {final_map_proxy:.4f}')\n    print(f'Annotated video saved to: {output_path}')\n\nif __name__ == '__main__':\n    main()\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 7469,
            "total": 8969,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.074981,
          "latency": 105223.0,
          "timeToFirstToken": 105.223,
          "updatedAt": "2026-02-05T01:45:10.237Z",
          "totalTokens": 8969,
          "costDetails": {
            "total": 0.074981
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 7469,
          "usageDetails": {
            "input": 1500,
            "output": 7469,
            "total": 8969
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T01:45:05.154Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-03-45-05-161078_chatcmpl-D5iiXD4FcxtxCJu2N2dC0YBzW0vXp",
          "traceId": "f41d8291",
          "type": "GENERATION",
          "name": "f41d_psg_gen_attempt#2",
          "startTime": "2026-02-05T01:45:05.161000+00:00",
          "endTime": "2026-02-05T01:46:27.980000+00:00",
          "completionStartTime": "2026-02-05T01:46:27.980000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\n- The previous Python script generated has error as logged under ### CAUSED ERROR BEFORE ###.\n- The \"CAUSED ERROR BEFORE\" lists the error message and your previous faulty code caused the error is under ### EXECUTED CODE BEFORE ###. \n- *MAKE SURE YOU USED THE PROVIDED PATHS AND PARAMETERS*\n\n### INSTRUCTIONS ###\n- Regenerate the complete script, adhering strictly to Application Description and Core Requirements.\n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### provided below, REWRITE the complete Python script for the application 'Object Detection via a video file' to avoid the error.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- Input Method Description: 'Read a single video file from the given input_path'\n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path = 'models/ssd-mobilenet_v1/labelmap.txt'  \n- input_path = 'data/object_detection/sheeps.mp4'  \n- output_path = 'results/object_detection/test_results/sheeps_detections.mp4' \n- confidence_threshold = '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n### CAUSED ERROR BEFORE ###\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205034504_psg_gpt-5-2025-08-07/tmp_20260205034504_psg_gpt-5-2025-08-07.py\", line 381, in <module>\n    main()\n  File \"/home/wuguangh/Projects/tinyml-autopilot/results/object_detection/sketches/tmp_20260205034504_psg_gpt-5-2025-08-07/tmp_20260205034504_psg_gpt-5-2025-08-07.py\", line 305, in main\n    cls_id_raw = int(round(float(classes[i])))\nIndexError: index 1 is out of bounds for axis 0 with size 1\n\n\n### EXECUTED CODE BEFORE ###\n```python\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nApplication: Object Detection via a video file\nTarget Device: Raspberry Pi 4B\n\nThis script loads a TFLite SSD model and performs object detection on a single\ninput video file. It writes an output video with rectangles and labels drawn\nfor detected objects and overlays a running \"mAP (proxy)\" metric computed as\nthe mean of average per-class confidences of detections above threshold.\n\nPhases implemented per Programming Guidelines:\n- Phase 1: Setup (imports, paths, labels, interpreter, I/O details)\n- Phase 2: Input Acquisition & Preprocessing Loop\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling (explicit 4.2 & 4.3 implemented)\n- Phase 5: Cleanup\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Imports: Import TFLite interpreter per guideline\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef load_labels(label_file_path):\n    \"\"\"\n    Load labels from a text file into a list.\n    Ignores empty lines and trims whitespace.\n    \"\"\"\n    labels = []\n    if os.path.isfile(label_file_path):\n        with open(label_file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line != '':\n                    labels.append(line)\n    return labels\n\ndef create_video_writer(out_path, fps, frame_w, frame_h):\n    \"\"\"\n    Create a cv2.VideoWriter ensuring parent directories exist.\n    \"\"\"\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n    # Use mp4v codec for mp4 output; widely supported on Raspberry Pi\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(out_path, fourcc, fps, (frame_w, frame_h))\n    return writer\n\ndef color_for_class(class_id):\n    \"\"\"\n    Deterministic color for a class id.\n    \"\"\"\n    # Simple hashing to map class id to BGR color\n    np.random.seed(int(class_id) * 97 + 23)\n    color = tuple(int(c) for c in np.random.randint(0, 255, size=3))\n    return color\n\ndef clip_bbox(b, width, height):\n    \"\"\"\n    Clip bounding box to image boundaries.\n    b: (ymin, xmin, ymax, xmax) in absolute pixel coords\n    \"\"\"\n    ymin, xmin, ymax, xmax = b\n    ymin = max(0, min(int(round(ymin)), height - 1))\n    xmin = max(0, min(int(round(xmin)), width - 1))\n    ymax = max(0, min(int(round(ymax)), height - 1))\n    xmax = max(0, min(int(round(xmax)), width - 1))\n    return ymin, xmin, ymax, xmax\n\ndef interpret_outputs(output_tensors):\n    \"\"\"\n    Try to parse standard SSD MobileNet TFLite outputs into:\n    boxes [N,4], classes [N], scores [N], num [scalar or N]\n    This handles common variants by inspecting tensor shapes and values.\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    for arr in output_tensors:\n        a = arr\n        # Remove batch dim if present and batch=1\n        if a.ndim == 3 and a.shape[0] == 1:\n            a = a[0]\n        if a.ndim == 2 and a.shape[-1] == 4:\n            # Candidate for boxes\n            boxes = a\n        elif a.ndim == 2 and a.shape[0] == 4 and a.shape[1] != 4:\n            # Some models may transpose; handle cautiously (rare)\n            # Not expected; ignore\n            pass\n        elif a.ndim == 1 and a.size == 1:\n            # num_detections\n            num = int(round(float(a[0])))\n        elif a.ndim == 2:\n            # Could be [1, N] already handled; now maybe [N]\n            pass\n        elif a.ndim == 1:\n            # Could be classes or scores\n            pass\n\n    # If still ambiguous, specifically search by heuristics\n    # Collect flattened variants as [N] for classes/scores and [N,4] for boxes\n    flats = []\n    for arr in output_tensors:\n        a = arr\n        if a.ndim == 3 and a.shape[0] == 1:\n            a = a[0]\n        if a.ndim == 2 and a.shape[0] == 1:\n            a = a[0]\n        flats.append(a)\n\n    # Assign by heuristics\n    for a in flats:\n        if a.ndim == 2 and a.shape[-1] == 4:\n            boxes = a\n    # For 1-D arrays of same length (N), decide class vs score\n    cands = [a for a in flats if a.ndim == 1]\n    if len(cands) >= 2:\n        # Pick scores as the one in [0,1] range, classes as integer-like\n        for a in cands:\n            if a.size == 0:\n                continue\n            maxv, minv = float(np.max(a)), float(np.min(a))\n            # Scores typically 0..1\n            if 0.0 <= minv and maxv <= 1.0:\n                scores = a\n        for a in cands:\n            if a is scores:\n                continue\n            # Classes: integer-like values; may be float dtype\n            if a.size > 0:\n                frac = np.mod(a, 1.0)\n                if np.mean(frac == 0) > 0.9 or np.mean(np.abs(frac) < 1e-3) > 0.9:\n                    classes = a\n        # Fallback if not found\n        if classes is None and len(cands) >= 2:\n            # Choose the one with larger max as classes\n            cands2 = [a for a in cands if a is not scores]\n            if len(cands2) > 0:\n                classes = max(cands2, key=lambda x: float(np.max(x)))\n    elif len(cands) == 1:\n        # If only one is present, assume scores; classes may be embedded differently (unlikely)\n        a = cands[0]\n        maxv, minv = float(np.max(a)), float(np.min(a))\n        if 0.0 <= minv and maxv <= 1.0:\n            scores = a\n        else:\n            classes = a\n\n    # If num not provided, infer from available arrays\n    if num is None:\n        lengths = []\n        for a in [scores, classes]:\n            if a is not None:\n                lengths.append(int(a.shape[0]))\n        if boxes is not None:\n            lengths.append(int(boxes.shape[0]))\n        num = min(lengths) if lengths else 0\n\n    # Slice to num\n    if boxes is not None and boxes.shape[0] >= num:\n        boxes = boxes[:num]\n    if classes is not None and classes.shape[0] >= num:\n        classes = classes[:num]\n    if scores is not None and scores.shape[0] >= num:\n        scores = scores[:num]\n\n    return boxes, classes, scores, num\n\ndef main():\n    # 1.2 Paths/Parameters\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    CONF_THRESHOLD = float('0.5')\n\n    # 1.3 Load Labels (conditional and relevant)\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details (input/output tensor specs)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Assume single input tensor for SSD model\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n    # Expected input shape [1, H, W, C]\n    in_h, in_w, in_c = int(input_shape[1]), int(input_shape[2]), int(input_shape[3])\n    # Quantization parameters (if any)\n    input_scale, input_zero_point = (1.0, 0)\n    if 'quantization' in input_details[0] and input_details[0]['quantization'] is not None:\n        input_scale, input_zero_point = input_details[0]['quantization']\n        if input_scale is None:\n            input_scale = 1.0\n        if input_zero_point is None:\n            input_zero_point = 0\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: open the specific video file\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f'ERROR: Failed to open input video: {input_path}')\n        return\n\n    # Get input video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 25.0  # fallback\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Prepare output writer\n    writer = create_video_writer(output_path, fps, frame_w, frame_h)\n\n    # Stats for mAP (proxy)\n    # Accumulate class_id -> list of detection confidences (kept after thresholding)\n    class_confidences = {}\n\n    frame_idx = 0\n    last_print_time = time.time()\n\n    print('Starting inference on video...')\n    print(f'Input: {input_path}')\n    print(f'Output: {output_path}')\n    print(f'Model: {model_path}')\n    print(f'Labels: {label_path} (loaded {len(labels)} labels)')\n    print(f'Confidence threshold: {CONF_THRESHOLD}')\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break  # End of video\n\n            frame_idx += 1\n            original_frame = frame.copy()\n            # 2.2 Preprocess Data: resize and convert color to match model input\n            resized = cv2.resize(frame, (in_w, in_h))\n            # Convert BGR (OpenCV) to RGB as many models expect RGB\n            resized_rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n            # 2.3 Quantization Handling and dtype preparation\n            if input_dtype == np.float32:\n                # Normalize to [-1,1]\n                input_data = (np.float32(resized_rgb) - 127.5) / 127.5\n            elif input_dtype == np.uint8:\n                # Feed as uint8; many quantized SSD models expect [0,255] uint8\n                input_data = np.uint8(resized_rgb)\n            elif input_dtype == np.int8:\n                # Use quantization parameters to quantize normalized float input\n                float_in = (np.float32(resized_rgb) - 127.5) / 127.5\n                if input_scale == 0:\n                    input_scale = 1.0\n                q = np.round(float_in / input_scale + input_zero_point)\n                q = np.clip(q, -128, 127)\n                input_data = q.astype(np.int8)\n            else:\n                # Fallback: cast resized RGB to required dtype directly\n                input_data = resized_rgb.astype(input_dtype)\n\n            # Add batch dimension [1, H, W, C]\n            input_data = np.expand_dims(input_data, axis=0)\n\n            # Phase 3: Inference\n            # 3.1 Set Input Tensor(s)\n            interpreter.set_tensor(input_index, input_data)\n            # 3.2 Run Inference\n            t0 = time.time()\n            interpreter.invoke()\n            infer_time = (time.time() - t0) * 1000.0  # ms\n\n            # Phase 4: Output Interpretation & Handling\n            # 4.1 Get Output Tensors\n            output_tensors = []\n            for od in output_details:\n                out_data = interpreter.get_tensor(od['index'])\n                output_tensors.append(out_data)\n\n            # 4.2 Interpret Results\n            boxes, classes, scores, num = interpret_outputs(output_tensors)\n            if boxes is None or scores is None or classes is None:\n                # Unable to parse outputs; write original frame\n                writer.write(original_frame)\n                continue\n\n            # 4.3 Post-processing: thresholding, coordinate scaling, clipping\n            kept = []\n            for i in range(num):\n                score = float(scores[i])\n                if score < CONF_THRESHOLD:\n                    continue\n                cls_id_raw = int(round(float(classes[i])))\n                # Many TFLite SSD models produce boxes in normalized ymin, xmin, ymax, xmax\n                ymin, xmin, ymax, xmax = boxes[i]\n                # Scale to original frame size\n                ymin = ymin * frame_h\n                ymax = ymax * frame_h\n                xmin = xmin * frame_w\n                xmax = xmax * frame_w\n                ymin, xmin, ymax, xmax = clip_bbox((ymin, xmin, ymax, xmax), frame_w, frame_h)\n\n                kept.append((cls_id_raw, score, (xmin, ymin, xmax, ymax)))\n\n                # Accumulate for mAP (proxy)\n                class_confidences.setdefault(cls_id_raw, []).append(score)\n\n            # Draw detections\n            for (cls_id, score, (xmin, ymin, xmax, ymax)) in kept:\n                color = color_for_class(cls_id)\n                cv2.rectangle(original_frame, (xmin, ymin), (xmax, ymax), color, 2)\n                # Label name resolution with robust fallback\n                label_name = None\n                if 0 <= cls_id < len(labels):\n                    label_name = labels[cls_id]\n                elif 0 <= cls_id - 1 < len(labels):\n                    # Some label maps include a background at index 0\n                    label_name = labels[cls_id - 1]\n                else:\n                    label_name = f'id_{cls_id}'\n\n                caption = f'{label_name}: {score:.2f}'\n                # Put a filled rectangle for text background\n                (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n                cv2.rectangle(original_frame, (xmin, max(0, ymin - th - baseline - 4)),\n                              (xmin + tw + 6, ymin), color, thickness=-1)\n                cv2.putText(original_frame, caption, (xmin + 3, ymin - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n            # Compute running mAP (proxy): mean over classes of the mean confidence of kept detections\n            ap_values = []\n            for cls_id, conf_list in class_confidences.items():\n                if len(conf_list) > 0:\n                    ap_values.append(float(np.mean(conf_list)))\n            mAP_proxy = float(np.mean(ap_values)) if len(ap_values) > 0 else 0.0\n\n            # Overlay runtime info and mAP proxy\n            info_text_1 = f'Frame: {frame_idx}/{total_frames if total_frames>0 else \"?\"}  Inference: {infer_time:.1f} ms'\n            info_text_2 = f'mAP (proxy): {mAP_proxy:.3f}  Detections: {len(kept)}  Thresh: {CONF_THRESHOLD:.2f}'\n            cv2.putText(original_frame, info_text_1, (8, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n            cv2.putText(original_frame, info_text_2, (8, 45), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n\n            # 4.4 Handle Output: write annotated frame to output video\n            writer.write(original_frame)\n\n            # 4.5 Optional progress print (not too frequent)\n            now = time.time()\n            if now - last_print_time > 2.0:\n                print(f'Processed frame {frame_idx} | {len(kept)} detections | mAP (proxy): {mAP_proxy:.3f}')\n                last_print_time = now\n\n    finally:\n        # Phase 5: Cleanup\n        cap.release()\n        writer.release()\n\n    # Final report\n    final_ap_values = []\n    for cls_id, conf_list in class_confidences.items():\n        if len(conf_list) > 0:\n            final_ap_values.append(float(np.mean(conf_list)))\n    final_map_proxy = float(np.mean(final_ap_values)) if len(final_ap_values) > 0 else 0.0\n\n    print('Inference complete.')\n    print(f'Approximate mAP (proxy, mean per-class confidence over kept detections): {final_map_proxy:.4f}')\n    print(f'Annotated video saved to: {output_path}')\n\nif __name__ == '__main__':\n    main()\n```\n\n### OUTPUT TEMPLATE ###\n```python\n<complete_code>\n```\n<REPORT THE FIX OF THE LAST ERROR>\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 994283
              }
            },
            "litellm_response_cost": 0.0727925,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nApplication: Object Detection via a video file\nDescription:\n- Reads a single video file from the given input_path.\n- Runs SSD MobileNet v1 TFLite model inference using ai_edge_litert Interpreter.\n- Outputs a video file with rectangles drawn on detected objects, labels, and an on-frame\n  running mAP (proxy) metric. Also prints a final proxy mAP summary.\n\nPhases implemented per Programming Guidelines:\n- Phase 1: Setup (imports, paths, labels, interpreter, I/O details)\n- Phase 2: Input Acquisition & Preprocessing Loop (explicitly implemented)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling (explicit 4.2 & 4.3 implemented)\n- Phase 5: Cleanup\n\nNote:\n- Only standard libraries plus numpy and cv2 are used for video/image processing.\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\n\n# Phase 1: Setup\n# 1.1 Import Interpreter exactly as specified\nfrom ai_edge_litert.interpreter import Interpreter\n\ndef load_labels(label_file_path):\n    \"\"\"\n    Load labels from a text file into a list, stripping whitespace and skipping empty lines.\n    \"\"\"\n    labels = []\n    if os.path.isfile(label_file_path):\n        with open(label_file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line != '':\n                    labels.append(line)\n    return labels\n\ndef create_video_writer(out_path, fps, frame_w, frame_h):\n    \"\"\"\n    Create a cv2.VideoWriter ensuring parent directories exist.\n    \"\"\"\n    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # mp4 codec\n    writer = cv2.VideoWriter(out_path, fourcc, float(fps), (int(frame_w), int(frame_h)))\n    return writer\n\ndef color_for_class(class_id):\n    \"\"\"\n    Deterministic BGR color for a class id.\n    \"\"\"\n    rng = np.random.RandomState(int(class_id) * 2654435761 % (2**32))\n    color = tuple(int(c) for c in rng.randint(0, 255, size=3))\n    return color\n\ndef clip_bbox_to_frame(b, width, height):\n    \"\"\"\n    Clip bounding box to image boundaries; input b=(ymin,xmin,ymax,xmax) in absolute pixels.\n    Returns ints: (ymin,xmin,ymax,xmax).\n    \"\"\"\n    ymin, xmin, ymax, xmax = b\n    ymin = max(0, min(int(round(ymin)), height - 1))\n    xmin = max(0, min(int(round(xmin)), width - 1))\n    ymax = max(0, min(int(round(ymax)), height - 1))\n    xmax = max(0, min(int(round(xmax)), width - 1))\n    # Ensure proper ordering after clipping\n    ymin, ymax = min(ymin, ymax), max(ymin, ymax)\n    xmin, xmax = min(xmin, xmax), max(xmin, xmax)\n    return ymin, xmin, ymax, xmax\n\ndef squeeze_out(arr):\n    \"\"\"\n    Remove leading batch dims of size 1 and trailing singleton dims where appropriate.\n    Keeps 2D for boxes and 1D for classes/scores.\n    \"\"\"\n    a = arr\n    # Remove leading batch dimension if 1\n    while a.ndim > 0 and a.shape[0] == 1 and a.ndim > 1:\n        a = a[0]\n    # If still has shape like [1, N], squeeze to [N]\n    if a.ndim == 2 and a.shape[0] == 1:\n        a = a[0]\n    return a\n\ndef parse_detection_outputs(output_details, output_arrays):\n    \"\"\"\n    Parse outputs from SSD-style TFLite detection model.\n\n    Returns:\n      boxes: np.ndarray of shape [N,4] (ymin, xmin, ymax, xmax)\n      classes: np.ndarray of shape [N] (float/int class ids)\n      scores: np.ndarray of shape [N] (float confidences in [0,1])\n      num: int, number of valid detections suggested by the model\n    \"\"\"\n    # Squeeze and collect\n    arrays = [squeeze_out(a) for a in output_arrays]\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # First, try to use names if present\n    names = []\n    try:\n        names = [od.get('name', '') for od in output_details]\n    except Exception:\n        names = [''] * len(output_details)\n\n    # Map by typical names when available\n    for a, n in zip(arrays, names):\n        n_low = (n or '').lower()\n        if 'box' in n_low or 'bbox' in n_low or (a.ndim == 2 and a.shape[-1] == 4):\n            if a.ndim == 2 and a.shape[-1] == 4:\n                boxes = a\n        if ('class' in n_low or 'label' in n_low) and a.ndim == 1:\n            classes = a\n        if ('score' in n_low or 'confidence' in n_low) and a.ndim == 1:\n            scores = a\n        if 'num' in n_low and a.size == 1:\n            try:\n                num = int(round(float(np.squeeze(a))))\n            except Exception:\n                pass\n\n    # Heuristic fallback if any missing\n    if boxes is None:\n        for a in arrays:\n            if a.ndim == 2 and a.shape[-1] == 4:\n                boxes = a\n                break\n\n    one_d = [a for a in arrays if a.ndim == 1 and a.size > 0]\n    if scores is None:\n        # scores expected to be in [0,1]\n        cand = None\n        best_range_fit = -1\n        for a in one_d:\n            amin, amax = float(np.min(a)), float(np.max(a))\n            # Score arrays should largely be within [0,1]\n            comp = -abs(0.0 - amin) - abs(1.0 - max(min(amax, 1.0), 0.0))\n            if comp > best_range_fit:\n                best_range_fit = comp\n                cand = a\n        scores = cand\n\n    if classes is None and len(one_d) > 0:\n        # Choose the 1-D array different from scores with more integer-like values\n        cand_list = [a for a in one_d if a is not scores]\n        if len(cand_list) == 1:\n            classes = cand_list[0]\n        elif len(cand_list) > 1:\n            best_int_like = -1.0\n            best = None\n            for a in cand_list:\n                frac = np.mod(a, 1.0)\n                int_like = float(np.mean(np.abs(frac) < 1e-3))\n                if int_like > best_int_like:\n                    best_int_like = int_like\n                    best = a\n            classes = best\n\n    if num is None:\n        # Infer num from the smallest consistent length\n        lengths = []\n        if boxes is not None:\n            lengths.append(int(boxes.shape[0]))\n        if classes is not None:\n            lengths.append(int(classes.shape[0]))\n        if scores is not None:\n            lengths.append(int(scores.shape[0]))\n        num = int(min(lengths)) if len(lengths) > 0 else 0\n\n    # Ensure proper shapes and consistent lengths\n    if boxes is not None and boxes.ndim != 2:\n        boxes = np.reshape(boxes, (-1, 4)) if boxes.size % 4 == 0 else None\n\n    # Final safety trims based on computed num\n    def safe_slice_1d(a, n):\n        if a is None:\n            return None\n        return a[:min(n, a.shape[0])] if a.ndim == 1 else a\n\n    def safe_slice_2d(a, n):\n        if a is None:\n            return None\n        return a[:min(n, a.shape[0]), :] if a.ndim == 2 else a\n\n    boxes = safe_slice_2d(boxes, num)\n    classes = safe_slice_1d(classes, num)\n    scores = safe_slice_1d(scores, num)\n\n    # Recompute n as the minimum available length to prevent OOB\n    lengths = []\n    if boxes is not None and boxes.ndim == 2:\n        lengths.append(int(boxes.shape[0]))\n    if classes is not None and classes.ndim == 1:\n        lengths.append(int(classes.shape[0]))\n    if scores is not None and scores.ndim == 1:\n        lengths.append(int(scores.shape[0]))\n    n_final = int(min(lengths)) if len(lengths) > 0 else 0\n\n    return boxes, classes, scores, n_final\n\ndef main():\n    # 1.2 Paths/Parameters (provided)\n    model_path = 'models/ssd-mobilenet_v1/detect.tflite'\n    label_path = 'models/ssd-mobilenet_v1/labelmap.txt'\n    input_path = 'data/object_detection/sheeps.mp4'\n    output_path = 'results/object_detection/test_results/sheeps_detections.mp4'\n    CONF_THRESHOLD = float('0.5')\n\n    # 1.3 Load Labels\n    labels = load_labels(label_path)\n\n    # 1.4 Load Interpreter\n    interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n\n    # 1.5 Get Model Details\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Single input expected: [1, H, W, C]\n    input_index = input_details[0]['index']\n    input_shape = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n    in_h, in_w, in_c = int(input_shape[1]), int(input_shape[2]), int(input_shape[3])\n\n    # Quantization params if present\n    input_scale, input_zero_point = (1.0, 0)\n    try:\n        q = input_details[0].get('quantization', (1.0, 0))\n        if isinstance(q, (tuple, list)) and len(q) == 2:\n            input_scale, input_zero_point = q\n            input_scale = 1.0 if input_scale is None else input_scale\n            input_zero_point = 0 if input_zero_point is None else input_zero_point\n    except Exception:\n        pass\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    # 2.1 Acquire Input Data: open the specific video file\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f'ERROR: Failed to open input video: {input_path}')\n        return\n\n    # Input video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    fps = fps if fps and not np.isnan(fps) and fps > 0 else 25.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) > 0 else -1\n\n    # Create output writer\n    writer = create_video_writer(output_path, fps, frame_w, frame_h)\n\n    # Stats for mAP (proxy): per-class list of confidences above threshold\n    class_confidences = {}\n\n    frame_idx = 0\n    last_print_time = time.time()\n\n    print('Starting inference on video...')\n    print(f'Input: {input_path}')\n    print(f'Output: {output_path}')\n    print(f'Model: {model_path}')\n    print(f'Labels: {label_path} (loaded {len(labels)} labels)')\n    print(f'Confidence threshold: {CONF_THRESHOLD:.2f}')\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break  # End of video\n            frame_idx += 1\n\n            original_frame = frame  # will annotate directly\n\n            # 2.2 Preprocess Data: resize to model input, convert BGR->RGB\n            resized = cv2.resize(original_frame, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n            rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n            # 2.3 Quantization Handling\n            if input_dtype == np.float32:\n                # Normalize to [-1, 1] as common for SSD MobileNet\n                input_data = (np.float32(rgb) - 127.5) / 127.5\n            elif input_dtype == np.uint8:\n                input_data = np.uint8(rgb)\n            elif input_dtype == np.int8:\n                # Quantize from normalized float using scale/zero-point when provided\n                float_in = (np.float32(rgb) - 127.5) / 127.5\n                scale = input_scale if input_scale not in (None, 0) else 1.0\n                zero = input_zero_point if input_zero_point is not None else 0\n                q = np.round(float_in / scale + zero)\n                q = np.clip(q, -128, 127)\n                input_data = q.astype(np.int8)\n            else:\n                input_data = rgb.astype(input_dtype)\n\n            # Add batch dimension [1, H, W, C]\n            input_data = np.expand_dims(input_data, axis=0)\n\n            # Phase 3: Inference\n            interpreter.set_tensor(input_index, input_data)\n            t0 = time.time()\n            interpreter.invoke()\n            infer_time_ms = (time.time() - t0) * 1000.0\n\n            # Phase 4: Output Interpretation & Handling\n            # 4.1 Get Output Tensor(s)\n            output_arrays = []\n            for od in output_details:\n                out = interpreter.get_tensor(od['index'])\n                output_arrays.append(out)\n\n            # 4.2 Interpret Results: parse SSD outputs (boxes/classes/scores/num)\n            boxes, classes, scores, num = parse_detection_outputs(output_details, output_arrays)\n\n            # 4.3 Post-processing: thresholding, coordinate scaling, clipping\n            kept = []\n            if boxes is not None and classes is not None and scores is not None and num > 0:\n                # Decide if boxes are normalized or absolute by checking range\n                # If typical SSD output: normalized [0,1]. If max coord > 2, treat as absolute.\n                max_box_val = float(np.max(np.abs(boxes))) if boxes.size > 0 else 0.0\n                boxes_are_normalized = not (max_box_val > 2.0)\n\n                N = int(min(num, boxes.shape[0], classes.shape[0], scores.shape[0]))\n                for i in range(N):\n                    score = float(scores[i])\n                    if score < CONF_THRESHOLD:\n                        continue\n\n                    # Class id (may be float from model); round then int\n                    cls_id_raw = int(round(float(classes[i])))\n\n                    y_min, x_min, y_max, x_max = boxes[i]\n                    if boxes_are_normalized:\n                        y_min *= frame_h\n                        y_max *= frame_h\n                        x_min *= frame_w\n                        x_max *= frame_w\n\n                    y_min, x_min, y_max, x_max = clip_bbox_to_frame((y_min, x_min, y_max, x_max), frame_w, frame_h)\n                    kept.append((cls_id_raw, score, (x_min, y_min, x_max, y_max)))\n\n                    # Accumulate for proxy mAP\n                    class_confidences.setdefault(cls_id_raw, []).append(score)\n\n            # Draw detections with labels\n            for (cls_id, score, (x_min, y_min, x_max, y_max)) in kept:\n                color = color_for_class(cls_id)\n                cv2.rectangle(original_frame, (x_min, y_min), (x_max, y_max), color, 2)\n\n                # Resolve label name\n                if 0 <= cls_id < len(labels):\n                    label_name = labels[cls_id]\n                elif 0 <= (cls_id - 1) < len(labels):\n                    # Some SSD label maps are 1-based (background at 0)\n                    label_name = labels[cls_id - 1]\n                else:\n                    label_name = f'id_{cls_id}'\n\n                caption = f'{label_name}: {score:.2f}'\n                (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n                y_text_top = max(0, y_min - th - bl - 4)\n                cv2.rectangle(original_frame, (x_min, y_text_top), (x_min + tw + 6, y_min), color, thickness=-1)\n                cv2.putText(original_frame, caption, (x_min + 3, y_min - 5),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n            # Compute running mAP (proxy): mean of per-class mean confidences\n            ap_vals = []\n            for _, conf_list in class_confidences.items():\n                if len(conf_list) > 0:\n                    ap_vals.append(float(np.mean(conf_list)))\n            mAP_proxy = float(np.mean(ap_vals)) if len(ap_vals) > 0 else 0.0\n\n            # 4.4 Handle Output: overlay info and write to output video\n            info1 = f'Frame: {frame_idx}/{total_frames if total_frames > 0 else \"?\"}  Inference: {infer_time_ms:.1f} ms'\n            info2 = f'mAP (proxy): {mAP_proxy:.3f}  Detections: {len(kept)}  Thresh: {CONF_THRESHOLD:.2f}'\n            cv2.putText(original_frame, info1, (8, 22), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n            cv2.putText(original_frame, info2, (8, 48), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (20, 220, 20), 2, cv2.LINE_AA)\n\n            writer.write(original_frame)\n\n            # 4.5 Progress logging (periodic)\n            now = time.time()\n            if now - last_print_time > 2.0:\n                print(f'Processed frame {frame_idx} | {len(kept)} detections | mAP (proxy): {mAP_proxy:.3f}')\n                last_print_time = now\n\n    finally:\n        # Phase 5: Cleanup\n        cap.release()\n        writer.release()\n\n    # Final report\n    final_ap_vals = []\n    for _, conf_list in class_confidences.items():\n        if len(conf_list) > 0:\n            final_ap_vals.append(float(np.mean(conf_list)))\n    final_map_proxy = float(np.mean(final_ap_vals)) if len(final_ap_vals) > 0 else 0.0\n\n    print('Inference complete.')\n    print(f'Approximate mAP (proxy, mean per-class confidence over kept detections): {final_map_proxy:.4f}')\n    print(f'Annotated video saved to: {output_path}')\n\n\nif __name__ == '__main__':\n    main()\n\n# -------------------------------------------------------------------------\n# REPORT OF THE FIX OF THE LAST ERROR:\n# The previous script crashed with IndexError when iterating detections:\n#   for i in range(num): cls_id_raw = int(round(float(classes[i])))\n# because 'num' exceeded the actual length of the 'classes' array.\n#\n# Fixes implemented:\n# 1) Robust output parsing (parse_detection_outputs) that:\n#    - Squeezes shapes, identifies boxes/classes/scores using names and heuristics.\n#    - Computes 'num' and then re-slices arrays safely.\n#    - Returns the final N as the minimum consistent length across outputs.\n# 2) In post-processing, iterate using N = min(num, len(boxes), len(classes), len(scores))\n#    to prevent out-of-bounds indexing.\n# These changes ensure safe indexing and prevent the IndexError.\n# -------------------------------------------------------------------------\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 5602,
            "output": 6579,
            "total": 12181,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0727925,
          "latency": 82819.0,
          "timeToFirstToken": 82.819,
          "updatedAt": "2026-02-05T01:46:33.347Z",
          "totalTokens": 12181,
          "costDetails": {
            "total": 0.0727925
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 6579,
          "usageDetails": {
            "input": 5602,
            "output": 6579,
            "total": 12181
          },
          "unit": "TOKENS",
          "promptTokens": 5602,
          "environment": "default",
          "createdAt": "2026-02-05T01:46:28.234Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T01:47:21.217Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T01:43:18.899Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "5e457efb",
      "timestamp": "2026-02-05T01:40:35.231000+00:00",
      "name": "5e45_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TFLite SSD model,\ndraws bounding boxes and labels for detected objects, and writes the annotated video to\nthe specified output path. It also computes and overlays a running \"mAP\" proxy (confidence-based)\nover the processed frames, given the lack of ground-truth annotations.\n\nPhases implemented per Programming Guideline:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video file reading and preprocessing)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop (detection parsing, thresholding, clipping,\n           drawing, running mAP-proxy computation, and video writing)\n- Phase 5: Cleanup\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# ==========================\n# Phase 1: Setup\n# ==========================\n\n# 1.2 Paths/Parameters (from provided configuration)\nmodel_path  = \"models/ssd-mobilenet_v1/detect.tflite\"\nlabel_path  = \"models/ssd-mobilenet_v1/labelmap.txt\"\ninput_path  = \"data/object_detection/sheeps.mp4\"\noutput_path  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = float('0.5')  # Confidence threshold, converted from string parameter\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n# 1.3 Load Labels (if provided and relevant)\ndef load_labels(path):\n    labels = []\n    if os.path.isfile(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(label_path)\n\n# Helper: Convert class id to label string\ndef get_label_name(class_id):\n    if 0 <= class_id < len(labels):\n        return labels[class_id]\n    else:\n        return f'class_{class_id}'\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input shape and dtype\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Parse expected input size\nif len(input_shape) == 4:\n    _, in_h, in_w, in_c = input_shape\nelse:\n    raise RuntimeError(f'Unexpected input tensor shape: {input_shape}')\n\n# ==========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# (Read a single video file from the given input_path)\n# ==========================\n\n# Open input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise FileNotFoundError(f'Unable to open video file: {input_path}')\n\n# Read first frame to get properties\nret, frame = cap.read()\nif not ret or frame is None:\n    cap.release()\n    raise RuntimeError('Failed to read the first frame from the input video.')\n\nframe_h, frame_w = frame.shape[:2]\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0:\n    fps = 30.0  # Fallback if FPS is not available\n\n# Prepare video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\nif not writer.isOpened():\n    cap.release()\n    raise RuntimeError(f'Unable to open VideoWriter for: {output_path}')\n\n# mAP proxy tracking: collect scores per class across frames\n# We use a confidence-based proxy due to lack of ground-truth annotations.\nclass_scores = {}  # dict: class_id -> list of detection scores\n\n# Utility: color generation per class id\ndef class_id_to_color(cid):\n    # Simple deterministic color from class id\n    np.random.seed(cid + 12345)\n    color = np.random.randint(0, 255, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\n# Helper: Compute running mAP proxy (mean of per-class mean confidences)\ndef compute_map_proxy(scores_dict):\n    per_class_aps = []\n    for cid, scores in scores_dict.items():\n        if len(scores) > 0:\n            per_class_aps.append(float(np.mean(scores)))\n    if len(per_class_aps) == 0:\n        return 0.0\n    return float(np.mean(per_class_aps))\n\n# Frame processing loop\nframe_index = 0\n\nwhile True:\n    # 2.2 Preprocess Data for current frame\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (in_w, in_h))\n    input_data = np.expand_dims(resized, axis=0)\n\n    # 2.3 Quantization Handling\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = input_data.astype(input_dtype)\n\n    # ==========================\n    # Phase 3: Inference\n    # ==========================\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\n    # ==========================\n    # Phase 4: Output Interpretation & Handling Loop\n    # ==========================\n\n    # 4.1 Get Output Tensors\n    raw_outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    # 4.2 Interpret Results (identify boxes, classes, scores, num_detections)\n    boxes = None\n    classes = None\n    scores = None\n    num_detections = None\n\n    # Robust detection of outputs by shapes/ranges\n    for arr in raw_outputs:\n        arr_np = np.squeeze(arr)\n        arr_shape = arr.shape\n        if len(arr_shape) == 3 and arr_shape[-1] == 4:\n            # Expect (1, N, 4) for boxes\n            boxes = arr[0]\n        elif arr_np.size == 1:\n            # num_detections\n            num_detections = int(round(float(arr_np)))\n        elif len(arr_shape) == 2 and arr_shape[0] == 1:\n            # Could be scores or classes\n            # Scores are usually in [0,1]; classes are ints (as float) > 1.0\n            max_val = float(np.max(arr))\n            min_val = float(np.min(arr))\n            if max_val <= 1.0 and min_val >= 0.0 and scores is None:\n                scores = arr[0]\n            elif classes is None:\n                classes = arr[0]\n\n    # Fallbacks if any are missing\n    if boxes is None:\n        # Try to find any array with last dim 4\n        for arr in raw_outputs:\n            if len(arr.shape) >= 2 and arr.shape[-1] == 4:\n                boxes = arr[0]\n                break\n    if classes is None:\n        # Identify non-scores array if possible\n        for arr in raw_outputs:\n            if len(arr.shape) == 2 and arr.shape[0] == 1:\n                max_val = float(np.max(arr))\n                min_val = float(np.min(arr))\n                if not (max_val <= 1.0 and min_val >= 0.0):\n                    classes = arr[0]\n                    break\n    if scores is None:\n        for arr in raw_outputs:\n            if len(arr.shape) == 2 and arr.shape[0] == 1:\n                max_val = float(np.max(arr))\n                min_val = float(np.min(arr))\n                if max_val <= 1.0 and min_val >= 0.0:\n                    scores = arr[0]\n                    break\n    if boxes is None or classes is None or scores is None:\n        # Cannot interpret outputs; write the original frame and continue\n        cv2.putText(frame, 'Detection outputs not found', (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n        # Overlay mAP proxy so far\n        map_proxy = compute_map_proxy(class_scores)\n        cv2.putText(frame, f'mAP: {map_proxy * 100:.2f}%', (10, 60),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 200, 0), 2, cv2.LINE_AA)\n        writer.write(frame)\n    else:\n        # Ensure 1D arrays and consistent length\n        if len(boxes.shape) == 3:\n            boxes = boxes[0]\n        if len(classes.shape) == 2:\n            classes = classes[0]\n        if len(scores.shape) == 2:\n            scores = scores[0]\n\n        if num_detections is None:\n            num_detections = min(len(scores), len(classes), len(boxes))\n        else:\n            num_detections = min(num_detections, len(scores), len(classes), len(boxes))\n\n        # 4.3 Post-processing: thresholding, coordinate scaling, and clipping\n        for i in range(num_detections):\n            score = float(scores[i])\n            if score < CONF_THRESHOLD:\n                continue\n\n            cid = int(classes[i])  # class id may come as float; cast to int\n            y_min, x_min, y_max, x_max = boxes[i]  # normalized coordinates\n\n            # Clip normalized coordinates to [0,1]\n            y_min = max(0.0, min(1.0, float(y_min)))\n            x_min = max(0.0, min(1.0, float(x_min)))\n            y_max = max(0.0, min(1.0, float(y_max)))\n            x_max = max(0.0, min(1.0, float(x_max)))\n\n            # Scale to image coordinates\n            left = int(round(x_min * frame_w))\n            top = int(round(y_min * frame_h))\n            right = int(round(x_max * frame_w))\n            bottom = int(round(y_max * frame_h))\n\n            # Ensure proper ordering\n            left, right = min(left, right), max(left, right)\n            top, bottom = min(top, bottom), max(top, bottom)\n\n            # Clip to frame bounds\n            left = max(0, min(frame_w - 1, left))\n            right = max(0, min(frame_w - 1, right))\n            top = max(0, min(frame_h - 1, top))\n            bottom = max(0, min(frame_h - 1, bottom))\n\n            # Skip invalid/empty boxes\n            if right <= left or bottom <= top:\n                continue\n\n            # Update running per-class scores for mAP proxy\n            if cid not in class_scores:\n                class_scores[cid] = []\n            class_scores[cid].append(score)\n\n            # Draw rectangle and label text\n            color = class_id_to_color(cid)\n            cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n\n            label_name = get_label_name(cid)\n            caption = f'{label_name}: {score:.2f}'\n            # Draw label background for readability\n            (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n            cv2.rectangle(frame, (left, max(0, top - th - baseline - 4)),\n                          (left + tw + 4, top), color, thickness=-1)\n            cv2.putText(frame, caption, (left + 2, top - baseline - 2),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n\n        # Compute and overlay running mAP proxy\n        map_proxy = compute_map_proxy(class_scores)\n        cv2.putText(frame, f'mAP: {map_proxy * 100:.2f}%', (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 200, 0), 2, cv2.LINE_AA)\n\n        # 4.4 Handle Output: write annotated frame to output video\n        writer.write(frame)\n\n    # 4.5 Loop continuation: read next frame or break\n    ret, frame = cap.read()\n    if not ret or frame is None:\n        break\n    frame_index += 1\n\n# ==========================\n# Phase 5: Cleanup\n# ==========================\ncap.release()\nwriter.release()\n\n# Print final mAP proxy summary\nfinal_map_proxy = compute_map_proxy(class_scores)\nprint(f'Processing completed. Estimated mAP (confidence-based proxy): {final_map_proxy * 100:.2f}%')\nprint(f'Annotated video written to: {output_path}')"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/5e457efb",
      "latency": 104.672,
      "total_cost": 0.077571,
      "observations": [
        {
          "id": "time-03-40-35-815355_chatcmpl-D5ieCLUEfKFLNe1qKTML8JSNWa5uG",
          "traceId": "5e457efb",
          "type": "GENERATION",
          "name": "5e45_psg_gen_attempt#1",
          "startTime": "2026-02-05T01:40:35.815000+00:00",
          "endTime": "2026-02-05T01:42:20.487000+00:00",
          "completionStartTime": "2026-02-05T01:42:20.487000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.077571,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\n#!/usr/bin/env python3\n\"\"\"\nApplication: Object Detection via a video file\nTarget device: Raspberry Pi 4B\n\nThis script performs object detection on a single input video using a TFLite SSD model,\ndraws bounding boxes and labels for detected objects, and writes the annotated video to\nthe specified output path. It also computes and overlays a running \"mAP\" proxy (confidence-based)\nover the processed frames, given the lack of ground-truth annotations.\n\nPhases implemented per Programming Guideline:\n- Phase 1: Setup (imports, paths, labels, interpreter, model details)\n- Phase 2: Input Acquisition & Preprocessing Loop (video file reading and preprocessing)\n- Phase 3: Inference\n- Phase 4: Output Interpretation & Handling Loop (detection parsing, thresholding, clipping,\n           drawing, running mAP-proxy computation, and video writing)\n- Phase 5: Cleanup\n\"\"\"\n\nimport os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# ==========================\n# Phase 1: Setup\n# ==========================\n\n# 1.2 Paths/Parameters (from provided configuration)\nmodel_path = 'models/ssd-mobilenet_v1/detect.tflite'\nlabel_path = 'models/ssd-mobilenet_v1/labelmap.txt'\ninput_path = 'data/object_detection/sheeps.mp4'\noutput_path = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = float('0.5')  # Confidence threshold, converted from string parameter\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n# 1.3 Load Labels (if provided and relevant)\ndef load_labels(path):\n    labels = []\n    if os.path.isfile(path):\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    return labels\n\nlabels = load_labels(label_path)\n\n# Helper: Convert class id to label string\ndef get_label_name(class_id):\n    if 0 <= class_id < len(labels):\n        return labels[class_id]\n    else:\n        return f'class_{class_id}'\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=model_path)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Extract input shape and dtype\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, H, W, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Parse expected input size\nif len(input_shape) == 4:\n    _, in_h, in_w, in_c = input_shape\nelse:\n    raise RuntimeError(f'Unexpected input tensor shape: {input_shape}')\n\n# ==========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# (Read a single video file from the given input_path)\n# ==========================\n\n# Open input video\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise FileNotFoundError(f'Unable to open video file: {input_path}')\n\n# Read first frame to get properties\nret, frame = cap.read()\nif not ret or frame is None:\n    cap.release()\n    raise RuntimeError('Failed to read the first frame from the input video.')\n\nframe_h, frame_w = frame.shape[:2]\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0:\n    fps = 30.0  # Fallback if FPS is not available\n\n# Prepare video writer\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\nif not writer.isOpened():\n    cap.release()\n    raise RuntimeError(f'Unable to open VideoWriter for: {output_path}')\n\n# mAP proxy tracking: collect scores per class across frames\n# We use a confidence-based proxy due to lack of ground-truth annotations.\nclass_scores = {}  # dict: class_id -> list of detection scores\n\n# Utility: color generation per class id\ndef class_id_to_color(cid):\n    # Simple deterministic color from class id\n    np.random.seed(cid + 12345)\n    color = np.random.randint(0, 255, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\n# Helper: Compute running mAP proxy (mean of per-class mean confidences)\ndef compute_map_proxy(scores_dict):\n    per_class_aps = []\n    for cid, scores in scores_dict.items():\n        if len(scores) > 0:\n            per_class_aps.append(float(np.mean(scores)))\n    if len(per_class_aps) == 0:\n        return 0.0\n    return float(np.mean(per_class_aps))\n\n# Frame processing loop\nframe_index = 0\n\nwhile True:\n    # 2.2 Preprocess Data for current frame\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (in_w, in_h))\n    input_data = np.expand_dims(resized, axis=0)\n\n    # 2.3 Quantization Handling\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = input_data.astype(input_dtype)\n\n    # ==========================\n    # Phase 3: Inference\n    # ==========================\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\n    # ==========================\n    # Phase 4: Output Interpretation & Handling Loop\n    # ==========================\n\n    # 4.1 Get Output Tensors\n    raw_outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    # 4.2 Interpret Results (identify boxes, classes, scores, num_detections)\n    boxes = None\n    classes = None\n    scores = None\n    num_detections = None\n\n    # Robust detection of outputs by shapes/ranges\n    for arr in raw_outputs:\n        arr_np = np.squeeze(arr)\n        arr_shape = arr.shape\n        if len(arr_shape) == 3 and arr_shape[-1] == 4:\n            # Expect (1, N, 4) for boxes\n            boxes = arr[0]\n        elif arr_np.size == 1:\n            # num_detections\n            num_detections = int(round(float(arr_np)))\n        elif len(arr_shape) == 2 and arr_shape[0] == 1:\n            # Could be scores or classes\n            # Scores are usually in [0,1]; classes are ints (as float) > 1.0\n            max_val = float(np.max(arr))\n            min_val = float(np.min(arr))\n            if max_val <= 1.0 and min_val >= 0.0 and scores is None:\n                scores = arr[0]\n            elif classes is None:\n                classes = arr[0]\n\n    # Fallbacks if any are missing\n    if boxes is None:\n        # Try to find any array with last dim 4\n        for arr in raw_outputs:\n            if len(arr.shape) >= 2 and arr.shape[-1] == 4:\n                boxes = arr[0]\n                break\n    if classes is None:\n        # Identify non-scores array if possible\n        for arr in raw_outputs:\n            if len(arr.shape) == 2 and arr.shape[0] == 1:\n                max_val = float(np.max(arr))\n                min_val = float(np.min(arr))\n                if not (max_val <= 1.0 and min_val >= 0.0):\n                    classes = arr[0]\n                    break\n    if scores is None:\n        for arr in raw_outputs:\n            if len(arr.shape) == 2 and arr.shape[0] == 1:\n                max_val = float(np.max(arr))\n                min_val = float(np.min(arr))\n                if max_val <= 1.0 and min_val >= 0.0:\n                    scores = arr[0]\n                    break\n    if boxes is None or classes is None or scores is None:\n        # Cannot interpret outputs; write the original frame and continue\n        cv2.putText(frame, 'Detection outputs not found', (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n        # Overlay mAP proxy so far\n        map_proxy = compute_map_proxy(class_scores)\n        cv2.putText(frame, f'mAP: {map_proxy * 100:.2f}%', (10, 60),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 200, 0), 2, cv2.LINE_AA)\n        writer.write(frame)\n    else:\n        # Ensure 1D arrays and consistent length\n        if len(boxes.shape) == 3:\n            boxes = boxes[0]\n        if len(classes.shape) == 2:\n            classes = classes[0]\n        if len(scores.shape) == 2:\n            scores = scores[0]\n\n        if num_detections is None:\n            num_detections = min(len(scores), len(classes), len(boxes))\n        else:\n            num_detections = min(num_detections, len(scores), len(classes), len(boxes))\n\n        # 4.3 Post-processing: thresholding, coordinate scaling, and clipping\n        for i in range(num_detections):\n            score = float(scores[i])\n            if score < CONF_THRESHOLD:\n                continue\n\n            cid = int(classes[i])  # class id may come as float; cast to int\n            y_min, x_min, y_max, x_max = boxes[i]  # normalized coordinates\n\n            # Clip normalized coordinates to [0,1]\n            y_min = max(0.0, min(1.0, float(y_min)))\n            x_min = max(0.0, min(1.0, float(x_min)))\n            y_max = max(0.0, min(1.0, float(y_max)))\n            x_max = max(0.0, min(1.0, float(x_max)))\n\n            # Scale to image coordinates\n            left = int(round(x_min * frame_w))\n            top = int(round(y_min * frame_h))\n            right = int(round(x_max * frame_w))\n            bottom = int(round(y_max * frame_h))\n\n            # Ensure proper ordering\n            left, right = min(left, right), max(left, right)\n            top, bottom = min(top, bottom), max(top, bottom)\n\n            # Clip to frame bounds\n            left = max(0, min(frame_w - 1, left))\n            right = max(0, min(frame_w - 1, right))\n            top = max(0, min(frame_h - 1, top))\n            bottom = max(0, min(frame_h - 1, bottom))\n\n            # Skip invalid/empty boxes\n            if right <= left or bottom <= top:\n                continue\n\n            # Update running per-class scores for mAP proxy\n            if cid not in class_scores:\n                class_scores[cid] = []\n            class_scores[cid].append(score)\n\n            # Draw rectangle and label text\n            color = class_id_to_color(cid)\n            cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n\n            label_name = get_label_name(cid)\n            caption = f'{label_name}: {score:.2f}'\n            # Draw label background for readability\n            (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n            cv2.rectangle(frame, (left, max(0, top - th - baseline - 4)),\n                          (left + tw + 4, top), color, thickness=-1)\n            cv2.putText(frame, caption, (left + 2, top - baseline - 2),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n\n        # Compute and overlay running mAP proxy\n        map_proxy = compute_map_proxy(class_scores)\n        cv2.putText(frame, f'mAP: {map_proxy * 100:.2f}%', (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 200, 0), 2, cv2.LINE_AA)\n\n        # 4.4 Handle Output: write annotated frame to output video\n        writer.write(frame)\n\n    # 4.5 Loop continuation: read next frame or break\n    ret, frame = cap.read()\n    if not ret or frame is None:\n        break\n    frame_index += 1\n\n# ==========================\n# Phase 5: Cleanup\n# ==========================\ncap.release()\nwriter.release()\n\n# Print final mAP proxy summary\nfinal_map_proxy = compute_map_proxy(class_scores)\nprint(f'Processing completed. Estimated mAP (confidence-based proxy): {final_map_proxy * 100:.2f}%')\nprint(f'Annotated video written to: {output_path}')\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 7728,
            "total": 9228,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.077571,
          "latency": 104672.0,
          "timeToFirstToken": 104.672,
          "updatedAt": "2026-02-05T01:42:26.224Z",
          "totalTokens": 9228,
          "costDetails": {
            "total": 0.077571
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 7728,
          "usageDetails": {
            "input": 1500,
            "output": 7728,
            "total": 9228
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T01:42:21.105Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T01:43:12.487Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T01:40:35.809Z",
      "environment": "default",
      "bookmarked": false
    },
    {
      "id": "ae41f990",
      "timestamp": "2026-02-05T01:37:33.393000+00:00",
      "name": "ae41_py_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# ================================================================\n# Application: Object Detection via a video file\n# Target Device: Raspberry Pi 4B\n# ================================================================\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.2 Paths/Parameters\nMODEL_PATH  = \"models/ssd-mobilenet_v1/detect.tflite\"\nLABEL_PATH  = \"models/ssd-mobilenet_v1/labelmap.txt\"\nINPUT_PATH  = \"data/object_detection/sheeps.mp4\"\nOUTPUT_PATH  = \"results/object_detection/test_results/sheeps_detections.mp4\"\nCONF_THRESHOLD = 0.5  # Confidence Threshold\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (Conditional)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name:\n                    labels.append(name)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Retrieve input tensor characteristics\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # [1, height, width, 3]\ninput_dtype = input_details[0]['dtype']\nin_height, in_width = int(input_shape[1]), int(input_shape[2])\n\n# Determine floating model\nfloating_model = (input_dtype == np.float32)\n\n# =========================\n# Utility Functions\n# =========================\n\ndef preprocess_frame(frame_bgr, in_w, in_h, dtype, floating):\n    # Resize and convert BGR -> RGB\n    resized = cv2.resize(frame_bgr, (in_w, in_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    if floating:\n        # Normalize to [-1, 1]\n        input_data = (np.float32(rgb) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(rgb, dtype=dtype)\n    return np.expand_dims(input_data, axis=0)\n\ndef clip_box(x1, y1, x2, y2, W, H):\n    x1 = max(0, min(W - 1, x1))\n    y1 = max(0, min(H - 1, y1))\n    x2 = max(0, min(W - 1, x2))\n    y2 = max(0, min(H - 1, y2))\n    return x1, y1, x2, y2\n\ndef parse_detection_outputs(raw_outputs):\n    \"\"\"\n    Identify indices of boxes, classes, scores, num_detections from raw output tensors.\n    Returns dict with keys: 'boxes_idx', 'classes_idx', 'scores_idx', 'num_idx' (num_idx can be None).\n    \"\"\"\n    boxes_idx = None\n    classes_idx = None\n    scores_idx = None\n    num_idx = None\n\n    # Identify indices by shape characteristics\n    for i, arr in enumerate(raw_outputs):\n        if arr.ndim == 3 and arr.shape[-1] == 4:\n            boxes_idx = i\n        elif arr.size == 1:\n            num_idx = i\n\n    # Among remaining, distinguish classes vs scores\n    # Typical: both have shape (1, N); classes are float of integers > 1, scores in [0,1]\n    candidate_idxs = [i for i in range(len(raw_outputs)) if i not in [boxes_idx] + ([num_idx] if num_idx is not None else [])]\n    for i in candidate_idxs:\n        arr = raw_outputs[i]\n        if arr.ndim == 2:\n            flat = arr.reshape(-1)\n            # Use value range heuristic\n            if np.all(flat >= 0.0) and np.all(flat <= 1.0):\n                scores_idx = i\n            else:\n                classes_idx = i\n\n    # Fallback: if ambiguous, assign deterministically\n    if scores_idx is None or classes_idx is None:\n        for i in candidate_idxs:\n            arr = raw_outputs[i]\n            if arr.ndim == 2:\n                flat = arr.reshape(-1)\n                if scores_idx is None:\n                    scores_idx = i\n                elif classes_idx is None:\n                    classes_idx = i\n\n    return {\n        'boxes_idx': boxes_idx,\n        'classes_idx': classes_idx,\n        'scores_idx': scores_idx,\n        'num_idx': num_idx\n    }\n\ndef scale_boxes_to_frame(norm_box, frame_w, frame_h):\n    # TFLite SSD: boxes in [ymin, xmin, ymax, xmax], normalized [0,1]\n    y_min, x_min, y_max, x_max = norm_box\n    x1 = int(x_min * frame_w)\n    y1 = int(y_min * frame_h)\n    x2 = int(x_max * frame_w)\n    y2 = int(y_max * frame_h)\n    # clip to image bounds\n    x1, y1, x2, y2 = clip_box(x1, y1, x2, y2, frame_w, frame_h)\n    return x1, y1, x2, y2\n\ndef iou_xyxy(a, b):\n    # boxes: [x1, y1, x2, y2]\n    ax1, ay1, ax2, ay2 = a\n    bx1, by1, bx2, by2 = b\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n    inter_w = max(0, inter_x2 - inter_x1)\n    inter_h = max(0, inter_y2 - inter_y1)\n    inter = inter_w * inter_h\n    area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)\n    area_b = max(0, bx2 - bx1) * max(0, by2 - by1)\n    union = area_a + area_b - inter\n    if union <= 0:\n        return 0.0\n    return inter / union\n\ndef evaluate_frame_predictions(prev_gt_by_class, detections_by_class, pred_stats, gt_counts, iou_thresh=0.5):\n    \"\"\"\n    Update running prediction stats for mAP calculation using previous frame's detections as pseudo-GT.\n    - prev_gt_by_class: dict[int, list of boxes]\n    - detections_by_class: dict[int, list of (score, box)]\n    - pred_stats: dict[int, list of (score, is_tp)]\n    - gt_counts: dict[int, int]\n    \"\"\"\n    for cls_id, gt_list in prev_gt_by_class.items():\n        gt_counts[cls_id] = gt_counts.get(cls_id, 0) + len(gt_list)\n\n    for cls_id, preds in detections_by_class.items():\n        # Sort predictions by score descending\n        preds_sorted = sorted(preds, key=lambda x: x[0], reverse=True)\n        gt_list = prev_gt_by_class.get(cls_id, [])\n        gt_matched = [False] * len(gt_list)\n\n        for score, p_box in preds_sorted:\n            best_iou = 0.0\n            best_idx = -1\n            for gi, g_box in enumerate(gt_list):\n                if not gt_matched[gi]:\n                    iou = iou_xyxy(p_box, g_box)\n                    if iou > best_iou:\n                        best_iou = iou\n                        best_idx = gi\n            is_tp = 0\n            if best_iou >= iou_thresh and best_idx >= 0:\n                is_tp = 1\n                gt_matched[best_idx] = True\n            pred_stats.setdefault(cls_id, []).append((score, is_tp))\n\ndef compute_map(pred_stats, gt_counts):\n    \"\"\"\n    Compute mAP using VOC 2007 11-point interpolation across classes present in gt_counts > 0.\n    pred_stats: dict[int, list of (score, is_tp)]\n    gt_counts: dict[int, int]\n    Returns mAP, per_class_AP dict\n    \"\"\"\n    ap_per_class = {}\n    for cls_id, gt_total in gt_counts.items():\n        if gt_total <= 0:\n            continue\n        preds = pred_stats.get(cls_id, [])\n        if not preds:\n            ap_per_class[cls_id] = 0.0\n            continue\n        preds_sorted = sorted(preds, key=lambda x: x[0], reverse=True)\n        tps = np.array([p[1] for p in preds_sorted], dtype=np.float32)\n        fps = 1.0 - tps\n        tp_cum = np.cumsum(tps)\n        fp_cum = np.cumsum(fps)\n        precision = tp_cum / np.maximum(1, tp_cum + fp_cum)\n        recall = tp_cum / float(gt_total)\n\n        # 11-point interpolation\n        ap = 0.0\n        for r in np.linspace(0, 1, 11):\n            mask = recall >= r\n            p = np.max(precision[mask]) if np.any(mask) else 0.0\n            ap += p / 11.0\n        ap_per_class[cls_id] = float(ap)\n\n    valid_aps = list(ap_per_class.values())\n    mAP = float(np.mean(valid_aps)) if len(valid_aps) > 0 else 0.0\n    return mAP, ap_per_class\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# 2.1 Acquire Input Data\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Retrieve properties for output writer\nframe_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0:\n    fps = 25.0\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_w, frame_h))\nif not out_writer.isOpened():\n    cap.release()\n    raise RuntimeError(f\"Failed to open output video writer: {OUTPUT_PATH}\")\n\n# Variables for mAP computation across frames (using previous frame as pseudo-GT)\nprev_gt_by_class = {}  # dict: class_id -> list of boxes [[x1,y1,x2,y2], ...]\npred_stats = {}        # dict: class_id -> list of (score, is_tp)\ngt_counts = {}         # dict: class_id -> int\n\n# Helper for output mapping (initialized after first inference)\noutput_map = None\n\nframe_index = 0\nstart_time = time.time()\n\n# =========================\n# Processing Loop\n# =========================\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame(frame_bgr, in_width, in_height, input_dtype, floating_model)\n\n    # 2.3 Quantization Handling already done in preprocess_frame based on floating_model\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling\n    # =========================\n\n    # 4.1 Get Output Tensors (raw)\n    raw_outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    # Initialize output mapping on first frame\n    if output_map is None:\n        output_map = parse_detection_outputs(raw_outputs)\n\n    boxes_arr = raw_outputs[output_map['boxes_idx']]\n    classes_arr = raw_outputs[output_map['classes_idx']]\n    scores_arr = raw_outputs[output_map['scores_idx']]\n    if output_map['num_idx'] is not None:\n        num_det = int(np.squeeze(raw_outputs[output_map['num_idx']]).tolist())\n    else:\n        num_det = boxes_arr.shape[1] if boxes_arr.ndim >= 2 else boxes_arr.shape[0]\n\n    # Squeeze to remove batch dimension\n    boxes = np.squeeze(boxes_arr, axis=0)\n    classes = np.squeeze(classes_arr, axis=0)\n    scores = np.squeeze(scores_arr, axis=0)\n\n    # 4.2 Interpret Results\n    # Prepare detections filtered by confidence and scaled to frame size\n    dets_for_drawing = []  # list of (class_id, score, (x1,y1,x2,y2))\n    detections_by_class = {}  # class_id -> list of (score, [x1,y1,x2,y2])\n\n    take_n = min(num_det, boxes.shape[0])\n    for i in range(take_n):\n        score = float(scores[i])\n        if score < CONF_THRESHOLD:\n            continue\n        cls_id = int(classes[i])\n        y_min, x_min, y_max, x_max = boxes[i]\n        x1, y1, x2, y2 = scale_boxes_to_frame((y_min, x_min, y_max, x_max), frame_w, frame_h)\n        # 4.3 Post-processing: bounding box clipping is already ensured in scale function\n        dets_for_drawing.append((cls_id, score, (x1, y1, x2, y2)))\n        detections_by_class.setdefault(cls_id, []).append((score, (x1, y1, x2, y2)))\n\n    # Update mAP statistics using previous frame detections as pseudo-ground truth\n    if frame_index > 0:\n        evaluate_frame_predictions(prev_gt_by_class, detections_by_class, pred_stats, gt_counts, iou_thresh=0.5)\n\n    # Prepare GT for next frame (current detections act as GT for next iteration)\n    prev_gt_by_class = {cid: [box for (_, box) in items] for cid, items in detections_by_class.items()}\n\n    # Compute running mAP\n    mAP_value, _ = compute_map(pred_stats, gt_counts)\n\n    # 4.4 Handle Output: Draw detections and write to output video\n    for cls_id, score, (x1, y1, x2, y2) in dets_for_drawing:\n        color = (0, 255, 0)\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        # Resolve label\n        if 0 <= cls_id < len(labels):\n            cls_name = labels[cls_id]\n        else:\n            cls_name = str(cls_id)\n        label_text = f\"{cls_name}: {score:.2f}\"\n        cv2.putText(frame_bgr, label_text, (x1, max(0, y1 - 8)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n\n    # Overlay running mAP on the frame\n    cv2.putText(frame_bgr, f\"mAP (proxy): {mAP_value:.3f}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 200, 255), 2, cv2.LINE_AA)\n    cv2.putText(frame_bgr, f\"Frame: {frame_index}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1, cv2.LINE_AA)\n\n    out_writer.write(frame_bgr)\n    frame_index += 1\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - start_time\nfinal_mAP, per_class_ap = compute_map(pred_stats, gt_counts)\n\nprint(\"Processing complete.\")\nprint(f\"Frames processed: {frame_index}\")\nprint(f\"Elapsed time: {elapsed:.2f} s, FPS (avg): {frame_index / elapsed if elapsed > 0 else 0:.2f}\")\nprint(f\"Proxy mAP over video (using previous frame as pseudo-GT): {final_mAP:.4f}\")\n\n# Optionally, print AP per class for classes present in GT\nif per_class_ap:\n    print(\"Per-class AP (classes with pseudo-GT present):\")\n    for cls_id in sorted(per_class_ap.keys()):\n        name = labels[cls_id] if 0 <= cls_id < len(labels) else str(cls_id)\n        print(f\" - {name} (id {cls_id}): {per_class_ap[cls_id]:.4f}\")"
      },
      "session_id": "gpt-5-2025-08-07_40f1_psg_batch",
      "release": null,
      "version": null,
      "user_id": null,
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "py_sketch_generator"
      ],
      "public": false,
      "html_path": "/project/cmavm4dny0006mo07fyz082g6/traces/ae41f990",
      "latency": 122.025,
      "total_cost": 0.095045,
      "observations": [
        {
          "id": "time-03-37-34-030522_chatcmpl-D5ibGpxSsZG1cpdyI72sGpYDr0yLF",
          "traceId": "ae41f990",
          "type": "GENERATION",
          "name": "ae41_psg_gen_attempt#1",
          "startTime": "2026-02-05T01:37:34.030000+00:00",
          "endTime": "2026-02-05T01:39:36.055000+00:00",
          "completionStartTime": "2026-02-05T01:39:36.055000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "input": {
            "messages": [
              {
                "role": "system",
                "content": "\nYou are an expert in Machine Learning on Edge Devices, highly skilled in the TinyML workflows, tools, techniques, and best practices. Your expertise covers both software and hardware, including microcontrollers and microprocessors. You will be asked questions related to various stages of the TinyML lifecycle, including data engineering, model design, model evaluation, model conversion and quantization, and deployment sketch development. The main task is to generate code to perform corresponding tasks (e.g., data cleaning, model quantization).\n\nYour output will be executed **directly**, without additional checks or modifications. Therefore, it is critical that your code strictly adheres to the given format requirements and task instructions.\n\n### RESPONSE FORMAT ###\n- Output must consist of a single Python code block (```python ... ```).\n- The code should be a complete, **directly** runnable script that  implements the provided Application Description and Requirements.\n- Use only standard Python libraries (`os`, `time`, `numpy`, and `cv2` if image/video processing is explicitly required).\n- Code must be:\n  - Fully executable without placeholders (no \"path_to...\").\n  - Well-structured and commented for clarity.\n  - Named consistently to reflect quantization details if relevant.\n- If model/tooling error info conflicts with requirements, regenerate strictly according to requirements.\n- Provide the complete Python script enclosed in a single ```python ... ``` block, you cannot skip outputting any part of the code.\n"
              },
              {
                "role": "user",
                "content": "\n### OBJECTIVE ###\nGenerate a complete Python script for the application named 'Object Detection via a video file'. The deployment target device is 'Raspberry Pi 4B'.\n\n### INSTRUCTIONS ###\n- Strictly follow Application Description and Core Requirements.  \n- Use the paths and parameters provided under ### CONFIGURATION PARAMETERS ###\n- Follow the phase structure outlined in the ### PROGRAMMING GUIDELINE ### reference provided below.\n- Explicitly implement Phases 2, 4.2, and 4.3 according to Input/Output method descriptions.  \n- Import standard libraries only as needed for the described application logic.\n- Only import libraries that are explicitly required by the application logic (no assumptions like `cv2` unless image/video is involved).  \n- Ensure **no placeholders** remain: all paths and parameters are already defined under ### CONFIGURATION PARAMETERS ###.\n\n### CONFIGURATION PARAMETERS ###\nAll the required paths and parameters are provided below, use them in generated code:\n- model_path: 'models/ssd-mobilenet_v1/detect.tflite'\n- label_path: 'models/ssd-mobilenet_v1/labelmap.txt'  \n- Input Method Description: 'Read a single video file from the given input_path'\n- input_path: 'data/object_detection/sheeps.mp4'  \n- Output Method Description: 'Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)'\n- output_path: 'results/object_detection/test_results/sheeps_detections.mp4' \n- Confidence Threshold: '0.5'\n\n### USEFUL INFORMATION ###\n**The first 10 lines of labelmap**:\n```\nperson\nbicycle\ncar\nmotorcycle\nairplane\nbus\ntrain\ntruck\nboat\ntraffic light\n```\n\n\n\n### PROGRAMMING GUIDELINE###\n\n**Programming Guidelines for TFLite Inference:**\n\n**Phase 1: Setup**\n1.1. **Imports**: Import interpreter literally by `from ai_edge_litert.interpreter import Interpreter`\n1.2. **Paths/Parameters**: Define necessary variables using the model path, input path (if provided and relevant), label path (if provided and relevant), output paths, or other parameters provided in the main prompt.\n1.3. **Load Labels (Conditional)**: If a label path variable was defined and is needed for the application (check main prompt's description), implement code to read the label file into a Python list.\n1.4. **Load Interpreter**: Instantiate interpreter with `interpreter = Interpreter(model_path=...)` using the provided model path variable, and call `interpreter.allocate_tensors()`.\n1.5. **Get Model Details**: Retrieve `input_details = interpreter.get_input_details()` and `output_details = interpreter.get_output_details()`. Store necessary indices and properties (shape, dtype) for input/output tensors.\n\n**Phase 2: Input Acquisition & Preprocessing Loop (Implement based on main prompt's input description)**\n2.1. **Acquire Input Data**: Implement code to get raw data according to the input description provided in the main prompt. **Use the input path variable if it was provided and is relevant to the described input method (e.g., reading a specific file/folder).** This phase is highly application-dependent.\n2.2. **Preprocess Data**: Implement code to transform the raw data into the `numpy` array(s) based on the extracted information of shape and `dtype` from the retrieved `input_details`. Store result in an `input_data` variable.\n2.3. **Quantization Handling**: Check if model uses floating point inputs: `floating_model = (input_details[0]['dtype'] == np.float32)`. If floating model, normalize: `input_data = (np.float32(input_data) - 127.5) / 127.5`.\n2.4. **Loop Control**: If the input description implies processing continuous data (stream, camera, multiple files), implement the appropriate loop structure and exit conditions. If processing single input, plan to run Phase 3 once.\n\n**Phase 3: Inference (Run per preprocessed input)**\n3.1. **Set Input Tensor(s)**: For each required input tensor: `interpreter.set_tensor(input_details[idx]['index'], input_data_variable[idx])`. Use the preprocessed `input_data` variable.\n3.2. **Run Inference**: `interpreter.invoke()`.\n\n**Phase 4: Output Interpretation & Handling Loop (Implement based on main prompt's application and output descriptions)**\n4.1. **Get Output Tensor(s)**: Retrieve results using `output_variable[idx] = interpreter.get_tensor(output_details[idx]['index'])`. Store in an `output_data` variable.\n4.2. **Interpret Results**: Implement code to process the raw `output_data` variable according to the specific task (defined in the main prompt's application name and description) to generate meaningful results. **If labels were loaded in Phase 1.3 and are relevant (e.g., for classification), use the label list here to map indices to names.**\n4.3. **Post-processing**: For detection models, apply confidence thresholding, coordinate scaling, and bounding box clipping to ensure results are within valid ranges.\n4.4. **Handle Output**: Implement code to deliver the interpreted results according to the output description provided in the main prompt (e.g., print, write to file using the provided output path variable, send data, control actuator).\n4.5. **Loop Continuation**: If inside a loop from Phase 2.4, implement logic to continue or break based on exit conditions.\n\n**Phase 5: Cleanup**\n5.1. Implement code to release any resources acquired in Phase 2 (e.g., close files, release camera).\n\n"
              }
            ]
          },
          "metadata": {
            "hidden_params": {
              "model_id": null,
              "cache_key": null,
              "api_base": null,
              "response_cost": null,
              "additional_headers": {
                "x_ratelimit_limit_requests": 5000,
                "x_ratelimit_limit_tokens": 1000000,
                "x_ratelimit_remaining_requests": 4999,
                "x_ratelimit_remaining_tokens": 998273
              }
            },
            "litellm_response_cost": 0.095045,
            "api_base": "https://api.openai.com/v1/",
            "cache_hit": false,
            "requester_metadata": {}
          },
          "output": {
            "content": "```python\nimport os\nimport time\nimport numpy as np\nimport cv2\nfrom ai_edge_litert.interpreter import Interpreter\n\n# ================================================================\n# Application: Object Detection via a video file\n# Target Device: Raspberry Pi 4B\n# ================================================================\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.2 Paths/Parameters\nMODEL_PATH = 'models/ssd-mobilenet_v1/detect.tflite'\nLABEL_PATH = 'models/ssd-mobilenet_v1/labelmap.txt'\nINPUT_PATH = 'data/object_detection/sheeps.mp4'\nOUTPUT_PATH = 'results/object_detection/test_results/sheeps_detections.mp4'\nCONF_THRESHOLD = 0.5  # Confidence Threshold\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n# 1.3 Load Labels (Conditional)\ndef load_labels(label_path):\n    labels = []\n    if os.path.isfile(label_path):\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name:\n                    labels.append(name)\n    return labels\n\nlabels = load_labels(LABEL_PATH)\n\n# 1.4 Load Interpreter\ninterpreter = Interpreter(model_path=MODEL_PATH)\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Retrieve input tensor characteristics\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # [1, height, width, 3]\ninput_dtype = input_details[0]['dtype']\nin_height, in_width = int(input_shape[1]), int(input_shape[2])\n\n# Determine floating model\nfloating_model = (input_dtype == np.float32)\n\n# =========================\n# Utility Functions\n# =========================\n\ndef preprocess_frame(frame_bgr, in_w, in_h, dtype, floating):\n    # Resize and convert BGR -> RGB\n    resized = cv2.resize(frame_bgr, (in_w, in_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    if floating:\n        # Normalize to [-1, 1]\n        input_data = (np.float32(rgb) - 127.5) / 127.5\n    else:\n        input_data = np.asarray(rgb, dtype=dtype)\n    return np.expand_dims(input_data, axis=0)\n\ndef clip_box(x1, y1, x2, y2, W, H):\n    x1 = max(0, min(W - 1, x1))\n    y1 = max(0, min(H - 1, y1))\n    x2 = max(0, min(W - 1, x2))\n    y2 = max(0, min(H - 1, y2))\n    return x1, y1, x2, y2\n\ndef parse_detection_outputs(raw_outputs):\n    \"\"\"\n    Identify indices of boxes, classes, scores, num_detections from raw output tensors.\n    Returns dict with keys: 'boxes_idx', 'classes_idx', 'scores_idx', 'num_idx' (num_idx can be None).\n    \"\"\"\n    boxes_idx = None\n    classes_idx = None\n    scores_idx = None\n    num_idx = None\n\n    # Identify indices by shape characteristics\n    for i, arr in enumerate(raw_outputs):\n        if arr.ndim == 3 and arr.shape[-1] == 4:\n            boxes_idx = i\n        elif arr.size == 1:\n            num_idx = i\n\n    # Among remaining, distinguish classes vs scores\n    # Typical: both have shape (1, N); classes are float of integers > 1, scores in [0,1]\n    candidate_idxs = [i for i in range(len(raw_outputs)) if i not in [boxes_idx] + ([num_idx] if num_idx is not None else [])]\n    for i in candidate_idxs:\n        arr = raw_outputs[i]\n        if arr.ndim == 2:\n            flat = arr.reshape(-1)\n            # Use value range heuristic\n            if np.all(flat >= 0.0) and np.all(flat <= 1.0):\n                scores_idx = i\n            else:\n                classes_idx = i\n\n    # Fallback: if ambiguous, assign deterministically\n    if scores_idx is None or classes_idx is None:\n        for i in candidate_idxs:\n            arr = raw_outputs[i]\n            if arr.ndim == 2:\n                flat = arr.reshape(-1)\n                if scores_idx is None:\n                    scores_idx = i\n                elif classes_idx is None:\n                    classes_idx = i\n\n    return {\n        'boxes_idx': boxes_idx,\n        'classes_idx': classes_idx,\n        'scores_idx': scores_idx,\n        'num_idx': num_idx\n    }\n\ndef scale_boxes_to_frame(norm_box, frame_w, frame_h):\n    # TFLite SSD: boxes in [ymin, xmin, ymax, xmax], normalized [0,1]\n    y_min, x_min, y_max, x_max = norm_box\n    x1 = int(x_min * frame_w)\n    y1 = int(y_min * frame_h)\n    x2 = int(x_max * frame_w)\n    y2 = int(y_max * frame_h)\n    # clip to image bounds\n    x1, y1, x2, y2 = clip_box(x1, y1, x2, y2, frame_w, frame_h)\n    return x1, y1, x2, y2\n\ndef iou_xyxy(a, b):\n    # boxes: [x1, y1, x2, y2]\n    ax1, ay1, ax2, ay2 = a\n    bx1, by1, bx2, by2 = b\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n    inter_w = max(0, inter_x2 - inter_x1)\n    inter_h = max(0, inter_y2 - inter_y1)\n    inter = inter_w * inter_h\n    area_a = max(0, ax2 - ax1) * max(0, ay2 - ay1)\n    area_b = max(0, bx2 - bx1) * max(0, by2 - by1)\n    union = area_a + area_b - inter\n    if union <= 0:\n        return 0.0\n    return inter / union\n\ndef evaluate_frame_predictions(prev_gt_by_class, detections_by_class, pred_stats, gt_counts, iou_thresh=0.5):\n    \"\"\"\n    Update running prediction stats for mAP calculation using previous frame's detections as pseudo-GT.\n    - prev_gt_by_class: dict[int, list of boxes]\n    - detections_by_class: dict[int, list of (score, box)]\n    - pred_stats: dict[int, list of (score, is_tp)]\n    - gt_counts: dict[int, int]\n    \"\"\"\n    for cls_id, gt_list in prev_gt_by_class.items():\n        gt_counts[cls_id] = gt_counts.get(cls_id, 0) + len(gt_list)\n\n    for cls_id, preds in detections_by_class.items():\n        # Sort predictions by score descending\n        preds_sorted = sorted(preds, key=lambda x: x[0], reverse=True)\n        gt_list = prev_gt_by_class.get(cls_id, [])\n        gt_matched = [False] * len(gt_list)\n\n        for score, p_box in preds_sorted:\n            best_iou = 0.0\n            best_idx = -1\n            for gi, g_box in enumerate(gt_list):\n                if not gt_matched[gi]:\n                    iou = iou_xyxy(p_box, g_box)\n                    if iou > best_iou:\n                        best_iou = iou\n                        best_idx = gi\n            is_tp = 0\n            if best_iou >= iou_thresh and best_idx >= 0:\n                is_tp = 1\n                gt_matched[best_idx] = True\n            pred_stats.setdefault(cls_id, []).append((score, is_tp))\n\ndef compute_map(pred_stats, gt_counts):\n    \"\"\"\n    Compute mAP using VOC 2007 11-point interpolation across classes present in gt_counts > 0.\n    pred_stats: dict[int, list of (score, is_tp)]\n    gt_counts: dict[int, int]\n    Returns mAP, per_class_AP dict\n    \"\"\"\n    ap_per_class = {}\n    for cls_id, gt_total in gt_counts.items():\n        if gt_total <= 0:\n            continue\n        preds = pred_stats.get(cls_id, [])\n        if not preds:\n            ap_per_class[cls_id] = 0.0\n            continue\n        preds_sorted = sorted(preds, key=lambda x: x[0], reverse=True)\n        tps = np.array([p[1] for p in preds_sorted], dtype=np.float32)\n        fps = 1.0 - tps\n        tp_cum = np.cumsum(tps)\n        fp_cum = np.cumsum(fps)\n        precision = tp_cum / np.maximum(1, tp_cum + fp_cum)\n        recall = tp_cum / float(gt_total)\n\n        # 11-point interpolation\n        ap = 0.0\n        for r in np.linspace(0, 1, 11):\n            mask = recall >= r\n            p = np.max(precision[mask]) if np.any(mask) else 0.0\n            ap += p / 11.0\n        ap_per_class[cls_id] = float(ap)\n\n    valid_aps = list(ap_per_class.values())\n    mAP = float(np.mean(valid_aps)) if len(valid_aps) > 0 else 0.0\n    return mAP, ap_per_class\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# 2.1 Acquire Input Data\ncap = cv2.VideoCapture(INPUT_PATH)\nif not cap.isOpened():\n    raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n# Retrieve properties for output writer\nframe_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0:\n    fps = 25.0\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_w, frame_h))\nif not out_writer.isOpened():\n    cap.release()\n    raise RuntimeError(f\"Failed to open output video writer: {OUTPUT_PATH}\")\n\n# Variables for mAP computation across frames (using previous frame as pseudo-GT)\nprev_gt_by_class = {}  # dict: class_id -> list of boxes [[x1,y1,x2,y2], ...]\npred_stats = {}        # dict: class_id -> list of (score, is_tp)\ngt_counts = {}         # dict: class_id -> int\n\n# Helper for output mapping (initialized after first inference)\noutput_map = None\n\nframe_index = 0\nstart_time = time.time()\n\n# =========================\n# Processing Loop\n# =========================\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame(frame_bgr, in_width, in_height, input_dtype, floating_model)\n\n    # 2.3 Quantization Handling already done in preprocess_frame based on floating_model\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling\n    # =========================\n\n    # 4.1 Get Output Tensors (raw)\n    raw_outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    # Initialize output mapping on first frame\n    if output_map is None:\n        output_map = parse_detection_outputs(raw_outputs)\n\n    boxes_arr = raw_outputs[output_map['boxes_idx']]\n    classes_arr = raw_outputs[output_map['classes_idx']]\n    scores_arr = raw_outputs[output_map['scores_idx']]\n    if output_map['num_idx'] is not None:\n        num_det = int(np.squeeze(raw_outputs[output_map['num_idx']]).tolist())\n    else:\n        num_det = boxes_arr.shape[1] if boxes_arr.ndim >= 2 else boxes_arr.shape[0]\n\n    # Squeeze to remove batch dimension\n    boxes = np.squeeze(boxes_arr, axis=0)\n    classes = np.squeeze(classes_arr, axis=0)\n    scores = np.squeeze(scores_arr, axis=0)\n\n    # 4.2 Interpret Results\n    # Prepare detections filtered by confidence and scaled to frame size\n    dets_for_drawing = []  # list of (class_id, score, (x1,y1,x2,y2))\n    detections_by_class = {}  # class_id -> list of (score, [x1,y1,x2,y2])\n\n    take_n = min(num_det, boxes.shape[0])\n    for i in range(take_n):\n        score = float(scores[i])\n        if score < CONF_THRESHOLD:\n            continue\n        cls_id = int(classes[i])\n        y_min, x_min, y_max, x_max = boxes[i]\n        x1, y1, x2, y2 = scale_boxes_to_frame((y_min, x_min, y_max, x_max), frame_w, frame_h)\n        # 4.3 Post-processing: bounding box clipping is already ensured in scale function\n        dets_for_drawing.append((cls_id, score, (x1, y1, x2, y2)))\n        detections_by_class.setdefault(cls_id, []).append((score, (x1, y1, x2, y2)))\n\n    # Update mAP statistics using previous frame detections as pseudo-ground truth\n    if frame_index > 0:\n        evaluate_frame_predictions(prev_gt_by_class, detections_by_class, pred_stats, gt_counts, iou_thresh=0.5)\n\n    # Prepare GT for next frame (current detections act as GT for next iteration)\n    prev_gt_by_class = {cid: [box for (_, box) in items] for cid, items in detections_by_class.items()}\n\n    # Compute running mAP\n    mAP_value, _ = compute_map(pred_stats, gt_counts)\n\n    # 4.4 Handle Output: Draw detections and write to output video\n    for cls_id, score, (x1, y1, x2, y2) in dets_for_drawing:\n        color = (0, 255, 0)\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        # Resolve label\n        if 0 <= cls_id < len(labels):\n            cls_name = labels[cls_id]\n        else:\n            cls_name = str(cls_id)\n        label_text = f\"{cls_name}: {score:.2f}\"\n        cv2.putText(frame_bgr, label_text, (x1, max(0, y1 - 8)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1, cv2.LINE_AA)\n\n    # Overlay running mAP on the frame\n    cv2.putText(frame_bgr, f\"mAP (proxy): {mAP_value:.3f}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 200, 255), 2, cv2.LINE_AA)\n    cv2.putText(frame_bgr, f\"Frame: {frame_index}\", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1, cv2.LINE_AA)\n\n    out_writer.write(frame_bgr)\n    frame_index += 1\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nout_writer.release()\n\nelapsed = time.time() - start_time\nfinal_mAP, per_class_ap = compute_map(pred_stats, gt_counts)\n\nprint(\"Processing complete.\")\nprint(f\"Frames processed: {frame_index}\")\nprint(f\"Elapsed time: {elapsed:.2f} s, FPS (avg): {frame_index / elapsed if elapsed > 0 else 0:.2f}\")\nprint(f\"Proxy mAP over video (using previous frame as pseudo-GT): {final_mAP:.4f}\")\n\n# Optionally, print AP per class for classes present in GT\nif per_class_ap:\n    print(\"Per-class AP (classes with pseudo-GT present):\")\n    for cls_id in sorted(per_class_ap.keys()):\n        name = labels[cls_id] if 0 <= cls_id < len(labels) else str(cls_id)\n        print(f\" - {name} (id {cls_id}): {per_class_ap[cls_id]:.4f}\")\n```",
            "role": "assistant",
            "tool_calls": null,
            "function_call": null
          },
          "usage": {
            "input": 1500,
            "output": 9317,
            "total": 10817,
            "unit": "TOKENS"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.095045,
          "latency": 122025.0,
          "timeToFirstToken": 122.025,
          "updatedAt": "2026-02-05T01:39:41.309Z",
          "totalTokens": 10817,
          "costDetails": {
            "total": 0.095045
          },
          "projectId": "cmavm4dny0006mo07fyz082g6",
          "completionTokens": 9317,
          "usageDetails": {
            "input": 1500,
            "output": 9317,
            "total": 10817
          },
          "unit": "TOKENS",
          "promptTokens": 1500,
          "environment": "default",
          "createdAt": "2026-02-05T01:39:36.196Z",
          "version": null,
          "statusMessage": null,
          "parentObservationId": null,
          "promptId": null,
          "promptName": null,
          "promptVersion": null,
          "modelId": null,
          "inputPrice": null,
          "outputPrice": null,
          "totalPrice": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "scores": [],
      "updatedAt": "2026-02-05T01:40:29.679Z",
      "projectId": "cmavm4dny0006mo07fyz082g6",
      "externalId": null,
      "createdAt": "2026-02-05T01:37:33.981Z",
      "environment": "default",
      "bookmarked": false
    }
  ],
  "meta": {
    "page": 1,
    "limit": 50,
    "total_items": 20,
    "total_pages": 1
  }
}