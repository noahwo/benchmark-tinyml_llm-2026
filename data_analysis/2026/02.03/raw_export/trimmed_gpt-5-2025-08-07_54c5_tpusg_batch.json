{
  "data": [
    {
      "id": "f897a6ae",
      "timestamp": "2026-02-04T21:16:37.886000+00:00",
      "name": "f897_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import os\nimport sys\nimport time\nimport numpy as np\nimport cv2\n\n# ============================\n# Phase 1: Setup\n# ============================\n\n# 1.1 Imports: TFLite Interpreter and EdgeTPU delegate with fallback imports\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\nexcept Exception:\n    try:\n        from tensorflow.lite import Interpreter  # type: ignore\n        from tensorflow.lite.experimental import load_delegate  # type: ignore\n    except Exception as e:\n        print(\"ERROR: Could not import TFLite Interpreter. Ensure tflite_runtime or TensorFlow Lite is installed.\")\n        print(f\"Detailed error: {e}\")\n        sys.exit(1)\n\n# 1.2 Paths/Parameters (provided - no placeholders)\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Parse confidence threshold safely\ntry:\n    conf_thr = float(confidence_threshold)\nexcept Exception:\n    conf_thr = 0.5\n\n# 1.3 Load Labels (Conditional)\ndef load_labels(path):\n    labels_list = []\n    if path and os.path.isfile(path):\n        try:\n            with open(path, 'r') as f:\n                for line in f:\n                    name = line.strip()\n                    if name:\n                        labels_list.append(name)\n        except Exception as e:\n            print(f\"WARNING: Failed to load labels from {path}. Proceeding without labels. Error: {e}\")\n    else:\n        print(f\"WARNING: Label file not found at {path}. Proceeding without labels.\")\n    return labels_list\n\nlabels = load_labels(label_path)\n\n# 1.4 Load Interpreter with EdgeTPU (with robust fallback handling)\ninterpreter = None\ndelegate_loaded = False\ndelegate_errors = []\n\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\n    delegate_loaded = True\nexcept Exception as e1:\n    delegate_errors.append(str(e1))\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n        delegate_loaded = True\n    except Exception as e2:\n        delegate_errors.append(str(e2))\n\nif interpreter is None or not delegate_loaded:\n    print(\"WARNING: Failed to load EdgeTPU delegate. Falling back to CPU execution.\")\n    print(\"Tried the following delegates and encountered errors:\")\n    print(\"  - libedgetpu.so.1.0\")\n    print(\"  - /usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\")\n    for idx, msg in enumerate(delegate_errors, 1):\n        print(f\"  Attempt {idx} error: {msg}\")\n    try:\n        interpreter = Interpreter(model_path=model_path)\n    except Exception as e:\n        print(\"ERROR: CPU fallback failed. This model likely requires EdgeTPU delegate.\")\n        print(f\"Details: {e}\")\n        sys.exit(1)\n\n# Allocate tensors\ntry:\n    interpreter.allocate_tensors()\nexcept Exception as e:\n    print(\"ERROR: Failed to allocate tensors. If using CPU fallback, the EdgeTPU-compiled model may not run on CPU.\")\n    print(f\"Details: {e}\")\n    sys.exit(1)\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nif not input_details or 'shape' not in input_details[0]:\n    print(\"ERROR: Could not retrieve input tensor details from model.\")\n    sys.exit(1)\n\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, height, width, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\nif len(input_shape) != 4 or input_shape[-1] != 3:\n    print(f\"ERROR: Unexpected model input shape: {input_shape}. Expected [1, height, width, 3].\")\n    sys.exit(1)\n\n# Model input dimensions (height, width) - ensure integers from tensor shape\nmodel_height  = \"gpt-5-2025-08-07\"\nmodel_width = int(input_shape[2])\n\n# ============================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# ============================\n\n# 2.1 Acquire Input Data: Open video file\nif not os.path.isfile(input_path):\n    print(f\"ERROR: Input video not found at path: {input_path}\")\n    sys.exit(1)\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"ERROR: Failed to open input video: {input_path}\")\n    sys.exit(1)\n\n# Prepare output writer using input video properties\norig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps_val = cap.get(cv2.CAP_PROP_FPS)\ntry:\n    fps = float(fps_val)\nexcept Exception:\n    fps = 0.0\nif not fps or fps <= 0:\n    fps = 25.0  # Fallback FPS if metadata missing\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(output_path, fourcc, fps, (orig_width, orig_height))\nif not out_writer.isOpened():\n    print(f\"ERROR: Failed to open output video writer at: {output_path}\")\n    cap.release()\n    sys.exit(1)\n\n# Utility for color generation per class id\ndef color_for_class(class_id):\n    np.random.seed(int(class_id) + 12345)\n    color = tuple(int(c) for c in np.random.randint(0, 255, size=3))\n    return color\n\n# ============================\n# Phase 3 and 4: Inference + Output Interpretation & Handling Loop\n# ============================\n\n# mAP requires ground truth annotations. Not available here.\nmap_text = \"mAP: N/A (no ground truth)\"\n\nframe_count = 0\ntotal_inference_time_ms = 0.0\n\ntry:\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # 2.2 Preprocess Data\n        # Convert BGR to RGB, resize to model's expected input, expand dims\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        resized_rgb = cv2.resize(frame_rgb, (model_width, model_height))\n        input_data = np.expand_dims(resized_rgb, axis=0)\n\n        # 2.3 Quantization Handling\n        if floating_model:\n            # Normalize to [-1, 1] (typical for MobileNet-based detectors)\n            input_data = (np.float32(input_data) - 127.5) / 127.5\n        else:\n            # Quantized uint8 model expects 0-255 RGB\n            input_data = np.uint8(input_data)\n\n        # Phase 3: Inference\n        interpreter.set_tensor(input_index, input_data)\n        t0 = time.time()\n        interpreter.invoke()\n        inference_ms = (time.time() - t0) * 1000.0\n        total_inference_time_ms += inference_ms\n\n        # 4.1 Get Output Tensor(s)\n        boxes = None\n        classes = None\n        scores = None\n        count = None\n\n        # Try to identify outputs by name first (if available), else by shape heuristics\n        for od in output_details:\n            try:\n                out = interpreter.get_tensor(od['index'])\n            except Exception:\n                continue\n\n            name = od.get('name', '').lower()\n            if 'box' in name and out.ndim in (2, 3) and out.shape[-1] == 4:\n                boxes = out\n                continue\n            if 'score' in name and out.ndim in (1, 2):\n                scores = out\n                continue\n            if ('class' in name or 'label' in name) and out.ndim in (1, 2):\n                classes = out\n                continue\n            if 'count' in name and out.size >= 1:\n                try:\n                    count = int(np.squeeze(out))\n                except Exception:\n                    count = None\n                continue\n\n            # Fallback by shape/value range\n            if out.ndim == 3 and out.shape[-1] == 4 and boxes is None:\n                boxes = out\n                continue\n            if out.ndim in (1, 2):\n                flat = out.flatten()\n                if flat.size == 0:\n                    continue\n                if scores is None and np.issubdtype(out.dtype, np.floating):\n                    if 0.0 <= float(np.min(flat)) and float(np.max(flat)) <= 1.0:\n                        scores = out\n                        continue\n                if classes is None and np.issubdtype(out.dtype, np.integer):\n                    classes = out\n                    continue\n\n        # If 'count' not provided, infer from scores/classes length\n        if count is None:\n            if scores is not None:\n                count = scores.shape[1] if scores.ndim == 2 else scores.shape[0]\n            elif classes is not None:\n                count = classes.shape[1] if classes.ndim == 2 else classes.shape[0]\n            else:\n                count = 0\n\n        # 4.2 Interpret Results and 4.3 Post-processing\n        output_frame = frame_bgr.copy()\n        if count and boxes is not None and scores is not None and classes is not None:\n            n = int(count)\n\n            det_boxes = boxes[0] if boxes.ndim == 3 else boxes\n            det_scores = scores[0] if scores.ndim == 2 else scores\n            det_classes = classes[0] if classes.ndim == 2 else classes\n\n            det_scores = det_scores.reshape(-1)\n            det_classes = det_classes.reshape(-1).astype(np.int32)\n\n            if det_boxes.ndim == 2 and det_boxes.shape[1] == 4:\n                for i in range(min(n, det_boxes.shape[0], det_scores.shape[0], det_classes.shape[0])):\n                    score = float(det_scores[i])\n                    if score < conf_thr:\n                        continue\n\n                    # Box is [ymin, xmin, ymax, xmax] in normalized [0, 1] coords\n                    ymin, xmin, ymax, xmax = [float(v) for v in det_boxes[i].tolist()]\n\n                    # Clip normalized coordinates\n                    xmin = max(0.0, min(1.0, xmin))\n                    xmax = max(0.0, min(1.0, xmax))\n                    ymin = max(0.0, min(1.0, ymin))\n                    ymax = max(0.0, min(1.0, ymax))\n\n                    # Scale to original frame size\n                    left = int(xmin * orig_width)\n                    right = int(xmax * orig_width)\n                    top = int(ymin * orig_height)\n                    bottom = int(ymax * orig_height)\n\n                    # Clip to frame boundaries\n                    left = max(0, min(orig_width - 1, left))\n                    right = max(0, min(orig_width - 1, right))\n                    top = max(0, min(orig_height - 1, top))\n                    bottom = max(0, min(orig_height - 1, bottom))\n\n                    # Skip invalid boxes\n                    if right <= left or bottom <= top:\n                        continue\n\n                    class_id = int(det_classes[i])\n                    if labels and 0 <= class_id < len(labels):\n                        label_name = labels[class_id]\n                    else:\n                        label_name = f\"id_{class_id}\"\n\n                    color = color_for_class(class_id)\n\n                    # Draw bounding box\n                    cv2.rectangle(output_frame, (left, top), (right, bottom), color, 2)\n\n                    # Prepare label text with score\n                    label_text = f\"{label_name}: {score*100:.1f}%\"\n                    (text_w, text_h), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n                    # Draw filled rectangle for text background\n                    text_bg_tl = (left, max(0, top - text_h - baseline - 4))\n                    text_bg_br = (left + text_w + 2, top)\n                    cv2.rectangle(output_frame, text_bg_tl, text_bg_br, color, thickness=-1)\n                    cv2.putText(output_frame, label_text, (left + 1, top - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n                                (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Overlay inference time and mAP info\n        info_text = f\"Inference: {inference_ms:.1f} ms | {map_text}\"\n        cv2.putText(output_frame, info_text, (8, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 0, 0), 3, cv2.LINE_AA)\n        cv2.putText(output_frame, info_text, (8, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # 4.4 Handle Output: write frame to output video file\n        out_writer.write(output_frame)\n\n        # 4.5 Loop continues until video ends\n\nexcept KeyboardInterrupt:\n    print(\"Interrupted by user. Finalizing...\")\n\n# ============================\n# Phase 5: Cleanup\n# ============================\n\n# 5.1 Release resources\ncap.release()\nout_writer.release()\n\n# Print summary\nif frame_count > 0:\n    avg_inference_ms = total_inference_time_ms / frame_count\n    print(f\"Processing complete. Frames processed: {frame_count}\")\n    print(f\"Average inference time per frame: {avg_inference_ms:.2f} ms\")\n    print(f\"Output saved to: {output_path}\")\n    print(\"Note: mAP not computed (ground truth annotations are required to calculate mAP).\")\nelse:\n    print(\"No frames were processed. Please check the input video.\")\n\n# FIX APPLIED:\n# Previous error was caused by assigning a string to model_height.\n# Now model_height and model_width are integers extracted from input tensor shape:\n#   model_height = int(input_shape[1]); model_width = int(input_shape[2])",
        "last_error": "Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_70665060_1770240284.py\", line 180, in <module>\n    resized_rgb = cv2.resize(frame_rgb, (model_width, model_height))\nTypeError: an integer is required (got type str)."
      },
      "session_id": "gpt-5-2025-08-07_54c5_tpusg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 493.577,
      "total_cost": 0.285672,
      "observations": [
        {
          "id": "time-23-16-38-578876_chatcmpl-D5eWl3t2ZpA3C980lljV9dfKVfCmb",
          "traceId": "f897a6ae",
          "type": "GENERATION",
          "name": "f897_tpusg_gen_attempt#1",
          "startTime": "2026-02-04T21:16:38.578000+00:00",
          "endTime": "2026-02-04T21:18:42.642000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.060743,
          "latency": 124064.0,
          "totalTokens": 7805,
          "costDetails": {
            "total": 0.060743
          },
          "completionTokens": 6041,
          "promptTokens": 1764,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-18-51-160685_chatcmpl-D5eYt1up9LfjngVuGpq5rMycFGHvI",
          "traceId": "f897a6ae",
          "type": "GENERATION",
          "name": "f897_tpusg_gen_attempt#2",
          "startTime": "2026-02-04T21:18:51.160000+00:00",
          "endTime": "2026-02-04T21:20:37.299000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06313875,
          "latency": 106139.0,
          "totalTokens": 10499,
          "costDetails": {
            "total": 0.06313875
          },
          "completionTokens": 5716,
          "promptTokens": 4783,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-20-45-759788_chatcmpl-D5eake28i4vsFZO5paVPfYvM1aTFL",
          "traceId": "f897a6ae",
          "type": "GENERATION",
          "name": "f897_tpusg_gen_attempt#3",
          "startTime": "2026-02-04T21:20:45.759000+00:00",
          "endTime": "2026-02-04T21:22:03.685000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05751625,
          "latency": 77926.0,
          "totalTokens": 10278,
          "costDetails": {
            "total": 0.05751625
          },
          "completionTokens": 5105,
          "promptTokens": 5173,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-22-12-037929_chatcmpl-D5ec8OR1Ww3XYHC8PgWEVwd3c5ULy",
          "traceId": "f897a6ae",
          "type": "GENERATION",
          "name": "f897_tpusg_gen_attempt#4",
          "startTime": "2026-02-04T21:22:12.037000+00:00",
          "endTime": "2026-02-04T21:23:23.276000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05171775,
          "latency": 71239.0,
          "totalTokens": 9814,
          "costDetails": {
            "total": 0.05171775
          },
          "completionTokens": 4739,
          "promptTokens": 5075,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-23-23-31-686185_chatcmpl-D5edQH22UlzCDoLuFR8uVBGuqSNsQ",
          "traceId": "f897a6ae",
          "type": "GENERATION",
          "name": "f897_tpusg_gen_attempt#5",
          "startTime": "2026-02-04T21:23:31.686000+00:00",
          "endTime": "2026-02-04T21:24:44.098000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05255625,
          "latency": 72412.0,
          "totalTokens": 9768,
          "costDetails": {
            "total": 0.05255625
          },
          "completionTokens": 4611,
          "promptTokens": 5157,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "a581b2b6-5ba4-407a-8d3a-ed6514c08f32",
          "traceId": "f897a6ae",
          "type": "SPAN",
          "name": "error_f8_tpusg_failure_signal_tpu_sketch_generator",
          "startTime": "2026-02-04T21:24:52.155000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed: Max retries reached with error. Last error: Traceback (most recent call last):\n  File \"script_70665060_1770240284.py\", line 180, in <module>\n    resized_rgb = cv2.resize(frame_rgb, (model_width, model_height))\nTypeError: an integer is required (got type str).",
          "calculatedTotalCost": 0.0,
          "totalTokens": 0,
          "costDetails": {},
          "completionTokens": 0,
          "promptTokens": 0,
          "environment": "default",
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "updatedAt": "2026-02-04T21:24:57.266Z",
      "createdAt": "2026-02-04T21:16:38.445Z",
      "environment": "default"
    },
    {
      "id": "678d7180",
      "timestamp": "2026-02-04T21:12:55.860000+00:00",
      "name": "678d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport sys\nimport time\nimport numpy as np\n\n# Phase 1: Setup\n# 1.1 Imports with fallback between tflite_runtime and tensorflow.lite\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\n    tflite_source = \"tflite_runtime\"\nexcept Exception:\n    try:\n        from tensorflow.lite import Interpreter  # type: ignore\n        from tensorflow.lite.experimental import load_delegate  # type: ignore\n        tflite_source = \"tensorflow.lite\"\n    except Exception as e:\n        print(\"ERROR: Unable to import TFLite Interpreter from either tflite_runtime or tensorflow.lite.\")\n        print(f\"Details: {e}\")\n        sys.exit(1)\n\n# Import cv2 only because the app explicitly needs video I/O and drawing\ntry:\n    import cv2\nexcept Exception as e:\n    print(\"ERROR: OpenCV (cv2) is required for video I/O and drawing but is not available.\")\n    print(f\"Details: {e}\")\n    sys.exit(1)\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\ntry:\n    confidence_threshold = float('0.5')\nexcept Exception:\n    confidence_threshold = 0.5\n\n# 1.3 Load Labels\ndef load_labels(labels_file):\n    labels = []\n    try:\n        with open(labels_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    except Exception as e:\n        print(f\"WARNING: Could not load label file at {labels_file}. Using empty labels. Details: {e}\")\n        labels = []\n    return labels\n\nlabels = load_labels(label_path)\n\n# 1.4 Load Interpreter with EdgeTPU delegate\ndef make_interpreter(model_file):\n    delegate = None\n    last_err = None\n    # Attempt to load the EdgeTPU delegate from common names/paths\n    for lib_name in ['libedgetpu.so.1.0', '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0']:\n        try:\n            delegate = load_delegate(lib_name)\n            print(f\"INFO: Loaded EdgeTPU delegate '{lib_name}' using {tflite_source}.\")\n            break\n        except Exception as e:\n            last_err = e\n            delegate = None\n    if delegate is None:\n        print(\"WARNING: Failed to load EdgeTPU delegate. Inference may not work with an EdgeTPU-compiled model.\")\n        print(\"Please ensure the EdgeTPU runtime is installed. Tried: 'libedgetpu.so.1.0' and '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0'.\")\n        print(f\"Last delegate load error: {last_err}\")\n\n    try:\n        if delegate is not None:\n            interpreter = Interpreter(model_path=model_file, experimental_delegates=[delegate])\n        else:\n            interpreter = Interpreter(model_path=model_file)\n        return interpreter\n    except Exception as e:\n        print(\"ERROR: Failed to create TFLite Interpreter. If this is an EdgeTPU-compiled model,\")\n        print(\"it typically requires the EdgeTPU delegate. Please install EdgeTPU runtime and retry.\")\n        print(f\"Details: {e}\")\n        sys.exit(1)\n\ninterpreter = make_interpreter(model_path)\ntry:\n    interpreter.allocate_tensors()\nexcept Exception as e:\n    print(\"ERROR: Failed to allocate tensors for the interpreter.\")\n    print(\"If using an EdgeTPU-compiled model without the EdgeTPU delegate, allocation will fail.\")\n    print(f\"Details: {e}\")\n    sys.exit(1)\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Determine input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\nin_height, in_width = int(input_shape[1]), int(input_shape[2])\n\n# Utility: detection output parsing\ndef parse_detection_outputs(interpreter, output_details):\n    \"\"\"\n    Attempts to find boxes, classes, scores, and count among the output tensors.\n    Returns:\n        boxes: np.ndarray shape (N, 4) with [ymin, xmin, ymax, xmax] normalized [0,1] if model outputs normalized.\n        classes: np.ndarray shape (N,)\n        scores: np.ndarray shape (N,)\n        count: int number of valid detections (if provided) else derived from scores length\n    \"\"\"\n    boxes = None\n    classes = None\n    scores = None\n    count = None\n\n    tensors = []\n    for od in output_details:\n        tensors.append(interpreter.get_tensor(od['index']))\n\n    # Flatten batch dimension if present\n    # Identify tensors based on shapes/dtypes heuristics\n    for t in tensors:\n        arr = t\n        if arr.ndim == 3 and arr.shape[0] == 1 and arr.shape[2] == 4:\n            boxes = arr[0]\n        elif arr.ndim == 2 and arr.shape[0] == 1:\n            # Could be classes or scores\n            flat = arr[0]\n            if flat.dtype == np.float32 or flat.dtype == np.float16:\n                scores = flat\n            else:\n                classes = flat\n        elif arr.ndim == 1 and arr.shape[0] == 1:\n            # num_detections\n            try:\n                count = int(arr[0])\n            except Exception:\n                pass\n\n    # Some models may return boxes shape (N,4) directly without batch dim\n    if boxes is None:\n        for t in tensors:\n            if t.ndim == 2 and t.shape[1] == 4:\n                boxes = t\n\n    # If classes are float, cast to int\n    if classes is not None and np.issubdtype(classes.dtype, np.floating):\n        classes = classes.astype(np.int32)\n\n    # Fallbacks if count not provided\n    if count is None:\n        if scores is not None:\n            count = scores.shape[0]\n        elif boxes is not None:\n            count = boxes.shape[0]\n        else:\n            count = 0\n\n    # Ensure shapes are consistent up to count\n    if boxes is not None and boxes.shape[0] < count:\n        count = boxes.shape[0]\n    if scores is not None and scores.shape[0] < count:\n        count = scores.shape[0]\n    if classes is not None and classes.shape[0] < count:\n        count = classes.shape[0]\n\n    # Slice to count\n    if boxes is not None:\n        boxes = boxes[:count]\n    if classes is not None:\n        classes = classes[:count]\n    if scores is not None:\n        scores = scores[:count]\n\n    return boxes, classes, scores, count\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n# 2.1 Acquire Input Data - open the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"ERROR: Failed to open input video at {input_path}\")\n    sys.exit(1)\n\n# Read first frame to initialize writer and check properties\nret, first_frame = cap.read()\nif not ret or first_frame is None:\n    print(\"ERROR: Could not read the first frame from the input video.\")\n    cap.release()\n    sys.exit(1)\n\n# Prepare VideoWriter with the same resolution and fps as input\nframe_h, frame_w = first_frame.shape[:2]\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0 or np.isnan(fps):\n    fps = 30.0  # fallback\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nout_writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\nif not out_writer.isOpened():\n    print(f\"ERROR: Failed to open output video for writing at {output_path}\")\n    cap.release()\n    sys.exit(1)\n\n# Stats for optional reporting\ntotal_frames = 0\ntotal_detections = 0\ninference_times_ms = []\n\n# Helper: Preprocess frame into model input tensor\ndef preprocess_frame_bgr(frame_bgr, target_w, target_h, floating):\n    # Resize\n    resized = cv2.resize(frame_bgr, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n    # Convert BGR -> RGB as most TFLite detection models expect RGB\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(rgb, axis=0)\n\n    # 2.3 Quantization Handling\n    if floating:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # For uint8 models, ensure dtype is uint8\n        input_data = np.asarray(input_data, dtype=np.uint8)\n    return input_data\n\n# Main processing loop\ncurrent_frame = first_frame\nwhile True:\n    frame = current_frame\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame_bgr(frame, in_width, in_height, floating_model)\n\n    # Phase 3: Inference\n    try:\n        interpreter.set_tensor(input_index, input_data)\n    except Exception as e:\n        print(\"ERROR: Failed to set input tensor. Ensure input tensor shape and dtype match the model.\")\n        print(f\"Details: {e}\")\n        break\n\n    t0 = time.time()\n    try:\n        interpreter.invoke()\n    except Exception as e:\n        print(\"ERROR: Inference failed. If using an EdgeTPU-compiled model, the EdgeTPU delegate must be loaded.\")\n        print(f\"Details: {e}\")\n        break\n    t1 = time.time()\n    inference_time_ms = (t1 - t0) * 1000.0\n    inference_times_ms.append(inference_time_ms)\n\n    # Phase 4: Output Interpretation & Handling\n    # 4.1 Get Output Tensors\n    boxes, classes, scores, count = parse_detection_outputs(interpreter, output_details)\n\n    # 4.2 Interpret Results and 4.3 Post-processing (thresholding, scaling, clipping)\n    detections = []\n    if boxes is not None and scores is not None and classes is not None:\n        for i in range(count):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n\n            # Boxes assumed normalized [ymin, xmin, ymax, xmax]\n            ymin, xmin, ymax, xmax = boxes[i]\n            # Scale to image coordinates\n            x1 = int(max(0, min(frame_w - 1, xmin * frame_w)))\n            y1 = int(max(0, min(frame_h - 1, ymin * frame_h)))\n            x2 = int(max(0, min(frame_w - 1, xmax * frame_w)))\n            y2 = int(max(0, min(frame_h - 1, ymax * frame_h)))\n\n            # Fix potential inverted coordinates\n            if x2 < x1:\n                x1, x2 = x2, x1\n            if y2 < y1:\n                y1, y2 = y2, y1\n\n            class_id = int(classes[i])\n            label_name = labels[class_id] if (labels and 0 <= class_id < len(labels)) else f\"id {class_id}\"\n\n            detections.append({\n                \"bbox\": (x1, y1, x2, y2),\n                \"score\": score,\n                \"class_id\": class_id,\n                \"label\": label_name\n            })\n\n    # 4.4 Handle Output - draw rectangles and labels; overlay mAP (N/A without GT)\n    # Draw detections\n    for det in detections:\n        x1, y1, x2, y2 = det[\"bbox\"]\n        label = det[\"label\"]\n        score = det[\"score\"]\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        caption = f\"{label}: {score:.2f}\"\n        # Text background\n        (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - baseline - 4), (x1 + tw + 2, y1), (0, 255, 0), thickness=-1)\n        cv2.putText(frame, caption, (x1 + 1, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    total_detections += len(detections)\n    total_frames += 1\n\n    # Overlay inference time and \"mAP\" info (no GT available -> N/A)\n    info_line1 = f\"Inference: {inference_time_ms:.1f} ms   Dets: {len(detections)}\"\n    info_line2 = \"mAP: N/A (no ground truth annotations provided)\"\n    cv2.putText(frame, info_line1, (8, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 50, 50), 2, cv2.LINE_AA)\n    cv2.putText(frame, info_line2, (8, 44), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50, 200, 255), 2, cv2.LINE_AA)\n\n    # Write frame to output\n    out_writer.write(frame)\n\n    # 4.5 Loop Continuation\n    ret, next_frame = cap.read()\n    if not ret or next_frame is None:\n        break\n    current_frame = next_frame\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\n# Final reporting\nif total_frames > 0:\n    avg_inf = np.mean(inference_times_ms) if inference_times_ms else 0.0\n    print(f\"Processing complete.\")\n    print(f\"Input video: {input_path}\")\n    print(f\"Output video: {output_path}\")\n    print(f\"Frames processed: {total_frames}\")\n    print(f\"Total detections (score >= {confidence_threshold}): {total_detections}\")\n    print(f\"Average inference time per frame: {avg_inf:.2f} ms\")\n    print(\"mAP: N/A (no ground truth available to compute mean Average Precision).\")\nelse:\n    print(\"No frames processed. Please check the input video file.\")"
      },
      "session_id": "gpt-5-2025-08-07_54c5_tpusg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 158.324,
      "total_cost": 0.064813,
      "observations": [
        {
          "id": "time-23-12-56-244970_chatcmpl-D5eTABU9uk8Ci8kSGpP2ECAg8Flme",
          "traceId": "678d7180",
          "type": "GENERATION",
          "name": "678d_tpusg_gen_attempt#1",
          "startTime": "2026-02-04T21:12:56.244000+00:00",
          "endTime": "2026-02-04T21:15:34.568000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.064813,
          "latency": 158324.0,
          "totalTokens": 8212,
          "costDetails": {
            "total": 0.064813
          },
          "completionTokens": 6448,
          "promptTokens": 1764,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:16:32.605Z",
      "createdAt": "2026-02-04T21:12:56.380Z",
      "environment": "default"
    },
    {
      "id": "4a9b8ab8",
      "timestamp": "2026-02-04T21:08:43.957000+00:00",
      "name": "4a9b_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport sys\nimport time\nimport numpy as np\n\n# Phase 1.1: Imports (Interpreter + EdgeTPU delegate with fallback)\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\nexcept Exception:\n    try:\n        from tensorflow.lite import Interpreter  # type: ignore\n        from tensorflow.lite.experimental import load_delegate  # type: ignore\n    except Exception as e:\n        print(\"ERROR: Unable to import tflite_runtime or tensorflow.lite Interpreter.\")\n        print(f\"Detail: {e}\")\n        sys.exit(1)\n\n# Import cv2 only because image/video processing is explicitly required\ntry:\n    import cv2\nexcept Exception as e:\n    print(\"ERROR: OpenCV (cv2) is required for video I/O but could not be imported.\")\n    print(f\"Detail: {e}\")\n    sys.exit(1)\n\n# Phase 1.2: Paths/Parameters (using provided configuration parameters)\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold_str  = 0.5\ntry:\n    confidence_threshold = float(confidence_threshold_str)\nexcept Exception:\n    confidence_threshold = 0.5\n\n# Utility: Load labels (Phase 1.3)\ndef load_labels(path):\n    labels = []\n    try:\n        with open(path, 'r', encoding='utf-8') as f:\n            for line in f:\n                name = line.strip()\n                if name:\n                    labels.append(name)\n    except Exception as e:\n        print(f\"WARNING: Failed to load labels from {path}. Detail: {e}\")\n    return labels\n\n# Phase 1.4: Load Interpreter with EdgeTPU delegate\ndef make_interpreter_tpu(model_file):\n    last_err = None\n    # Try default shared object name used on Coral\n    try:\n        interpreter = Interpreter(\n            model_path=model_file,\n            experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n        )\n        return interpreter\n    except Exception as e1:\n        last_err = e1\n    # Try absolute path used on aarch64 Linux\n    try:\n        interpreter = Interpreter(\n            model_path=model_file,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n        return interpreter\n    except Exception as e2:\n        print(\"ERROR: Failed to load EdgeTPU delegate. Inference requires the EdgeTPU on the Coral Dev Board.\")\n        print(\"Attempts:\")\n        print(f\" - load_delegate('libedgetpu.so.1.0') -> {last_err}\")\n        print(f\" - load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0') -> {e2}\")\n        print(\"Please ensure the EdgeTPU runtime is installed and the TPU is accessible.\")\n        sys.exit(1)\n\n# Helper: Parse detection outputs robustly\ndef parse_tflite_detection_outputs(interpreter, output_details):\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    for od in output_details:\n        out = interpreter.get_tensor(od['index'])\n        # num detections\n        if out.size == 1:\n            try:\n                num = int(np.squeeze(out).astype(np.int32))\n            except Exception:\n                num = int(np.squeeze(out))\n            continue\n        # boxes: [1, N, 4]\n        if out.ndim == 3 and out.shape[-1] == 4:\n            boxes = out[0]\n            continue\n        # classes or scores: [1, N]\n        if out.ndim == 2 and out.shape[0] == 1:\n            candidate = out[0]\n            # Heuristic: scores typically in [0,1]\n            if candidate.dtype.kind in ('f',):\n                maxv = float(np.max(candidate)) if candidate.size else 0.0\n                minv = float(np.min(candidate)) if candidate.size else 0.0\n                if 0.0 <= minv and maxv <= 1.0:\n                    scores = candidate\n                else:\n                    classes = candidate.astype(np.int32)\n            else:\n                classes = candidate.astype(np.int32)\n\n    # Fallbacks if num is None\n    if num is None:\n        if scores is not None:\n            num = scores.shape[0]\n        elif boxes is not None:\n            num = boxes.shape[0]\n        elif classes is not None:\n            num = classes.shape[0]\n        else:\n            num = 0\n\n    # Ensure not None arrays for downstream logic\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n\n    return boxes, classes, scores, num\n\n# Helper: Draw detections on frame\ndef draw_detections_on_frame(frame_bgr, detections, labels, map_text):\n    for det in detections:\n        x0, y0, x1, y1, cls_id, score = det\n        color = (0, 255, 0)\n        cv2.rectangle(frame_bgr, (x0, y0), (x1, y1), color, 2)\n        label_text = labels[cls_id] if 0 <= cls_id < len(labels) else f\"id:{cls_id}\"\n        caption = f\"{label_text} {score:.2f}\"\n        cv2.putText(frame_bgr, caption, (x0, max(y0 - 5, 0)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2, cv2.LINE_AA)\n\n    # Draw mAP text on the top-left corner\n    cv2.rectangle(frame_bgr, (5, 5), (240, 30), (0, 0, 0), thickness=-1)\n    cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n    return frame_bgr\n\n# Main execution\ndef main():\n    # Load labels (Phase 1.3)\n    labels = load_labels(label_path)\n\n    # Load interpreter with EdgeTPU (Phase 1.4)\n    interpreter = make_interpreter_tpu(model_path)\n    try:\n        interpreter.allocate_tensors()\n    except Exception as e:\n        print(f\"ERROR: Failed to allocate tensors: {e}\")\n        sys.exit(1)\n\n    # Get model I/O details (Phase 1.5)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    in_shape = input_details[0]['shape']  # [1, H, W, C]\n    in_height, in_width = int(in_shape[1]), int(in_shape[2])\n    in_dtype = input_details[0]['dtype']\n    floating_model = (in_dtype == np.float32)\n\n    # Phase 2.1: Acquire Input Data - open video file\n    if not os.path.exists(input_path):\n        print(f\"ERROR: Input video file not found: {input_path}\")\n        sys.exit(1)\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"ERROR: Failed to open video file: {input_path}\")\n        sys.exit(1)\n\n    # Prepare output video writer\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # Fallback FPS\n    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (orig_w, orig_h))\n    if not writer.isOpened():\n        print(f\"ERROR: Failed to open output video writer for: {output_path}\")\n        cap.release()\n        sys.exit(1)\n\n    # For mAP: Without ground truth annotations, true mAP cannot be computed.\n    # We will annotate as \"mAP: N/A (no GT)\" to be informative while keeping processing functional.\n    map_text = \"mAP: N/A (no ground truth)\"\n\n    # Phase 2.4: Processing loop for single input video file\n    frame_index = 0\n    try:\n        while True:\n            ret, frame_bgr = cap.read()\n            if not ret:\n                break  # End of video\n\n            frame_index += 1\n            # Phase 2.2: Preprocess Data (resize and convert color to match model expectation RGB)\n            frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n            resized_rgb = cv2.resize(frame_rgb, (in_width, in_height), interpolation=cv2.INTER_LINEAR)\n            input_data = np.expand_dims(resized_rgb, axis=0)\n\n            # Phase 2.3: Quantization Handling\n            if floating_model:\n                input_data = (np.float32(input_data) - 127.5) / 127.5\n            else:\n                # Ensure dtype matches model input\n                input_data = input_data.astype(in_dtype, copy=False)\n\n            # Phase 3.1: Set input tensor\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n\n            # Phase 3.2: Inference\n            interpreter.invoke()\n\n            # Phase 4.1: Get outputs\n            boxes, classes, scores, num = parse_tflite_detection_outputs(interpreter, output_details)\n\n            # Phase 4.2: Interpret Results\n            # Convert normalized boxes to pixel coordinates and pair with labels/scores\n            detections = []\n            for i in range(int(num)):\n                score = float(scores[i]) if i < len(scores) else 0.0\n                if score < confidence_threshold:\n                    continue\n                cls_id = int(classes[i]) if i < len(classes) else -1\n                if i < len(boxes):\n                    ymin, xmin, ymax, xmax = boxes[i]\n                else:\n                    ymin, xmin, ymax, xmax = 0.0, 0.0, 0.0, 0.0\n\n                # Phase 4.3: Post-processing (scale + clip)\n                x0 = int(max(0, min(orig_w - 1, round(xmin * orig_w))))\n                y0 = int(max(0, min(orig_h - 1, round(ymin * orig_h))))\n                x1 = int(max(0, min(orig_w - 1, round(xmax * orig_w))))\n                y1 = int(max(0, min(orig_h - 1, round(ymax * orig_h))))\n\n                # Ensure proper box orientation\n                if x1 < x0:\n                    x0, x1 = x1, x0\n                if y1 < y0:\n                    y0, y1 = y1, y0\n\n                detections.append((x0, y0, x1, y1, cls_id, score))\n\n            # Phase 4.4: Handle Output (draw and write to file)\n            annotated = draw_detections_on_frame(frame_bgr, detections, labels, map_text)\n            writer.write(annotated)\n\n            # Phase 4.5: Loop continuation automatically by while True and break on EOF\n\n    finally:\n        # Phase 5.1: Cleanup resources\n        cap.release()\n        writer.release()\n\n    print(f\"Processing completed. Output video with detections saved to: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_54c5_tpusg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 187.248,
      "total_cost": 0.067995,
      "observations": [
        {
          "id": "time-23-08-44-374639_chatcmpl-D5eP62mPyRAxpv7vO8Wj7MJSQeahy",
          "traceId": "4a9b8ab8",
          "type": "GENERATION",
          "name": "4a9b_tpusg_gen_attempt#1",
          "startTime": "2026-02-04T21:08:44.374000+00:00",
          "endTime": "2026-02-04T21:11:51.622000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.067995,
          "latency": 187248.0,
          "totalTokens": 8343,
          "costDetails": {
            "total": 0.067995
          },
          "completionTokens": 6579,
          "promptTokens": 1764,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:12:50.386Z",
      "createdAt": "2026-02-04T21:08:44.486Z",
      "environment": "default"
    },
    {
      "id": "bb0a8235",
      "timestamp": "2026-02-04T21:05:12.333000+00:00",
      "name": "bb0a_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.1 Imports: TFLite Interpreter with EdgeTPU delegate\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\n    runtime_source = \"tflite_runtime\"\nexcept Exception:\n    try:\n        from tensorflow.lite import Interpreter\n        from tensorflow.lite.experimental import load_delegate\n        runtime_source = \"tensorflow.lite\"\n    except Exception as e:\n        raise SystemExit(f\"ERROR: Failed to import TFLite Interpreter. Details: {e}\")\n\n# 1.2 Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\ntry:\n    confidence_threshold = float('0.5')\nexcept Exception:\n    confidence_threshold = 0.5\n\n# Ensure output directory exists\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n# 1.3 Load Labels\ndef load_labels(path):\n    labels = []\n    try:\n        with open(path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    # Handle possible \"id label\" or \"label\" formats\n                    parts = line.split(maxsplit=1)\n                    if len(parts) == 2 and parts[0].isdigit():\n                        labels.append(parts[1])\n                    else:\n                        labels.append(line)\n    except Exception as e:\n        raise SystemExit(f\"ERROR: Failed to load labels from {path}. Details: {e}\")\n    return labels\n\nlabels = load_labels(label_path)\n\n# 1.4 Load Interpreter with EdgeTPU\nusing_edgetpu = True\ninterpreter = None\ndelegate_error_msgs = []\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept Exception as e1:\n    delegate_error_msgs.append(str(e1))\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e2:\n        delegate_error_msgs.append(str(e2))\n        # Fallback to CPU interpreter with informative message\n        using_edgetpu = False\n        try:\n            interpreter = Interpreter(model_path=model_path)\n            print(\"WARNING: EdgeTPU delegate failed to load. Falling back to CPU execution.\")\n            print(\"Details (attempts):\")\n            for idx, msg in enumerate(delegate_error_msgs, 1):\n                print(f\"  Attempt {idx}: {msg}\")\n            print(\"Ensure the EdgeTPU runtime is installed: sudo apt-get install libedgetpu1-std\")\n        except Exception as e3:\n            raise SystemExit(f\"ERROR: Failed to create TFLite Interpreter. Details: {e3}\")\n\n# Allocate tensors\ninterpreter.allocate_tensors()\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\ninput_height, input_width = int(input_shape[1]), int(input_shape[2])\nfloating_model = (input_dtype == np.float32)\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# 2.1 Acquire Input Data: open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    raise SystemExit(f\"ERROR: Cannot open input video: {input_path}\")\n\n# Prepare video writer with the same resolution/fps as input\norig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0 or np.isnan(fps):\n    fps = 30.0  # default fallback\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(output_path, fourcc, fps, (orig_width, orig_height))\nif not writer.isOpened():\n    cap.release()\n    raise SystemExit(f\"ERROR: Cannot open output video writer: {output_path}\")\n\n# Data structures for \"mAP\" proxy computation (mean of average confidences per class)\nper_class_confidences = {}  # class_id -> list of confidences\n\nframe_index = 0\ninference_times = []\n\n# 2.4 Loop control: process until the end of video\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break\n    frame_index += 1\n\n    # 2.2 Preprocess Data: resize, color convert, batch dimension\n    # Keep a copy of original frame for drawing\n    original_frame = frame_bgr.copy()\n    resized_bgr = cv2.resize(frame_bgr, (input_width, input_height))\n    input_rgb = cv2.cvtColor(resized_bgr, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(input_rgb, axis=0)\n\n    # 2.3 Quantization Handling\n    if floating_model:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = input_data.astype(input_dtype)\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    interpreter.set_tensor(input_index, input_data)\n    t0 = time.time()\n    interpreter.invoke()\n    t1 = time.time()\n    inference_time_ms = (t1 - t0) * 1000.0\n    inference_times.append(inference_time_ms)\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling\n    # =========================\n\n    # 4.1 Get Output Tensor(s)\n    # The detector typically outputs: boxes [1,N,4], classes [1,N], scores [1,N], count [1]\n    boxes = None\n    classes = None\n    scores = None\n    count = None\n\n    # Collect raw outputs\n    raw_outputs = []\n    for od in output_details:\n        raw = interpreter.get_tensor(od['index'])\n        raw_outputs.append(raw)\n\n    # Heuristics to parse outputs\n    for arr in raw_outputs:\n        # boxes detection\n        if arr.ndim == 3 and arr.shape[0] == 1 and arr.shape[2] == 4:\n            boxes = arr[0]\n        elif arr.ndim == 2 and arr.shape[0] == 1:\n            # Could be classes or scores\n            candidate = arr[0]\n            if candidate.dtype in (np.float32, np.float64):\n                # Scores are in [0, 1], classes typically >1 for COCO\n                if candidate.size > 0 and np.nanmax(candidate) <= 1.0 + 1e-3:\n                    scores = candidate.astype(np.float32)\n                else:\n                    classes = candidate.astype(np.int32)\n            else:\n                # integer classes\n                classes = candidate.astype(np.int32)\n        elif arr.ndim == 1 and arr.size == 1:\n            try:\n                count = int(arr[0])\n            except Exception:\n                pass\n        elif arr.ndim == 2 and arr.shape[1] == 4:\n            # some models may output [N,4] directly\n            boxes = arr\n\n    if scores is None and boxes is not None:\n        # If scores didn't parse but boxes exist, assume length N and set default scores of 1\n        scores = np.ones((boxes.shape[0],), dtype=np.float32)\n    if classes is None and scores is not None:\n        classes = np.zeros_like(scores, dtype=np.int32)\n    if boxes is None:\n        # No boxes returned, skip drawing and write frame as is\n        writer.write(original_frame)\n        continue\n    if count is None:\n        count = min(len(scores), boxes.shape[0]) if scores is not None else boxes.shape[0]\n    count = int(count)\n\n    # 4.2 Interpret Results: Convert raw outputs to human-readable detections\n    detections = []\n    # Determine if boxes are normalized [0,1] or absolute\n    max_box_val = float(np.max(boxes)) if boxes.size > 0 else 0.0\n    boxes_are_normalized = max_box_val <= 1.5  # heuristic threshold\n\n    for i in range(count):\n        score = float(scores[i]) if i < len(scores) else 0.0\n        class_id = int(classes[i]) if i < len(classes) else 0\n        y_min, x_min, y_max, x_max = boxes[i].tolist()\n\n        # 4.3 Post-processing: confidence filtering and coordinate scaling/clipping\n        if score < confidence_threshold:\n            continue\n\n        if boxes_are_normalized:\n            # scale to original frame\n            y_min_px = int(max(0, min(orig_height - 1, y_min * orig_height)))\n            x_min_px = int(max(0, min(orig_width - 1, x_min * orig_width)))\n            y_max_px = int(max(0, min(orig_height - 1, y_max * orig_height)))\n            x_max_px = int(max(0, min(orig_width - 1, x_max * orig_width)))\n        else:\n            # if model outputs absolute coords relative to model input, scale to original\n            scale_y = orig_height / float(input_height)\n            scale_x = orig_width / float(input_width)\n            y_min_px = int(max(0, min(orig_height - 1, y_min * scale_y)))\n            x_min_px = int(max(0, min(orig_width - 1, x_min * scale_x)))\n            y_max_px = int(max(0, min(orig_height - 1, y_max * scale_y)))\n            x_max_px = int(max(0, min(orig_width - 1, x_max * scale_x)))\n\n        # Ensure valid box\n        if x_max_px <= x_min_px or y_max_px <= y_min_px:\n            continue\n\n        detections.append({\n            'bbox': (x_min_px, y_min_px, x_max_px, y_max_px),\n            'score': score,\n            'class_id': class_id,\n        })\n\n    # Draw detections and maintain per-class confidences for \"mAP\" proxy\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        score = det['score']\n        class_id = det['class_id']\n        class_name = labels[class_id] if 0 <= class_id < len(labels) else f'id_{class_id}'\n\n        # Draw rectangle\n        cv2.rectangle(original_frame, (x1, y1), (x2, y2), (0, 200, 0), 2)\n\n        # Put label text\n        label_text = f\"{class_name}: {score:.2f}\"\n        # Determine text position\n        text_x, text_y = x1, max(0, y1 - 10)\n        cv2.putText(original_frame, label_text, (text_x, text_y),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), thickness=3, lineType=cv2.LINE_AA)\n        cv2.putText(original_frame, label_text, (text_x, text_y),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), thickness=1, lineType=cv2.LINE_AA)\n\n        # Update per-class confidences for mAP proxy computation\n        if class_id not in per_class_confidences:\n            per_class_confidences[class_id] = []\n        per_class_confidences[class_id].append(score)\n\n    # Compute a simple \"mAP\" proxy as mean of per-class average confidences (since no ground-truth is provided)\n    ap_values = []\n    for cls_id, conf_list in per_class_confidences.items():\n        if len(conf_list) > 0:\n            ap_values.append(float(np.mean(conf_list)))\n    approx_map = float(np.mean(ap_values)) if len(ap_values) > 0 else 0.0\n\n    # Overlay stats: mAP (proxy), frame index, inference time\n    stats_text_1 = f\"mAP: {approx_map:.3f}  |  Detections: {len(detections)}\"\n    stats_text_2 = f\"Inference: {inference_time_ms:.1f} ms  |  Backend: {'EdgeTPU' if using_edgetpu else runtime_source}\"\n    cv2.putText(original_frame, stats_text_1, (10, 25),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 50, 50), 3, cv2.LINE_AA)\n    cv2.putText(original_frame, stats_text_1, (10, 25),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 1, cv2.LINE_AA)\n    cv2.putText(original_frame, stats_text_2, (10, 50),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50, 50, 50), 3, cv2.LINE_AA)\n    cv2.putText(original_frame, stats_text_2, (10, 50),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # 4.4 Handle Output: write annotated frame to output video\n    writer.write(original_frame)\n\n# 4.5 Loop Continuation handled by while loop until frames are exhausted\n\n# =========================\n# Phase 5: Cleanup\n# =========================\n\ncap.release()\nwriter.release()\n\n# Final summary printout\nif len(inference_times) > 0:\n    print(f\"Processed {frame_index} frames.\")\n    print(f\"Average inference time: {np.mean(inference_times):.2f} ms \"\n          f\"(min {np.min(inference_times):.2f} ms, max {np.max(inference_times):.2f} ms)\")\nelse:\n    print(\"No frames processed.\")\n\nprint(f\"Output saved to: {output_path}\")"
      },
      "session_id": "gpt-5-2025-08-07_54c5_tpusg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 146.339,
      "total_cost": 0.072863,
      "observations": [
        {
          "id": "time-23-05-12-743715_chatcmpl-D5eLh6mr49jzV1CZQkDCu7U74fxju",
          "traceId": "bb0a8235",
          "type": "GENERATION",
          "name": "bb0a_tpusg_gen_attempt#1",
          "startTime": "2026-02-04T21:05:12.743000+00:00",
          "endTime": "2026-02-04T21:07:39.082000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.072863,
          "latency": 146339.0,
          "totalTokens": 9017,
          "costDetails": {
            "total": 0.072863
          },
          "completionTokens": 7253,
          "promptTokens": 1764,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:08:38.559Z",
      "createdAt": "2026-02-04T21:05:12.853Z",
      "environment": "default"
    },
    {
      "id": "74405acd",
      "timestamp": "2026-02-04T21:00:22.421000+00:00",
      "name": "7440_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport sys\nimport time\nimport numpy as np\n\n# Phase 1: Setup\n# 1.1 Imports: Interpreter and EdgeTPU delegate with fallback paths\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\n    tflite_backend = \"tflite_runtime\"\nexcept Exception:\n    try:\n        from tensorflow.lite import Interpreter  # type: ignore\n        from tensorflow.lite.experimental import load_delegate  # type: ignore\n        tflite_backend = \"tensorflow.lite\"\n    except Exception as e:\n        print(\"ERROR: Unable to import TFLite Interpreter from either 'tflite_runtime' or 'tensorflow'.\")\n        print(f\"Detail: {e}\")\n        sys.exit(1)\n\n# Import cv2 only because video processing is explicitly required\ntry:\n    import cv2\nexcept Exception as e:\n    print(\"ERROR: OpenCV (cv2) is required for video processing but could not be imported.\")\n    print(f\"Detail: {e}\")\n    sys.exit(1)\n\n# 1.2 Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Sanity check for files\nif not os.path.isfile(model_path):\n    print(f\"ERROR: Model file not found at: {model_path}\")\n    sys.exit(1)\n\nif not os.path.isfile(input_path):\n    print(f\"ERROR: Input video file not found at: {input_path}\")\n    sys.exit(1)\n\nif not os.path.isfile(label_path):\n    print(f\"WARNING: Label file not found at: {label_path}. Proceeding without labels.\")\n    labels = []\nelse:\n    # 1.3 Load Labels\n    try:\n        with open(label_path, 'r', encoding='utf-8') as f:\n            labels = [line.strip() for line in f.readlines() if line.strip()]\n    except Exception as e:\n        print(f\"WARNING: Failed to read labels from {label_path}. Detail: {e}\")\n        labels = []\n\n# 1.4 Load Interpreter with EdgeTPU delegate\ninterpreter = None\ndelegate_error_messages = []\ntry:\n    # First try default shared object name\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept Exception as e1:\n    delegate_error_messages.append(str(e1))\n    try:\n        # Fallback to explicit system path often used on Coral Dev Board\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e2:\n        delegate_error_messages.append(str(e2))\n        print(\"ERROR: Failed to load the EdgeTPU delegate. Ensure the EdgeTPU runtime is installed.\")\n        print(\"Tried delegates: 'libedgetpu.so.1.0' and '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0'\")\n        print(\"Details:\")\n        for idx, msg in enumerate(delegate_error_messages, 1):\n            print(f\"  Attempt {idx}: {msg}\")\n        sys.exit(1)\n\n# Allocate tensors\ntry:\n    interpreter.allocate_tensors()\nexcept Exception as e:\n    print(f\"ERROR: Failed to allocate tensors for the interpreter. Detail: {e}\")\n    sys.exit(1)\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nif len(input_details) < 1:\n    print(\"ERROR: The model does not have any input tensors.\")\n    sys.exit(1)\n\n# Determine input tensor properties\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Parse confidence threshold\ntry:\n    conf_thresh = float(confidence_threshold)\nexcept Exception:\n    conf_thresh = 0.5\n\ndef preprocess_frame(frame_bgr, target_h, target_w, floating):\n    \"\"\"\n    Resize and convert the frame to match model input requirements.\n    Returns input tensor of shape [1, H, W, 3] with appropriate dtype.\n    \"\"\"\n    # Convert BGR to RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # Resize to model input size\n    resized = cv2.resize(frame_rgb, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n\n    if floating:\n        # Normalize to [-1, 1]\n        input_data = (resized.astype(np.float32) - 127.5) / 127.5\n    else:\n        # Handle quantized types\n        if input_dtype == np.uint8:\n            input_data = resized.astype(np.uint8)\n        elif input_dtype == np.int8:\n            # Quantize with scale/zero_point if available\n            scale, zero_point = input_details[0].get('quantization', (0.0, 0))\n            if scale is None or scale == 0.0:\n                # Fallback heuristic: assume input in [0,255] -> [-128,127]\n                input_data = np.clip(np.round(resized.astype(np.float32) - 128.0), -128, 127).astype(np.int8)\n            else:\n                # Convert from [0,255] to [0,1], then quantize\n                norm = resized.astype(np.float32) / 255.0\n                quantized = np.round(norm / scale + zero_point)\n                input_data = np.clip(quantized, -128, 127).astype(np.int8)\n        else:\n            # Unknown type; attempt safe cast\n            input_data = resized.astype(input_dtype)\n\n    # Add batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef decode_detection_outputs(interpreter, output_details):\n    \"\"\"\n    Robustly extract boxes, classes, scores, and num_detections from detector outputs.\n    Returns:\n        boxes: np.ndarray [N,4] (ymin, xmin, ymax, xmax) in normalized coords\n        classes: np.ndarray [N] int\n        scores: np.ndarray [N] float\n        num_dets: int\n    \"\"\"\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Try by name hints first\n    name_map = {od.get('name', f'out_{i}'): (i, od) for i, od in enumerate(output_details)}\n    for key in name_map:\n        lower = key.lower()\n        idx = name_map[key][0]\n        val = outputs[idx]\n        if 'postprocess' in lower and 'box' in lower:\n            boxes = val\n        elif 'postprocess' in lower and 'class' in lower:\n            classes = val\n        elif 'postprocess' in lower and ('score' in lower or 'prob' in lower):\n            scores = val\n        elif 'num' in lower and 'detection' in lower:\n            num = val\n\n    # Fallback by shape pattern\n    if boxes is None or classes is None or scores is None or num is None:\n        for val in outputs:\n            if val.ndim == 3 and val.shape[-1] == 4 and boxes is None:\n                boxes = val\n        for val in outputs:\n            if val.ndim == 2 and scores is None and val.dtype == np.float32:\n                # Heuristic: the larger variance array is likely the scores\n                if scores is None:\n                    scores = val\n                else:\n                    # pick the one with more unique values as scores\n                    if np.unique(val).size > np.unique(scores).size:\n                        scores = val\n        for val in outputs:\n            if val.ndim == 2 and classes is None and val.dtype in (np.float32, np.int32, np.int64):\n                classes = val\n        for val in outputs:\n            if val.ndim == 1 and val.size == 1 and num is None:\n                num = val\n\n    # Safety checks and reshaping\n    if boxes is None or classes is None or scores is None:\n        raise RuntimeError(\"Failed to parse detection outputs: boxes/classes/scores not all found.\")\n    if boxes.ndim == 3:\n        boxes = boxes[0]\n    if classes.ndim == 2:\n        classes = classes[0]\n    if scores.ndim == 2:\n        scores = scores[0]\n    if num is None:\n        num_dets = min(len(scores), len(classes), len(boxes))\n    else:\n        num_dets = int(num.reshape([-1])[0])\n\n    # Clip num_dets to valid range\n    num_dets = max(0, min(num_dets, min(len(scores), len(classes), len(boxes))))\n\n    # Cast classes to int\n    classes = classes.astype(np.int32)\n\n    return boxes[:num_dets], classes[:num_dets], scores[:num_dets], num_dets\n\ndef get_label_name(class_id):\n    if labels and 0 <= class_id < len(labels):\n        return labels[class_id]\n    return f'class_{class_id}'\n\ndef draw_detections(frame, detections, map_text):\n    \"\"\"\n    Draw bounding boxes and labels on the frame.\n    detections: list of dicts with keys: 'box'(xmin,ymin,xmax,ymax), 'score', 'class_id', 'label'\n    map_text: string to overlay indicating mAP or proxy metric\n    \"\"\"\n    for det in detections:\n        (xmin, ymin, xmax, ymax) = det['box']\n        label = det['label']\n        score = det['score']\n\n        # Draw rectangle\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)\n\n        # Prepare label text\n        caption = f\"{label}: {score:.2f}\"\n        # Text background for readability\n        (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (xmin, max(0, ymin - th - baseline - 2)), (xmin + tw + 2, ymin), (0, 255, 0), thickness=-1)\n        cv2.putText(frame, caption, (xmin + 1, max(0, ymin - baseline - 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Draw mAP/proxy text at top-left\n    cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 200, 255), 2, cv2.LINE_AA)\n\ndef compute_proxy_map(per_class_conf_hist):\n    \"\"\"\n    IMPORTANT: True mAP requires ground-truth annotations.\n    Since none are provided, we compute a proxy metric:\n    - For each class, average its detection confidences across frames (AP_proxy = mean of confidences).\n    - mAP_proxy = mean of AP_proxy over classes that had detections.\n    Returns (mAP_proxy_value, per_class_AP_proxy_dict)\n    \"\"\"\n    ap_list = []\n    ap_per_class = {}\n    for cid, confs in per_class_conf_hist.items():\n        if len(confs) > 0:\n            ap = float(np.mean(confs))\n            ap_per_class[cid] = ap\n            ap_list.append(ap)\n    if len(ap_list) == 0:\n        return 0.0, ap_per_class\n    return float(np.mean(ap_list)), ap_per_class\n\ndef main():\n    # Determine input tensor spatial size\n    if len(input_shape) != 4 or input_shape[-1] != 3:\n        print(f\"ERROR: Unexpected input tensor shape: {input_shape}. Expected [1, H, W, 3].\")\n        return\n    model_in_h = int(input_shape[1])\n    model_in_w = int(input_shape[2])\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"ERROR: Unable to open input video: {input_path}\")\n        return\n\n    # Determine video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    if width <= 0 or height <= 0:\n        # Read one frame to determine size\n        ret_probe, frame_probe = cap.read()\n        if not ret_probe:\n            print(\"ERROR: Could not read frame to determine video size.\")\n            cap.release()\n            return\n        height, width = frame_probe.shape[:2]\n        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # reset to start\n\n    # Prepare VideoWriter\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out_writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    if not out_writer.isOpened():\n        print(f\"ERROR: Unable to open VideoWriter for: {output_path}\")\n        cap.release()\n        return\n\n    # Accumulate confidences per class for proxy mAP\n    per_class_conf_hist = {}\n    frame_count = 0\n    start_time = time.time()\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break  # End of video\n\n        frame_count += 1\n\n        # 2.2 Preprocess Data\n        input_data = preprocess_frame(frame_bgr, model_in_h, model_in_w, floating_model).astype(input_dtype)\n\n        # 2.3 Quantization Handling handled within preprocess_frame via dtype check\n\n        # Phase 3: Inference\n        try:\n            interpreter.set_tensor(input_index, input_data)\n            interpreter.invoke()\n        except Exception as e:\n            print(f\"ERROR during inference on frame {frame_count}: {e}\")\n            break\n\n        # Phase 4: Output Interpretation & Handling Loop\n        # 4.1 Get Output Tensors\n        try:\n            boxes_norm, classes, scores, num_dets = decode_detection_outputs(interpreter, output_details)\n        except Exception as e:\n            print(f\"ERROR: Failed to decode detection outputs on frame {frame_count}. Detail: {e}\")\n            break\n\n        # 4.2 Interpret Results\n        detections = []\n        for i in range(num_dets):\n            score = float(scores[i])\n            class_id = int(classes[i])\n            label_name = get_label_name(class_id)\n            ymin, xmin, ymax, xmax = boxes_norm[i].tolist()\n\n            # 4.3 Post-processing: threshold, scale, clip\n            if score < conf_thresh:\n                continue\n\n            # Clip normalized coordinates\n            ymin = min(max(ymin, 0.0), 1.0)\n            xmin = min(max(xmin, 0.0), 1.0)\n            ymax = min(max(ymax, 0.0), 1.0)\n            xmax = min(max(xmax, 0.0), 1.0)\n\n            # Scale to pixel coordinates\n            x_min_px = int(round(xmin * width))\n            y_min_px = int(round(ymin * height))\n            x_max_px = int(round(xmax * width))\n            y_max_px = int(round(ymax * height))\n\n            # Ensure proper box ordering\n            x_min_px, x_max_px = max(0, min(x_min_px, width - 1)), max(0, min(x_max_px, width - 1))\n            y_min_px, y_max_px = max(0, min(y_min_px, height - 1)), max(0, min(y_max_px, height - 1))\n            if x_max_px <= x_min_px or y_max_px <= y_min_px:\n                continue  # invalid box after clipping/scaling\n\n            det = {\n                'box': (x_min_px, y_min_px, x_max_px, y_max_px),\n                'score': score,\n                'class_id': class_id,\n                'label': label_name\n            }\n            detections.append(det)\n\n            # Accumulate confidences for proxy mAP\n            if class_id not in per_class_conf_hist:\n                per_class_conf_hist[class_id] = []\n            per_class_conf_hist[class_id].append(score)\n\n        # Compute proxy mAP (since ground-truth is not available)\n        map_proxy, _ = compute_proxy_map(per_class_conf_hist)\n        map_text = f\"mAP (proxy): {map_proxy:.3f}\"\n\n        # 4.4 Handle Output: draw and write frame\n        draw_detections(frame_bgr, detections, map_text)\n        out_writer.write(frame_bgr)\n\n    elapsed = time.time() - start_time\n\n    # Final proxy mAP computation\n    final_map_proxy, per_class_ap_proxy = compute_proxy_map(per_class_conf_hist)\n    print(f\"Processing complete. Frames: {frame_count}, Time: {elapsed:.2f}s, FPS: {frame_count / max(elapsed, 1e-6):.2f}\")\n    print(f\"Computed proxy mAP (mean of average detection confidences across classes): {final_map_proxy:.4f}\")\n    if per_class_ap_proxy:\n        # Print per-class proxy AP summary (limited to top 10 classes by AP)\n        sorted_items = sorted(per_class_ap_proxy.items(), key=lambda kv: kv[1], reverse=True)\n        print(\"Top classes by proxy AP:\")\n        for idx, (cid, ap) in enumerate(sorted_items[:10], 1):\n            print(f\"  {idx}. {get_label_name(cid)} (id {cid}): {ap:.4f}\")\n    else:\n        print(\"No detections above threshold; proxy mAP is 0.0\")\n\n    # Phase 5: Cleanup\n    cap.release()\n    out_writer.release()\n    # No UI windows to destroy\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_54c5_tpusg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 225.279,
      "total_cost": 0.086063,
      "observations": [
        {
          "id": "time-23-00-22-740017_chatcmpl-D5eH1e7zZsYtF6BAiK1ahvG4eNqib",
          "traceId": "74405acd",
          "type": "GENERATION",
          "name": "7440_tpusg_gen_attempt#1",
          "startTime": "2026-02-04T21:00:22.740000+00:00",
          "endTime": "2026-02-04T21:04:08.019000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.086063,
          "latency": 225279.0,
          "totalTokens": 10337,
          "costDetails": {
            "total": 0.086063
          },
          "completionTokens": 8573,
          "promptTokens": 1764,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:05:06.278Z",
      "createdAt": "2026-02-04T21:00:23.012Z",
      "environment": "default"
    },
    {
      "id": "a3973587",
      "timestamp": "2026-02-04T20:56:22.477000+00:00",
      "name": "a397_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport sys\nimport time\nimport numpy as np\nimport cv2\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.1 Imports: Interpreter and EdgeTPU delegate with fallback import paths\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\nexcept Exception:\n    try:\n        from tensorflow.lite import Interpreter  # type: ignore\n        from tensorflow.lite.experimental import load_delegate  # type: ignore\n    except Exception as e:\n        print(\"ERROR: Failed to import TFLite Interpreter. Please ensure tflite_runtime or tensorflow is installed.\")\n        print(f\"Details: {e}\")\n        sys.exit(1)\n\n# 1.2 Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# 1.3 Load Labels (if label path is provided and needed)\ndef load_labels(label_file_path):\n    labels = []\n    try:\n        with open(label_file_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                # Handle possible formats: either \"label\" or \"id label\"\n                # Given \"Useful Information\", labels are in plain text lines.\n                parts = line.split(maxsplit=1)\n                if len(parts) == 2 and parts[0].isdigit():\n                    labels.append(parts[1])\n                else:\n                    labels.append(line)\n    except Exception as e:\n        print(f\"WARNING: Failed to read labels from {label_file_path}. Details: {e}\")\n    return labels\n\nlabels = load_labels(label_path)\n\n# 1.4 Load Interpreter with EdgeTPU\ninterpreter = None\ndelegate_loaded = False\ndelegate_error_msgs = []\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\n    delegate_loaded = True\nexcept Exception as e1:\n    delegate_error_msgs.append(f\"Attempt 1 with 'libedgetpu.so.1.0' failed: {e1}\")\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n        delegate_loaded = True\n    except Exception as e2:\n        delegate_error_msgs.append(f\"Attempt 2 with '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0' failed: {e2}\")\n\nif not delegate_loaded or interpreter is None:\n    print(\"ERROR: Failed to load EdgeTPU delegate. Ensure the Coral EdgeTPU runtime is installed and accessible.\")\n    for msg in delegate_error_msgs:\n        print(\"-\", msg)\n    sys.exit(1)\n\ntry:\n    interpreter.allocate_tensors()\nexcept Exception as e:\n    print(f\"ERROR: Failed to allocate tensors for the interpreter. Details: {e}\")\n    sys.exit(1)\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nif len(input_details) < 1:\n    print(\"ERROR: Model has no input tensors.\")\n    sys.exit(1)\n\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # Expected [1, height, width, channels]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Helper: Identify outputs (boxes, classes, scores, count) by shape\ndef identify_detection_outputs(output_details_list):\n    idx_boxes = idx_classes = idx_scores = idx_count = None\n    for i, od in enumerate(output_details_list):\n        od_shape = od.get('shape', [])\n        od_dtype = od.get('dtype', None)\n        # Boxes: [1, N, 4]\n        if len(od_shape) == 3 and od_shape[-1] == 4:\n            idx_boxes = i\n        # Count: [1] or scalar\n        elif len(od_shape) == 1 and od_shape[0] == 1:\n            idx_count = i\n        # Classes and scores: [1, N]\n        elif len(od_shape) == 2 and od_shape[0] == 1:\n            # Heuristic: scores are float32, classes often float32 but representable as ints\n            if od_dtype == np.float32:\n                # If there are two float32 [1,N], prefer the one with name containing 'scores' if exists\n                tensor_name = od.get('name', '').lower()\n                if 'score' in tensor_name:\n                    idx_scores = i\n                elif 'class' in tensor_name:\n                    idx_classes = i\n                else:\n                    # Fallback: assign scores first, then classes\n                    if idx_scores is None:\n                        idx_scores = i\n                    else:\n                        idx_classes = i\n            else:\n                # Non-float for classes (rare): assume classes\n                idx_classes = i\n    # Final sanity fallback if names weren't helpful\n    # If both classes and scores are float and ambiguous, leave as assigned\n    return idx_boxes, idx_classes, idx_scores, idx_count\n\nout_idx_boxes, out_idx_classes, out_idx_scores, out_idx_count = identify_detection_outputs(output_details)\nif None in (out_idx_boxes, out_idx_classes, out_idx_scores, out_idx_count):\n    print(\"ERROR: Failed to identify detection output tensors (boxes, classes, scores, count).\")\n    sys.exit(1)\n\n# =========================\n# Utility Functions\n# =========================\n\ndef preprocess_frame_bgr(frame_bgr, expected_shape, dtype):\n    \"\"\"\n    Resize and normalize BGR frame to model input.\n    expected_shape: [1, height, width, channels]\n    Returns input_data ready to set into interpreter.\n    \"\"\"\n    _, in_h, in_w, in_c = expected_shape\n    # Convert BGR to RGB as most detection models expect RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n    input_data = np.expand_dims(resized, axis=0)\n    if dtype == np.float32:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = np.uint8(input_data)\n    return input_data\n\ndef clip_bbox(x1, y1, x2, y2, width, height):\n    x1 = max(0, min(int(x1), width - 1))\n    y1 = max(0, min(int(y1), height - 1))\n    x2 = max(0, min(int(x2), width - 1))\n    y2 = max(0, min(int(y2), height - 1))\n    return x1, y1, x2, y2\n\ndef get_label_name(class_id, labels_list):\n    if labels_list and 0 <= class_id < len(labels_list):\n        return labels_list[class_id]\n    return f\"Class {class_id}\"\n\ndef compute_iou(box_a, box_b):\n    \"\"\"\n    IoU between two boxes: each box is (x1, y1, x2, y2)\n    \"\"\"\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n    inter_w = max(0, inter_x2 - inter_x1 + 1)\n    inter_h = max(0, inter_y2 - inter_y1 + 1)\n    inter_area = inter_w * inter_h\n    area_a = max(0, ax2 - ax1 + 1) * max(0, ay2 - ay1 + 1)\n    area_b = max(0, bx2 - bx1 + 1) * max(0, by2 - by1 + 1)\n    union = area_a + area_b - inter_area\n    if union <= 0:\n        return 0.0\n    return inter_area / union\n\ndef voc_ap(rec, prec):\n    \"\"\"\n    Compute AP using PASCAL VOC method.\n    rec and prec are numpy arrays.\n    \"\"\"\n    mrec = np.concatenate(([0.0], rec, [1.0]))\n    mpre = np.concatenate(([0.0], prec, [0.0]))\n    for i in range(mpre.size - 1, 0, -1):\n        mpre[i - 1] = max(mpre[i - 1], mpre[i])\n    idx = np.where(mrec[1:] != mrec[:-1])[0]\n    ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n    return ap\n\ndef compute_map_approx_per_frame(detections, iou_threshold=0.5):\n    \"\"\"\n    Approximate mAP without ground truth by self-consistent clustering:\n    - For each class, cluster detections by IoU > threshold (greedy by score).\n    - Treat each cluster as a single 'object'.\n    - Walk detections sorted by score; the first detection hitting a cluster counts as TP, others as FP.\n    - Compute AP per class and average across classes that have at least one cluster.\n\n    detections: list of dicts with keys: 'bbox'=(x1,y1,x2,y2), 'score', 'class_id'\n    Returns (mAP, per_class_AP_dict)\n    \"\"\"\n    # Group detections by class_id\n    det_by_class = {}\n    for d in detections:\n        cid = d['class_id']\n        det_by_class.setdefault(cid, []).append(d)\n\n    ap_list = []\n    per_class_ap = {}\n\n    for cid, dets in det_by_class.items():\n        if len(dets) == 0:\n            continue\n        # Sort by score desc\n        dets_sorted = sorted(dets, key=lambda x: x['score'], reverse=True)\n        # Build clusters (unique object proxies) greedily\n        clusters = []  # each cluster is represented by the first (highest score) box\n        for d in dets_sorted:\n            assigned = False\n            for rep in clusters:\n                if compute_iou(d['bbox'], rep['bbox']) >= iou_threshold:\n                    assigned = True\n                    break\n            if not assigned:\n                clusters.append(d)\n        K = len(clusters)\n        if K == 0:\n            continue\n\n        # Assign each detection to the \"closest\" cluster (by IoU) if IoU >= threshold; else to a new 'no-cluster' (-1)\n        # Then compute TP/FP along the ranked list: first time a cluster is hit -> TP, else FP\n        cluster_hits = set()\n        tp = []\n        fp = []\n\n        # Precompute IoUs to speed up\n        for d in dets_sorted:\n            best_iou = 0.0\n            best_c = -1\n            for idx_c, rep in enumerate(clusters):\n                iou = compute_iou(d['bbox'], rep['bbox'])\n                if iou > best_iou:\n                    best_iou = iou\n                    best_c = idx_c\n            if best_iou >= iou_threshold and best_c >= 0:\n                if best_c not in cluster_hits:\n                    tp.append(1)\n                    fp.append(0)\n                    cluster_hits.add(best_c)\n                else:\n                    tp.append(0)\n                    fp.append(1)\n            else:\n                # Does not match any cluster (consider as FP)\n                tp.append(0)\n                fp.append(1)\n\n        tp_cum = np.cumsum(tp)\n        fp_cum = np.cumsum(fp)\n        rec = tp_cum / float(K)\n        prec = tp_cum / np.maximum(tp_cum + fp_cum, np.finfo(np.float32).eps)\n        ap = voc_ap(rec, prec)\n        ap_list.append(ap)\n        per_class_ap[cid] = ap\n\n    if len(ap_list) == 0:\n        return 0.0, per_class_ap\n    return float(np.mean(ap_list)), per_class_ap\n\ndef draw_detections_on_frame(frame, detections, labels_list, threshold):\n    \"\"\"\n    Draw bounding boxes and labels for detections with score >= threshold.\n    \"\"\"\n    h, w = frame.shape[:2]\n    for det in detections:\n        score = det['score']\n        if score < threshold:\n            continue\n        x1, y1, x2, y2 = det['bbox']\n        cid = det['class_id']\n        label = get_label_name(cid, labels_list)\n        color = (0, 255, 0)  # Green boxes\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        caption = f\"{label}: {score:.2f}\"\n        # Text background for readability\n        (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - baseline), (x1 + tw, y1), color, -1)\n        cv2.putText(frame, caption, (x1, y1 - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# 2.1 Acquire Input Data: open video file\nif not os.path.exists(input_path):\n    print(f\"ERROR: Input video file not found at {input_path}\")\n    sys.exit(1)\n\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"ERROR: Failed to open input video file: {input_path}\")\n    sys.exit(1)\n\n# Prepare output video writer\norig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\norig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps is None or fps <= 0 or np.isnan(fps):\n    fps = 30.0  # default fallback\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nwriter = cv2.VideoWriter(output_path, fourcc, fps, (orig_width, orig_height))\nif not writer.isOpened():\n    print(f\"ERROR: Failed to open output video for writing: {output_path}\")\n    cap.release()\n    sys.exit(1)\n\n# 2.2 Preprocess Data will be done per-frame using preprocess_frame_bgr\n# 2.3 Quantization Handling via floating_model flag inside preprocessing\n# 2.4 Loop Control: process all frames in the single video\n\n# Performance tracking\nframe_count = 0\ninference_times_ms = []\nmap_values = []\n\n# =========================\n# Processing Loop\n# =========================\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break\n    frame_count += 1\n    frame_h, frame_w = frame_bgr.shape[:2]\n\n    # Preprocess frame\n    input_data = preprocess_frame_bgr(frame_bgr, input_shape, input_dtype)\n\n    # =========================\n    # Phase 3: Inference\n    # =========================\n    interpreter.set_tensor(input_index, input_data)\n    t0 = time.time()\n    interpreter.invoke()\n    t1 = time.time()\n    infer_time_ms = (t1 - t0) * 1000.0\n    inference_times_ms.append(infer_time_ms)\n\n    # =========================\n    # Phase 4: Output Interpretation & Handling\n    # =========================\n\n    # 4.1 Get Output Tensor(s)\n    boxes = interpreter.get_tensor(output_details[out_idx_boxes]['index'])\n    classes = interpreter.get_tensor(output_details[out_idx_classes]['index'])\n    scores = interpreter.get_tensor(output_details[out_idx_scores]['index'])\n    count = interpreter.get_tensor(output_details[out_idx_count]['index'])\n\n    # Flatten outputs (expected shapes: boxes [1,N,4], classes [1,N], scores [1,N], count [1])\n    if boxes.ndim == 3:\n        boxes = boxes[0]\n    if classes.ndim == 2:\n        classes = classes[0]\n    if scores.ndim == 2:\n        scores = scores[0]\n    if np.ndim(count) > 0:\n        num = int(count.flatten()[0])\n    else:\n        num = int(count)\n\n    # 4.2 Interpret Results\n    # Convert normalized box coordinates to absolute pixel coordinates and assemble detections\n    detections = []\n    for i in range(num):\n        score = float(scores[i])\n        cid = int(classes[i])\n        y_min, x_min, y_max, x_max = boxes[i]  # typically normalized [0,1]\n        # 4.3 Post-processing: apply confidence thresholding, coordinate scaling, clipping\n        # We will build all detections first; filtering for drawing uses confidence_threshold\n        x1 = int(x_min * frame_w)\n        y1 = int(y_min * frame_h)\n        x2 = int(x_max * frame_w)\n        y2 = int(y_max * frame_h)\n        # Ensure x1<=x2, y1<=y2\n        x1, y1, x2, y2 = min(x1, x2), min(y1, y2), max(x1, x2), max(y1, y2)\n        x1, y1, x2, y2 = clip_bbox(x1, y1, x2, y2, frame_w, frame_h)\n        det = {\n            'bbox': (x1, y1, x2, y2),\n            'score': score,\n            'class_id': cid\n        }\n        detections.append(det)\n\n    # 4.3 Post-processing continued: compute approximate mAP using all detections with a self-consistent clustering approach\n    # This provides an approximate metric in absence of ground-truth annotations.\n    frame_map, per_class_ap = compute_map_approx_per_frame(detections, iou_threshold=0.5)\n    map_values.append(frame_map)\n\n    # 4.4 Handle Output: draw boxes and overlay info; write to output video\n    annotated = frame_bgr.copy()\n    draw_detections_on_frame(annotated, detections, labels, confidence_threshold)\n\n    avg_map = float(np.mean(map_values)) if len(map_values) > 0 else 0.0\n    avg_infer_ms = float(np.mean(inference_times_ms)) if len(inference_times_ms) > 0 else 0.0\n    # Overlay metrics\n    info_lines = [\n        f\"mAP (approx, this frame): {frame_map:.3f}\",\n        f\"mAP (approx, avg): {avg_map:.3f}\",\n        f\"Inference: {infer_time_ms:.1f} ms (avg {avg_infer_ms:.1f} ms)\",\n        f\"Detections (>= {confidence_threshold:.2f}): {sum(1 for d in detections if d['score'] >= confidence_threshold)}\"\n    ]\n    y0 = 25\n    for idx, text in enumerate(info_lines):\n        y = y0 + idx * 20\n        cv2.putText(annotated, text, (10, y), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (50, 255, 50), 2, cv2.LINE_AA)\n\n    writer.write(annotated)\n\n# =========================\n# Phase 5: Cleanup\n# =========================\ncap.release()\nwriter.release()\n\n# Final report\nif frame_count > 0:\n    final_avg_map = float(np.mean(map_values)) if len(map_values) > 0 else 0.0\n    final_avg_infer_ms = float(np.mean(inference_times_ms)) if len(inference_times_ms) > 0 else 0.0\n    print(\"Processing complete.\")\n    print(f\"Input video: {input_path}\")\n    print(f\"Output video: {output_path}\")\n    print(f\"Frames processed: {frame_count}\")\n    print(f\"Average inference time: {final_avg_infer_ms:.2f} ms\")\n    print(f\"Approximate mAP over video: {final_avg_map:.3f}\")\nelse:\n    print(\"No frames processed from the input video.\")"
      },
      "session_id": "gpt-5-2025-08-07_54c5_tpusg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 174.358,
      "total_cost": 0.094753,
      "observations": [
        {
          "id": "time-22-56-22-900146_chatcmpl-D5eD9fFTa8poOiWm66QMEK2bpHLC8",
          "traceId": "a3973587",
          "type": "GENERATION",
          "name": "a397_tpusg_gen_attempt#1",
          "startTime": "2026-02-04T20:56:22.900000+00:00",
          "endTime": "2026-02-04T20:59:17.258000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.094753,
          "latency": 174358.0,
          "totalTokens": 11206,
          "costDetails": {
            "total": 0.094753
          },
          "completionTokens": 9442,
          "promptTokens": 1764,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T21:00:17.212Z",
      "createdAt": "2026-02-04T20:56:22.997Z",
      "environment": "default"
    },
    {
      "id": "57182ac0",
      "timestamp": "2026-02-04T20:52:27.709000+00:00",
      "name": "5718_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport sys\nimport time\nimport numpy as np\n\n# Phase 1: Setup\n# 1.1 Imports: Interpreter and EdgeTPU delegate with fallback\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\nexcept Exception:\n    try:\n        from tensorflow.lite import Interpreter  # type: ignore\n        from tensorflow.lite.experimental import load_delegate  # type: ignore\n    except Exception as e:\n        print(\"ERROR: Unable to import TFLite Interpreter. Ensure tflite_runtime or tensorflow is installed.\")\n        print(f\"Import error detail: {e}\")\n        sys.exit(1)\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\ntry:\n    CONF_THRESHOLD = float(confidence_threshold)\nexcept Exception:\n    CONF_THRESHOLD = 0.5\n\n# Only import cv2 when image/video processing is required\nimport cv2  # OpenCV is required for video I/O and drawing\n\ndef load_labels(path):\n    labels = []\n    try:\n        with open(path, 'r') as f:\n            for line in f:\n                label = line.strip()\n                if label:\n                    labels.append(label)\n    except Exception as e:\n        print(f\"WARNING: Failed to load labels from {path}. Proceeding without labels. Detail: {e}\")\n    return labels\n\ndef make_interpreter_with_edgetpu(model_path_str):\n    # 1.4 Load Interpreter with EdgeTPU and proper error handling\n    last_error = None\n    for lib in ('libedgetpu.so.1.0', '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0'):\n        try:\n            delegate = load_delegate(lib)\n            interpreter = Interpreter(model_path=model_path_str, experimental_delegates=[delegate])\n            interpreter.allocate_tensors()\n            print(f\"INFO: EdgeTPU delegate loaded using: {lib}\")\n            return interpreter\n        except Exception as e:\n            last_error = e\n            continue\n    print(\"ERROR: Failed to load EdgeTPU delegate. Please ensure the EdgeTPU runtime is installed and the device is connected.\")\n    print(f\"Delegate loading detail: {last_error}\")\n    sys.exit(1)\n\ndef get_model_io_details(interpreter):\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    if not input_details or not output_details:\n        print(\"ERROR: Failed to retrieve model IO details.\")\n        sys.exit(1)\n    return input_details, output_details\n\ndef preprocess_frame(frame_bgr, input_details):\n    # 2.2 Preprocess Data to match model input\n    # Convert BGR to RGB and resize to model input size\n    _, in_h, in_w, _ = input_details[0]['shape']\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n\n    # Add batch dimension\n    input_data = np.expand_dims(resized, axis=0)\n\n    # 2.3 Quantization Handling\n    input_dtype = input_details[0]['dtype']\n    if input_dtype == np.float32:\n        # Normalize to [-1, 1]\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # For quantized models (commonly uint8 on EdgeTPU), pass raw uint8\n        input_data = np.asarray(input_data, dtype=input_dtype)\n    return input_data\n\ndef interpret_detection_outputs(output_tensors, frame_w, frame_h, labels, threshold):\n    # 4.2 Interpret Results and 4.3 Post-processing (confidence thresholding, scaling, clipping)\n    # Typical TFLite detection postprocess output:\n    # boxes: [1, N, 4], classes: [1, N], scores: [1, N], count: [1]\n    boxes, classes, scores, count = None, None, None, None\n\n    # Identify tensors by shape/value characteristics\n    candidates = output_tensors[:]\n    # boxes detection\n    for t in candidates:\n        if isinstance(t, np.ndarray) and t.ndim == 3 and t.shape[0] == 1 and t.shape[-1] == 4:\n            boxes = t\n            break\n    # scores and classes detection\n    remaining = [t for t in candidates if t is not boxes]\n    score_candidate, class_candidate = None, None\n    for t in remaining:\n        if isinstance(t, np.ndarray) and t.ndim == 2 and t.shape[0] == 1:\n            # Heuristic: scores are in [0,1]\n            if np.all((t >= 0.0) & (t <= 1.0)):\n                score_candidate = t if score_candidate is None else score_candidate\n            else:\n                class_candidate = t if class_candidate is None else class_candidate\n    scores = score_candidate\n    classes = class_candidate\n\n    # count detection (shape [1] or [1,1])\n    for t in candidates:\n        if isinstance(t, np.ndarray) and t.size == 1 and (t.ndim == 1 or t.ndim == 2):\n            # Likely num_detections\n            count = int(np.round(float(t.reshape(-1)[0])))\n            break\n    if count is None and scores is not None:\n        count = scores.shape[1]\n    if boxes is None or scores is None or classes is None or count is None:\n        raise RuntimeError(\"Unexpected detection model outputs. Cannot parse tensors.\")\n\n    detections = []\n    n = min(count, boxes.shape[1], scores.shape[1], classes.shape[1])\n    for i in range(n):\n        score = float(scores[0, i])\n        if score < threshold:\n            continue\n        ymin, xmin, ymax, xmax = boxes[0, i]\n        # Clip coordinates to [0,1]\n        ymin = max(0.0, min(1.0, float(ymin)))\n        xmin = max(0.0, min(1.0, float(xmin)))\n        ymax = max(0.0, min(1.0, float(ymax)))\n        xmax = max(0.0, min(1.0, float(xmax)))\n        # Scale to frame size\n        x0 = int(xmin * frame_w)\n        y0 = int(ymin * frame_h)\n        x1 = int(xmax * frame_w)\n        y1 = int(ymax * frame_h)\n        # Clip to frame boundaries\n        x0 = max(0, min(frame_w - 1, x0))\n        y0 = max(0, min(frame_h - 1, y0))\n        x1 = max(0, min(frame_w - 1, x1))\n        y1 = max(0, min(frame_h - 1, y1))\n        class_id = int(classes[0, i])\n        label = labels[class_id] if (labels and 0 <= class_id < len(labels)) else f\"class_{class_id}\"\n        detections.append({\n            'bbox': (x0, y0, x1, y1),\n            'score': score,\n            'class_id': class_id,\n            'label': label\n        })\n    return detections\n\ndef draw_detections(frame, detections, fps_text=None, map_text=None):\n    for det in detections:\n        x0, y0, x1, y1 = det['bbox']\n        label = det['label']\n        score = det['score']\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (x0, y0), (x1, y1), color, 2)\n        caption = f\"{label}: {score:.2f}\"\n        # Text background for readability\n        (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x0, y0 - th - baseline), (x0 + tw, y0), color, -1)\n        cv2.putText(frame, caption, (x0, y0 - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n    # Overlay FPS and mAP text\n    y_offset = 20\n    if fps_text:\n        cv2.putText(frame, fps_text, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50, 200, 255), 2, cv2.LINE_AA)\n        y_offset += 25\n    if map_text:\n        cv2.putText(frame, map_text, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2, cv2.LINE_AA)\n    return frame\n\ndef ensure_output_path(path):\n    out_dir = os.path.dirname(path)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\ndef main():\n    # Phase 1 continued\n    labels = load_labels(label_path)\n\n    interpreter = make_interpreter_with_edgetpu(model_path)\n    input_details, output_details = get_model_io_details(interpreter)\n\n    # Extract input tensor info\n    in_shape = input_details[0]['shape']\n    in_dtype = input_details[0]['dtype']\n    if len(in_shape) != 4 or in_shape[0] != 1 or in_shape[3] != 3:\n        print(f\"ERROR: Unexpected input shape {in_shape}. Expected [1, H, W, 3].\")\n        sys.exit(1)\n\n    # Phase 2: Input Acquisition & Preprocessing Loop\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"ERROR: Unable to open input video: {input_path}\")\n        sys.exit(1)\n\n    # Prepare output writer\n    ensure_output_path(output_path)\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # default fallback\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out_writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n    if not out_writer.isOpened():\n        print(f\"ERROR: Unable to open output video for writing: {output_path}\")\n        cap.release()\n        sys.exit(1)\n\n    # Placeholder for mAP: No ground truth provided; will annotate as N/A\n    computed_map_text = \"mAP: N/A (requires ground truth)\"\n\n    print(\"INFO: Starting inference on video...\")\n    frame_index = 0\n    inference_times = []\n\n    try:\n        while True:\n            ret, frame_bgr = cap.read()\n            if not ret:\n                break  # 2.4 Loop Control: single video; stop at end\n\n            # 2.2 Preprocess data\n            input_data = preprocess_frame(frame_bgr, input_details)\n\n            # Phase 3: Inference\n            # 3.1 Set input tensor\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n            # 3.2 Run Inference\n            t0 = time.time()\n            interpreter.invoke()\n            t1 = time.time()\n            infer_ms = (t1 - t0) * 1000.0\n            inference_times.append(infer_ms)\n\n            # Phase 4: Output Interpretation & Handling\n            # 4.1 Get Output Tensor(s)\n            output_tensors = [interpreter.get_tensor(od['index']) for od in output_details]\n\n            # 4.2 and 4.3 Interpret and Post-process detections\n            try:\n                detections = interpret_detection_outputs(output_tensors, frame_w, frame_h, labels, CONF_THRESHOLD)\n            except Exception as e:\n                print(f\"ERROR: Failed to interpret detection outputs at frame {frame_index}. Detail: {e}\")\n                detections = []\n\n            # Prepare FPS text\n            avg_infer_ms = infer_ms\n            if len(inference_times) >= 5:\n                # Smoothing average over last 5 frames\n                avg_infer_ms = float(np.mean(inference_times[-5:]))\n            fps_text = f\"Inference: {avg_infer_ms:.1f} ms ({1000.0/avg_infer_ms:.1f} FPS)\" if avg_infer_ms > 0 else \"Inference: N/A\"\n\n            # 4.4 Handle Output: draw and write to file\n            annotated = draw_detections(frame_bgr.copy(), detections, fps_text=fps_text, map_text=computed_map_text)\n            out_writer.write(annotated)\n\n            frame_index += 1\n\n        print(\"INFO: Inference completed.\")\n        if inference_times:\n            print(f\"Average inference time: {float(np.mean(inference_times)):.2f} ms per frame\")\n        print(\"NOTE: mAP was not computed because no ground truth annotations were provided.\")\n        print(f\"Output saved to: {output_path}\")\n\n    finally:\n        # Phase 5: Cleanup\n        cap.release()\n        out_writer.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_54c5_tpusg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 170.702,
      "total_cost": 0.076863,
      "observations": [
        {
          "id": "time-22-52-28-143578_chatcmpl-D5e9MJkxvF22vDjcawcOUiVg0z5K0",
          "traceId": "57182ac0",
          "type": "GENERATION",
          "name": "5718_tpusg_gen_attempt#1",
          "startTime": "2026-02-04T20:52:28.143000+00:00",
          "endTime": "2026-02-04T20:55:18.845000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.076863,
          "latency": 170702.0,
          "totalTokens": 9417,
          "costDetails": {
            "total": 0.076863
          },
          "completionTokens": 7653,
          "promptTokens": 1764,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T20:56:17.422Z",
      "createdAt": "2026-02-04T20:52:28.238Z",
      "environment": "default"
    },
    {
      "id": "6aca9a6a",
      "timestamp": "2026-02-04T20:48:40.620000+00:00",
      "name": "6aca_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport sys\nimport time\nimport numpy as np\n\n# Phase 1: Setup\n\n# 1.1 Imports with fallback\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\nexcept Exception:\n    try:\n        from tensorflow.lite import Interpreter\n        from tensorflow.lite.experimental import load_delegate\n    except Exception as e:\n        print(\"Error: Unable to import TFLite Interpreter. Ensure either 'tflite_runtime' or 'tensorflow' is installed.\")\n        sys.exit(1)\n\n# Only import cv2 because video/image processing is explicitly required\ntry:\n    import cv2\nexcept Exception as e:\n    print(\"Error: OpenCV (cv2) is required for video I/O. Please install it.\")\n    sys.exit(1)\n\n# 1.2 Paths/Parameters\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\n\n# Utility: deterministic color per class id\ndef class_color(class_id: int):\n    np.random.seed(class_id + 12345)\n    color = tuple(int(x) for x in np.random.randint(0, 255, size=3))\n    return color\n\n# 1.3 Load Labels\ndef load_labels(label_file_path):\n    labels = []\n    try:\n        with open(label_file_path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    except Exception as e:\n        print(f\"Error: Unable to read label file at {label_file_path}: {e}\")\n        sys.exit(1)\n    return labels\n\nlabels = load_labels(label_path)\n\n# 1.4 Load Interpreter with EdgeTPU\ndef make_interpreter_with_edgetpu(tflite_model_path):\n    last_err = None\n    try:\n        interpreter = Interpreter(\n            model_path=tflite_model_path,\n            experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n        )\n        return interpreter\n    except Exception as e:\n        last_err = e\n        # Try specific path (common on aarch64)\n        try:\n            interpreter = Interpreter(\n                model_path=tflite_model_path,\n                experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n            )\n            return interpreter\n        except Exception as e2:\n            last_err = e2\n    print(\"Error: Failed to load EdgeTPU delegate. Ensure the Coral EdgeTPU runtime is installed and \"\n          \"the device is connected. Details:\", last_err)\n    sys.exit(1)\n\ninterpreter = make_interpreter_with_edgetpu(model_path)\ntry:\n    interpreter.allocate_tensors()\nexcept Exception as e:\n    print(\"Error: Failed to allocate tensors for the TFLite interpreter:\", e)\n    sys.exit(1)\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nif len(input_details) < 1:\n    print(\"Error: Model does not have any input tensors.\")\n    sys.exit(1)\n\ninput_shape = input_details[0]['shape']  # Expected shape: [1, height, width, 3]\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\n# 2.1 Acquire Input Data - open the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"Error: Unable to open input video file: {input_path}\")\n    sys.exit(1)\n\n# Prepare output writer with the same dimensions as the input video frames\nin_fps = cap.get(cv2.CAP_PROP_FPS)\nif in_fps is None or in_fps <= 0 or np.isnan(in_fps):\n    in_fps = 30.0  # fallback\n\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nout_writer = cv2.VideoWriter(output_path, fourcc, in_fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    print(f\"Error: Unable to open output video writer at: {output_path}\")\n    cap.release()\n    sys.exit(1)\n\n# 2.2 + 2.3 Preprocess frames into model input\ndef preprocess_frame_bgr_to_model_input(bgr_frame, input_shape, floating_model):\n    # Model expects [1, h, w, c]\n    _, in_h, in_w, in_c = input_shape\n    # Convert BGR to RGB\n    rgb = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (in_w, in_h))\n    input_data = np.expand_dims(resized, axis=0)\n\n    if floating_model:\n        # Normalize to [-1, 1] as per common MobileNet SSD preprocessing\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = np.uint8(input_data)\n    return input_data\n\n# Helper: parse TFLite detection outputs robustly\ndef parse_detection_outputs(interpreter, output_details):\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n    boxes = classes = scores = count = None\n    # First try common order: [boxes, classes, scores, count]\n    try:\n        boxes = outputs[0][0]\n        classes = outputs[1][0]\n        scores = outputs[2][0]\n        count = int(outputs[3][0]) if outputs[3].size > 0 else len(scores)\n        return boxes, classes, scores, count\n    except Exception:\n        pass\n    # Fallback by shape heuristics\n    for out in outputs:\n        shp = out.shape\n        try:\n            if len(shp) == 3 and shp[-1] == 4:\n                boxes = out[0]\n            elif len(shp) == 2:\n                arr = out[0]\n                # Scores typically float in [0,1]\n                if arr.dtype == np.float32 and np.all(arr >= 0) and np.all(arr <= 1.0):\n                    scores = arr\n                else:\n                    classes = arr.astype(np.int32) if arr.dtype != np.int32 else arr\n            elif len(shp) == 1 and out.size >= 1:\n                # count tensor\n                count = int(out[0])\n        except Exception:\n            continue\n    if count is None and scores is not None:\n        count = len(scores)\n    if boxes is None or classes is None or scores is None or count is None:\n        raise RuntimeError(\"Unable to parse detection outputs from the model.\")\n    return boxes, classes, scores, count\n\n# 2.4 Loop control variables\ntotal_detections = 0\nsum_confidences = 0.0  # For a proxy mAP measure (mean confidence across detections)\nframe_index = 0\n\n# Phase 2.4 + 3 + 4 Loop over video frames\nwhile True:\n    ret, frame_bgr = cap.read()\n    if not ret:\n        break  # End of video\n\n    frame_index += 1\n    orig_h, orig_w = frame_bgr.shape[:2]\n\n    # Preprocess to model input\n    input_tensor = preprocess_frame_bgr_to_model_input(frame_bgr, input_shape, floating_model)\n\n    # Phase 3: Inference\n    interpreter.set_tensor(input_details[0]['index'], input_tensor)\n    interpreter.invoke()\n\n    # Phase 4: Output Interpretation & Handling\n    # 4.1 Get output tensors\n    try:\n        boxes, classes, scores, count = parse_detection_outputs(interpreter, output_details)\n    except Exception as e:\n        print(f\"Warning: Failed to parse model outputs at frame {frame_index}: {e}\")\n        boxes, classes, scores, count = None, None, None, 0\n\n    # 4.2 Interpret Results\n    detections_to_draw = []\n    if count and boxes is not None and classes is not None and scores is not None:\n        for i in range(int(count)):\n            score = float(scores[i])\n            class_id = int(classes[i]) if i < len(classes) else -1\n            label_name = labels[class_id] if 0 <= class_id < len(labels) else f\"class_{class_id}\"\n            # 4.3 Post-processing: thresholding and clipping\n            if score >= confidence_threshold:\n                # boxes are [ymin, xmin, ymax, xmax] normalized [0,1]\n                y_min, x_min, y_max, x_max = boxes[i]\n                y_min = max(0.0, min(1.0, float(y_min)))\n                x_min = max(0.0, min(1.0, float(x_min)))\n                y_max = max(0.0, min(1.0, float(y_max)))\n                x_max = max(0.0, min(1.0, float(x_max)))\n\n                # Scale to image coordinates\n                x1 = int(x_min * orig_w)\n                y1 = int(y_min * orig_h)\n                x2 = int(x_max * orig_w)\n                y2 = int(y_max * orig_h)\n\n                # Ensure proper box\n                x1, x2 = max(0, min(x1, orig_w - 1)), max(0, min(x2, orig_w - 1))\n                y1, y2 = max(0, min(y1, orig_h - 1)), max(0, min(y2, orig_h - 1))\n\n                detections_to_draw.append((x1, y1, x2, y2, class_id, label_name, score))\n                total_detections += 1\n                sum_confidences += score\n\n    # Compute a running \"mAP\" proxy: mean confidence over all detections so far\n    # Note: True mAP requires ground truth annotations. Here we provide a proxy metric due to lack of ground truth.\n    map_proxy = (sum_confidences / total_detections) if total_detections > 0 else 0.0\n\n    # 4.4 Handle Output: draw detections and write frame\n    # Draw detection rectangles and labels\n    for (x1, y1, x2, y2, cid, lname, score) in detections_to_draw:\n        color = class_color(cid)\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        label_text = f\"{lname}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        # draw filled rectangle for label background\n        cv2.rectangle(frame_bgr, (x1, max(0, y1 - th - baseline - 2)), (x1 + tw + 2, y1), color, thickness=-1)\n        cv2.putText(frame_bgr, label_text, (x1 + 1, y1 - baseline - 1), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n\n    # Overlay mAP proxy on frame\n    map_text = f\"mAP (proxy): {map_proxy:.3f}\"\n    cv2.putText(frame_bgr, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (50, 255, 50), 2)\n\n    # Write to output video\n    out_writer.write(frame_bgr)\n\n# Phase 5: Cleanup\ncap.release()\nout_writer.release()\n\n# Print final proxy mAP to console\nfinal_map_proxy = (sum_confidences / total_detections) if total_detections > 0 else 0.0\nprint(f\"Processing complete. Output saved at: {output_path}\")\nprint(f\"Detections: {total_detections}, mAP (proxy): {final_map_proxy:.4f}\")"
      },
      "session_id": "gpt-5-2025-08-07_54c5_tpusg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 162.842,
      "total_cost": 0.072195,
      "observations": [
        {
          "id": "time-22-48-41-056374_chatcmpl-D5e5hF65rhEenpfcirA4IWHztLmH4",
          "traceId": "6aca9a6a",
          "type": "GENERATION",
          "name": "6aca_tpusg_gen_attempt#1",
          "startTime": "2026-02-04T20:48:41.056000+00:00",
          "endTime": "2026-02-04T20:51:23.898000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.072195,
          "latency": 162842.0,
          "totalTokens": 8763,
          "costDetails": {
            "total": 0.072195
          },
          "completionTokens": 6999,
          "promptTokens": 1764,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T20:52:21.858Z",
      "createdAt": "2026-02-04T20:48:41.149Z",
      "environment": "default"
    },
    {
      "id": "d540443c",
      "timestamp": "2026-02-04T20:44:10.433000+00:00",
      "name": "d540_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# =========================\n# Phase 1: Setup\n# =========================\n\n# 1.1 Imports: TFLite Interpreter and EdgeTPU delegate with fallback imports\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\n    tflite_source = \"tflite_runtime\"\nexcept ImportError:\n    try:\n        from tensorflow.lite import Interpreter\n        from tensorflow.lite.experimental import load_delegate\n        tflite_source = \"tensorflow.lite\"\n    except Exception as e:\n        print(\"ERROR: Unable to import TFLite Interpreter. Ensure tflite_runtime or TensorFlow Lite is installed.\")\n        raise\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold  = 0.5\npseudo_gt_high_conf_threshold = 0.75  # threshold to create pseudo ground-truth from high-confidence detections\niou_threshold = 0.5  # IoU threshold for a correct match in mAP computation\n\n# 1.3 Load Labels (if provided and needed)\ndef load_labels(path):\n    labels = []\n    try:\n        with open(path, 'r') as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                # If labels may contain \"id label\" format, split and take the last token(s)\n                # Given snippet shows plain names per line; we keep simple\n                labels.append(line)\n    except Exception as e:\n        print(f\"WARNING: Failed to load labels from {path}. Error: {e}\")\n        labels = []\n    return labels\n\nlabels = load_labels(label_path)\n\n# 1.4 Load Interpreter with EdgeTPU delegate\ndef make_interpreter_with_edgetpu(model_path_local):\n    last_error = None\n    for libpath in ['libedgetpu.so.1.0', '/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0']:\n        try:\n            interpreter_local = Interpreter(\n                model_path=model_path_local,\n                experimental_delegates=[load_delegate(libpath)]\n            )\n            return interpreter_local\n        except Exception as e:\n            last_error = e\n    raise RuntimeError(f\"ERROR: Failed to load EdgeTPU delegate. Last error: {last_error}. \"\n                       f\"Ensure the EdgeTPU runtime is installed and the Coral is connected.\")\n\n# Check files\nif not os.path.exists(model_path):\n    raise FileNotFoundError(f\"Model file not found: {model_path}\")\nif not os.path.exists(input_path):\n    raise FileNotFoundError(f\"Input video file not found: {input_path}\")\nif not os.path.exists(os.path.dirname(output_path)):\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n# Create interpreter\ntry:\n    interpreter = make_interpreter_with_edgetpu(model_path)\nexcept Exception as e:\n    print(str(e))\n    raise SystemExit(1)\n\n# Allocate tensors\ntry:\n    interpreter.allocate_tensors()\nexcept Exception as e:\n    print(f\"ERROR: Failed to allocate tensors: {e}\")\n    raise SystemExit(1)\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\n# Determine input shape and dtype\ninput_index = input_details[0]['index']\ninput_shape = input_details[0]['shape']  # typically [1, height, width, 3]\ninput_height, input_width = int(input_shape[1]), int(input_shape[2])\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Helper: color per class for drawing\ndef color_for_class(cid):\n    np.random.seed(cid + 7)\n    color = np.random.randint(0, 255, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\n# =========================\n# Phase 2: Input Acquisition & Preprocessing Loop\n# =========================\n\n# 2.1 Acquire Input Data: open the video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"ERROR: Cannot open input video: {input_path}\")\n    raise SystemExit(1)\n\n# Determine video properties for output writer\nfps = cap.get(cv2.CAP_PROP_FPS)\nif fps <= 1e-3:\n    fps = 30.0  # fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Prepare VideoWriter for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout_writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\nif not out_writer.isOpened():\n    print(f\"ERROR: Cannot open output video for writing: {output_path}\")\n    cap.release()\n    raise SystemExit(1)\n\ndef preprocess_frame(frame_bgr):\n    # 2.2 Preprocess Data: resize and normalize as needed based on input_details\n    frame_resized = cv2.resize(frame_bgr, (input_width, input_height))\n    # Convert BGR to RGB as TFLite models typically expect RGB\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n    input_data = np.expand_dims(frame_rgb, axis=0)\n    if floating_model:\n        # 2.3 Quantization Handling: normalize to [-1, 1] for floating models\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        # For quantized models (uint8), pass as-is\n        if input_dtype == np.uint8:\n            input_data = np.uint8(input_data)\n        else:\n            # Fallback: cast to required dtype without normalization\n            input_data = input_data.astype(input_dtype)\n    return input_data\n\n# =========================\n# Utilities for Detection and mAP\n# =========================\n\ndef interpret_outputs(interpreter_obj, output_details_list):\n    # 4.1 Get Output Tensor(s)\n    outputs = [interpreter_obj.get_tensor(od['index']) for od in output_details_list]\n    # Attempt to map outputs by common SSD order: [boxes, classes, scores, count]\n    # Many EdgeTPU detection models follow this order.\n    if len(outputs) >= 4:\n        boxes = outputs[0]\n        classes = outputs[1]\n        scores = outputs[2]\n        count = outputs[3]\n    else:\n        # If unusual, best-effort mapping by shapes\n        boxes, classes, scores, count = None, None, None, None\n        for out, det in zip(outputs, output_details_list):\n            shp = det['shape']\n            if len(shp) == 3 and shp[-1] == 4:\n                boxes = out\n            elif len(shp) == 2 and shp[1] > 1 and det['dtype'] in [np.float32, np.int64, np.int32, np.uint8]:\n                # Could be classes or scores; we check dtype and value ranges later\n                if classes is None:\n                    classes = out\n                else:\n                    scores = out\n            elif len(shp) == 1 and shp[0] == 1:\n                count = out\n        if boxes is None or classes is None or scores is None or count is None:\n            raise RuntimeError(\"Unexpected model output format; cannot interpret detection outputs.\")\n    return boxes, classes, scores, count\n\ndef clip_bbox(x1, y1, x2, y2, w, h):\n    x1 = max(0, min(w - 1, x1))\n    y1 = max(0, min(h - 1, y1))\n    x2 = max(0, min(w - 1, x2))\n    y2 = max(0, min(h - 1, y2))\n    return x1, y1, x2, y2\n\ndef iou(boxA, boxB):\n    # boxes: [x1, y1, x2, y2]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    interW = max(0, xB - xA + 1)\n    interH = max(0, yB - yA + 1)\n    interArea = interW * interH\n    if interArea <= 0:\n        return 0.0\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    return interArea / float(boxAArea + boxBArea - interArea + 1e-10)\n\ndef compute_map(predictions, ground_truths, num_classes, iou_thr=0.5):\n    # predictions: list of dicts {class_id, score, bbox:[x1,y1,x2,y2], frame_id}\n    # ground_truths: list of dicts {class_id, bbox:[x1,y1,x2,y2], frame_id}\n    # Returns: mAP (float or None if no GT)\n    # Prepare GT structures\n    gts_by_class_frame = {}\n    for gt in ground_truths:\n        cid = gt['class_id']\n        fid = gt['frame_id']\n        gts_by_class_frame.setdefault(cid, {}).setdefault(fid, []).append(gt['bbox'])\n    # For each class, compute AP\n    ap_list = []\n    for cid in range(num_classes):\n        preds_c = [p for p in predictions if p['class_id'] == cid]\n        if cid not in gts_by_class_frame:\n            continue\n        total_gt = sum(len(bxs) for bxs in gts_by_class_frame[cid].values())\n        if total_gt == 0:\n            continue\n        # sort predictions by confidence descending\n        preds_c_sorted = sorted(preds_c, key=lambda d: d['score'], reverse=True)\n        # matched flags for GT\n        matched_flags = {fid: [False] * len(gts_by_class_frame[cid][fid]) for fid in gts_by_class_frame[cid]}\n        tp = np.zeros(len(preds_c_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_c_sorted), dtype=np.float32)\n        for i, p in enumerate(preds_c_sorted):\n            fid = p['frame_id']\n            bbox_p = p['bbox']\n            best_iou = 0.0\n            best_gt_idx = -1\n            if fid in gts_by_class_frame[cid]:\n                gt_boxes = gts_by_class_frame[cid][fid]\n                for j, gt_box in enumerate(gt_boxes):\n                    if matched_flags[fid][j]:\n                        continue\n                    iou_val = iou(bbox_p, gt_box)\n                    if iou_val > best_iou:\n                        best_iou = iou_val\n                        best_gt_idx = j\n            if best_iou >= iou_thr and best_gt_idx >= 0:\n                tp[i] = 1.0\n                matched_flags[fid][best_gt_idx] = True\n            else:\n                fp[i] = 1.0\n        if len(tp) == 0:\n            continue\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-9)\n        recall = cum_tp / float(total_gt)\n        # 11-point interpolated AP\n        ap = 0.0\n        for t in np.linspace(0, 1, 11):\n            if np.any(recall >= t):\n                p = np.max(precision[recall >= t])\n            else:\n                p = 0.0\n            ap += p / 11.0\n        ap_list.append(ap)\n    if len(ap_list) == 0:\n        return None\n    return float(np.mean(ap_list))\n\ndef draw_detections(frame, detections_to_draw, labels_list, map_value):\n    # Draw detections and mAP value on frame\n    for det in detections_to_draw:\n        x1, y1, x2, y2 = det['bbox']\n        cid = det['class_id']\n        score = det['score']\n        color = color_for_class(cid)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        if 0 <= cid < len(labels_list):\n            label_name = labels_list[cid]\n        else:\n            label_name = f\"id:{cid}\"\n        text = f\"{label_name} {score:.2f}\"\n        # Text background\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - baseline), (x1 + tw, y1), color, -1)\n        cv2.putText(frame, text, (x1, y1 - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n    # Draw mAP\n    if map_value is None:\n        map_text = \"mAP: N/A\"\n    else:\n        map_text = f\"mAP: {map_value:.3f}\"\n    cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n    return frame\n\n# =========================\n# Phase 3: Inference and Phase 4: Output Interpretation & Handling\n# =========================\n\nrunning_predictions = []  # list of dicts: {class_id, score, bbox:[x1,y1,x2,y2], frame_id}\nrunning_ground_truths = []  # pseudo GT from high-confidence detections\n\nframe_index = 0\nstart_time = time.time()\n\ntry:\n    while True:\n        # 2.4 Loop Control: read next frame; break if none\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocess\n        input_data = preprocess_frame(frame)\n\n        # Phase 3.1: Set Input Tensor\n        interpreter.set_tensor(input_index, input_data)\n\n        # Phase 3.2: Run Inference\n        interpreter.invoke()\n\n        # Phase 4.1: Get outputs\n        boxes, classes, scores, count = interpret_outputs(interpreter, output_details)\n\n        # Convert outputs to numpy arrays\n        boxes = np.squeeze(boxes)\n        classes = np.squeeze(classes).astype(np.int32)\n        scores = np.squeeze(scores)\n        if np.isscalar(count):\n            num = int(count)\n        elif isinstance(count, np.ndarray):\n            num = int(np.squeeze(count))\n        else:\n            num = len(scores)\n\n        # 4.2 Interpret Results and 4.3 Post-processing:\n        detections_for_draw = []\n        predictions_this_frame = []\n        gts_this_frame = []\n\n        for i in range(num):\n            score = float(scores[i])\n            cid = int(classes[i])\n            # The model returns boxes in normalized [ymin, xmin, ymax, xmax]\n            y1_norm, x1_norm, y2_norm, x2_norm = boxes[i]\n            x1 = int(x1_norm * frame_width)\n            y1 = int(y1_norm * frame_height)\n            x2 = int(x2_norm * frame_width)\n            y2 = int(y2_norm * frame_height)\n            # Ensure x1 <= x2, y1 <= y2\n            x1, x2 = min(x1, x2), max(x1, x2)\n            y1, y2 = min(y1, y2), max(y1, y2)\n            # 4.3 Bounding box clipping\n            x1, y1, x2, y2 = clip_bbox(x1, y1, x2, y2, frame_width, frame_height)\n\n            # Collect detections for drawing if above main confidence threshold\n            if score >= confidence_threshold:\n                detections_for_draw.append({\n                    'class_id': cid,\n                    'score': score,\n                    'bbox': [x1, y1, x2, y2]\n                })\n                predictions_this_frame.append({\n                    'class_id': cid,\n                    'score': score,\n                    'bbox': [x1, y1, x2, y2],\n                    'frame_id': frame_index\n                })\n\n            # Collect pseudo ground-truth using higher confidence threshold\n            if score >= pseudo_gt_high_conf_threshold:\n                gts_this_frame.append({\n                    'class_id': cid,\n                    'bbox': [x1, y1, x2, y2],\n                    'frame_id': frame_index\n                })\n\n        # Update running lists\n        running_predictions.extend(predictions_this_frame)\n        running_ground_truths.extend(gts_this_frame)\n\n        # Compute running mAP across frames processed so far\n        num_classes = max(len(labels), int(np.max(classes)) + 1 if classes.size > 0 else 1)\n        current_map = compute_map(running_predictions, running_ground_truths, num_classes, iou_thr=iou_threshold)\n\n        # 4.4 Handle Output: draw and write frame\n        frame_out = draw_detections(frame, detections_for_draw, labels, current_map)\n        out_writer.write(frame_out)\n\n        frame_index += 1\n\nexcept Exception as e:\n    print(f\"ERROR during processing: {e}\")\nfinally:\n    # =========================\n    # Phase 5: Cleanup\n    # =========================\n    cap.release()\n    out_writer.release()\n\n# Print summary\nelapsed = time.time() - start_time\nprint(f\"Processed {frame_index} frames in {elapsed:.2f}s ({(frame_index / max(elapsed,1e-6)):.2f} FPS).\")\n# Final mAP computation\nfinal_num_classes = len(labels) if len(labels) > 0 else 90\nfinal_map = compute_map(running_predictions, running_ground_truths, final_num_classes, iou_thr=iou_threshold)\nif final_map is None:\n    print(\"Final mAP: N/A (no pseudo ground-truth was collected; increase pseudo_gt_high_conf_threshold if needed).\")\nelse:\n    print(f\"Final mAP (pseudo-GT based): {final_map:.4f}\")\nprint(f\"Output video saved to: {output_path}\")"
      },
      "session_id": "gpt-5-2025-08-07_54c5_tpusg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 201.821,
      "total_cost": 0.094675,
      "observations": [
        {
          "id": "time-22-44-10-696523_chatcmpl-D5e1Lvc3yCJIHS5OxElfzdPnk9MnV",
          "traceId": "d540443c",
          "type": "GENERATION",
          "name": "d540_tpusg_gen_attempt#1",
          "startTime": "2026-02-04T20:44:10.696000+00:00",
          "endTime": "2026-02-04T20:47:32.517000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.094675,
          "latency": 201821.0,
          "totalTokens": 11011,
          "costDetails": {
            "total": 0.094675
          },
          "completionTokens": 9247,
          "promptTokens": 1764,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T20:48:34.821Z",
      "createdAt": "2026-02-04T20:44:10.956Z",
      "environment": "default"
    },
    {
      "id": "d2b83727",
      "timestamp": "2026-02-04T20:40:16.365000+00:00",
      "name": "d2b8_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport sys\nimport time\nimport numpy as np\n\n# Phase 1: Setup\n\n# 1.1 Imports: TFLite Interpreter and EdgeTPU delegate with fallback\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\nexcept Exception:\n    try:\n        from tensorflow.lite import Interpreter\n        from tensorflow.lite.experimental import load_delegate\n    except Exception as e:\n        print(\"ERROR: Unable to import TFLite Interpreter. Ensure tflite_runtime or tensorflow is installed.\")\n        sys.exit(1)\n\n# Import cv2 only because image/video processing is explicitly required\ntry:\n    import cv2\nexcept Exception as e:\n    print(\"ERROR: OpenCV (cv2) is required for video processing but could not be imported.\")\n    sys.exit(1)\n\n# 1.2 Paths/Parameters (from CONFIGURATION PARAMETERS)\nmodel_path  = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path  = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path  = \"/home/mendel/tinyml_autopilot/data//sheeps.mp4\"\noutput_path  = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold_str  = 0.5\ntry:\n    confidence_threshold = float(confidence_threshold_str)\nexcept ValueError:\n    print(\"ERROR: confidence_threshold is not a float. Check configuration.\")\n    sys.exit(1)\n\n# 1.3 Load Labels (if provided and relevant)\nlabels = []\nif label_path and os.path.isfile(label_path):\n    try:\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if line:\n                    labels.append(line)\n    except Exception as e:\n        print(f\"WARNING: Failed to read label file at {label_path}. Proceeding without labels. Error: {e}\")\nelse:\n    if label_path:\n        print(f\"WARNING: Label file not found at {label_path}. Proceeding without labels.\")\n\n# 1.4 Load Interpreter with EdgeTPU delegate\ninterpreter = None\ndelegate_errors = []\ntry:\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('libedgetpu.so.1.0')]\n    )\nexcept Exception as e1:\n    delegate_errors.append((\"libedgetpu.so.1.0\", str(e1)))\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except Exception as e2:\n        delegate_errors.append((\"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\", str(e2)))\n        print(\"ERROR: Failed to load EdgeTPU delegate with both default and fallback paths.\")\n        for lib, err in delegate_errors:\n            print(f\" - Attempted delegate '{lib}' failed with error: {err}\")\n        print(\"Ensure the EdgeTPU runtime is installed and the correct shared library is available.\")\n        sys.exit(1)\n\n# Allocate tensors\ntry:\n    interpreter.allocate_tensors()\nexcept Exception as e:\n    print(f\"ERROR: Failed to allocate tensors: {e}\")\n    sys.exit(1)\n\n# 1.5 Get Model Details\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\nif len(input_details) < 1:\n    print(\"ERROR: Model does not have any input tensors.\")\n    sys.exit(1)\n\ninput_shape = input_details[0]['shape']\ninput_height = int(input_shape[1])\ninput_width = int(input_shape[2])\ninput_dtype = input_details[0]['dtype']\nfloating_model = (input_dtype == np.float32)\n\n# Utility functions\ndef preprocess_frame_bgr_to_model_input(bgr_frame, target_width, target_height, floating):\n    # Convert BGR to RGB, resize, and prepare tensor\n    rgb = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (target_width, target_height))\n    input_data = np.expand_dims(resized, axis=0)\n    if floating:\n        input_data = (np.float32(input_data) - 127.5) / 127.5\n    else:\n        input_data = np.uint8(input_data)\n    return input_data\n\ndef safe_label(class_id):\n    if labels and 0 <= class_id < len(labels):\n        return labels[class_id]\n    return f\"id_{class_id}\"\n\ndef clip(value, min_v, max_v):\n    return max(min_v, min(value, max_v))\n\n# Phase 2: Input Acquisition & Preprocessing Loop\n\n# 2.1 Acquire Input Data - open video file\ncap = cv2.VideoCapture(input_path)\nif not cap.isOpened():\n    print(f\"ERROR: Unable to open input video file: {input_path}\")\n    sys.exit(1)\n\n# Retrieve video properties\ninput_fps = cap.get(cv2.CAP_PROP_FPS)\nif input_fps <= 0 or np.isnan(input_fps):\n    input_fps = 30.0  # fallback\nframe_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nframe_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Prepare VideoWriter for output\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nwriter = cv2.VideoWriter(output_path, fourcc, input_fps, (frame_width, frame_height))\nif not writer.isOpened():\n    # Try fallback codecs\n    for codec in ['avc1', 'XVID', 'MJPG']:\n        fourcc_alt = cv2.VideoWriter_fourcc(*codec)\n        writer = cv2.VideoWriter(output_path, fourcc_alt, input_fps, (frame_width, frame_height))\n        if writer.isOpened():\n            break\nif not writer.isOpened():\n    print(f\"ERROR: Unable to open VideoWriter for output file: {output_path}\")\n    cap.release()\n    sys.exit(1)\n\n# Tracking for mAP proxy computation\n# We approximate AP per class by average of detection scores above threshold; mAP is mean across classes observed\nclass_scores = {}  # class_id -> list of scores\nframe_count = 0\ntotal_inference_time_ms = 0.0\n\n# 2.4 Loop Control - process entire video\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n    frame_count += 1\n\n    # 2.2 Preprocess Data\n    input_data = preprocess_frame_bgr_to_model_input(frame, input_width, input_height, floating_model)\n\n    # Phase 3: Inference\n    # 3.1 Set Input Tensor\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    # 3.2 Run Inference\n    t0 = time.time()\n    interpreter.invoke()\n    t1 = time.time()\n    infer_ms = (t1 - t0) * 1000.0\n    total_inference_time_ms += infer_ms\n\n    # Phase 4: Output Interpretation & Handling Loop\n\n    # 4.1 Get Output Tensors\n    # Typical SSD output tensors: boxes [1,N,4], classes [1,N], scores [1,N], count [1]\n    raw_outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    boxes = None\n    classes = None\n    scores = None\n    count = None\n\n    for out in raw_outputs:\n        arr = np.squeeze(out)\n        if arr.ndim == 2 and arr.shape[1] == 4:\n            # boxes\n            boxes = arr\n        elif arr.ndim == 1:\n            # count or 1D of N (classes or scores)\n            if arr.shape[0] == 1:\n                count = int(arr[0])\n            else:\n                # Need to distinguish classes vs scores; try by value range\n                if np.max(arr) <= 1.0 and np.min(arr) >= 0.0:\n                    scores = arr\n                else:\n                    classes = arr.astype(np.int32)\n        elif arr.ndim == 3 and arr.shape[0] == 1 and arr.shape[2] == 4:\n            boxes = arr[0]\n        elif arr.ndim == 2:\n            # Could be classes or scores\n            if np.max(arr) <= 1.0 and np.min(arr) >= 0.0:\n                scores = arr[0] if arr.shape[0] == 1 else arr\n            else:\n                classes = arr[0].astype(np.int32) if arr.shape[0] == 1 else arr.astype(np.int32)\n\n    # Fallbacks if any is still None due to variant output ordering\n    if boxes is None:\n        # Attempt by shape from output_details directly\n        for od in output_details:\n            o = interpreter.get_tensor(od['index'])\n            if o.ndim == 3 and o.shape[-1] == 4:\n                boxes = o[0]\n                break\n    if count is None:\n        # Estimate count by available size\n        if scores is not None:\n            count = int(len(scores))\n        elif classes is not None:\n            count = int(len(classes))\n        elif boxes is not None:\n            count = int(boxes.shape[0])\n        else:\n            count = 0\n\n    if scores is None or classes is None or boxes is None:\n        # If outputs are still not identified, skip this frame gracefully\n        overlay = frame.copy()\n        cv2.putText(overlay, \"Detection output parsing failed.\", (10, 30),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n        writer.write(overlay)\n        continue\n\n    # Ensure sizes match\n    N = min(count, len(scores), len(classes), boxes.shape[0])\n\n    # 4.2 Interpret Results\n    detections = []\n    for i in range(N):\n        score = float(scores[i])\n        cls_id = int(classes[i])\n        # 4.3 Post-processing: confidence thresholding\n        if score < confidence_threshold:\n            continue\n        y_min, x_min, y_max, x_max = boxes[i]\n        # Scale from normalized coordinates [0,1] to pixel coordinates\n        x_min_abs = int(clip(x_min * frame_width, 0, frame_width - 1))\n        y_min_abs = int(clip(y_min * frame_height, 0, frame_height - 1))\n        x_max_abs = int(clip(x_max * frame_width, 0, frame_width - 1))\n        y_max_abs = int(clip(y_max * frame_height, 0, frame_height - 1))\n\n        # Ensure box has positive area after clipping\n        if x_max_abs <= x_min_abs or y_max_abs <= y_min_abs:\n            continue\n\n        label_text = safe_label(cls_id)\n        detections.append({\n            'bbox': (x_min_abs, y_min_abs, x_max_abs, y_max_abs),\n            'score': score,\n            'class_id': cls_id,\n            'label': label_text\n        })\n\n        # Accumulate scores for mAP proxy computation\n        if cls_id not in class_scores:\n            class_scores[cls_id] = []\n        class_scores[cls_id].append(score)\n\n    # Compute running mAP proxy: mean of per-class average scores\n    map_proxy = 0.0\n    if len(class_scores) > 0:\n        per_class_avg = [np.mean(scores_list) for scores_list in class_scores.values() if len(scores_list) > 0]\n        if len(per_class_avg) > 0:\n            map_proxy = float(np.mean(per_class_avg))\n\n    # 4.4 Handle Output: draw rectangles, labels, and mAP on the frame, then write to file\n    overlay = frame.copy()\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        score = det['score']\n        label = det['label']\n\n        # Draw bounding box\n        cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        # Prepare label text\n        text = f\"{label}: {score:.2f}\"\n        # Draw background for text for better readability\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        text_x, text_y = x1, y1 - 10 if y1 - 10 > 10 else y1 + th + 10\n        cv2.rectangle(overlay, (text_x, text_y - th - baseline), (text_x + tw, text_y + baseline),\n                      (0, 255, 0), thickness=cv2.FILLED)\n        cv2.putText(overlay, text, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.6,\n                    (0, 0, 0), 2, cv2.LINE_AA)\n\n    # Add inference time and mAP proxy on the frame\n    status_text = f\"Infer: {infer_ms:.1f} ms | mAP: {map_proxy:.3f}\"\n    cv2.putText(overlay, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8,\n                (255, 255, 0), 2, cv2.LINE_AA)\n\n    writer.write(overlay)\n\n# Phase 5: Cleanup\ncap.release()\nwriter.release()\ncv2.destroyAllWindows()\n\n# Print summary\nif frame_count > 0:\n    avg_infer_ms = total_inference_time_ms / frame_count\nelse:\n    avg_infer_ms = 0.0\n\nfinal_map = 0.0\nif len(class_scores) > 0:\n    per_class_avg = [np.mean(scores_list) for scores_list in class_scores.values() if len(scores_list) > 0]\n    if len(per_class_avg) > 0:\n        final_map = float(np.mean(per_class_avg))\n\nprint(\"Processing complete.\")\nprint(f\"Frames processed: {frame_count}\")\nprint(f\"Average inference time per frame: {avg_infer_ms:.2f} ms\")\nprint(f\"mAP (proxy based on average detection confidences per class): {final_map:.3f}\")\nprint(f\"Output video saved to: {output_path}\")"
      },
      "session_id": "gpt-5-2025-08-07_54c5_tpusg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 168.302,
      "total_cost": 0.071815,
      "observations": [
        {
          "id": "time-22-40-17-012986_chatcmpl-D5dxZDXIJ2sYlIRTseoEZeBxuczeE",
          "traceId": "d2b83727",
          "type": "GENERATION",
          "name": "d2b8_tpusg_gen_attempt#1",
          "startTime": "2026-02-04T20:40:17.012000+00:00",
          "endTime": "2026-02-04T20:43:05.314000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.071815,
          "latency": 168302.0,
          "totalTokens": 8725,
          "costDetails": {
            "total": 0.071815
          },
          "completionTokens": 6961,
          "promptTokens": 1764,
          "environment": "default",
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "updatedAt": "2026-02-04T20:44:04.631Z",
      "createdAt": "2026-02-04T20:40:16.922Z",
      "environment": "default"
    }
  ],
  "meta": {
    "total_items": 10
  }
}