{
  "data": [
    {
      "id": "ddd7470e",
      "timestamp": "2026-02-04T11:07:16.188000+00:00",
      "name": "ddd7_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Load your saved model\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverter = tf.lite.TFLiteConverter.from_keras_model(tf.keras.models.load_model(original_model_path))\n\n# Set conversion parameters for quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\ndef representative_dataset_gen():\n    for _ in range(100):\n        yield [np.array(np.random.random_sample((1, 224, 224, 3)), dtype=np.float32)]\nconverter.representative_dataset = representative_dataset_gen\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.uint8\nconverter.inference_output_type = tf.uint8\n\n# Convert the model to TFLite format with quantization\ntflite_model_quant = converter.convert()\n\n# Save the quantized model to a file\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 61.107,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-07-16-190793_chatcmpl-2ad09b7b-24c6-4b28-b862-063f61152139",
          "traceId": "ddd7470e",
          "type": "GENERATION",
          "name": "dd_mc_conversion_code_gen",
          "startTime": "2026-02-04T11:07:16.190000+00:00",
          "endTime": "2026-02-04T11:07:27.446000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11256.0,
          "completionTokens": 283,
          "environment": "default",
          "totalTokens": 715,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-07-31-057509_chatcmpl-522281a1-9446-48f5-ae8d-924e403e388b",
          "traceId": "ddd7470e",
          "type": "GENERATION",
          "name": "dd_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T11:07:31.057000+00:00",
          "endTime": "2026-02-04T11:07:46.745000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15688.0,
          "completionTokens": 324,
          "environment": "default",
          "totalTokens": 2372,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-07-49-991712_chatcmpl-65d020b5-0089-400a-adf8-479b646edbb1",
          "traceId": "ddd7470e",
          "type": "GENERATION",
          "name": "dd_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T11:07:49.991000+00:00",
          "endTime": "2026-02-04T11:08:02.075000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12084.0,
          "completionTokens": 240,
          "environment": "default",
          "totalTokens": 1848,
          "promptTokens": 1608,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-08-05-305563_chatcmpl-2be0e0f8-a658-4722-983a-0e77fe29e4f0",
          "traceId": "ddd7470e",
          "type": "GENERATION",
          "name": "dd_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T11:08:05.305000+00:00",
          "endTime": "2026-02-04T11:08:17.295000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11990.0,
          "completionTokens": 255,
          "environment": "default",
          "totalTokens": 1883,
          "promptTokens": 1628,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "9cfcfd3c-1e92-45b6-bd98-31c4b884dfaa",
          "traceId": "ddd7470e",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T11:07:16.188000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T11:07:16.711Z",
      "updatedAt": "2026-02-04T11:08:27.404Z",
      "environment": "default"
    },
    {
      "id": "234aea1d",
      "timestamp": "2026-02-04T11:05:28.460000+00:00",
      "name": "234a_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define input shape\ninput_shape = (1, 224, 224, 3)  # Example input shape for an image model\n\n# Load or define your Keras model\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Convert the model to a TFLite model\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Define representative dataset generator for quantization\ndef representative_data_gen():\n    for _ in range(100):\n        yield [np.array(np.random.random_sample(input_shape), dtype=np.float32)]\n\n# Set the optimization flag to optimize for size (quantize)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # or tf.uint8\nconverter.inference_output_type = tf.uint8  # or tf.int8\n\n# Convert and save the TFLite model\ntflite_model = converter.convert()\nwith open(converted_model_path, \"wb\") as f:\n    f.write(tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 77.195,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-05-28-462495_chatcmpl-2e4529c1-b44e-4765-87e8-6d45b859552d",
          "traceId": "234aea1d",
          "type": "GENERATION",
          "name": "23_mc_conversion_code_gen",
          "startTime": "2026-02-04T11:05:28.462000+00:00",
          "endTime": "2026-02-04T11:05:41.730000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13268.0,
          "completionTokens": 332,
          "environment": "default",
          "totalTokens": 764,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-05-45-340122_chatcmpl-9ded8c50-c0b7-4639-8e1b-17b81e738110",
          "traceId": "234aea1d",
          "type": "GENERATION",
          "name": "23_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T11:05:45.340000+00:00",
          "endTime": "2026-02-04T11:06:09.772000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24432.0,
          "completionTokens": 545,
          "environment": "default",
          "totalTokens": 2593,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-06-13-264111_chatcmpl-49d96643-72c3-4511-becd-1958690f9037",
          "traceId": "234aea1d",
          "type": "GENERATION",
          "name": "23_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T11:06:13.264000+00:00",
          "endTime": "2026-02-04T11:06:27.379000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14115.0,
          "completionTokens": 286,
          "environment": "default",
          "totalTokens": 2081,
          "promptTokens": 1795,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-06-30-937201_chatcmpl-a0afdf09-df53-4584-843a-09682e4969dd",
          "traceId": "234aea1d",
          "type": "GENERATION",
          "name": "23_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T11:06:30.937000+00:00",
          "endTime": "2026-02-04T11:06:45.655000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14718.0,
          "completionTokens": 324,
          "environment": "default",
          "totalTokens": 1699,
          "promptTokens": 1375,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "ecd1c08c-7810-4586-9a72-c807a451cc84",
          "traceId": "234aea1d",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T11:05:28.460000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T11:05:28.985Z",
      "updatedAt": "2026-02-04T11:06:55.659Z",
      "environment": "default"
    },
    {
      "id": "de1bca1e",
      "timestamp": "2026-02-04T11:03:09.910000+00:00",
      "name": "de1b_model_converter",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import tensorflow as tf\n\n# Assuming `model` is your Keras model\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Define a generator function that yields input data with FLOAT32 dtype\ndef dataset_gen():\n    # Replace the following lines with your actual data loading and preprocessing logic\n    for _ in range(100):  # Example: yield 100 samples\n        # Generate or load your input data\n        input_data = tf.random.normal([1, 28, 28, 3])  # Example shape: (batch_size, height, width, channels)\n        \n        # Ensure the dtype is FLOAT32\n        input_data = tf.cast(input_data, tf.float32)\n        \n        yield [input_data]  # Return a list of input tensors\n\n# Set the optimization flag for quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Provide a representative dataset generator\nconverter.representative_dataset = dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.int8  # or tf.uint8\nconverter.inference_output_type = tf.int8  # or tf.uint8\n\n# Convert the model to TFLite format\ntflite_quant_model = converter.convert()\n\n# Save the quantized TFLite model to a file\nwith open('model.tflite', 'wb') as f:\n    f.write(tflite_quant_model)",
        "last_error": "Failed to generate valid code after the max 5 attempts. Last error from code execution: 2026-02-04 13:04:59.235847: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 13:04:59.239441: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 13:04:59.251262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2026-02-04 13:04:59.270432: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2026-02-04 13:04:59.276006: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-02-04 13:04:59.290153: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2026-02-04 13:05:00.221061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204130458.py/tmp_20260204130458_mc_qwen2.5-coder:32b/tmp_20260204130458_mc_qwen2.5-coder:32b.py\", line 4, in <module>\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\nNameError: name 'model' is not defined\n"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 112.329,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-03-09-912281_chatcmpl-b11aeea6-4795-4c21-b7f1-b5db7af4387d",
          "traceId": "de1bca1e",
          "type": "GENERATION",
          "name": "de_mc_conversion_code_gen",
          "startTime": "2026-02-04T11:03:09.912000+00:00",
          "endTime": "2026-02-04T11:03:22.011000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12099.0,
          "completionTokens": 310,
          "environment": "default",
          "totalTokens": 742,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-03-25-813905_chatcmpl-9f5ba5c5-319d-4205-8ab6-2a097ae5337b",
          "traceId": "de1bca1e",
          "type": "GENERATION",
          "name": "de_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T11:03:25.813000+00:00",
          "endTime": "2026-02-04T11:03:49.897000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24084.0,
          "completionTokens": 536,
          "environment": "default",
          "totalTokens": 2584,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-03-53-164880_chatcmpl-0d0db1e3-4b26-4369-9fa3-23c75043f333",
          "traceId": "de1bca1e",
          "type": "GENERATION",
          "name": "de_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T11:03:53.164000+00:00",
          "endTime": "2026-02-04T11:04:08.844000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15680.0,
          "completionTokens": 326,
          "environment": "default",
          "totalTokens": 2125,
          "promptTokens": 1799,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-04-12-009389_chatcmpl-8e800769-c621-449d-8a53-6e52da869bee",
          "traceId": "de1bca1e",
          "type": "GENERATION",
          "name": "de_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T11:04:12.009000+00:00",
          "endTime": "2026-02-04T11:04:28.498000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16489.0,
          "completionTokens": 368,
          "environment": "default",
          "totalTokens": 1785,
          "promptTokens": 1417,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-04-32-162579_chatcmpl-796ef3ca-2ac5-4511-87fb-14ba2e73ee41",
          "traceId": "de1bca1e",
          "type": "GENERATION",
          "name": "de_mc_error_handling_attempt#5",
          "startTime": "2026-02-04T11:04:32.162000+00:00",
          "endTime": "2026-02-04T11:04:58.967000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26805.0,
          "completionTokens": 606,
          "environment": "default",
          "totalTokens": 2654,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "d0201315-df9a-4d2c-9255-3af1f9c2e6d1",
          "traceId": "de1bca1e",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T11:03:09.911000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        },
        {
          "id": "d36de267-aec4-4eac-a46a-0c7844021ba1",
          "traceId": "de1bca1e",
          "type": "SPAN",
          "name": "error_de_mc_failure_signal_model_converter",
          "startTime": "2026-02-04T11:05:02.240000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Failed to generate valid code after the max 5 attempts. Last error from code execution: 2026-02-04 13:04:59.235847: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 13:04:59.239441: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 13:04:59.251262: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2026-02-04 13:04:59.270432: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2026-02-04 13:04:59.276006: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-02-04 13:04:59.290153: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2026-02-04 13:05:00.221061: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204130458.py/tmp_20260204130458_mc_qwen2.5-coder:32b/tmp_20260204130458_mc_qwen2.5-coder:32b.py\", line 4, in <module>\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\nNameError: name 'model' is not defined\n",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T11:03:10.433Z",
      "updatedAt": "2026-02-04T11:05:07.662Z",
      "environment": "default"
    },
    {
      "id": "218f8fc9",
      "timestamp": "2026-02-04T11:02:27.175000+00:00",
      "name": "218f_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Convert the model to TFLite with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = supported_ops\n\n# Define representative dataset generator for calibration\ndef representative_data_gen():\n    for _ in range(100):\n        # Replace with actual data loading and preprocessing logic\n        input_data = tf.random.normal([1, 224, 224, 3], dtype=tf.float32)\n        yield [input_data]\n\nconverter.representative_dataset = representative_data_gen\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert the model and save it to a file\nquantized_tflite_model = converter.convert()\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 11.902,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-02-27-177779_chatcmpl-5c55bf71-b6b7-4d40-97b6-7744bd09537d",
          "traceId": "218f8fc9",
          "type": "GENERATION",
          "name": "21_mc_conversion_code_gen",
          "startTime": "2026-02-04T11:02:27.177000+00:00",
          "endTime": "2026-02-04T11:02:39.078000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11901.0,
          "completionTokens": 298,
          "environment": "default",
          "totalTokens": 730,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "93ffacb0-373f-44a7-9805-d64f46a0a1d2",
          "traceId": "218f8fc9",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T11:02:27.176000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T11:02:27.701Z",
      "updatedAt": "2026-02-04T11:02:49.323Z",
      "environment": "default"
    },
    {
      "id": "00f1a6e8",
      "timestamp": "2026-02-04T11:00:33.374000+00:00",
      "name": "00f1_model_converter",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Define representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):\n        # Generate a random input data sample\n        data = tf.random.normal([1] + original_model.input_shape[1:], dtype=tf.float32)\n        yield [data]\n\n# Create a converter object from the Keras model\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Set quantization parameters\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset_gen\nconverter.target_spec.supported_ops = supported_ops\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert the model to TFLite format with quantization\nquantized_tflite_model = converter.convert()\n\n# Save the converted model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)",
        "last_error": "Failed to generate valid code after the max 5 attempts. Last error from code execution: 2026-02-04 13:01:57.304712: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 13:01:57.308962: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 13:01:57.321733: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2026-02-04 13:01:57.341596: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2026-02-04 13:01:57.347522: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-02-04 13:01:57.362995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2026-02-04 13:01:58.270006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 7 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n  warnings.warn(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1770202919.923638 2843947 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\nW0000 00:00:1770202919.923668 2843947 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n2026-02-04 13:01:59.924200: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpsyuqm_wu\n2026-02-04 13:01:59.924763: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n2026-02-04 13:01:59.924779: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpsyuqm_wu\n2026-02-04 13:01:59.928397: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n2026-02-04 13:01:59.929244: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n2026-02-04 13:01:59.953955: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpsyuqm_wu\n2026-02-04 13:01:59.960699: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 36501 microseconds.\n2026-02-04 13:01:59.968514: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204130157.py/tmp_20260204130157_mc_qwen2.5-coder:32b/tmp_20260204130157_mc_qwen2.5-coder:32b.py\", line 29, in <module>\n    quantized_tflite_model = converter.convert()\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1231, in wrapper\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1183, in _convert_and_export_metrics\n    result = convert_func(self, *args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1744, in convert\n    saved_model_convert_result = self._convert_as_saved_model()\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1725, in _convert_as_saved_model\n    return super(TFLiteKerasModelConverterV2, self).convert(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1473, in convert\n    return self._optimize_tflite_model(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 215, in wrapper\n    raise error from None  # Re-throws the exception.\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 205, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1127, in _optimize_tflite_model\n    model = self._quantize(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 748, in _quantize\n    calibrated = calibrate_quantize.calibrate(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 215, in wrapper\n    raise error from None  # Re-throws the exception.\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 205, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 254, in calibrate\n    self._feed_tensors(dataset_gen, resize_input=True)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 101, in _feed_tensors\n    for sample in dataset_gen():\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204130157.py/tmp_20260204130157_mc_qwen2.5-coder:32b/tmp_20260204130157_mc_qwen2.5-coder:32b.py\", line 15, in representative_dataset_gen\n    data = tf.random.normal([1] + original_model.input_shape[1:], dtype=tf.float32)\nTypeError: can only concatenate list (not \"tuple\") to list\n"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 87.32,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-13-00-33-376408_chatcmpl-40d4d7d7-ccaf-4cb1-b6cc-fc1288d22304",
          "traceId": "00f1a6e8",
          "type": "GENERATION",
          "name": "00_mc_conversion_code_gen",
          "startTime": "2026-02-04T11:00:33.376000+00:00",
          "endTime": "2026-02-04T11:00:44.947000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11571.0,
          "completionTokens": 302,
          "environment": "default",
          "totalTokens": 734,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-00-48-753490_chatcmpl-a0691925-b43c-4e54-9837-8340b623d8bf",
          "traceId": "00f1a6e8",
          "type": "GENERATION",
          "name": "00_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T11:00:48.753000+00:00",
          "endTime": "2026-02-04T11:01:07.190000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18437.0,
          "completionTokens": 386,
          "environment": "default",
          "totalTokens": 2434,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-01-07-195669_chatcmpl-74510d2c-e4e6-406f-9ca0-30f20567254a",
          "traceId": "00f1a6e8",
          "type": "GENERATION",
          "name": "00_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T11:01:07.195000+00:00",
          "endTime": "2026-02-04T11:01:20.149000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12954.0,
          "completionTokens": 302,
          "environment": "default",
          "totalTokens": 1090,
          "promptTokens": 788,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-01-24-260869_chatcmpl-f704cb0b-f057-4b7d-b86c-d97daa25c5d6",
          "traceId": "00f1a6e8",
          "type": "GENERATION",
          "name": "00_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T11:01:24.260000+00:00",
          "endTime": "2026-02-04T11:01:44.138000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19878.0,
          "completionTokens": 427,
          "environment": "default",
          "totalTokens": 2475,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-01-44-141475_chatcmpl-879ef7bb-45e6-4cf5-977a-ff693b473ae4",
          "traceId": "00f1a6e8",
          "type": "GENERATION",
          "name": "00_mc_error_handling_attempt#5",
          "startTime": "2026-02-04T11:01:44.141000+00:00",
          "endTime": "2026-02-04T11:01:57.013000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12872.0,
          "completionTokens": 302,
          "environment": "default",
          "totalTokens": 1090,
          "promptTokens": 788,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "30467c94-4615-454e-bb27-029b38c5a0d7",
          "traceId": "00f1a6e8",
          "type": "SPAN",
          "name": "error_00_mc_failure_signal_model_converter",
          "startTime": "2026-02-04T11:02:00.694000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Failed to generate valid code after the max 5 attempts. Last error from code execution: 2026-02-04 13:01:57.304712: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 13:01:57.308962: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 13:01:57.321733: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2026-02-04 13:01:57.341596: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2026-02-04 13:01:57.347522: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-02-04 13:01:57.362995: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2026-02-04 13:01:58.270006: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 7 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n  warnings.warn(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1770202919.923638 2843947 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\nW0000 00:00:1770202919.923668 2843947 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n2026-02-04 13:01:59.924200: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpsyuqm_wu\n2026-02-04 13:01:59.924763: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n2026-02-04 13:01:59.924779: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpsyuqm_wu\n2026-02-04 13:01:59.928397: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n2026-02-04 13:01:59.929244: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n2026-02-04 13:01:59.953955: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpsyuqm_wu\n2026-02-04 13:01:59.960699: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 36501 microseconds.\n2026-02-04 13:01:59.968514: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204130157.py/tmp_20260204130157_mc_qwen2.5-coder:32b/tmp_20260204130157_mc_qwen2.5-coder:32b.py\", line 29, in <module>\n    quantized_tflite_model = converter.convert()\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1231, in wrapper\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1183, in _convert_and_export_metrics\n    result = convert_func(self, *args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1744, in convert\n    saved_model_convert_result = self._convert_as_saved_model()\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1725, in _convert_as_saved_model\n    return super(TFLiteKerasModelConverterV2, self).convert(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1473, in convert\n    return self._optimize_tflite_model(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 215, in wrapper\n    raise error from None  # Re-throws the exception.\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 205, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1127, in _optimize_tflite_model\n    model = self._quantize(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 748, in _quantize\n    calibrated = calibrate_quantize.calibrate(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 215, in wrapper\n    raise error from None  # Re-throws the exception.\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 205, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 254, in calibrate\n    self._feed_tensors(dataset_gen, resize_input=True)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 101, in _feed_tensors\n    for sample in dataset_gen():\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204130157.py/tmp_20260204130157_mc_qwen2.5-coder:32b/tmp_20260204130157_mc_qwen2.5-coder:32b.py\", line 15, in representative_dataset_gen\n    data = tf.random.normal([1] + original_model.input_shape[1:], dtype=tf.float32)\nTypeError: can only concatenate list (not \"tuple\") to list\n",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        },
        {
          "id": "9df00316-bfa2-47a1-8955-90c91bd33e5c",
          "traceId": "00f1a6e8",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T11:00:33.374000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T11:00:33.898Z",
      "updatedAt": "2026-02-04T11:02:06.064Z",
      "environment": "default"
    },
    {
      "id": "0ab55d4c",
      "timestamp": "2026-02-04T10:59:48.839000+00:00",
      "name": "0ab5_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Define paths\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\n# Load the original model\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Define quantization parameters\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Convert to TFLite with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = supported_ops\n\n# Define representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):\n        # Replace this with actual data generation logic\n        input_data = tf.random.normal([1, 224, 224, 3], dtype=tf.float32)\n        yield [input_data]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, they are still converted to float\nconverter.target_spec.supported_types = [tf.int8]\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert the model\ntflite_model_quant = converter.convert()\n\n# Save the quantized TFLite model\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 13.805,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-59-48-842321_chatcmpl-55e4ea3e-abcf-4858-80b7-c72c420294f1",
          "traceId": "0ab55d4c",
          "type": "GENERATION",
          "name": "0a_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:59:48.842000+00:00",
          "endTime": "2026-02-04T11:00:02.645000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13803.0,
          "completionTokens": 348,
          "environment": "default",
          "totalTokens": 780,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "5b1315a1-4884-48a6-9945-7482f4645a4f",
          "traceId": "0ab55d4c",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:59:48.840000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:59:49.362Z",
      "updatedAt": "2026-02-04T11:00:12.521Z",
      "environment": "default"
    },
    {
      "id": "6e0b728f",
      "timestamp": "2026-02-04T10:58:12.137000+00:00",
      "name": "6e0b_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define paths\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\n# Load the original model\noriginal_model = tf.keras.models.load_model(original_model_path)\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\ndef representative_data_gen():\n    # Generate random input data that matches the model's input shape\n    for _ in range(100):\n        yield [np.array(np.random.random_sample(original_model.input_shape[1:]), dtype=np.float32)]\n\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Define quantization configuration\ninference_input_type = tf.int8\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\nconverter.target_spec.supported_ops = supported_ops\n\nquantized_tflite_model = converter.convert()\n\n# Save the quantized model to a file\nwith open(converted_model_path, 'wb') as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 66.855,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-58-12-140026_chatcmpl-a9c79107-a1c1-4a4f-b0e8-c85cd601c113",
          "traceId": "6e0b728f",
          "type": "GENERATION",
          "name": "6e_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:58:12.140000+00:00",
          "endTime": "2026-02-04T10:58:26.042000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13902.0,
          "completionTokens": 347,
          "environment": "default",
          "totalTokens": 779,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-58-30-097640_chatcmpl-46013cdd-b3ed-4f89-b2e5-f0f53b3c3914",
          "traceId": "6e0b728f",
          "type": "GENERATION",
          "name": "6e_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:58:30.097000+00:00",
          "endTime": "2026-02-04T10:58:48.438000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18341.0,
          "completionTokens": 389,
          "environment": "default",
          "totalTokens": 2437,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-58-51-646715_chatcmpl-9abd2d0c-a06b-47ec-9313-85b2f85d80ce",
          "traceId": "6e0b728f",
          "type": "GENERATION",
          "name": "6e_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:58:51.646000+00:00",
          "endTime": "2026-02-04T10:59:03.215000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11569.0,
          "completionTokens": 241,
          "environment": "default",
          "totalTokens": 1664,
          "promptTokens": 1423,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-59-06-588589_chatcmpl-09c0aecc-9af8-484a-8d3a-9c3cb80ec1b2",
          "traceId": "6e0b728f",
          "type": "GENERATION",
          "name": "6e_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:59:06.588000+00:00",
          "endTime": "2026-02-04T10:59:18.993000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12405.0,
          "completionTokens": 283,
          "environment": "default",
          "totalTokens": 1614,
          "promptTokens": 1331,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "730945fc-52f4-41fc-b33b-a9d9c7f1f06c",
          "traceId": "6e0b728f",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:58:12.138000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:58:12.659Z",
      "updatedAt": "2026-02-04T10:59:27.855Z",
      "environment": "default"
    },
    {
      "id": "39980f24",
      "timestamp": "2026-02-04T10:55:57.104000+00:00",
      "name": "3998_model_converter",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import tensorflow as tf\n\n# Load the SavedModel\nmodel = tf.keras.models.load_model('path_to_your_saved_model')\n\n# Create a TFLite converter object\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set optimization flags\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define a generator function for calibration data\ndef representative_dataset_gen():\n    # Replace this with your actual dataset generation logic\n    for _ in range(100):\n        # Generate input data of the same shape as the model's input\n        # Make sure the data is of type FLOAT32\n        input_data = tf.random.normal([1, 224, 224, 3], dtype=tf.float32)\n        yield [input_data]\n\n# Set the representative dataset for calibration\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.int8  # or tf.uint8\nconverter.inference_output_type = tf.int8  # or tf.uint8\n\n# Convert the model to TFLite format\ntry:\n    tflite_model = converter.convert()\n    # Save the converted model to a file\n    with open('model.tflite', 'wb') as f:\n        f.write(tflite_model)\n    print(\"Model conversion successful!\")\nexcept Exception as e:\n    print(f\"Error during conversion: {e}\")",
        "last_error": "Failed to generate valid code after the max 5 attempts. Last error from code execution: 2026-02-04 12:57:43.021032: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:57:43.025779: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:57:43.041235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2026-02-04 12:57:43.064359: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2026-02-04 12:57:43.071430: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-02-04 12:57:43.087058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2026-02-04 12:57:43.991382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204125742.py/tmp_20260204125742_mc_qwen2.5-coder:32b/tmp_20260204125742_mc_qwen2.5-coder:32b.py\", line 4, in <module>\n    model = tf.keras.models.load_model('path_to_your_saved_model')\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_api.py\", line 206, in load_model\n    raise ValueError(\nValueError: File format not supported: filepath=path_to_your_saved_model. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(path_to_your_saved_model, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).\n"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 108.924,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-55-57-106827_chatcmpl-478fcb5a-02e0-4e3d-bc60-99160466a6a8",
          "traceId": "39980f24",
          "type": "GENERATION",
          "name": "39_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:55:57.106000+00:00",
          "endTime": "2026-02-04T10:56:10.434000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13328.0,
          "completionTokens": 336,
          "environment": "default",
          "totalTokens": 768,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-56-14-194836_chatcmpl-2f82407d-d5af-41f0-b1a7-7178cdb8f395",
          "traceId": "39980f24",
          "type": "GENERATION",
          "name": "39_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:56:14.194000+00:00",
          "endTime": "2026-02-04T10:56:35.991000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21797.0,
          "completionTokens": 476,
          "environment": "default",
          "totalTokens": 2524,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-56-39-511149_chatcmpl-5ce78b6a-296b-4b69-8bf4-a77d45e77b2d",
          "traceId": "39980f24",
          "type": "GENERATION",
          "name": "39_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:56:39.511000+00:00",
          "endTime": "2026-02-04T10:56:54.478000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14967.0,
          "completionTokens": 308,
          "environment": "default",
          "totalTokens": 2066,
          "promptTokens": 1758,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-56-58-019701_chatcmpl-503e80bc-7fbc-4f20-8fdd-68023976992b",
          "traceId": "39980f24",
          "type": "GENERATION",
          "name": "39_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:56:58.019000+00:00",
          "endTime": "2026-02-04T10:57:13.775000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15756.0,
          "completionTokens": 349,
          "environment": "default",
          "totalTokens": 1747,
          "promptTokens": 1398,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-57-17-380736_chatcmpl-aa116fd0-0d03-47f5-870e-81796f7a32f6",
          "traceId": "39980f24",
          "type": "GENERATION",
          "name": "39_mc_error_handling_attempt#5",
          "startTime": "2026-02-04T10:57:17.380000+00:00",
          "endTime": "2026-02-04T10:57:42.693000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25313.0,
          "completionTokens": 566,
          "environment": "default",
          "totalTokens": 2614,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "00301ad5-439e-4030-b469-5e8c62061b59",
          "traceId": "39980f24",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:55:57.104000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        },
        {
          "id": "e0225b64-2cef-4ca6-a9c0-5f6d604f83ad",
          "traceId": "39980f24",
          "type": "SPAN",
          "name": "error_39_mc_failure_signal_model_converter",
          "startTime": "2026-02-04T10:57:46.028000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Failed to generate valid code after the max 5 attempts. Last error from code execution: 2026-02-04 12:57:43.021032: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:57:43.025779: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:57:43.041235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2026-02-04 12:57:43.064359: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2026-02-04 12:57:43.071430: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-02-04 12:57:43.087058: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2026-02-04 12:57:43.991382: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204125742.py/tmp_20260204125742_mc_qwen2.5-coder:32b/tmp_20260204125742_mc_qwen2.5-coder:32b.py\", line 4, in <module>\n    model = tf.keras.models.load_model('path_to_your_saved_model')\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_api.py\", line 206, in load_model\n    raise ValueError(\nValueError: File format not supported: filepath=path_to_your_saved_model. Keras 3 only supports V3 `.keras` files and legacy H5 format files (`.h5` extension). Note that the legacy SavedModel format is not supported by `load_model()` in Keras 3. In order to reload a TensorFlow SavedModel as an inference-only layer in Keras 3, use `keras.layers.TFSMLayer(path_to_your_saved_model, call_endpoint='serving_default')` (note that your `call_endpoint` might have a different name).\n",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:55:57.625Z",
      "updatedAt": "2026-02-04T10:57:51.289Z",
      "environment": "default"
    },
    {
      "id": "731f3457",
      "timestamp": "2026-02-04T10:54:11.499000+00:00",
      "name": "731f_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define a generator function that provides representative dataset entries.\ndef representative_dataset_gen():\n    # Generate a random input data of the same shape and type as used when training.\n    for _ in range(100):  # Adjust the number of samples as needed\n        input_data = np.random.rand(1, 224, 224, 3).astype(np.float32)  # Example shape, adjust as needed\n        yield [input_data]\n\n# Define paths\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\n# Load your Keras model\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Convert the Keras model to a TensorFlow Lite model using post-training quantization.\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset_gen\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]  # Set the target operations to INT8\n\n# Define inference input and output types\nconverter.inference_input_type = tf.float32\nconverter.inference_output_type = tf.uint8\n\n# Convert the model\nquantized_tflite_model = converter.convert()\n\n# Save the quantized TFLite model to a file\nwith open(converted_model_path, \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 75.069,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-54-11-503651_chatcmpl-791e01a7-e840-4c30-b428-938753e38467",
          "traceId": "731f3457",
          "type": "GENERATION",
          "name": "73_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:54:11.503000+00:00",
          "endTime": "2026-02-04T10:54:23.685000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12182.0,
          "completionTokens": 303,
          "environment": "default",
          "totalTokens": 735,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-54-27-477250_chatcmpl-1e984101-a800-410f-89b0-97a0a32de04d",
          "traceId": "731f3457",
          "type": "GENERATION",
          "name": "73_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:54:27.477000+00:00",
          "endTime": "2026-02-04T10:54:49.682000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22205.0,
          "completionTokens": 487,
          "environment": "default",
          "totalTokens": 2535,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-54-52-889662_chatcmpl-b4ddb0fd-fb64-4729-991a-634b3cbb49b4",
          "traceId": "731f3457",
          "type": "GENERATION",
          "name": "73_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:54:52.889000+00:00",
          "endTime": "2026-02-04T10:55:07.906000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15017.0,
          "completionTokens": 300,
          "environment": "default",
          "totalTokens": 2082,
          "promptTokens": 1782,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-55-11-078230_chatcmpl-0d41fa67-3ea4-4040-8536-046a1302af95",
          "traceId": "731f3457",
          "type": "GENERATION",
          "name": "73_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:55:11.078000+00:00",
          "endTime": "2026-02-04T10:55:26.569000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15491.0,
          "completionTokens": 342,
          "environment": "default",
          "totalTokens": 1732,
          "promptTokens": 1390,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "b9756b9a-1bdc-484f-ac41-7f31399ba2a8",
          "traceId": "731f3457",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:54:11.500000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:54:12.022Z",
      "updatedAt": "2026-02-04T10:55:36.174Z",
      "environment": "default"
    },
    {
      "id": "d1e279d7",
      "timestamp": "2026-02-04T10:52:21.644000+00:00",
      "name": "d1e2_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define input shape\ninput_shape = (1, 224, 224, 3)  # Example input shape for an image model\n\n# Load your Keras model\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Convert the Keras model to a TensorFlow Lite converter object\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Define a representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):  # You can adjust the number of samples as needed\n        yield [np.array(np.random.random_sample(input_shape), dtype=np.float32)]\n\n# Set the optimization flag to optimize for size (quantization)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32\nconverter.inference_output_type = tf.uint8\n\n# Provide a representative dataset generator to ensure we quantize correctly\nconverter.representative_dataset = representative_dataset_gen\n\n# Convert the model and save it to disk\ntflite_quant_model = converter.convert()\n\n# Save the TFLite model to a file\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 78.41,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-52-21-646400_chatcmpl-c8ba4fe4-a0ff-4769-84f2-156b0cf3d52a",
          "traceId": "d1e279d7",
          "type": "GENERATION",
          "name": "d1_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:52:21.646000+00:00",
          "endTime": "2026-02-04T10:52:34.419000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12773.0,
          "completionTokens": 323,
          "environment": "default",
          "totalTokens": 755,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-52-38-087961_chatcmpl-03cea6b8-5b88-47ca-aa04-c72e32b59442",
          "traceId": "d1e279d7",
          "type": "GENERATION",
          "name": "d1_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:52:38.087000+00:00",
          "endTime": "2026-02-04T10:53:03.789000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25702.0,
          "completionTokens": 578,
          "environment": "default",
          "totalTokens": 2626,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-53-07-034533_chatcmpl-db29cb31-cba0-4913-b680-431ae9aa7fc4",
          "traceId": "d1e279d7",
          "type": "GENERATION",
          "name": "d1_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:53:07.034000+00:00",
          "endTime": "2026-02-04T10:53:21.626000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14592.0,
          "completionTokens": 311,
          "environment": "default",
          "totalTokens": 1886,
          "promptTokens": 1575,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-53-24-994116_chatcmpl-9124fccb-5c59-44b7-a7cb-04f4777d5de8",
          "traceId": "d1e279d7",
          "type": "GENERATION",
          "name": "d1_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:53:24.994000+00:00",
          "endTime": "2026-02-04T10:53:40.054000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15060.0,
          "completionTokens": 349,
          "environment": "default",
          "totalTokens": 1749,
          "promptTokens": 1400,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "4cbb6491-badf-4b46-94f5-6778c7ddaffc",
          "traceId": "d1e279d7",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:52:21.644000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:52:22.163Z",
      "updatedAt": "2026-02-04T10:53:50.360Z",
      "environment": "default"
    },
    {
      "id": "429c195f",
      "timestamp": "2026-02-04T10:50:29.090000+00:00",
      "name": "429c_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define the input shape\ninput_shape = (1, 224, 224, 3)  # Example input shape for an image model\n\n# Load your pre-trained Keras model\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Convert the Keras model to a TensorFlow Lite model\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set post-training quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define a generator function for representative dataset\ndef representative_dataset_gen():\n    for _ in range(100):  # Number of samples to use for calibration\n        yield [np.array(np.random.random_sample(input_shape), dtype=np.float32)]\n\n# Set the representative dataset\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, they will remain as float\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # or tf.uint8\nconverter.inference_output_type = tf.uint8\n\n# Convert the model to a TensorFlow Lite model with post-training quantization\ntflite_model = converter.convert()\n\n# Save the TFLite model to file\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\nwith open(converted_model_path, \"wb\") as f:\n    f.write(tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 82.346,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-50-29-093235_chatcmpl-8e2ccf16-20e7-472e-97c6-b798c083cfc9",
          "traceId": "429c195f",
          "type": "GENERATION",
          "name": "42_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:50:29.093000+00:00",
          "endTime": "2026-02-04T10:50:42.480000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13387.0,
          "completionTokens": 340,
          "environment": "default",
          "totalTokens": 772,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-50-46-280991_chatcmpl-443bdeef-ec8f-4732-8f6a-1f662cb8eaa3",
          "traceId": "429c195f",
          "type": "GENERATION",
          "name": "42_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:50:46.280000+00:00",
          "endTime": "2026-02-04T10:51:14.272000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 27992.0,
          "completionTokens": 630,
          "environment": "default",
          "totalTokens": 2678,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-51-17-517703_chatcmpl-f657179a-caa0-447a-8b19-cdb48c4985e1",
          "traceId": "429c195f",
          "type": "GENERATION",
          "name": "42_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:51:17.517000+00:00",
          "endTime": "2026-02-04T10:51:32.525000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15008.0,
          "completionTokens": 307,
          "environment": "default",
          "totalTokens": 2095,
          "promptTokens": 1788,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-51-35-824538_chatcmpl-498ec69f-b976-431c-a9f7-a4726c16b89b",
          "traceId": "429c195f",
          "type": "GENERATION",
          "name": "42_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:51:35.824000+00:00",
          "endTime": "2026-02-04T10:51:51.437000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15613.0,
          "completionTokens": 345,
          "environment": "default",
          "totalTokens": 1741,
          "promptTokens": 1396,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "c20680f3-232b-49a9-9e43-14f6d71dffe2",
          "traceId": "429c195f",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:50:29.091000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:50:29.609Z",
      "updatedAt": "2026-02-04T10:52:01.271Z",
      "environment": "default"
    },
    {
      "id": "c9654c0e",
      "timestamp": "2026-02-04T10:48:46.289000+00:00",
      "name": "c965_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define input shape\ninput_shape = (1, 224, 224, 3)  # Example input shape for an image model\n\n# Function to generate representative dataset for quantization\ndef representative_dataset_gen():\n    for _ in range(100):\n        yield [np.array(np.random.random_sample(input_shape), dtype=np.float32)]\n\n# Load your trained Keras model\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Convert the model to a TFLite model with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # Input data type should match model's input\nconverter.inference_output_type = tf.uint8\n\n# Convert and save the model to a TFLite file\ntflite_quant_model = converter.convert()\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 71.445,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-48-46-291827_chatcmpl-6bf75d5d-cbe1-4c45-a999-97807cfbf01d",
          "traceId": "c9654c0e",
          "type": "GENERATION",
          "name": "c9_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:48:46.291000+00:00",
          "endTime": "2026-02-04T10:48:57.904000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11613.0,
          "completionTokens": 287,
          "environment": "default",
          "totalTokens": 719,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-49-01-987123_chatcmpl-f80d5f2c-205b-41e9-bb43-e74e8c7213d6",
          "traceId": "c9654c0e",
          "type": "GENERATION",
          "name": "c9_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:49:01.987000+00:00",
          "endTime": "2026-02-04T10:49:23.699000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 21712.0,
          "completionTokens": 476,
          "environment": "default",
          "totalTokens": 2524,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-49-26-862409_chatcmpl-c3444735-bbf4-44aa-95d8-aff9b048a0df",
          "traceId": "c9654c0e",
          "type": "GENERATION",
          "name": "c9_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:49:26.862000+00:00",
          "endTime": "2026-02-04T10:49:40.657000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13795.0,
          "completionTokens": 279,
          "environment": "default",
          "totalTokens": 2033,
          "promptTokens": 1754,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-49-43-887438_chatcmpl-5a7e52f2-647e-45d0-9f05-309418708318",
          "traceId": "c9654c0e",
          "type": "GENERATION",
          "name": "c9_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:49:43.887000+00:00",
          "endTime": "2026-02-04T10:49:57.735000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13848.0,
          "completionTokens": 317,
          "environment": "default",
          "totalTokens": 1686,
          "promptTokens": 1369,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "a47883dc-76ca-43d7-a45e-63cbd32b27ea",
          "traceId": "c9654c0e",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:48:46.290000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:48:46.807Z",
      "updatedAt": "2026-02-04T10:50:07.942Z",
      "environment": "default"
    },
    {
      "id": "4679f179",
      "timestamp": "2026-02-04T10:47:15.391000+00:00",
      "name": "4679_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define a function to generate representative data for calibration\ndef representative_dataset_gen():\n    # Generate random input data that matches the input shape and type of your model\n    for _ in range(100):  # Adjust the number of samples as needed\n        yield [np.array(np.random.random_sample((1, 224, 224, 3)), dtype=np.float32)]\n\n# Load the TensorFlow SavedModel\nsaved_model_dir = \"models/fruit_to_emoji/og_model/model.keras\"  # Corrected path to the saved model\nconverter = tf.lite.TFLiteConverter.from_keras_model(tf.keras.models.load_model(saved_model_dir))\n\n# Set the optimization flag to optimize for size (quantization)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define a generator function that provides representative dataset examples.\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # Input type as float32\nconverter.inference_output_type = tf.uint8   # Output type as uint8\n\n# Convert the model to the TensorFlow Lite format with quantization\ntflite_model_quant = converter.convert()\n\n# Save the converted TFLite model\nwith open('models/fruit_to_emoji/tflite_model/model_quant.tflite', 'wb') as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 60.038,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-47-15-393347_chatcmpl-f915a46e-4825-4fb9-9ce9-f947d0aa52bf",
          "traceId": "4679f179",
          "type": "GENERATION",
          "name": "46_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:47:15.393000+00:00",
          "endTime": "2026-02-04T10:47:29.219000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13826.0,
          "completionTokens": 352,
          "environment": "default",
          "totalTokens": 784,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-47-32-872880_chatcmpl-b3adf612-cf28-4a40-af0b-8f48863efdd1",
          "traceId": "4679f179",
          "type": "GENERATION",
          "name": "46_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:47:32.872000+00:00",
          "endTime": "2026-02-04T10:47:55.886000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 23014.0,
          "completionTokens": 508,
          "environment": "default",
          "totalTokens": 2556,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-47-59-047573_chatcmpl-53902e19-b445-4957-96eb-9cc4ad9d6101",
          "traceId": "4679f179",
          "type": "GENERATION",
          "name": "46_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:47:59.047000+00:00",
          "endTime": "2026-02-04T10:48:15.429000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16382.0,
          "completionTokens": 344,
          "environment": "default",
          "totalTokens": 2050,
          "promptTokens": 1706,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "294637e5-aa8f-4d47-842d-762baf7e296f",
          "traceId": "4679f179",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:47:15.391000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:47:15.909Z",
      "updatedAt": "2026-02-04T10:48:25.049Z",
      "environment": "default"
    },
    {
      "id": "6b9f2285",
      "timestamp": "2026-02-04T10:45:30.545000+00:00",
      "name": "6b9f_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define a function to generate representative data for quantization\ndef representative_data_gen():\n    # Generate random data with shape (1, 224, 224, 3)\n    data = np.random.rand(1, 224, 224, 3)\n    yield [data.astype(np.float32)]\n\n# Predefined paths\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\n# Load your trained Keras model\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Convert the Keras model to a TensorFlow Lite model with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Set the input and output tensors to int8 (only in static range quantization)\nconverter.inference_input_type = tf.float32  # input datatype is float32\nconverter.inference_output_type = tf.uint8   # output datatype is uint8\n\n# Convert the model\nquantized_tflite_model = converter.convert()\n\n# Save the quantized TFLite model to a file\nwith open(converted_model_path, 'wb') as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 74.706,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-45-30-547334_chatcmpl-246b784e-9b31-4e22-9eaf-b0cab53bf1d9",
          "traceId": "6b9f2285",
          "type": "GENERATION",
          "name": "6b_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:45:30.547000+00:00",
          "endTime": "2026-02-04T10:45:44.677000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14130.0,
          "completionTokens": 370,
          "environment": "default",
          "totalTokens": 802,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-45-48-403712_chatcmpl-d3165cd8-bc7c-4152-bfb0-f33c2e3aa6f6",
          "traceId": "6b9f2285",
          "type": "GENERATION",
          "name": "6b_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:45:48.403000+00:00",
          "endTime": "2026-02-04T10:46:07.728000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19325.0,
          "completionTokens": 415,
          "environment": "default",
          "totalTokens": 2463,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-46-11-292942_chatcmpl-f0c453a7-5abd-44e9-b95e-89ca77be07ea",
          "traceId": "6b9f2285",
          "type": "GENERATION",
          "name": "6b_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:46:11.292000+00:00",
          "endTime": "2026-02-04T10:46:26.189000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14897.0,
          "completionTokens": 307,
          "environment": "default",
          "totalTokens": 2085,
          "promptTokens": 1778,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-46-29-589841_chatcmpl-7e4e313d-a9fb-439c-a1ac-f572459b6ba4",
          "traceId": "6b9f2285",
          "type": "GENERATION",
          "name": "6b_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:46:29.589000+00:00",
          "endTime": "2026-02-04T10:46:45.252000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15663.0,
          "completionTokens": 349,
          "environment": "default",
          "totalTokens": 1746,
          "promptTokens": 1397,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "7dc0bf80-d622-45b5-9c62-efc13f0208ca",
          "traceId": "6b9f2285",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:45:30.546000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:45:31.068Z",
      "updatedAt": "2026-02-04T10:46:54.206Z",
      "environment": "default"
    },
    {
      "id": "de71e729",
      "timestamp": "2026-02-04T10:44:47+00:00",
      "name": "de71_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Define a representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):\n        # Generate input data similar to your training data\n        # Replace this with actual data generation logic if necessary\n        input_data = tf.random.uniform(shape=[1, 224, 224, 3], minval=0.0, maxval=1.0, dtype=tf.float32)\n        yield [input_data]\n\n# Convert the model to TFLite with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset_gen\nconverter.target_spec.supported_ops = supported_ops\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert and save the model\nquantized_tflite_model = converter.convert()\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 12.775,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-44-47-002979_chatcmpl-fae30946-39ef-4edb-afb5-de70a624d956",
          "traceId": "de71e729",
          "type": "GENERATION",
          "name": "de_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:44:47.002000+00:00",
          "endTime": "2026-02-04T10:44:59.776000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12774.0,
          "completionTokens": 319,
          "environment": "default",
          "totalTokens": 751,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "0050eebe-a470-4f2b-961e-f1dedc8a0380",
          "traceId": "de71e729",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:44:47.001000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:44:47.520Z",
      "updatedAt": "2026-02-04T10:45:09.707Z",
      "environment": "default"
    },
    {
      "id": "cbb0ebee",
      "timestamp": "2026-02-04T10:42:30.233000+00:00",
      "name": "cbb0_model_converter",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import tensorflow as tf\n\n# Assuming load_test_dataset is defined somewhere in your script or imported\ndef load_test_dataset():\n    # This is a placeholder. Replace with actual dataset loading logic.\n    # For example, you might be using something like:\n    # return tf.data.Dataset.from_tensor_slices((images, labels))\n    pass\n\n# Load the test dataset\ntest_dataset = load_test_dataset()\n\n# Define a representative dataset generator function\ndef representative_data_gen():\n    for input_value in test_dataset.take(100):  # Use a subset of the dataset for calibration\n        yield [input_value]\n\n# Load your Keras model\nmodel = tf.keras.models.load_model('path_to_your_keras_model.h5')\n\n# Convert the Keras model to a TensorFlow Lite model with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Ensure that if any ops can't be quantized, they are still converted\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,  # Enable TensorFlow Lite ops.\n    tf.lite.OpsSet.SELECT_TF_OPS       # Enable TensorFlow ops.\n]\n\n# Set the input and output tensors to int8 (optional)\nconverter.inference_input_type = tf.int8\nconverter.inference_output_type = tf.int8\n\ntflite_model = converter.convert()\n\n# Save the TFLite model\nwith open('converted_model.tflite', 'wb') as f:\n    f.write(tflite_model)\n\nprint(\"TFLite model conversion completed and saved.\")",
        "last_error": "Failed to generate valid code after the max 5 attempts. Last error from code execution: 2026-02-04 12:44:18.126937: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:44:18.130595: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:44:18.142016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2026-02-04 12:44:18.161025: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2026-02-04 12:44:18.167304: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-02-04 12:44:18.181128: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2026-02-04 12:44:19.082763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204124417.py/tmp_20260204124417_mc_qwen2.5-coder:32b/tmp_20260204124417_mc_qwen2.5-coder:32b.py\", line 19, in <module>\n    model = tf.keras.models.load_model('path_to_your_keras_model.h5')\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_api.py\", line 196, in load_model\n    return legacy_h5_format.load_model_from_hdf5(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/legacy/saving/legacy_h5_format.py\", line 116, in load_model_from_hdf5\n    f = h5py.File(filepath, mode=\"r\")\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/h5py/_hl/files.py\", line 561, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n    fid = h5f.open(name, flags, fapl=fapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\nFileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'path_to_your_keras_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 110.877,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-42-30-235779_chatcmpl-6401a394-c542-4190-a51b-c74886d0b650",
          "traceId": "cbb0ebee",
          "type": "GENERATION",
          "name": "cb_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:42:30.235000+00:00",
          "endTime": "2026-02-04T10:42:40.638000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10403.0,
          "completionTokens": 272,
          "environment": "default",
          "totalTokens": 704,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-42-44-539386_chatcmpl-c629e9bf-c89c-4be5-8d6f-e4bc35b5b6ac",
          "traceId": "cbb0ebee",
          "type": "GENERATION",
          "name": "cb_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:42:44.539000+00:00",
          "endTime": "2026-02-04T10:43:10.908000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26369.0,
          "completionTokens": 590,
          "environment": "default",
          "totalTokens": 2638,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-43-14-426423_chatcmpl-12c87857-5524-4755-ad9b-7e2bce7806f6",
          "traceId": "cbb0ebee",
          "type": "GENERATION",
          "name": "cb_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:43:14.426000+00:00",
          "endTime": "2026-02-04T10:43:29.460000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15034.0,
          "completionTokens": 309,
          "environment": "default",
          "totalTokens": 2131,
          "promptTokens": 1822,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-43-32-563323_chatcmpl-bf0b16ea-3fb1-4ea5-a5cd-85fbe210212e",
          "traceId": "cbb0ebee",
          "type": "GENERATION",
          "name": "cb_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:43:32.563000+00:00",
          "endTime": "2026-02-04T10:43:48.383000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15820.0,
          "completionTokens": 352,
          "environment": "default",
          "totalTokens": 1751,
          "promptTokens": 1399,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-43-52-157746_chatcmpl-680d0fb2-70e0-48d0-a05e-bbfe92590891",
          "traceId": "cbb0ebee",
          "type": "GENERATION",
          "name": "cb_mc_error_handling_attempt#5",
          "startTime": "2026-02-04T10:43:52.157000+00:00",
          "endTime": "2026-02-04T10:44:17.855000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 25698.0,
          "completionTokens": 578,
          "environment": "default",
          "totalTokens": 2626,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "393a3e34-e91e-4b44-9ff0-1773ea76d705",
          "traceId": "cbb0ebee",
          "type": "SPAN",
          "name": "error_cb_mc_failure_signal_model_converter",
          "startTime": "2026-02-04T10:44:21.111000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Failed to generate valid code after the max 5 attempts. Last error from code execution: 2026-02-04 12:44:18.126937: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:44:18.130595: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:44:18.142016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2026-02-04 12:44:18.161025: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2026-02-04 12:44:18.167304: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-02-04 12:44:18.181128: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2026-02-04 12:44:19.082763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204124417.py/tmp_20260204124417_mc_qwen2.5-coder:32b/tmp_20260204124417_mc_qwen2.5-coder:32b.py\", line 19, in <module>\n    model = tf.keras.models.load_model('path_to_your_keras_model.h5')\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_api.py\", line 196, in load_model\n    return legacy_h5_format.load_model_from_hdf5(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/legacy/saving/legacy_h5_format.py\", line 116, in load_model_from_hdf5\n    f = h5py.File(filepath, mode=\"r\")\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/h5py/_hl/files.py\", line 561, in __init__\n    fid = make_fid(name, mode, userblock_size, fapl, fcpl, swmr=swmr)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/h5py/_hl/files.py\", line 235, in make_fid\n    fid = h5f.open(name, flags, fapl=fapl)\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"h5py/h5f.pyx\", line 102, in h5py.h5f.open\nFileNotFoundError: [Errno 2] Unable to synchronously open file (unable to open file: name = 'path_to_your_keras_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)\n",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        },
        {
          "id": "4cdab470-d50d-46cf-98da-0267137a99d4",
          "traceId": "cbb0ebee",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:42:30.234000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:42:30.753Z",
      "updatedAt": "2026-02-04T10:44:26.447Z",
      "environment": "default"
    },
    {
      "id": "eac29909",
      "timestamp": "2026-02-04T10:41:46.636000+00:00",
      "name": "eac2_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Convert the model to a TensorFlow Lite model with 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\nconverter.target_spec.supported_ops = supported_ops\n\n# Define a representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):\n        # Replace with actual input data generation logic\n        yield [tf.random.uniform(shape=(1, 224, 224, 3), minval=0.0, maxval=1.0, dtype=tf.float32)]\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, they are still converted to TFLite\nconverter.target_spec.supported_types = [tf.int8]\n\n# Convert and save the model\ntflite_model = converter.convert()\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 13.474,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-41-46-638972_chatcmpl-1054f353-b531-438d-a6d8-97d680d57b9f",
          "traceId": "eac29909",
          "type": "GENERATION",
          "name": "ea_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:41:46.638000+00:00",
          "endTime": "2026-02-04T10:42:00.111000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13473.0,
          "completionTokens": 341,
          "environment": "default",
          "totalTokens": 773,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "ce1d05f4-9a44-4645-a182-10db2e393493",
          "traceId": "eac29909",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:41:46.637000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:41:47.157Z",
      "updatedAt": "2026-02-04T10:42:09.818Z",
      "environment": "default"
    },
    {
      "id": "554a8e3d",
      "timestamp": "2026-02-04T10:39:59.039000+00:00",
      "name": "554a_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define paths\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\n# Load the original Keras model from the specified path\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Define the converter\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set quantization parameters\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\ndef representative_data_gen():\n    for _ in range(100):\n        # Generate input data with a size of (1, 32, 32, 3)\n        yield [np.array(np.random.random_sample((1, 32, 32, 3)), dtype=np.float32)]\n\n# Set the representative dataset\nconverter.representative_dataset = representative_data_gen\n\n# Define input and output types for quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\n\n# Ensure that if any ops can't be quantized, they are still converted to TFLite\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Set the input and output tensors to int8 (optional)\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Set supported operations for quantization\nconverter.target_spec.supported_ops = supported_ops\n\n# Convert the model\ntflite_model = converter.convert()\n\n# Save the TFLite model to the specified path\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 78.125,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-39-59-041939_chatcmpl-3eebedf9-d5d7-453e-9d06-8820ad856984",
          "traceId": "554a8e3d",
          "type": "GENERATION",
          "name": "55_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:39:59.041000+00:00",
          "endTime": "2026-02-04T10:40:12.506000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13465.0,
          "completionTokens": 341,
          "environment": "default",
          "totalTokens": 773,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-40-16-264089_chatcmpl-2fa3195a-12bf-4e8b-8242-4006b0eb7edb",
          "traceId": "554a8e3d",
          "type": "GENERATION",
          "name": "55_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:40:16.264000+00:00",
          "endTime": "2026-02-04T10:40:38.599000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 22335.0,
          "completionTokens": 493,
          "environment": "default",
          "totalTokens": 2541,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-40-41-872961_chatcmpl-7cc452ac-6a19-425f-ace3-e2b0f721d424",
          "traceId": "554a8e3d",
          "type": "GENERATION",
          "name": "55_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:40:41.872000+00:00",
          "endTime": "2026-02-04T10:40:57.497000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15625.0,
          "completionTokens": 326,
          "environment": "default",
          "totalTokens": 2109,
          "promptTokens": 1783,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-41-00-706214_chatcmpl-8877d59a-4b9d-4ab9-9d41-7b581d841816",
          "traceId": "554a8e3d",
          "type": "GENERATION",
          "name": "55_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:41:00.706000+00:00",
          "endTime": "2026-02-04T10:41:17.165000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16459.0,
          "completionTokens": 368,
          "environment": "default",
          "totalTokens": 1783,
          "promptTokens": 1415,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "c598039b-996b-4de9-93c6-0870e6d84154",
          "traceId": "554a8e3d",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:39:59.040000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:39:59.559Z",
      "updatedAt": "2026-02-04T10:41:26.250Z",
      "environment": "default"
    },
    {
      "id": "2981e3f9",
      "timestamp": "2026-02-04T10:38:19.204000+00:00",
      "name": "2981_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Assuming representative_dataset_gen and other necessary functions are defined here\n\ndef representative_dataset_gen():\n    for input_sample in range(10):  # Example loop, replace with actual data source\n        yield [np.array(input_sample, dtype=np.float32)]  # Adjust dtype as needed\n\n# Load your Keras model\nkeras_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n\n# Set the optimization flag to optimize for size and enable quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # Input datatype as specified\nconverter.inference_output_type = tf.uint8   # Output datatype as specified\n\n# Define a generator function that provides representative dataset examples.\nconverter.representative_dataset = representative_dataset_gen\n\n# Convert the model to the TensorFlow Lite format with quantization\ntflite_model_quant = converter.convert()\n\n# Save the converted model to a file\nwith open('models/fruit_to_emoji/tflite_model/model_quant.tflite', 'wb') as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 69.929,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-38-19-206709_chatcmpl-f9f0012c-e299-443d-a701-f0cf33e4451b",
          "traceId": "2981e3f9",
          "type": "GENERATION",
          "name": "29_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:38:19.206000+00:00",
          "endTime": "2026-02-04T10:38:31.099000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11893.0,
          "completionTokens": 300,
          "environment": "default",
          "totalTokens": 732,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-38-35-081305_chatcmpl-b5123265-3a56-4eee-9fd6-5ce8a40a5566",
          "traceId": "2981e3f9",
          "type": "GENERATION",
          "name": "29_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:38:35.081000+00:00",
          "endTime": "2026-02-04T10:38:53.964000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18883.0,
          "completionTokens": 397,
          "environment": "default",
          "totalTokens": 2445,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-38-57-172186_chatcmpl-9a293185-c460-4404-8540-863435eae623",
          "traceId": "2981e3f9",
          "type": "GENERATION",
          "name": "29_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:38:57.172000+00:00",
          "endTime": "2026-02-04T10:39:11.729000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14557.0,
          "completionTokens": 301,
          "environment": "default",
          "totalTokens": 1971,
          "promptTokens": 1670,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-39-15-017884_chatcmpl-641574df-66a4-414c-8400-55da6c254f86",
          "traceId": "2981e3f9",
          "type": "GENERATION",
          "name": "29_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:39:15.017000+00:00",
          "endTime": "2026-02-04T10:39:29.134000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14117.0,
          "completionTokens": 308,
          "environment": "default",
          "totalTokens": 1998,
          "promptTokens": 1690,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "028ed484-9702-43b8-91d3-3f2c294fd502",
          "traceId": "2981e3f9",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:38:19.205000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:38:19.724Z",
      "updatedAt": "2026-02-04T10:39:38.349Z",
      "environment": "default"
    },
    {
      "id": "6134020d",
      "timestamp": "2026-02-04T10:36:32.294000+00:00",
      "name": "6134_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Assuming representative_dataset_gen and other required functions are defined here\ndef representative_dataset_gen():\n    # Example input data. Replace this with your actual data generation logic.\n    for _ in range(100):\n        input_sample = [0.5]  # Example input sample\n        yield [np.array(input_sample, dtype=np.float32)]\n\n# Define paths\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\n# Load your model\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Convert the model to TFLite format\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set the optimization flag to optimize for size and enable quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define a generator function that provides a representative dataset\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, they will still be converted to TFLite\nconverter.target_spec.supported_ops = [\n    tf.lite.OpsSet.TFLITE_BUILTINS_INT8,  # Enable TensorFlow Lite built-in int8 ops.\n]\n\n# Set the input and output tensors to uint8 (APIs added in r2.3)\nconverter.inference_input_type = tf.uint8\nconverter.inference_output_type = tf.uint8\n\n# Convert the model to TFLite format\ntflite_model_quant = converter.convert()\n\n# Save the quantized TFLite model to a file\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 76.936,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-36-32-296572_chatcmpl-03745565-a280-40b2-b771-bc529e3e5c07",
          "traceId": "6134020d",
          "type": "GENERATION",
          "name": "61_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:36:32.296000+00:00",
          "endTime": "2026-02-04T10:36:43.368000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11072.0,
          "completionTokens": 273,
          "environment": "default",
          "totalTokens": 705,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-36-47-481405_chatcmpl-c418548f-2fb5-4528-a8a4-2d375f9ab8ab",
          "traceId": "6134020d",
          "type": "GENERATION",
          "name": "61_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:36:47.481000+00:00",
          "endTime": "2026-02-04T10:37:11.527000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24046.0,
          "completionTokens": 536,
          "environment": "default",
          "totalTokens": 2584,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-37-15-017082_chatcmpl-3c463c52-01b7-4320-a5ef-237d5e5fdc90",
          "traceId": "6134020d",
          "type": "GENERATION",
          "name": "61_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:37:15.017000+00:00",
          "endTime": "2026-02-04T10:37:30.253000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15236.0,
          "completionTokens": 328,
          "environment": "default",
          "totalTokens": 1913,
          "promptTokens": 1585,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-37-33-415403_chatcmpl-224528ed-2eef-45f2-a255-b085d0969b62",
          "traceId": "6134020d",
          "type": "GENERATION",
          "name": "61_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:37:33.415000+00:00",
          "endTime": "2026-02-04T10:37:49.230000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15815.0,
          "completionTokens": 370,
          "environment": "default",
          "totalTokens": 1788,
          "promptTokens": 1418,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "a2950b31-cbc5-4ae0-9acc-8a8efee66da8",
          "traceId": "6134020d",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:36:32.294000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:36:32.815Z",
      "updatedAt": "2026-02-04T10:37:58.472Z",
      "environment": "default"
    },
    {
      "id": "0d3b5398",
      "timestamp": "2026-02-04T10:34:54.545000+00:00",
      "name": "0d3b_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Assuming you have a trained Keras model saved as 'model.keras'\nmodel = tf.keras.models.load_model('models/fruit_to_emoji/og_model/model.keras')\n\n# Define a representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):  # Generate 100 samples\n        data = np.random.rand(1, 224, 224, 3).astype(np.float32)  # Random input data with shape (1, 224, 224, 3)\n        yield [data]  # Yield each sample as a list\n\n# Convert the Keras model to a TensorFlow Lite model\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set the optimization flag to optimize for size (quantization)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Define input and output tensors for quantization\ninference_input_type = tf.int8  # or tf.uint8\ninference_output_type = tf.int8  # or tf.uint8\n\n# Set the input and output tensors to int8 (optional)\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Provide a representative dataset to ensure we quantize correctly.\nconverter.representative_dataset = representative_dataset_gen\n\n# Convert the model to the TensorFlow Lite format with post-training quantization\ntflite_model = converter.convert()\n\n# Save the converted model to a file\nwith open('models/fruit_to_emoji/tflite_model/model_int8.tflite', 'wb') as f:\n    f.write(tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 66.962,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-34-54-547080_chatcmpl-c1d7749f-ac40-46fe-bffe-d8f35d5425c7",
          "traceId": "0d3b5398",
          "type": "GENERATION",
          "name": "0d_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:34:54.547000+00:00",
          "endTime": "2026-02-04T10:35:05.804000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11257.0,
          "completionTokens": 287,
          "environment": "default",
          "totalTokens": 719,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-35-09-959038_chatcmpl-9d40fa48-727a-4ecd-93b5-7eb2fea5e952",
          "traceId": "0d3b5398",
          "type": "GENERATION",
          "name": "0d_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:35:09.959000+00:00",
          "endTime": "2026-02-04T10:35:39.538000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29579.0,
          "completionTokens": 671,
          "environment": "default",
          "totalTokens": 2719,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-35-42-805051_chatcmpl-6aaf1dd2-1383-4cc4-8c97-4565b4a84d62",
          "traceId": "0d3b5398",
          "type": "GENERATION",
          "name": "0d_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:35:42.805000+00:00",
          "endTime": "2026-02-04T10:36:01.507000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 18702.0,
          "completionTokens": 402,
          "environment": "default",
          "totalTokens": 2215,
          "promptTokens": 1813,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "f0e487a8-d43a-44a7-8f49-c1ff84273b9a",
          "traceId": "0d3b5398",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:34:54.545000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:34:55.068Z",
      "updatedAt": "2026-02-04T10:36:11.265Z",
      "environment": "default"
    },
    {
      "id": "b43e84dc",
      "timestamp": "2026-02-04T10:34:10.810000+00:00",
      "name": "b43e_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Define a representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):\n        # Replace with actual input data shape and type\n        yield [tf.random.uniform(shape=(1, 224, 224, 3), minval=0, maxval=255, dtype=tf.float32)]\n\n# Create a converter object from the original model\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Set conversion parameters for quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset_gen\nconverter.target_spec.supported_ops = supported_ops\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Convert the model to TFLite format with quantization\nquantized_tflite_model = converter.convert()\n\n# Save the converted model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 12.696,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-34-10-812826_chatcmpl-cc725480-0e20-483f-966f-f450a1637440",
          "traceId": "b43e84dc",
          "type": "GENERATION",
          "name": "b4_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:34:10.812000+00:00",
          "endTime": "2026-02-04T10:34:23.507000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12695.0,
          "completionTokens": 319,
          "environment": "default",
          "totalTokens": 751,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "1f5a9154-edfa-4cae-a867-1d22694dda8f",
          "traceId": "b43e84dc",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:34:10.811000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:34:11.332Z",
      "updatedAt": "2026-02-04T10:34:33.497Z",
      "environment": "default"
    },
    {
      "id": "f33c7691",
      "timestamp": "2026-02-04T10:32:26.203000+00:00",
      "name": "f33c_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define input shape for demonstration purposes.\ninput_shape = (1, 224, 224, 3)\n\ndef representative_dataset_gen():\n    for _ in range(100):\n        yield [np.array(np.random.random_sample(input_shape), dtype=np.float32)]\n\n# Load your Keras model\noriginal_model_path=\"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path=\"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\nkeras_model = tf.keras.models.load_model(original_model_path)\n\n# Convert the Keras model to a TensorFlow Lite model\nconverter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n\n# Set the optimization flag.\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Define a generator function that provides representative dataset examples.\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32\nconverter.inference_output_type = tf.uint8\n\n# Convert the model to the TensorFlow Lite format with quantization\ntflite_model = converter.convert()\n\n# Save the quantized TFLite model\nwith open(converted_model_path, 'wb') as f:\n    f.write(tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 73.811,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-32-26-205324_chatcmpl-114b8286-6855-4498-b695-488451e44fc7",
          "traceId": "f33c7691",
          "type": "GENERATION",
          "name": "f3_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:32:26.205000+00:00",
          "endTime": "2026-02-04T10:32:37.052000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10847.0,
          "completionTokens": 280,
          "environment": "default",
          "totalTokens": 712,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-32-40-899432_chatcmpl-5cf91456-1596-4a46-b602-cf57e6cb9ee4",
          "traceId": "f33c7691",
          "type": "GENERATION",
          "name": "f3_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:32:40.899000+00:00",
          "endTime": "2026-02-04T10:33:05.380000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 24481.0,
          "completionTokens": 532,
          "environment": "default",
          "totalTokens": 2580,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-33-08-481005_chatcmpl-d4d5816d-55e6-4cea-9121-c0a1f6fc2007",
          "traceId": "f33c7691",
          "type": "GENERATION",
          "name": "f3_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:33:08.481000+00:00",
          "endTime": "2026-02-04T10:33:22.449000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13968.0,
          "completionTokens": 278,
          "environment": "default",
          "totalTokens": 2074,
          "promptTokens": 1796,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-33-25-683096_chatcmpl-cbbe99c0-1063-4ed2-8dc2-179516657242",
          "traceId": "f33c7691",
          "type": "GENERATION",
          "name": "f3_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:33:25.683000+00:00",
          "endTime": "2026-02-04T10:33:40.014000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14331.0,
          "completionTokens": 314,
          "environment": "default",
          "totalTokens": 1683,
          "promptTokens": 1369,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "a7927b13-48ba-4384-a3cb-96cc57803485",
          "traceId": "f33c7691",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:32:26.203000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:32:26.725Z",
      "updatedAt": "2026-02-04T10:33:49.936Z",
      "environment": "default"
    },
    {
      "id": "37c8f786",
      "timestamp": "2026-02-04T10:31:40.384000+00:00",
      "name": "37c8_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Define representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):\n        # Replace with actual input data of shape (1, height, width, channels)\n        input_data = tf.random.normal([1, 224, 224, 3], dtype=tf.float32)\n        yield [input_data]\n\n# Create a converter object\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Set the optimization flag to optimize for size and convert to integer quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = supported_ops\n\n# Define input and output tensors to int8 (optional but recommended)\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\n\n# Set the representative dataset for quantization\nconverter.representative_dataset = representative_dataset_gen\n\n# Ensure that if any ops can't be quantized, an error is thrown\nconverter.target_spec.supported_types = [tf.int8]\n\n# Convert the model to the TensorFlow Lite format with quantization\nquantized_tflite_model = converter.convert()\n\n# Save the converted model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 14.5,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-31-40-386143_chatcmpl-297cf980-21ea-4a43-b21a-789d02ddf591",
          "traceId": "37c8f786",
          "type": "GENERATION",
          "name": "37_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:31:40.386000+00:00",
          "endTime": "2026-02-04T10:31:54.884000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14498.0,
          "completionTokens": 365,
          "environment": "default",
          "totalTokens": 797,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "643a4b10-cba5-42c0-a09f-2b1f03823986",
          "traceId": "37c8f786",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:31:40.384000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:31:40.905Z",
      "updatedAt": "2026-02-04T10:32:05.545Z",
      "environment": "default"
    },
    {
      "id": "23e13c14",
      "timestamp": "2026-02-04T10:29:22.763000+00:00",
      "name": "23e1_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Example dataset generator function\ndef dataset_generator():\n    # Replace this with your actual dataset loading logic\n    for _ in range(100):  # Assuming 100 samples\n        input_data = tf.random.uniform(shape=(224, 224, 3), minval=0, maxval=255, dtype=tf.float32)\n        label = tf.constant([0], dtype=tf.int32)  # Replace with your actual labels\n        yield (input_data, label)\n\n# Load the saved model\nsaved_model_dir = \"models/fruit_to_emoji/og_model/model.keras\"\nconverter = tf.lite.TFLiteConverter.from_keras_model(tf.keras.models.load_model(saved_model_dir))\n\n# Define representative data generator for calibration\ndef representative_data_gen():\n    for input_value, _ in dataset_generator():\n        # Add batch dimension\n        yield [input_value]\n\n# Set the optimization flag\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_data_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Set the input and output tensors to int8 (optional)\nconverter.inference_input_type = tf.float32  # Based on the configuration\nconverter.inference_output_type = tf.int8    # Based on the configuration\n\n# Convert the model\ntflite_quant_model = converter.convert()\n\n# Save the quantized model\nwith open('models/fruit_to_emoji/tflite_model/quantized_model.tflite', 'wb') as f:\n    f.write(tflite_quant_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 107.345,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-29-22-765712_chatcmpl-89829ad5-9899-4bb8-9f88-8a0cea422101",
          "traceId": "23e13c14",
          "type": "GENERATION",
          "name": "23_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:29:22.765000+00:00",
          "endTime": "2026-02-04T10:29:35.085000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12320.0,
          "completionTokens": 309,
          "environment": "default",
          "totalTokens": 741,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-29-38-813147_chatcmpl-01d21ff7-386a-4ef1-ae0c-b0771548c666",
          "traceId": "23e13c14",
          "type": "GENERATION",
          "name": "23_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:29:38.813000+00:00",
          "endTime": "2026-02-04T10:30:08.594000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 29781.0,
          "completionTokens": 680,
          "environment": "default",
          "totalTokens": 2728,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-30-11-850294_chatcmpl-74915d83-81a0-41ce-8e13-4d31f135e0f5",
          "traceId": "23e13c14",
          "type": "GENERATION",
          "name": "23_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:30:11.850000+00:00",
          "endTime": "2026-02-04T10:30:29.336000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17486.0,
          "completionTokens": 374,
          "environment": "default",
          "totalTokens": 2118,
          "promptTokens": 1744,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-30-32-568080_chatcmpl-1e7a5dff-ed44-4d06-8112-cc9ee02e8c53",
          "traceId": "23e13c14",
          "type": "GENERATION",
          "name": "23_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:30:32.568000+00:00",
          "endTime": "2026-02-04T10:30:49.481000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16913.0,
          "completionTokens": 359,
          "environment": "default",
          "totalTokens": 2122,
          "promptTokens": 1763,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-30-52-676619_chatcmpl-d6c9d93f-4cba-4ba2-89f6-b22549345926",
          "traceId": "23e13c14",
          "type": "GENERATION",
          "name": "23_mc_error_handling_attempt#5",
          "startTime": "2026-02-04T10:30:52.676000+00:00",
          "endTime": "2026-02-04T10:31:10.108000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 17432.0,
          "completionTokens": 372,
          "environment": "default",
          "totalTokens": 2120,
          "promptTokens": 1748,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "30f5b993-be8d-4dc4-878d-b476868f4392",
          "traceId": "23e13c14",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:29:22.763000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:29:23.283Z",
      "updatedAt": "2026-02-04T10:31:19.477Z",
      "environment": "default"
    },
    {
      "id": "f0bdd67b",
      "timestamp": "2026-02-04T10:27:54.895000+00:00",
      "name": "f0bd_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model('models/fruit_to_emoji/og_model/model.keras')\n\n# Convert the Keras model to a TensorFlow Lite converter object\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\n\n# Define a generator function that provides representative dataset examples.\ndef representative_data_gen():\n    for _ in range(100):\n        # Get sample input data as a numpy array in a method of your choosing.\n        yield [np.array(np.random.rand(1, 224, 224, 3), dtype=np.float32)]\n\n# Set the optimization flag to optimize for size\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # Input is float32\nconverter.inference_output_type = tf.uint8   # Output is uint8\n\n# Provide a representative dataset to ensure we quantize correctly.\nconverter.representative_dataset = representative_data_gen\n\n# Convert the model to the TensorFlow Lite format with quantization\ntflite_model_quant = converter.convert()\n\n# Save the model to disk\nwith open('models/fruit_to_emoji/tflite_model/model_quant.tflite', 'wb') as f:\n    f.write(tflite_model_quant)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 57.008,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-27-54-896981_chatcmpl-d7b481fc-4bdf-417b-b6ed-9f8837b98ca1",
          "traceId": "f0bdd67b",
          "type": "GENERATION",
          "name": "f0_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:27:54.896000+00:00",
          "endTime": "2026-02-04T10:28:09.037000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 14141.0,
          "completionTokens": 359,
          "environment": "default",
          "totalTokens": 791,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-28-12-771211_chatcmpl-5b9fd0db-20f3-4120-9b62-b0b36eb8dbbc",
          "traceId": "f0bdd67b",
          "type": "GENERATION",
          "name": "f0_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:28:12.771000+00:00",
          "endTime": "2026-02-04T10:28:32.520000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 19749.0,
          "completionTokens": 427,
          "environment": "default",
          "totalTokens": 2475,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-28-36-126545_chatcmpl-8d9c46da-49f3-4a3f-9632-e091c7f18db7",
          "traceId": "f0bdd67b",
          "type": "GENERATION",
          "name": "f0_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:28:36.126000+00:00",
          "endTime": "2026-02-04T10:28:51.903000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 15777.0,
          "completionTokens": 329,
          "environment": "default",
          "totalTokens": 2010,
          "promptTokens": 1681,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "4bc41422-1ad9-480e-9b0b-70b167191abc",
          "traceId": "f0bdd67b",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:27:54.895000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:27:55.414Z",
      "updatedAt": "2026-02-04T10:29:02.062Z",
      "environment": "default"
    },
    {
      "id": "6bd6c539",
      "timestamp": "2026-02-04T10:26:19.131000+00:00",
      "name": "6bd6_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import numpy as np\nimport tensorflow as tf\n\n# Define input shape\ninput_shape = (1, 224, 224, 3)\n\ndef representative_dataset_gen():\n    for _ in range(100):\n        yield [np.array(np.random.random_sample(input_shape), dtype=np.float32)]\n\n# Load the Keras model\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Convert the model to a TensorFlow Lite model with quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset_gen\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32\nconverter.inference_output_type = tf.uint8\n\nquantized_tflite_model = converter.convert()\n\n# Save the quantized TFLite model to a file\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\nwith open(converted_model_path, 'wb') as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 65.251,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-26-19-133450_chatcmpl-d3d3f065-2692-4fd0-9f6c-d5567893db99",
          "traceId": "6bd6c539",
          "type": "GENERATION",
          "name": "6b_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:26:19.133000+00:00",
          "endTime": "2026-02-04T10:26:32.763000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13630.0,
          "completionTokens": 346,
          "environment": "default",
          "totalTokens": 778,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-26-36-582860_chatcmpl-97a48756-580f-45c0-adca-e24bf2728e4b",
          "traceId": "6bd6c539",
          "type": "GENERATION",
          "name": "6b_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:26:36.582000+00:00",
          "endTime": "2026-02-04T10:26:53.377000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16795.0,
          "completionTokens": 352,
          "environment": "default",
          "totalTokens": 2400,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-26-57-005196_chatcmpl-6c82ba5e-00fe-4871-a54e-963405a46b05",
          "traceId": "6bd6c539",
          "type": "GENERATION",
          "name": "6b_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:26:57.005000+00:00",
          "endTime": "2026-02-04T10:27:09.173000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 12168.0,
          "completionTokens": 240,
          "environment": "default",
          "totalTokens": 1923,
          "promptTokens": 1683,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-27-12-509113_chatcmpl-9f920d41-5691-40ff-9dac-6320074225f8",
          "traceId": "6bd6c539",
          "type": "GENERATION",
          "name": "6b_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:27:12.509000+00:00",
          "endTime": "2026-02-04T10:27:24.383000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 11874.0,
          "completionTokens": 270,
          "environment": "default",
          "totalTokens": 1600,
          "promptTokens": 1330,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "8fe53e3e-3904-40ca-903e-294323f1a7f9",
          "traceId": "6bd6c539",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:26:19.132000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:26:19.650Z",
      "updatedAt": "2026-02-04T10:27:34.340Z",
      "environment": "default"
    },
    {
      "id": "1c40dd3e",
      "timestamp": "2026-02-04T10:23:29.287000+00:00",
      "name": "1c40_model_converter",
      "input": null,
      "output": {
        "status": "failed",
        "last_code": "import tensorflow as tf\n\n# Assuming you have a function or dataset generator that provides your input data\ndef representative_data_gen():\n    for input_value in ...:  # Replace with your actual data source\n        # Ensure the input value is of type float32\n        yield [input_value.astype(tf.float32)]\n\n# Define paths\noriginal_model_path = \"models/fruit_to_emoji/og_model/model.keras\"\nconverted_model_path = \"models/fruit_to_emoji/tflite_model/model_quant.tflite\"\n\n# Load your model\nmodel = tf.keras.models.load_model(original_model_path)\n\n# Create a converter object from the loaded model\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n\n# Set the optimization flag to optimize for size\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.float32  # Input datatype is float32\nconverter.inference_output_type = tf.uint8   # Output datatype is uint8\n\n# Provide a representative dataset to ensure we quantize correctly.\nconverter.representative_dataset = representative_data_gen\n\n# Convert the model\nquantized_tflite_model = converter.convert()\n\n# Save the quantized model to disk\nwith open(converted_model_path, 'wb') as f:\n    f.write(quantized_tflite_model)",
        "last_error": "Failed to generate valid code after the max 5 attempts. Last error from code execution: 2026-02-04 12:25:49.146713: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:25:49.150371: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:25:49.162250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2026-02-04 12:25:49.181964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2026-02-04 12:25:49.187700: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-02-04 12:25:49.201686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2026-02-04 12:25:50.119503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 7 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n  warnings.warn(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1770200751.799048 2830339 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\nW0000 00:00:1770200751.799082 2830339 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n2026-02-04 12:25:51.799621: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpznuq2h96\n2026-02-04 12:25:51.800203: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n2026-02-04 12:25:51.800222: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpznuq2h96\n2026-02-04 12:25:51.803813: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n2026-02-04 12:25:51.804551: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n2026-02-04 12:25:51.828872: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpznuq2h96\n2026-02-04 12:25:51.835603: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 35982 microseconds.\n2026-02-04 12:25:51.844006: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204122548.py/tmp_20260204122548_mc_qwen2.5-coder:32b/tmp_20260204122548_mc_qwen2.5-coder:32b.py\", line 31, in <module>\n    quantized_tflite_model = converter.convert()\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1231, in wrapper\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1183, in _convert_and_export_metrics\n    result = convert_func(self, *args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1744, in convert\n    saved_model_convert_result = self._convert_as_saved_model()\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1725, in _convert_as_saved_model\n    return super(TFLiteKerasModelConverterV2, self).convert(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1473, in convert\n    return self._optimize_tflite_model(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 215, in wrapper\n    raise error from None  # Re-throws the exception.\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 205, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1127, in _optimize_tflite_model\n    model = self._quantize(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 748, in _quantize\n    calibrated = calibrate_quantize.calibrate(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 215, in wrapper\n    raise error from None  # Re-throws the exception.\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 205, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 254, in calibrate\n    self._feed_tensors(dataset_gen, resize_input=True)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 101, in _feed_tensors\n    for sample in dataset_gen():\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204122548.py/tmp_20260204122548_mc_qwen2.5-coder:32b/tmp_20260204122548_mc_qwen2.5-coder:32b.py\", line 5, in representative_data_gen\n    for input_value in ...:  # Replace with your actual data source\nTypeError: 'ellipsis' object is not iterable\n"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 143.352,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-23-29-289835_chatcmpl-239bb46b-bfe0-49af-bdc9-e68fcf6e1592",
          "traceId": "1c40dd3e",
          "type": "GENERATION",
          "name": "1c_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:23:29.289000+00:00",
          "endTime": "2026-02-04T10:23:40.191000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 10902.0,
          "completionTokens": 286,
          "environment": "default",
          "totalTokens": 718,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-23-43-881398_chatcmpl-3245b87e-67eb-4810-b21a-8f94a01e1868",
          "traceId": "1c40dd3e",
          "type": "GENERATION",
          "name": "1c_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:23:43.881000+00:00",
          "endTime": "2026-02-04T10:24:19.693000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 35812.0,
          "completionTokens": 836,
          "environment": "default",
          "totalTokens": 2884,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-24-47-646197_chatcmpl-48523378-1eb5-4bbf-be2c-20368a09f361",
          "traceId": "1c40dd3e",
          "type": "GENERATION",
          "name": "1c_mc_error_handling_attempt#3",
          "startTime": "2026-02-04T10:24:47.646000+00:00",
          "endTime": "2026-02-04T10:25:14.553000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 26907.0,
          "completionTokens": 609,
          "environment": "default",
          "totalTokens": 2657,
          "promptTokens": 2048,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-25-18-432849_chatcmpl-acbcf1e7-3668-4561-ac7e-44773d36a31a",
          "traceId": "1c40dd3e",
          "type": "GENERATION",
          "name": "1c_mc_error_handling_attempt#4",
          "startTime": "2026-02-04T10:25:18.432000+00:00",
          "endTime": "2026-02-04T10:25:31.609000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13177.0,
          "completionTokens": 281,
          "environment": "default",
          "totalTokens": 1816,
          "promptTokens": 1535,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-25-34-910684_chatcmpl-237670f1-7e18-43c3-9edd-47ab2d8106e1",
          "traceId": "1c40dd3e",
          "type": "GENERATION",
          "name": "1c_mc_error_handling_attempt#5",
          "startTime": "2026-02-04T10:25:34.910000+00:00",
          "endTime": "2026-02-04T10:25:48.875000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13965.0,
          "completionTokens": 323,
          "environment": "default",
          "totalTokens": 1694,
          "promptTokens": 1371,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "4e39ec78-d93e-424e-b3e4-d3ad82b9dca4",
          "traceId": "1c40dd3e",
          "type": "SPAN",
          "name": "error_1c_mc_failure_signal_model_converter",
          "startTime": "2026-02-04T10:25:52.639000+00:00",
          "level": "ERROR",
          "statusMessage": "Failed. Last error: Failed to generate valid code after the max 5 attempts. Last error from code execution: 2026-02-04 12:25:49.146713: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:25:49.150371: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n2026-02-04 12:25:49.162250: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2026-02-04 12:25:49.181964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2026-02-04 12:25:49.187700: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2026-02-04 12:25:49.201686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2026-02-04 12:25:50.119503: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 8 variables whereas the saved optimizer has 7 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n  warnings.warn(\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1770200751.799048 2830339 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\nW0000 00:00:1770200751.799082 2830339 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n2026-02-04 12:25:51.799621: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpznuq2h96\n2026-02-04 12:25:51.800203: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n2026-02-04 12:25:51.800222: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpznuq2h96\n2026-02-04 12:25:51.803813: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n2026-02-04 12:25:51.804551: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n2026-02-04 12:25:51.828872: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpznuq2h96\n2026-02-04 12:25:51.835603: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 35982 microseconds.\n2026-02-04 12:25:51.844006: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\nTraceback (most recent call last):\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204122548.py/tmp_20260204122548_mc_qwen2.5-coder:32b/tmp_20260204122548_mc_qwen2.5-coder:32b.py\", line 31, in <module>\n    quantized_tflite_model = converter.convert()\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1231, in wrapper\n    return self._convert_and_export_metrics(convert_func, *args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1183, in _convert_and_export_metrics\n    result = convert_func(self, *args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1744, in convert\n    saved_model_convert_result = self._convert_as_saved_model()\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1725, in _convert_as_saved_model\n    return super(TFLiteKerasModelConverterV2, self).convert(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1473, in convert\n    return self._optimize_tflite_model(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 215, in wrapper\n    raise error from None  # Re-throws the exception.\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 205, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 1127, in _optimize_tflite_model\n    model = self._quantize(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/lite.py\", line 748, in _quantize\n    calibrated = calibrate_quantize.calibrate(\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 215, in wrapper\n    raise error from None  # Re-throws the exception.\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/convert_phase.py\", line 205, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 254, in calibrate\n    self._feed_tensors(dataset_gen, resize_input=True)\n  File \"/home/wuguangh/.conda/envs/tinyml/lib/python3.10/site-packages/tensorflow/lite/python/optimize/calibrator.py\", line 101, in _feed_tensors\n    for sample in dataset_gen():\n  File \"/home/wuguangh/Projects/tinyml-autopilot/models/fruit_to_emoji/tflite_model/tmp_converter20260204122548.py/tmp_20260204122548_mc_qwen2.5-coder:32b/tmp_20260204122548_mc_qwen2.5-coder:32b.py\", line 5, in representative_data_gen\n    for input_value in ...:  # Replace with your actual data source\nTypeError: 'ellipsis' object is not iterable\n",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        },
        {
          "id": "c4906da5-2f1a-4fa6-af2d-4edd57b5a777",
          "traceId": "1c40dd3e",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:23:29.287000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:23:29.804Z",
      "updatedAt": "2026-02-04T10:25:57.977Z",
      "environment": "default"
    },
    {
      "id": "5e01f79c",
      "timestamp": "2026-02-04T10:22:45.789000+00:00",
      "name": "5e01_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Set the optimization flag to optimize for size and specify quantization parameters\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_types = [tf.int8]\nconverter.inference_input_type = inference_input_type\nconverter.inference_output_type = inference_output_type\nconverter.target_spec.supported_ops = supported_ops\n\n# Define a representative dataset generator function\ndef representative_dataset_gen():\n    for _ in range(100):\n        # Generate input data from a real distribution.\n        input_data = tf.random.normal([1, 224, 224, 3], dtype=tf.float32)\n        yield [input_data]\n\n# Set the representative dataset\nconverter.representative_dataset = representative_dataset_gen\n\n# Convert the model to TFLite with 8-bit integer quantization\nquantized_tflite_model = converter.convert()\n\n# Save the converted model\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 13.084,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-22-45-790934_chatcmpl-3de31b8b-b852-42d4-8cb4-f0f457ca0ca3",
          "traceId": "5e01f79c",
          "type": "GENERATION",
          "name": "5e_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:22:45.790000+00:00",
          "endTime": "2026-02-04T10:22:58.873000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 13083.0,
          "completionTokens": 327,
          "environment": "default",
          "totalTokens": 759,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "6d796002-c875-46cf-aed6-1934d9e798c7",
          "traceId": "5e01f79c",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:22:45.789000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:22:46.307Z",
      "updatedAt": "2026-02-04T10:23:08.929Z",
      "environment": "default"
    },
    {
      "id": "4e99b483",
      "timestamp": "2026-02-04T10:21:02.119000+00:00",
      "name": "4e99_model_converter",
      "input": null,
      "output": {
        "status": "success",
        "code": "import tensorflow as tf\nimport numpy as np\n\n# Load the original Keras model\noriginal_model = tf.keras.models.load_model(\"models/fruit_to_emoji/og_model/model.keras\")\n\n# Define conversion parameters for 8-bit integer quantization\ninference_input_type = tf.float32\ninference_output_type = tf.uint8\nsupported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n\n# Convert the model to a TensorFlow Lite Quantized Model\nconverter = tf.lite.TFLiteConverter.from_keras_model(original_model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.target_spec.supported_ops = supported_ops\n\n# Define a generator function that provides representative dataset examples\ndef representative_data_gen():\n    for _ in range(100):\n        # Get sample input data as a numpy array in a method of your choosing.\n        yield [np.random.uniform(low=0, high=255, size=(1, *original_model.input_shape[1:])).astype(np.float32)]\n\nconverter.representative_dataset = representative_data_gen\n\n# Ensure that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_types = [tf.int8]\n\n# Set the input and output tensors to int8 (optional)\nconverter.inference_input_type = tf.uint8\nconverter.inference_output_type = tf.uint8\n\n# Convert the model\nquantized_tflite_model = converter.convert()\n\n# Save the quantized model to a file\nwith open(\"models/fruit_to_emoji/tflite_model/model_quant.tflite\", \"wb\") as f:\n    f.write(quantized_tflite_model)"
      },
      "session_id": "qwen2.5-coder:32b_7cb7_mc_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "model_converter",
        "qwen2.5-coder:32b"
      ],
      "latency": 71.077,
      "total_cost": 0.0,
      "observations": [
        {
          "id": "time-12-21-02-121690_chatcmpl-68ef938f-3a05-4fb0-93f8-1737df483ab4",
          "traceId": "4e99b483",
          "type": "GENERATION",
          "name": "4e_mc_conversion_code_gen",
          "startTime": "2026-02-04T10:21:02.121000+00:00",
          "endTime": "2026-02-04T10:21:39.832000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 37711.0,
          "completionTokens": 382,
          "environment": "default",
          "totalTokens": 814,
          "promptTokens": 432,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-12-21-56-743469_chatcmpl-5475c24d-66fa-4d39-8291-d0973e2ceb78",
          "traceId": "4e99b483",
          "type": "GENERATION",
          "name": "4e_mc_error_handling_attempt#2",
          "startTime": "2026-02-04T10:21:56.743000+00:00",
          "endTime": "2026-02-04T10:22:13.196000+00:00",
          "model": "qwen2.5-coder:32b",
          "modelParameters": {},
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "latency": 16453.0,
          "completionTokens": 357,
          "environment": "default",
          "totalTokens": 2216,
          "promptTokens": 1859,
          "costDetails": {},
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "a875f11b-838c-45e9-9ae4-e1975ac912db",
          "traceId": "4e99b483",
          "type": "SPAN",
          "name": "start_model_converter",
          "startTime": "2026-02-04T10:21:02.119000+00:00",
          "level": "DEFAULT",
          "calculatedTotalCost": 0.0,
          "completionTokens": 0,
          "environment": "default",
          "totalTokens": 0,
          "promptTokens": 0,
          "costDetails": {},
          "endTime": null,
          "model": null,
          "modelParameters": null,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null,
          "latency": null
        }
      ],
      "createdAt": "2026-02-04T10:21:02.636Z",
      "updatedAt": "2026-02-04T10:22:24.766Z",
      "environment": "default"
    }
  ],
  "meta": {
    "total_items": 30
  }
}