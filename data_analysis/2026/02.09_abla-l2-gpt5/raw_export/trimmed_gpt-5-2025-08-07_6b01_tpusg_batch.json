{
  "data": [
    {
      "id": "6b1372d8",
      "timestamp": "2026-02-06T13:00:59.222000+00:00",
      "name": "6b13_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# TFLite Runtime (EdgeTPU)\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# ==============================\n# Configuration Parameters\n# ==============================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# ==============================\n# Utilities\n# ==============================\ndef load_labels(path):\n    labels = {}\n    if not os.path.isfile(path):\n        print(f\"Warning: Label file not found at {path}. Using empty label map.\")\n        return labels\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = [l.strip() for l in f.readlines() if l.strip()]\n    # Try to parse \"id: name\" or \"id name\" or plain label list\n    for idx, line in enumerate(lines):\n        if ':' in line:\n            left, right = line.split(':', 1)\n            try:\n                labels[int(left.strip())] = right.strip()\n                continue\n            except ValueError:\n                pass\n        parts = line.split()\n        if parts and parts[0].isdigit():\n            try:\n                labels[int(parts[0])] = ' '.join(parts[1:]).strip()\n                continue\n            except ValueError:\n                pass\n        # Fallback: assign by enumerated index\n        labels[idx] = line\n    return labels\n\ndef color_for_class(class_id):\n    # Deterministic color from class id (avoid importing random)\n    r = (37 * (class_id + 1)) % 255\n    g = (17 * (class_id + 1)) % 255\n    b = (29 * (class_id + 1)) % 255\n    # Avoid too-dark colors\n    r = 80 + (r % 175)\n    g = 80 + (g % 175)\n    b = 80 + (b % 175)\n    return int(b), int(g), int(r)  # BGR for OpenCV\n\ndef ensure_dir_exists(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\ndef get_video_writer(path, width, height, fps):\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    return cv2.VideoWriter(path, fourcc, fps, (width, height))\n\ndef preprocess_frame(frame_bgr, input_size):\n    ih, iw = input_size\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (iw, ih))\n    return np.expand_dims(resized, axis=0)\n\ndef find_output_tensors(interpreter):\n    # Return indices for boxes, classes, scores, count based on shapes/values\n    output_details = interpreter.get_output_details()\n    tensors = [interpreter.get_tensor(od['index']) for od in output_details]\n    idx_boxes = idx_classes = idx_scores = idx_count = None\n\n    # Identify count: shape length 1 and size 1\n    for i, t in enumerate(tensors):\n        if t.ndim == 1 and t.shape[0] == 1:\n            idx_count = i\n            break\n\n    # Identify boxes: shape [1, N, 4]\n    for i, t in enumerate(tensors):\n        if t.ndim == 3 and t.shape[0] == 1 and t.shape[2] == 4:\n            idx_boxes = i\n            break\n\n    # Identify classes & scores among remaining 2D [1, N]\n    candidates = []\n    for i, t in enumerate(tensors):\n        if t.ndim == 2 and t.shape[0] == 1:\n            candidates.append(i)\n    if len(candidates) >= 2:\n        a, b = candidates[0], candidates[1]\n        max_a = float(np.max(tensors[a])) if tensors[a].size else 0.0\n        max_b = float(np.max(tensors[b])) if tensors[b].size else 0.0\n        # Scores expected in [0, 1], classes typically > 1\n        if max_a <= 1.05 and max_b > 1.05:\n            idx_scores, idx_classes = a, b\n        elif max_b <= 1.05 and max_a > 1.05:\n            idx_scores, idx_classes = b, a\n        else:\n            # Fallback to assume standard order if ambiguous\n            idx_scores, idx_classes = a, b\n\n    # As ultimate fallback, assume standard TFLite SSD order: 0:boxes,1:classes,2:scores,3:num\n    if None in (idx_boxes, idx_classes, idx_scores, idx_count):\n        # Attempt standard order mapping\n        try:\n            if len(output_details) >= 4:\n                idx_boxes = 0 if idx_boxes is None else idx_boxes\n                idx_classes = 1 if idx_classes is None else idx_classes\n                idx_scores = 2 if idx_scores is None else idx_scores\n                idx_count = 3 if idx_count is None else idx_count\n        except Exception:\n            pass\n\n    return (output_details[idx_boxes]['index'],\n            output_details[idx_classes]['index'],\n            output_details[idx_scores]['index'],\n            output_details[idx_count]['index'])\n\n# ==============================\n# Main Pipeline\n# ==============================\ndef main():\n    # 1. Setup: Load interpreter with EdgeTPU, allocate tensors, load labels, open input video\n    if not os.path.isfile(model_path):\n        raise FileNotFoundError(f\"Model not found: {model_path}\")\n    print(\"Loading TFLite model with EdgeTPU delegate...\")\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()[0]\n    input_index = input_details['index']\n    input_height, input_width = input_details['shape'][1], input_details['shape'][2]\n    input_dtype = input_details['dtype']\n    if input_dtype != np.uint8:\n        print(\"Warning: Model input is not uint8. Proceeding, but EdgeTPU models typically use uint8.\")\n\n    labels = load_labels(label_path)\n    print(f\"Loaded {len(labels)} labels from {label_path}\")\n\n    if not os.path.isfile(input_path):\n        raise FileNotFoundError(f\"Input video not found: {input_path}\")\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Could not open input video: {input_path}\")\n    in_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    in_fps = cap.get(cv2.CAP_PROP_FPS)\n    if not in_fps or in_fps <= 0 or np.isnan(in_fps):\n        in_fps = 30.0\n\n    ensure_dir_exists(output_path)\n    writer = get_video_writer(output_path, in_w, in_h, in_fps)\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Could not open VideoWriter for output: {output_path}\")\n\n    # Determine output tensor indices reliably once we have a first inference\n    # Do a dry-run with a black frame of correct size\n    dummy = np.zeros((1, input_height, input_width, 3), dtype=np.uint8)\n    interpreter.set_tensor(input_index, dummy)\n    interpreter.invoke()\n    out_idx_boxes, out_idx_classes, out_idx_scores, out_idx_count = find_output_tensors(interpreter)\n\n    # mAP proxy: running mean of detection confidences over all processed frames/detections\n    sum_confidences = 0.0\n    count_detections = 0\n    frame_index = 0\n\n    print(\"Processing video...\")\n    t_start = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_index += 1\n        orig_h, orig_w = frame.shape[:2]\n\n        # 2. Preprocessing\n        input_tensor = preprocess_frame(frame, (input_height, input_width))\n        if input_dtype == np.float32:\n            # Typical normalization if float32 model (not common for EdgeTPU models)\n            input_tensor = (input_tensor.astype(np.float32) - 127.5) / 127.5\n        else:\n            input_tensor = input_tensor.astype(np.uint8)\n\n        # 3. Inference\n        t0 = time.time()\n        interpreter.set_tensor(input_index, input_tensor)\n        interpreter.invoke()\n        infer_ms = (time.time() - t0) * 1000.0\n\n        boxes = interpreter.get_tensor(out_idx_boxes)  # [1, num, 4] in [ymin, xmin, ymax, xmax]\n        classes = interpreter.get_tensor(out_idx_classes)  # [1, num]\n        scores = interpreter.get_tensor(out_idx_scores)  # [1, num]\n        count = interpreter.get_tensor(out_idx_count)  # [1]\n\n        if boxes.ndim != 3 or scores.ndim != 2 or classes.ndim != 2:\n            # Fallback re-detection of indices if shapes are unexpected\n            out_idx_boxes, out_idx_classes, out_idx_scores, out_idx_count = find_output_tensors(interpreter)\n            boxes = interpreter.get_tensor(out_idx_boxes)\n            classes = interpreter.get_tensor(out_idx_classes)\n            scores = interpreter.get_tensor(out_idx_scores)\n            count = interpreter.get_tensor(out_idx_count)\n\n        num = int(count[0]) if count.size else boxes.shape[1]\n        boxes = boxes[0]\n        classes = classes[0]\n        scores = scores[0]\n\n        # 4. Output handling: draw detection boxes with labels and update mAP proxy\n        for i in range(min(num, boxes.shape[0])):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n            ymin, xmin, ymax, xmax = boxes[i]\n            # Convert to absolute coordinates\n            left = max(0, min(orig_w - 1, int(xmin * orig_w)))\n            top = max(0, min(orig_h - 1, int(ymin * orig_h)))\n            right = max(0, min(orig_w - 1, int(xmax * orig_w)))\n            bottom = max(0, min(orig_h - 1, int(ymax * orig_h)))\n\n            class_id = int(classes[i])\n            label = labels.get(class_id, f\"id:{class_id}\")\n            color = color_for_class(class_id)\n\n            cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n            label_text = f\"{label} {score:.2f}\"\n            # Text background\n            (tw, th), baseline = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame, (left, top - th - baseline), (left + tw, top), color, -1)\n            cv2.putText(frame, label_text, (left, top - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n            # Update mAP proxy stats\n            sum_confidences += score\n            count_detections += 1\n\n        # Compute running mAP proxy (mean of detection confidences)\n        map_proxy = (sum_confidences / count_detections) if count_detections > 0 else 0.0\n        fps_infer = 1000.0 / infer_ms if infer_ms > 0 else 0.0\n        hud_text = f\"mAP: {map_proxy:.3f}  Inference: {infer_ms:.1f} ms  FPS: {fps_infer:.1f}\"\n        cv2.putText(frame, hud_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (15, 15, 240), 2, cv2.LINE_AA)\n\n        writer.write(frame)\n\n        # Optional: print progress every 50 frames\n        if frame_index % 50 == 0:\n            print(f\"Processed {frame_index} frames - mAP proxy: {map_proxy:.3f}, last inference: {infer_ms:.1f} ms\")\n\n    cap.release()\n    writer.release()\n    total_time = time.time() - t_start\n    total_frames = frame_index\n    overall_fps = total_frames / total_time if total_time > 0 else 0.0\n    final_map_proxy = (sum_confidences / count_detections) if count_detections > 0 else 0.0\n\n    print(\"Processing complete.\")\n    print(f\"Saved output video to: {output_path}\")\n    print(f\"Frames: {total_frames}, Time: {total_time:.2f}s, Avg FPS: {overall_fps:.2f}\")\n    print(f\"Final mAP (proxy by mean confidence): {final_map_proxy:.4f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 30
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 73.875,
      "total_cost": 0.06429375,
      "observations": [
        {
          "id": "time-15-00-59-464107_chatcmpl-D6FkBxcJpbKIyPY9ju1pawq32zLiu",
          "traceId": "6b1372d8",
          "type": "GENERATION",
          "name": "6b13_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T13:00:59.464000+00:00",
          "endTime": "2026-02-06T13:02:13.339000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06429375,
          "latency": 73875.0,
          "costDetails": {
            "total": 0.06429375
          },
          "totalTokens": 6824,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 6373,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T13:00:59.746Z",
      "environment": "default",
      "updatedAt": "2026-02-06T13:03:11.601Z"
    },
    {
      "id": "c54f5ee8",
      "timestamp": "2026-02-06T12:58:13.088000+00:00",
      "name": "c54f_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# ==============================\n# Configuration Parameters\n# ==============================\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\nEDGETPU_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\n# ==============================\n# Utilities\n# ==============================\ndef load_labels(path):\n    \"\"\"\n    Loads labels from a label map file.\n    Supports:\n      - one label per line (index inferred from line number)\n      - \"index label\" (space or colon separated)\n    \"\"\"\n    labels = {}\n    try:\n        with open(path, 'r') as f:\n            lines = [line.strip() for line in f.readlines() if line.strip()]\n    except Exception as e:\n        print(\"Failed to read label file:\", e)\n        return labels\n\n    for idx, line in enumerate(lines):\n        # Try split by common separators\n        label = None\n        key = None\n        if ':' in line:\n            parts = line.split(':', 1)\n            try:\n                key = int(parts[0].strip())\n                label = parts[1].strip()\n            except ValueError:\n                pass\n        elif ' ' in line:\n            parts = line.split(' ', 1)\n            try:\n                key = int(parts[0].strip())\n                label = parts[1].strip()\n            except ValueError:\n                pass\n\n        if key is None or label is None:\n            # Fallback: use line index as id\n            key = idx\n            label = line\n\n        labels[key] = label\n    return labels\n\n\ndef make_interpreter(model_path, edgetpu_lib_path):\n    \"\"\"\n    Creates and returns a TFLite interpreter loaded with EdgeTPU delegate.\n    \"\"\"\n    try:\n        delegate = load_delegate(edgetpu_lib_path)\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate])\n        interpreter.allocate_tensors()\n        return interpreter\n    except Exception as e:\n        raise RuntimeError(f\"Failed to create TFLite interpreter with EdgeTPU delegate: {e}\")\n\n\ndef preprocess_frame(frame_bgr, input_details):\n    \"\"\"\n    Preprocess frame to match model input requirements.\n    - Resize to expected input size\n    - Convert BGR to RGB\n    - Type conversion based on model input dtype\n    \"\"\"\n    ih, iw = input_details['shape'][1], input_details['shape'][2]\n    frame_resized = cv2.resize(frame_bgr, (iw, ih))\n    frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n    input_dtype = input_details['dtype']\n\n    if input_dtype == np.float32:\n        input_data = (frame_rgb.astype(np.float32) / 255.0).reshape(1, ih, iw, 3)\n    else:\n        # Assume quantized uint8 input\n        input_data = frame_rgb.astype(np.uint8).reshape(1, ih, iw, 3)\n\n    return input_data\n\n\ndef get_detections(interpreter, frame_w, frame_h, confidence_threshold):\n    \"\"\"\n    Extract detections from model outputs, convert to pixel coordinates, and filter by confidence threshold.\n    Returns a list of dicts: { 'bbox': (x1, y1, x2, y2), 'score': float, 'class_id': int }\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    # Typical order for TFLite SSD: boxes, classes, scores, count\n    try:\n        boxes = interpreter.get_tensor(output_details[0]['index'])[0]  # [N,4] in ymin,xmin,ymax,xmax normalized\n        classes = interpreter.get_tensor(output_details[1]['index'])[0]  # [N]\n        scores = interpreter.get_tensor(output_details[2]['index'])[0]  # [N]\n        count = int(interpreter.get_tensor(output_details[3]['index'])[0])\n    except Exception:\n        # Attempt to find tensors by shape heuristics if order differs\n        boxes = None\n        classes = None\n        scores = None\n        count = None\n        for od in output_details:\n            tensor = interpreter.get_tensor(od['index'])\n            shp = tensor.shape\n            if len(shp) == 3 and shp[-1] == 4:\n                boxes = tensor[0]\n            elif len(shp) == 2:\n                # could be classes or scores\n                if tensor.dtype == np.float32:\n                    # Heuristic: keep higher variance as scores if both float32\n                    if scores is None:\n                        scores = tensor[0]\n                    else:\n                        # choose the one whose values are in [0,1] as scores\n                        t0 = np.clip(tensor[0], 0.0, 1.0)\n                        if np.allclose(t0, tensor[0], atol=1e-5):\n                            scores = tensor[0]\n                        else:\n                            classes = tensor[0]\n                else:\n                    classes = tensor[0]\n            elif len(shp) == 1 and shp[0] == 1:\n                count = int(tensor[0])\n\n        if boxes is None or classes is None or scores is None or count is None:\n            raise RuntimeError(\"Unable to parse model outputs for detection.\")\n\n    detections = []\n    n = min(len(scores), count)\n    for i in range(n):\n        score = float(scores[i])\n        if score < confidence_threshold:\n            continue\n        cls_id = int(classes[i]) if not np.isnan(classes[i]) else -1\n        ymin, xmin, ymax, xmax = boxes[i]\n        # Clamp and scale to pixel coordinates\n        xmin = max(0.0, min(1.0, float(xmin)))\n        xmax = max(0.0, min(1.0, float(xmax)))\n        ymin = max(0.0, min(1.0, float(ymin)))\n        ymax = max(0.0, min(1.0, float(ymax)))\n\n        x1 = int(xmin * frame_w)\n        y1 = int(ymin * frame_h)\n        x2 = int(xmax * frame_w)\n        y2 = int(ymax * frame_h)\n\n        detections.append({\n            'bbox': (x1, y1, x2, y2),\n            'score': score,\n            'class_id': cls_id\n        })\n    return detections\n\n\ndef draw_detections(frame, detections, labels, map_value=None):\n    \"\"\"\n    Draws bounding boxes and labels on the frame.\n    Optionally overlay an mAP value (or proxy) at the top-left.\n    \"\"\"\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        score = det['score']\n        cls_id = det['class_id']\n        label = labels.get(cls_id, f\"id:{cls_id}\")\n        caption = f\"{label} {score:.2f}\"\n\n        # Draw rectangle\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 200, 0), 2)\n\n        # Draw label background\n        (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), (0, 200, 0), -1)\n        # Draw label text\n        cv2.putText(frame, caption, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n\n    # Overlay mAP (or proxy) at the top-left corner\n    if map_value is not None:\n        text = f\"mAP: {map_value:.3f}\"\n    else:\n        text = \"mAP: N/A\"\n    cv2.putText(frame, text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 10, 230), 2)\n\n\ndef compute_proxy_map(all_scores):\n    \"\"\"\n    Computes a proxy for mAP in absence of ground truth.\n    This proxy computes mean precision across thresholds [0.5, 0.95] where\n    precision at threshold t is (#detections with score >= t) / (total detections).\n    Note: This is NOT true mAP. It is a placeholder metric.\n    \"\"\"\n    if not all_scores:\n        return 0.0\n    scores = np.array(all_scores, dtype=np.float32)\n    thresholds = np.arange(0.5, 1.0, 0.05)\n    precisions = []\n    total = float(len(scores))\n    for t in thresholds:\n        precisions.append(float(np.sum(scores >= t)) / total)\n    return float(np.mean(precisions)) if precisions else 0.0\n\n\n# ==============================\n# Main pipeline\n# ==============================\ndef main():\n    # Prepare output directory\n    out_dir = os.path.dirname(OUTPUT_PATH)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Setup TFLite + EdgeTPU\n    interpreter = make_interpreter(MODEL_PATH, EDGETPU_LIB)\n    input_details = interpreter.get_input_details()[0]\n\n    # Setup video IO\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0:\n        fps = 30.0  # Fallback\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    # For proxy mAP computation\n    all_detection_scores = []\n\n    frame_index = 0\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break  # End of video\n\n            # Preprocess\n            input_data = preprocess_frame(frame, input_details)\n            interpreter.set_tensor(input_details['index'], input_data)\n\n            # Inference\n            start_time = time.time()\n            interpreter.invoke()\n            _ = (time.time() - start_time) * 1000.0  # inference time in ms (not displayed but measured)\n\n            # Postprocess detections\n            detections = get_detections(interpreter, frame_w=width, frame_h=height, confidence_threshold=CONFIDENCE_THRESHOLD)\n\n            # Update scores for proxy mAP\n            for d in detections:\n                all_detection_scores.append(d['score'])\n\n            proxy_map = compute_proxy_map(all_detection_scores)\n\n            # Draw results and overlay proxy mAP\n            draw_detections(frame, detections, labels, map_value=proxy_map)\n\n            # Write frame\n            writer.write(frame)\n            frame_index += 1\n\n    finally:\n        cap.release()\n        writer.release()\n\n    print(f\"Processing complete. Output saved to: {OUTPUT_PATH}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 29
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 86.944,
      "total_cost": 0.06916375,
      "observations": [
        {
          "id": "time-14-58-13-718268_chatcmpl-D6FhV9MJhPiLxvWPj8cfnu7tCwUNq",
          "traceId": "c54f5ee8",
          "type": "GENERATION",
          "name": "c54f_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:58:13.718000+00:00",
          "endTime": "2026-02-06T12:59:40.662000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06916375,
          "latency": 86944.0,
          "costDetails": {
            "total": 0.06916375
          },
          "totalTokens": 7311,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 6860,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:58:13.643Z",
      "environment": "default",
      "updatedAt": "2026-02-06T13:00:38.337Z"
    },
    {
      "id": "8a9cc70d",
      "timestamp": "2026-02-06T12:55:44.084000+00:00",
      "name": "8a9c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration parameters\n# =========================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"  # corrected path\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# =========================\n# Helper functions\n# =========================\ndef load_labels(path):\n    \"\"\"\n    Load labels from a label map file.\n    Supports the following formats per line:\n      - \"id label\"\n      - \"id: label\"\n      - \"label\" (implicit incremental id starting at 0)\n    Returns dict: {id(int): label(str)}\n    \"\"\"\n    labels = {}\n    try:\n        with open(path, 'r', encoding='utf-8') as f:\n            for idx, line in enumerate(f):\n                line = line.strip()\n                if not line:\n                    continue\n                # Try \"id: label\"\n                if ':' in line:\n                    left, right = line.split(':', 1)\n                    left = left.strip()\n                    right = right.strip()\n                    if left.isdigit():\n                        labels[int(left)] = right\n                        continue\n                # Try \"id label\"\n                parts = line.split(maxsplit=1)\n                if len(parts) == 2 and parts[0].isdigit():\n                    labels[int(parts[0])] = parts[1].strip()\n                    continue\n                # Fallback: implicit id\n                labels[idx] = line\n    except Exception as e:\n        print(f\"Warning: Failed to load labels from {path}: {e}\")\n    return labels\n\ndef make_interpreter(model_file, delegate_lib=\"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"):\n    \"\"\"\n    Create a TFLite interpreter with EdgeTPU delegate.\n    \"\"\"\n    try:\n        interpreter = Interpreter(\n            model_path=model_file,\n            experimental_delegates=[load_delegate(delegate_lib)]\n        )\n    except ValueError as e:\n        raise RuntimeError(f\"Failed to load EdgeTPU delegate '{delegate_lib}': {e}\")\n    return interpreter\n\ndef preprocess_frame(frame_bgr, input_size, input_dtype):\n    \"\"\"\n    Resize and convert BGR frame to model's expected input tensor.\n    - Convert BGR to RGB.\n    - Resize to input_size (width, height).\n    - If input_dtype is float32, normalize to [0,1].\n    - Return a numpy array of shape (1, height, width, 3) with proper dtype.\n    \"\"\"\n    ih, iw = input_size[1], input_size[0]\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (iw, ih), interpolation=cv2.INTER_LINEAR)\n\n    if input_dtype == np.float32:\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        input_data = resized.astype(np.uint8)\n\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef get_output(interpreter):\n    \"\"\"\n    Extract common object detection model outputs:\n      - boxes: [N, 4] in normalized coordinates [ymin, xmin, ymax, xmax]\n      - classes: [N] float class indices\n      - scores: [N] float confidence scores\n      - count: number of detections (int)\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    count = interpreter.get_tensor(output_details[3]['index'])\n\n    # Squeeze batch dimension\n    boxes = np.squeeze(boxes)\n    classes = np.squeeze(classes).astype(np.int32)\n    scores = np.squeeze(scores)\n    if np.isscalar(count):\n        num = int(count)\n    else:\n        num = int(np.squeeze(count))\n    return boxes, classes, scores, num\n\ndef draw_detections(frame, detections, labels, mAP_value):\n    \"\"\"\n    Draw bounding boxes and labels on frame.\n    detections: list of dicts with keys: 'box' (xmin, ymin, xmax, ymax), 'score', 'class_id'\n    labels: dict {id: label}\n    mAP_value: float, will be displayed on frame as \"mAP\"\n    \"\"\"\n    h, w = frame.shape[:2]\n\n    for det in detections:\n        xmin, ymin, xmax, ymax = det['box']\n        score = det['score']\n        class_id = det['class_id']\n        label = labels.get(class_id, f\"class_{class_id}\")\n        caption = f\"{label}: {score:.2f}\"\n\n        # Choose a color based on class_id for consistency\n        color = (\n            int((37 * (class_id + 1)) % 255),\n            int((17 * (class_id + 1)) % 255),\n            int((29 * (class_id + 1)) % 255),\n        )\n\n        # Draw rectangle\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n\n        # Draw filled rectangle for text background\n        (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (xmin, max(0, ymin - th - baseline - 4)),\n                             (xmin + tw + 4, ymin), color, thickness=-1)\n        # Put text\n        cv2.putText(frame, caption, (xmin + 2, ymin - 4),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Draw mAP on the frame (top-left corner)\n    cv2.putText(frame, f\"mAP (proxy): {mAP_value*100:.2f}%\",\n                (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 200, 10), 2, cv2.LINE_AA)\n\ndef clip_bbox(xmin, ymin, xmax, ymax, w, h):\n    \"\"\"\n    Clip bounding box coordinates to image dimensions.\n    \"\"\"\n    xmin = max(0, min(w - 1, xmin))\n    ymin = max(0, min(h - 1, ymin))\n    xmax = max(0, min(w - 1, xmax))\n    ymax = max(0, min(h - 1, ymax))\n    return xmin, ymin, xmax, ymax\n\ndef compute_proxy_map(class_to_scores):\n    \"\"\"\n    Compute a proxy mAP metric without ground-truth:\n    - For each class with any detections, AP_c = mean(confidence scores of detections above threshold).\n    - mAP = mean(AP_c) across classes with any detections.\n    NOTE: This is NOT a true mAP since no ground-truth is provided.\n    \"\"\"\n    ap_values = []\n    for _, scores in class_to_scores.items():\n        if len(scores) > 0:\n            ap_values.append(float(np.mean(np.array(scores, dtype=np.float32))))\n    if len(ap_values) == 0:\n        return 0.0\n    return float(np.mean(np.array(ap_values, dtype=np.float32)))\n\n# =========================\n# Main application\n# =========================\ndef main():\n    # Ensure output directory exists\n    out_dir = os.path.dirname(output_path)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(label_path)\n    if not labels:\n        print(\"Warning: No labels were loaded; classes will be shown as numeric IDs.\")\n\n    # Load interpreter with EdgeTPU delegate\n    print(\"Loading TFLite model with EdgeTPU delegate...\")\n    interpreter = make_interpreter(model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_index = input_details[0]['index']\n    _, in_h, in_w, in_c = input_details[0]['shape']\n    input_dtype = input_details[0]['dtype']\n    print(f\"Model input: shape=({in_h}, {in_w}, {in_c}), dtype={input_dtype}\")\n    print(f\"Model outputs: {len(output_details)} tensors\")\n\n    # Video I/O setup\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0:\n        fps = 30.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to create output video: {output_path}\")\n\n    print(f\"Processing video: {input_path}\")\n    print(f\"Saving annotated video to: {output_path}\")\n    print(f\"Frame size: {width}x{height}, FPS: {fps}\")\n\n    # Metrics containers\n    class_to_scores = {}  # class_id -> list of confidences\n    total_frames = 0\n    t0 = time.time()\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            total_frames += 1\n\n            # Preprocess\n            input_tensor = preprocess_frame(frame, (in_w, in_h), input_dtype)\n\n            # Inference\n            interpreter.set_tensor(input_index, input_tensor)\n            interpreter.invoke()\n\n            # Post-process\n            boxes, classes, scores, count = get_output(interpreter)\n            detections = []\n            for i in range(count):\n                score = float(scores[i])\n                if score < confidence_threshold:\n                    continue\n                class_id = int(classes[i])\n\n                # boxes are [ymin, xmin, ymax, xmax] in normalized coordinates\n                ymin = boxes[i][0]\n                xmin = boxes[i][1]\n                ymax = boxes[i][2]\n                xmax = boxes[i][3]\n\n                # Convert to pixel coordinates\n                x0 = int(xmin * width)\n                y0 = int(ymin * height)\n                x1 = int(xmax * width)\n                y1 = int(ymax * height)\n\n                x0, y0, x1, y1 = clip_bbox(x0, y0, x1, y1, width, height)\n                if x1 <= x0 or y1 <= y0:\n                    continue\n\n                detections.append({\n                    'box': (x0, y0, x1, y1),\n                    'score': score,\n                    'class_id': class_id\n                })\n\n                # Update proxy AP metrics store\n                if class_id not in class_to_scores:\n                    class_to_scores[class_id] = []\n                class_to_scores[class_id].append(score)\n\n            # Compute running proxy mAP\n            proxy_map = compute_proxy_map(class_to_scores)\n\n            # Draw and write frame\n            draw_detections(frame, detections, labels, proxy_map)\n            writer.write(frame)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    elapsed = time.time() - t0\n    proxy_map_final = compute_proxy_map(class_to_scores)\n    print(f\"Done. Processed {total_frames} frames in {elapsed:.2f}s \"\n          f\"({(total_frames / elapsed) if elapsed > 0 else 0:.2f} FPS).\")\n    print(f\"Final proxy mAP (no ground-truth): {proxy_map_final*100:.2f}%\")\n\nif __name__ == \"__main__\":\n    \"\"\"\n    TFLite object detection with TPU\n    - Setup: load TFLite interpreter with EdgeTPU and labels, open input video.\n    - Preprocessing: resize and normalize frames as required by the model.\n    - Inference: run model per frame, retrieve boxes/classes/scores.\n    - Output: draw detection boxes with labels and confidence, compute a proxy mAP\n      (note: without ground-truth annotations, this is an approximation based on mean\n       detection confidence per class), and save annotated video.\n    \"\"\"\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 28
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 69.597,
      "total_cost": 0.06052375,
      "observations": [
        {
          "id": "time-14-55-44-700550_chatcmpl-D6Ff7JdRLFVaNyMFayMe8jSqTh7Hz",
          "traceId": "8a9cc70d",
          "type": "GENERATION",
          "name": "8a9c_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:55:44.700000+00:00",
          "endTime": "2026-02-06T12:56:54.297000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06052375,
          "latency": 69597.0,
          "costDetails": {
            "total": 0.06052375
          },
          "totalTokens": 6447,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 5996,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:55:44.635Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:57:53.027Z"
    },
    {
      "id": "54ceff3a",
      "timestamp": "2026-02-06T12:53:19.269000+00:00",
      "name": "54ce_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# =========================\n# Utilities\n# =========================\ndef load_labels(path):\n    labels = {}\n    if not os.path.exists(path):\n        print(f\"Label file not found at: {path}\")\n        return labels\n    with open(path, 'r') as f:\n        for idx, line in enumerate(f):\n            line = line.strip()\n            if not line:\n                continue\n            # Try formats:\n            # 1) \"0 label\", \"1 sheep\"\n            parts = line.split(maxsplit=1)\n            if len(parts) == 2 and parts[0].isdigit():\n                labels[int(parts[0])] = parts[1].strip()\n                continue\n            # 2) \"0: label\"\n            if ':' in line:\n                left, right = line.split(':', 1)\n                if left.strip().isdigit():\n                    labels[int(left.strip())] = right.strip()\n                    continue\n            # 3) Just label per line\n            labels[idx] = line\n    return labels\n\ndef make_interpreter_with_edgetpu(model_path, edgetpu_lib=\"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"):\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate(edgetpu_lib)]\n        )\n    except (ValueError, OSError) as e:\n        raise RuntimeError(f\"Failed to load EdgeTPU delegate from {edgetpu_lib}: {e}\")\n    interpreter.allocate_tensors()\n    return interpreter\n\ndef get_output_tensors_dict(interpreter):\n    # Maps typical detection output tensors by name when available.\n    outputs = {\"boxes\": None, \"classes\": None, \"scores\": None, \"count\": None}\n    details = interpreter.get_output_details()\n    for d in details:\n        name = d.get(\"name\", \"\")\n        if isinstance(name, bytes):\n            name = name.decode(\"utf-8\", errors=\"ignore\")\n        lname = name.lower()\n        if \"box\" in lname:\n            outputs[\"boxes\"] = d\n        elif \"class\" in lname:\n            outputs[\"classes\"] = d\n        elif \"score\" in lname:\n            outputs[\"scores\"] = d\n        elif \"count\" in lname or \"num\" in lname:\n            outputs[\"count\"] = d\n    # Fallback heuristics if names are not informative\n    if outputs[\"boxes\"] is None or outputs[\"scores\"] is None or outputs[\"classes\"] is None:\n        for d in details:\n            shape = d.get(\"shape\", [])\n            if len(shape) == 3 and shape[-1] == 4:\n                outputs[\"boxes\"] = outputs[\"boxes\"] or d\n        # Collect 2D tensors [1, N]\n        two_d = [d for d in details if len(d.get(\"shape\", [])) == 2 and d[\"shape\"][0] == 1]\n        # Among 2D tensors, classes likely int/float with smaller dtype or named accordingly.\n        # Try to assign based on dtype and uniqueness\n        for d in two_d:\n            name = d.get(\"name\", \"\")\n            if isinstance(name, bytes):\n                name = name.decode(\"utf-8\", errors=\"ignore\")\n            lname = name.lower()\n            if \"class\" in lname and outputs[\"classes\"] is None:\n                outputs[\"classes\"] = d\n            elif \"score\" in lname and outputs[\"scores\"] is None:\n                outputs[\"scores\"] = d\n        # Last resort: assign remaining by dtype guesses\n        remaining = [d for d in two_d if d not in outputs.values()]\n        for d in remaining:\n            if outputs[\"scores\"] is None and d[\"dtype\"] == np.float32:\n                outputs[\"scores\"] = d\n            elif outputs[\"classes\"] is None:\n                outputs[\"classes\"] = d\n        # Count tensor often [1]\n        for d in details:\n            shape = d.get(\"shape\", [])\n            if len(shape) == 1 and shape[0] == 1:\n                outputs[\"count\"] = outputs[\"count\"] or d\n    return outputs\n\ndef draw_detections(frame, detections, labels, map_value=None):\n    # detections: list of dicts with keys: 'bbox' (xmin, ymin, xmax, ymax), 'score', 'class_id'\n    for det in detections:\n        (xmin, ymin, xmax, ymax) = det[\"bbox\"]\n        score = det[\"score\"]\n        class_id = det[\"class_id\"]\n        label = labels.get(class_id, str(class_id))\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n        text = f\"{label}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (xmin, ymin - th - baseline), (xmin + tw, ymin), color, -1)\n        cv2.putText(frame, text, (xmin, ymin - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Overlay mAP\n    map_text = \"mAP: N/A (no ground truth)\"\n    if isinstance(map_value, (float, int)):\n        map_text = f\"mAP: {map_value:.3f}\"\n    cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (50, 200, 255), 2, cv2.LINE_AA)\n\ndef preprocess_frame(frame, input_shape, input_dtype):\n    # input_shape: [1, height, width, channels]\n    _, in_h, in_w, in_c = input_shape\n    # Resize and convert color\n    resized = cv2.resize(frame, (in_w, in_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    if input_dtype == np.float32:\n        input_data = rgb.astype(np.float32) / 255.0\n    else:\n        input_data = rgb.astype(input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef postprocess_detections(interpreter, outputs, frame_size, threshold=0.5):\n    # frame_size: (width, height) of original frame\n    fw, fh = frame_size\n    # Retrieve tensors\n    def get_tensor(d):\n        return interpreter.get_tensor(d[\"index\"]) if d is not None else None\n\n    boxes = get_tensor(outputs[\"boxes\"])\n    classes = get_tensor(outputs[\"classes\"])\n    scores = get_tensor(outputs[\"scores\"])\n    count = get_tensor(outputs[\"count\"])\n\n    # Normalize expected shapes\n    if boxes is None or classes is None or scores is None:\n        return []\n\n    # Typical shapes: boxes [1, N, 4], classes [1, N], scores [1, N], count [1]\n    boxes = np.squeeze(boxes)\n    classes = np.squeeze(classes)\n    scores = np.squeeze(scores)\n    if count is not None:\n        count = int(np.squeeze(count))\n    else:\n        # Infer count from scores/boxes length\n        count = min(len(scores), len(boxes))\n\n    detections = []\n    for i in range(count):\n        score = float(scores[i])\n        if score < threshold:\n            continue\n        # TFLite boxes are [ymin, xmin, ymax, xmax] in normalized coordinates (0..1)\n        y_min, x_min, y_max, x_max = boxes[i]\n        xmin = max(0, min(int(x_min * fw), fw - 1))\n        ymin = max(0, min(int(y_min * fh), fh - 1))\n        xmax = max(0, min(int(x_max * fw), fw - 1))\n        ymax = max(0, min(int(y_max * fh), fh - 1))\n        if xmax <= xmin or ymax <= ymin:\n            continue\n        class_id = int(classes[i]) if i < len(classes) else -1\n        detections.append({\n            \"bbox\": (xmin, ymin, xmax, ymax),\n            \"score\": score,\n            \"class_id\": class_id\n        })\n    return detections\n\n# =========================\n# Main Application\n# =========================\ndef main():\n    # Setup: Load labels\n    labels = load_labels(label_path)\n    if not labels:\n        print(\"Warning: Labels could not be loaded or file is empty. Class IDs will be shown instead.\")\n\n    # Setup: Load TFLite interpreter with EdgeTPU delegate\n    print(\"Initializing TFLite Interpreter with EdgeTPU delegate...\")\n    interpreter = make_interpreter_with_edgetpu(model_path)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    if len(input_details) != 1:\n        print(f\"Warning: Model has {len(input_details)} input tensors; using the first one.\")\n\n    input_index = input_details[0][\"index\"]\n    input_shape = input_details[0][\"shape\"]\n    input_dtype = input_details[0][\"dtype\"]\n\n    # Map output tensors\n    outputs = get_output_tensors_dict(interpreter)\n\n    # Video IO\n    print(f\"Opening input video: {input_path}\")\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video at {input_path}\")\n\n    in_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 1e-2:\n        fps = 30.0  # default fallback\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    print(f\"Saving output to: {output_path} ({in_w}x{in_h} @ {fps:.2f} FPS)\")\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (in_w, in_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open VideoWriter for output at {output_path}\")\n\n    # Inference loop\n    frame_idx = 0\n    t0 = time.time()\n    # Placeholder for mAP (cannot be computed without ground truth)\n    computed_map = None\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_idx += 1\n\n            # Preprocessing\n            input_data = preprocess_frame(frame, input_shape, input_dtype)\n\n            # Set input tensor\n            interpreter.set_tensor(input_index, input_data)\n\n            # Inference\n            inf_start = time.time()\n            interpreter.invoke()\n            inf_end = time.time()\n\n            # Postprocess\n            detections = postprocess_detections(\n                interpreter, outputs, frame_size=(in_w, in_h), threshold=confidence_threshold\n            )\n\n            # Draw and annotate\n            annotated = frame.copy()\n            draw_detections(annotated, detections, labels, map_value=computed_map)\n\n            # Optionally overlay FPS and frame index\n            inf_ms = (inf_end - inf_start) * 1000.0\n            info_text = f\"Frame: {frame_idx}/{total_frames if total_frames>0 else '?'}  Inference: {inf_ms:.1f} ms\"\n            cv2.putText(annotated, info_text, (10, in_h - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 150, 0), 2, cv2.LINE_AA)\n\n            # Write to output\n            writer.write(annotated)\n\n            # Optional: print progress every 50 frames\n            if frame_idx % 50 == 0:\n                print(f\"Processed {frame_idx} frames...\")\n\n    finally:\n        cap.release()\n        writer.release()\n        elapsed = time.time() - t0\n        fps_effective = frame_idx / elapsed if elapsed > 0 else 0.0\n        print(f\"Done. Processed {frame_idx} frames in {elapsed:.2f}s ({fps_effective:.2f} FPS).\")\n        print(f\"Output saved to: {output_path}\")\n        if computed_map is None:\n            print(\"Note: mAP cannot be computed because no ground-truth annotations were provided.\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 27
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 65.579,
      "total_cost": 0.05714375,
      "observations": [
        {
          "id": "time-14-53-19-897866_chatcmpl-D6FcmF2CKmSbGvEplZYKaXguCZcJi",
          "traceId": "54ceff3a",
          "type": "GENERATION",
          "name": "54ce_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:53:19.897000+00:00",
          "endTime": "2026-02-06T12:54:25.476000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05714375,
          "latency": 65579.0,
          "costDetails": {
            "total": 0.05714375
          },
          "totalTokens": 6109,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 5658,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:53:19.833Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:55:23.976Z"
    },
    {
      "id": "2589ce9a",
      "timestamp": "2026-02-06T12:50:28.314000+00:00",
      "name": "2589_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# ================== Configuration Parameters ==================\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\nEDGETPU_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n# Optional ground-truth for mAP (per-frame text annotations). If this file exists,\n# it is used to compute mAP across frames.\n# Format per line: frame_index class_id x_min y_min x_max y_max  (pixel coordinates)\nGT_PATH = INPUT_PATH + \".gt.txt\"\n\n# ================== Utility Functions ==================\ndef load_labels(path):\n    labels = {}\n    if not os.path.exists(path):\n        return labels\n    with open(path, \"r\") as f:\n        for i, line in enumerate(f.readlines()):\n            line = line.strip()\n            if not line:\n                continue\n            # Try formats: \"id label\" or \"label\"\n            parts = line.split(maxsplit=1)\n            if len(parts) == 2 and parts[0].isdigit():\n                labels[int(parts[0])] = parts[1].strip()\n            else:\n                labels[i] = line\n    return labels\n\ndef load_ground_truth(path):\n    # Returns dict: frame_idx -> list of dicts {class_id, bbox:[x1,y1,x2,y2]}\n    gts = {}\n    if not os.path.exists(path):\n        return gts\n    with open(path, \"r\") as f:\n        for line in f:\n            s = line.strip()\n            if not s or s.startswith(\"#\"):\n                continue\n            parts = s.split()\n            if len(parts) != 6:\n                continue\n            try:\n                frame_idx = int(parts[0])\n                class_id = int(parts[1])\n                x1 = float(parts[2]); y1 = float(parts[3]); x2 = float(parts[4]); y2 = float(parts[5])\n            except:\n                continue\n            if frame_idx not in gts:\n                gts[frame_idx] = []\n            gts[frame_idx].append({\"class_id\": class_id, \"bbox\": [x1, y1, x2, y2]})\n    return gts\n\ndef make_interpreter(model_path, edgetpu_lib):\n    return Interpreter(model_path=model_path, experimental_delegates=[load_delegate(edgetpu_lib)])\n\ndef set_input_tensor(interpreter, input_image_rgb):\n    input_details = interpreter.get_input_details()[0]\n    input_index = input_details[\"index\"]\n    # Ensure dtype matches model\n    if input_details[\"dtype\"] == np.uint8:\n        input_data = input_image_rgb.astype(np.uint8)\n    else:\n        # If model expects float, convert accordingly (rare for EdgeTPU)\n        input_data = input_image_rgb.astype(np.float32)\n    input_data = np.expand_dims(input_data, axis=0)\n    interpreter.set_tensor(input_index, input_data)\n\ndef get_input_size(interpreter):\n    input_details = interpreter.get_input_details()[0]\n    # Expect shape: [1, height, width, channels]\n    return int(input_details[\"shape\"][2]), int(input_details[\"shape\"][1])  # width, height\n\ndef detect_objects(interpreter, image_rgb, threshold):\n    # Assumes typical EdgeTPU SSD detection outputs:\n    # 0: boxes [1, num, 4], 1: classes [1, num], 2: scores [1, num], 3: count [1]\n    set_input_tensor(interpreter, image_rgb)\n    t0 = time.time()\n    interpreter.invoke()\n    infer_ms = (time.time() - t0) * 1000.0\n\n    output_details = interpreter.get_output_details()\n    boxes = interpreter.get_tensor(output_details[0][\"index\"])[0]\n    classes = interpreter.get_tensor(output_details[1][\"index\"])[0]\n    scores = interpreter.get_tensor(output_details[2][\"index\"])[0]\n    # Count may be float or int; ensure int\n    raw_count = interpreter.get_tensor(output_details[3][\"index\"])\n    count = int(raw_count.flatten()[0]) if raw_count.size > 0 else len(scores)\n\n    results = []\n    for i in range(count):\n        score = float(scores[i])\n        if score < threshold:\n            continue\n        cls = int(classes[i])\n        ymin, xmin, ymax, xmax = boxes[i]\n        results.append({\n            \"bbox_norm\": [xmin, ymin, xmax, ymax],  # normalized [0..1]\n            \"score\": score,\n            \"class_id\": cls\n        })\n    return results, infer_ms\n\ndef norm_to_abs_bbox(bbox_norm, frame_w, frame_h):\n    xmin_n, ymin_n, xmax_n, ymax_n = bbox_norm\n    x1 = max(0, min(frame_w - 1, int(xmin_n * frame_w)))\n    y1 = max(0, min(frame_h - 1, int(ymin_n * frame_h)))\n    x2 = max(0, min(frame_w - 1, int(xmax_n * frame_w)))\n    y2 = max(0, min(frame_h - 1, int(ymax_n * frame_h)))\n    return [x1, y1, x2, y2]\n\ndef draw_prediction(frame, bbox, label_text, color=(0, 255, 0)):\n    x1, y1, x2, y2 = bbox\n    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n    # Draw label background\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 0.5\n    thickness = 1\n    (text_w, text_h), baseline = cv2.getTextSize(label_text, font, font_scale, thickness)\n    y_text = max(0, y1 - 5)\n    cv2.rectangle(frame, (x1, y_text - text_h - baseline), (x1 + text_w + 2, y_text + 2), color, -1)\n    cv2.putText(frame, label_text, (x1 + 1, y_text), font, font_scale, (0, 0, 0), thickness, cv2.LINE_AA)\n\ndef compute_iou(boxA, boxB):\n    # boxes: [x1, y1, x2, y2]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = xB - xA + 1\n    inter_h = yB - yA + 1\n    if inter_w <= 0 or inter_h <= 0:\n        return 0.0\n    inter_area = inter_w * inter_h\n    boxA_area = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxB_area = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    union = boxA_area + boxB_area - inter_area\n    if union <= 0:\n        return 0.0\n    return inter_area / union\n\ndef update_map_stats(stats, preds_frame, gts_frame, iou_thresh=0.5):\n    # stats: class_id -> {\"scores\": [], \"matches\": [], \"gt_count\": int}\n    # Greedy match per class within this frame\n    # Prepare GT availability flags per class\n    gt_by_class = {}\n    for gt in gts_frame:\n        cid = gt[\"class_id\"]\n        if cid not in gt_by_class:\n            gt_by_class[cid] = []\n        gt_by_class[cid].append({\"bbox\": gt[\"bbox\"], \"used\": False})\n    pred_by_class = {}\n    for pr in preds_frame:\n        cid = pr[\"class_id\"]\n        if cid not in pred_by_class:\n            pred_by_class[cid] = []\n        pred_by_class[cid].append(pr)\n\n    # Update per-class stats\n    for cid in set(list(gt_by_class.keys()) + list(pred_by_class.keys())):\n        if cid not in stats:\n            stats[cid] = {\"scores\": [], \"matches\": [], \"gt_count\": 0}\n        gt_list = gt_by_class.get(cid, [])\n        pr_list = pred_by_class.get(cid, [])\n\n        # Count ground truths\n        stats[cid][\"gt_count\"] += len(gt_list)\n\n        # Sort predictions in this frame by score descending for greedy matching\n        pr_list_sorted = sorted(pr_list, key=lambda x: x[\"score\"], reverse=True)\n\n        for pr in pr_list_sorted:\n            best_iou = 0.0\n            best_j = -1\n            for j, gt in enumerate(gt_list):\n                if gt[\"used\"]:\n                    continue\n                iou = compute_iou(pr[\"bbox\"], gt[\"bbox\"])\n                if iou > best_iou:\n                    best_iou = iou\n                    best_j = j\n            is_tp = 0\n            if best_iou >= iou_thresh and best_j >= 0:\n                is_tp = 1\n                gt_list[best_j][\"used\"] = True\n            stats[cid][\"scores\"].append(float(pr[\"score\"]))\n            stats[cid][\"matches\"].append(int(is_tp))\n\ndef compute_ap(scores, matches, gt_count):\n    # scores: list of floats\n    # matches: list of 0/1 (1 means TP), ordered by any time, we will sort by scores desc\n    if gt_count == 0:\n        return None\n    if len(scores) == 0:\n        return 0.0\n    scores_np = np.array(scores, dtype=np.float32)\n    matches_np = np.array(matches, dtype=np.int32)\n\n    order = np.argsort(-scores_np)\n    matches_sorted = matches_np[order]\n\n    tp_cum = np.cumsum(matches_sorted)\n    fp_cum = np.cumsum(1 - matches_sorted)\n\n    denom = tp_cum + fp_cum\n    denom[denom == 0] = 1  # avoid div by zero\n    precisions = tp_cum / denom\n    recalls = tp_cum / float(gt_count)\n\n    # VOC-style AP: integrate precision envelope over recall\n    mrec = np.concatenate(([0.0], recalls, [1.0]))\n    mpre = np.concatenate(([0.0], precisions, [0.0]))\n    # Precision envelope\n    for i in range(mpre.size - 1, 0, -1):\n        if mpre[i - 1] < mpre[i]:\n            mpre[i - 1] = mpre[i]\n    # Compute area under PR curve\n    ap = 0.0\n    for i in range(1, mrec.size):\n        if mrec[i] != mrec[i - 1]:\n            ap += (mrec[i] - mrec[i - 1]) * mpre[i]\n    return float(ap)\n\ndef compute_map(stats):\n    # stats: class_id -> {\"scores\": [], \"matches\": [], \"gt_count\": int}\n    aps = []\n    for cid, d in stats.items():\n        ap = compute_ap(d[\"scores\"], d[\"matches\"], d[\"gt_count\"])\n        if ap is not None:\n            aps.append(ap)\n    if len(aps) == 0:\n        return None\n    return float(np.mean(np.array(aps, dtype=np.float32)))\n\ndef pick_label_name(class_id, labels):\n    # Try exact, then +1 offset\n    if class_id in labels:\n        return labels[class_id]\n    if (class_id + 1) in labels:\n        return labels[class_id + 1]\n    return str(class_id)\n\n# ================== Main Pipeline ==================\ndef main():\n    # Prepare output directory\n    out_dir = os.path.dirname(OUTPUT_PATH)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Load optional ground-truth annotations\n    gt_all = load_ground_truth(GT_PATH)\n    has_gt = len(gt_all) > 0\n\n    # Initialize TFLite Interpreter with EdgeTPU\n    interpreter = make_interpreter(MODEL_PATH, EDGETPU_LIB)\n    interpreter.allocate_tensors()\n    in_w, in_h = get_input_size(interpreter)\n\n    # Open input video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(\"ERROR: Cannot open input video:\", INPUT_PATH)\n        return\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 1e-3:\n        fps = 30.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Initialize writer\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        print(\"ERROR: Cannot open output video for writing:\", OUTPUT_PATH)\n        cap.release()\n        return\n\n    # mAP stats across frames\n    map_stats = {}  # class_id -> {\"scores\": [], \"matches\": [], \"gt_count\": int}\n    running_map = None\n\n    frame_idx = 0\n    last_map_str = \"mAP: N/A\"\n    try:\n        while True:\n            ret, frame_bgr = cap.read()\n            if not ret:\n                break\n\n            # Preprocessing: resize to model input and convert BGR->RGB\n            resized = cv2.resize(frame_bgr, (in_w, in_h))\n            frame_rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n            # Inference\n            detections, infer_ms = detect_objects(interpreter, frame_rgb, CONFIDENCE_THRESHOLD)\n\n            # Convert normalized boxes to absolute on original frame size\n            preds_abs = []\n            for det in detections:\n                abs_box = norm_to_abs_bbox(det[\"bbox_norm\"], frame_w, frame_h)\n                preds_abs.append({\n                    \"bbox\": abs_box,\n                    \"score\": det[\"score\"],\n                    \"class_id\": det[\"class_id\"]\n                })\n\n            # Prepare ground truths for this frame, if available\n            gts_frame = gt_all.get(frame_idx, []) if has_gt else []\n\n            # Update mAP stats and compute running mAP\n            if has_gt:\n                update_map_stats(map_stats, preds_abs, gts_frame, iou_thresh=0.5)\n                running_map = compute_map(map_stats)\n                if running_map is not None:\n                    last_map_str = \"mAP: {:.3f}\".format(running_map)\n                else:\n                    last_map_str = \"mAP: N/A\"\n            else:\n                last_map_str = \"mAP: N/A\"\n\n            # Draw detections\n            overlay = frame_bgr.copy()\n            for pr in preds_abs:\n                cls_id = pr[\"class_id\"]\n                label = pick_label_name(cls_id, labels)\n                conf = pr[\"score\"]\n                text = \"{}: {:.2f}\".format(label, conf)\n                draw_prediction(overlay, pr[\"bbox\"], text, color=(0, 255, 0))\n\n            # Draw mAP and inference time\n            info_text = \"{} | {:.1f} ms\".format(last_map_str, infer_ms)\n            cv2.putText(overlay, info_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2, cv2.LINE_AA)\n\n            # Write frame\n            writer.write(overlay)\n\n            frame_idx += 1\n\n    finally:\n        cap.release()\n        writer.release()\n\n    # Print final summary\n    if has_gt:\n        final_map = compute_map(map_stats)\n        if final_map is not None:\n            print(\"Final mAP over processed video: {:.4f}\".format(final_map))\n        else:\n            print(\"Final mAP: N/A (no valid ground-truth instances)\")\n    else:\n        print(\"No ground-truth file found at {}. mAP not computed.\".format(GT_PATH))\n    print(\"Output saved to:\", OUTPUT_PATH)\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 26
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 91.395,
      "total_cost": 0.09133375,
      "observations": [
        {
          "id": "time-14-50-28-899921_chatcmpl-D6Fa1gtR3rSDxB5gSdFAJfY7XBy0K",
          "traceId": "2589ce9a",
          "type": "GENERATION",
          "name": "2589_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:50:28.899000+00:00",
          "endTime": "2026-02-06T12:52:00.294000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.09133375,
          "latency": 91395.0,
          "costDetails": {
            "total": 0.09133375
          },
          "totalTokens": 9528,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 9077,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:50:28.867Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:52:58.542Z"
    },
    {
      "id": "7eb3bea6",
      "timestamp": "2026-02-06T12:47:24.428000+00:00",
      "name": "7eb3_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\nedgetpu_lib = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\ndef load_labels(path):\n    labels = {}\n    try:\n        with open(path, \"r\") as f:\n            for idx, line in enumerate(f):\n                line = line.strip()\n                if not line or line.startswith(\"#\"):\n                    continue\n                # Support \"id label\", \"id: label\", or plain \"label\"\n                if \":\" in line:\n                    parts = [p.strip() for p in line.split(\":\", 1)]\n                    if len(parts) == 2 and parts[0].isdigit():\n                        labels[int(parts[0])] = parts[1]\n                        continue\n                parts = line.split(maxsplit=1)\n                if len(parts) == 2 and parts[0].isdigit():\n                    labels[int(parts[0])] = parts[1].strip()\n                else:\n                    labels[idx] = line\n    except Exception as e:\n        print(f\"Warning: Failed to load labels from {path}: {e}\")\n    return labels\n\n\ndef make_interpreter(model_path, delegate_path):\n    return Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate(delegate_path)]\n    )\n\n\ndef preprocess(frame_bgr, input_size, input_dtype):\n    # Convert BGR (OpenCV) to RGB as most TFLite detection models expect RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (input_size[1], input_size[0]))\n    if input_dtype == np.float32:\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        input_data = resized.astype(np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n\ndef postprocess(frame_shape, boxes, classes, scores, num, conf_thres):\n    h, w = frame_shape[:2]\n    detections = []\n    count = int(num)\n    for i in range(count):\n        score = float(scores[i])\n        if score < conf_thres:\n            continue\n        cls_id = int(classes[i])\n        # boxes are typically in [ymin, xmin, ymax, xmax] normalized coordinates\n        y_min, x_min, y_max, x_max = boxes[i]\n        x0 = max(0, min(w - 1, int(x_min * w)))\n        y0 = max(0, min(h - 1, int(y_min * h)))\n        x1 = max(0, min(w - 1, int(x_max * w)))\n        y1 = max(0, min(h - 1, int(y_max * h)))\n        # Ensure proper rectangle coordinates\n        x0, x1 = (x0, x1) if x0 <= x1 else (x1, x0)\n        y0, y1 = (y0, y1) if y0 <= y1 else (y1, y0)\n        detections.append((x0, y0, x1, y1, cls_id, score))\n    return detections\n\n\ndef draw_detections(frame, detections, labels, map_value=None):\n    for (x0, y0, x1, y1, cls_id, score) in detections:\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (x0, y0), (x1, y1), color, 2)\n        # Try both 0-based and 1-based indexing for labels\n        label_text = labels.get(cls_id, labels.get(cls_id + 1, str(cls_id)))\n        text = f\"{label_text}: {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x0, y0 - th - baseline), (x0 + tw, y0), color, -1)\n        cv2.putText(frame, text, (x0, y0 - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    if map_value is not None:\n        map_text = f\"mAP (proxy): {map_value:.3f}\"\n        cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (50, 200, 255), 2, cv2.LINE_AA)\n\n\n# Heuristic, proxy mAP computation without ground-truth:\n# - For each class and frame, the highest-scoring detection is treated as TP, all other detections of that class in that frame are FP.\n# - AP is computed across frames based on scores, integrating the precision-recall curve where recall is TP / (number of frames with at least one detection for that class).\n# This does not reflect true mAP without ground-truth, but serves as a proxy metric for demonstration.\ndef update_ap_data_for_frame(detections, ap_data, num_pos_by_class):\n    # Sort detections in current frame by score descending per class\n    per_class = {}\n    for det in detections:\n        x0, y0, x1, y1, cls_id, score = det\n        per_class.setdefault(cls_id, []).append(score)\n    # For each class, mark the highest-scoring detection as TP, others as FP\n    for cls_id, scores in per_class.items():\n        scores_sorted = sorted(scores, reverse=True)\n        for j, s in enumerate(scores_sorted):\n            tp_flag = 1 if j == 0 else 0\n            ap_data.setdefault(cls_id, []).append((float(s), tp_flag))\n        # Count one positive \"instance\" for this frame for that class (proxy)\n        num_pos_by_class[cls_id] = num_pos_by_class.get(cls_id, 0) + 1\n\n\ndef compute_ap(scores_and_tp, num_pos):\n    if num_pos <= 0 or len(scores_and_tp) == 0:\n        return None\n    # Sort by score descending\n    arr = sorted(scores_and_tp, key=lambda x: x[0], reverse=True)\n    tps = np.array([tp for _, tp in arr], dtype=np.float32)\n    fps = 1.0 - tps\n    cum_tp = np.cumsum(tps)\n    cum_fp = np.cumsum(fps)\n    precisions = cum_tp / (cum_tp + cum_fp + 1e-8)\n    recalls = cum_tp / (num_pos + 1e-8)\n\n    # Precision envelope (monotonic decreasing)\n    mprec = precisions.copy()\n    for i in range(len(mprec) - 2, -1, -1):\n        if mprec[i] < mprec[i + 1]:\n            mprec[i] = mprec[i + 1]\n\n    # Integrate area under precision-recall curve\n    ap = 0.0\n    prev_recall = 0.0\n    for i in range(len(recalls)):\n        if i == 0 or recalls[i] != recalls[i - 1]:\n            ap += (recalls[i] - prev_recall) * mprec[i]\n            prev_recall = recalls[i]\n    # Bound AP to [0,1]\n    ap = float(max(0.0, min(1.0, ap)))\n    return ap\n\n\ndef compute_map(ap_data, num_pos_by_class):\n    aps = []\n    for cls_id, data in ap_data.items():\n        ap = compute_ap(data, num_pos_by_class.get(cls_id, 0))\n        if ap is not None:\n            aps.append(ap)\n    if len(aps) == 0:\n        return None\n    return float(np.mean(aps))\n\n\ndef main():\n    # 1) Setup: Interpreter, labels, video IO\n    labels = load_labels(label_path)\n    interpreter = make_interpreter(model_path, edgetpu_lib)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    input_index = input_details[0][\"index\"]\n    in_h, in_w = input_details[0][\"shape\"][1], input_details[0][\"shape\"][2]\n    input_dtype = input_details[0][\"dtype\"]\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"Error: Failed to open input video: {input_path}\")\n        return\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        print(f\"Error: Failed to open video writer for: {output_path}\")\n        cap.release()\n        return\n\n    # For proxy mAP calculation\n    ap_data = {}  # class_id -> list of (score, tp_flag)\n    num_pos_by_class = {}  # class_id -> count of frames with at least one detection\n\n    frame_count = 0\n    t0 = time.time()\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_count += 1\n\n            # 2) Preprocessing\n            input_data = preprocess(frame, (in_h, in_w), input_dtype)\n\n            # 3) Inference\n            interpreter.set_tensor(input_index, input_data)\n            interpreter.invoke()\n\n            # Attempt standard TF Lite detection postprocess outputs ordering\n            try:\n                boxes = interpreter.get_tensor(output_details[0][\"index\"])[0]\n                classes = interpreter.get_tensor(output_details[1][\"index\"])[0]\n                scores = interpreter.get_tensor(output_details[2][\"index\"])[0]\n                num = interpreter.get_tensor(output_details[3][\"index\"])[0]\n            except Exception:\n                # Fallback: re-read in case ordering differs; here we still assume standard 4 outputs.\n                outs = [interpreter.get_tensor(od[\"index\"]) for od in output_details]\n                # Heuristic assignment\n                boxes = None\n                classes = None\n                scores = None\n                num = None\n                for out in outs:\n                    arr = np.squeeze(out)\n                    if arr.ndim == 2 and arr.shape[-1] == 4:\n                        boxes = arr\n                    elif arr.ndim == 1 and arr.size > 4 and np.issubdtype(arr.dtype, np.floating):\n                        # could be scores or classes\n                        if scores is None:\n                            scores = arr\n                        else:\n                            classes = arr\n                    elif arr.ndim == 0 or (arr.ndim == 1 and arr.size == 1):\n                        num = arr\n                if boxes is None or classes is None or scores is None or num is None:\n                    boxes = np.zeros((0, 4), dtype=np.float32)\n                    classes = np.zeros((0,), dtype=np.int32)\n                    scores = np.zeros((0,), dtype=np.float32)\n                    num = 0\n\n            # Ensure correct types\n            classes = classes.astype(np.int32, copy=False)\n            detections = postprocess(frame.shape, boxes, classes, scores, num, confidence_threshold)\n\n            # 4) Output handling: update proxy mAP and draw results\n            update_ap_data_for_frame(detections, ap_data, num_pos_by_class)\n            current_map = compute_map(ap_data, num_pos_by_class)\n            draw_detections(frame, detections, labels, map_value=(current_map if current_map is not None else 0.0))\n\n            writer.write(frame)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    elapsed = time.time() - t0\n    final_map = compute_map(ap_data, num_pos_by_class)\n    if final_map is None:\n        final_map = 0.0\n    print(f\"Processed {frame_count} frames in {elapsed:.2f}s ({(frame_count/elapsed) if elapsed>0 else 0:.2f} FPS).\")\n    print(f\"Saved output video to: {output_path}\")\n    print(f\"Proxy mAP over the processed video: {final_map:.4f} (note: computed without ground-truth)\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 25
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 103.814,
      "total_cost": 0.07889375,
      "observations": [
        {
          "id": "time-14-47-25-042361_chatcmpl-D6FX3FVx2irgEffqBFYD9YuqatRzT",
          "traceId": "7eb3bea6",
          "type": "GENERATION",
          "name": "7eb3_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:47:25.042000+00:00",
          "endTime": "2026-02-06T12:49:08.856000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07889375,
          "latency": 103814.0,
          "costDetails": {
            "total": 0.07889375
          },
          "totalTokens": 8284,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 7833,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:47:24.986Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:50:07.708Z"
    },
    {
      "id": "6d015089",
      "timestamp": "2026-02-06T12:45:16.559000+00:00",
      "name": "6d01_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# =========================\n# Utility Functions\n# =========================\ndef load_labels(path):\n    \"\"\"Load label map from file. Supports:\n       - 'index label' per line, or\n       - single label per line (index inferred from line number).\n    \"\"\"\n    labels = {}\n    try:\n        with open(path, 'r') as f:\n            lines = [l.strip() for l in f if l.strip()]\n        for i, line in enumerate(lines):\n            parts = line.split()\n            if len(parts) >= 2 and parts[0].isdigit():\n                idx = int(parts[0])\n                name = \" \".join(parts[1:])\n            else:\n                idx = i\n                name = line\n            labels[idx] = name\n    except Exception as e:\n        print(f\"Warning: Failed to load labels from {path}: {e}\")\n        labels = {}\n    return labels\n\ndef make_interpreter_tpu(model_path, edgetpu_lib='/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0'):\n    \"\"\"Create TFLite interpreter with EdgeTPU delegate.\"\"\"\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate(edgetpu_lib)]\n        )\n        print(\"EdgeTPU delegate loaded successfully.\")\n    except Exception as e:\n        # If delegate fails, raise error, as guideline requests TPU usage.\n        raise RuntimeError(f\"Failed to load EdgeTPU delegate from {edgetpu_lib}: {e}\")\n    return interpreter\n\ndef preprocess_frame_bgr(frame_bgr, input_details):\n    \"\"\"Resize and format frame to model input shape/dtype.\"\"\"\n    _, in_h, in_w, in_c = input_details[0]['shape']\n    dtype = input_details[0]['dtype']\n    # Convert BGR->RGB and resize\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n    input_data = np.expand_dims(resized, axis=0)\n    if dtype == np.float32:\n        input_data = (input_data.astype(np.float32) / 255.0)\n    elif dtype == np.uint8:\n        input_data = input_data.astype(np.uint8)\n    else:\n        # Default to original dtype casting\n        input_data = input_data.astype(dtype)\n    return input_data\n\ndef parse_detections(interpreter, output_details):\n    \"\"\"Parse model outputs assuming SSD-style outputs:\n       - boxes: [1, num, 4] (ymin, xmin, ymax, xmax), normalized [0,1]\n       - classes: [1, num]\n       - scores: [1, num]\n       - num_detections: [1]\n    \"\"\"\n    # Attempt to infer by common ordering\n    try:\n        boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n        classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(np.int32)\n        scores = interpreter.get_tensor(output_details[2]['index'])[0]\n        if len(output_details) > 3:\n            num = int(interpreter.get_tensor(output_details[3]['index'])[0])\n        else:\n            num = len(scores)\n    except Exception:\n        # Fallback: search by shapes\n        boxes = classes = scores = None\n        num = None\n        for od in output_details:\n            out = interpreter.get_tensor(od['index'])\n            shp = out.shape\n            if len(shp) == 3 and shp[-1] == 4:\n                boxes = out[0]\n            elif len(shp) == 2 and shp[0] == 1:\n                # Could be classes or scores\n                arr = out[0]\n                # Heuristic: scores are floats in [0,1]; classes often near small ints or floats of ints\n                if arr.dtype.kind == 'f' and np.all((arr >= 0.0) & (arr <= 1.0)):\n                    scores = arr\n                else:\n                    classes = arr.astype(np.int32)\n            elif len(shp) == 1 and shp[0] == 1:\n                num = int(out[0])\n        if num is None and scores is not None:\n            num = len(scores)\n        if boxes is None or classes is None or scores is None:\n            raise RuntimeError(\"Unable to parse detection outputs from the model.\")\n    return boxes, classes, scores, num\n\ndef draw_detections(frame, detections, labels, approx_map):\n    \"\"\"Draw bounding boxes, labels, and mAP on the frame.\"\"\"\n    h, w = frame.shape[:2]\n    for det in detections:\n        ymin, xmin, ymax, xmax, cls_id, score = det\n        # Scale to pixel coordinates\n        left = max(0, int(xmin * w))\n        top = max(0, int(ymin * h))\n        right = min(w - 1, int(xmax * w))\n        bottom = min(h - 1, int(ymax * h))\n        # Choose a color based on class id for consistency\n        np.random.seed(cls_id + 12345)\n        color = tuple(int(c) for c in np.random.randint(0, 255, 3))\n        # Draw rectangle\n        cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n        # Label text\n        label_text = labels.get(cls_id, str(cls_id))\n        caption = f\"{label_text} {score:.2f}\"\n        # Put text background\n        (txt_w, txt_h), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (left, max(0, top - txt_h - baseline)), (left + txt_w, top), color, thickness=-1)\n        cv2.putText(frame, caption, (left, top - baseline), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)\n\n    # Draw approximate mAP on the top-left\n    cv2.putText(frame, f\"mAP (approx.): {approx_map:.3f}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (30, 220, 30), 2, cv2.LINE_AA)\n\ndef compute_running_map_approx(per_class_sum, per_class_count):\n    \"\"\"Approximate mAP as the mean of per-class average confidence over observed classes.\n       Note: True mAP requires ground-truth and IoU matching; this is a proxy metric.\n    \"\"\"\n    aps = []\n    for cls_id, cnt in per_class_count.items():\n        if cnt > 0:\n            aps.append(per_class_sum[cls_id] / float(cnt))\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\n# =========================\n# Main Pipeline\n# =========================\ndef main():\n    # Validate input\n    if not os.path.isfile(model_path):\n        raise FileNotFoundError(f\"Model not found at {model_path}\")\n    if not os.path.isfile(label_path):\n        print(f\"Warning: Label file not found at {label_path}. Proceeding without labels.\")\n    if not os.path.isfile(input_path):\n        raise FileNotFoundError(f\"Input video not found at {input_path}\")\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize interpreter with EdgeTPU\n    interpreter = make_interpreter_tpu(model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Initialize video I/O\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    in_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 1e-3:\n        fps = 30.0  # default fallback\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (in_w, in_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video writer at {output_path}\")\n\n    # Stats for approximate mAP\n    per_class_sum = {}    # cls_id -> sum(confidence)\n    per_class_count = {}  # cls_id -> count\n    total_frames = 0\n    t0 = time.time()\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            total_frames += 1\n\n            # Preprocess\n            input_data = preprocess_frame_bgr(frame, input_details)\n            interpreter.set_tensor(input_details[0]['index'], input_data)\n\n            # Inference\n            interpreter.invoke()\n\n            # Parse detections\n            boxes, classes, scores, num = parse_detections(interpreter, output_details)\n\n            # Collect filtered detections\n            filtered = []\n            for i in range(num):\n                score = float(scores[i])\n                if score < confidence_threshold:\n                    continue\n                cls_id = int(classes[i])\n                ymin, xmin, ymax, xmax = boxes[i]\n                filtered.append((ymin, xmin, ymax, xmax, cls_id, score))\n                # Update stats\n                per_class_sum[cls_id] = per_class_sum.get(cls_id, 0.0) + score\n                per_class_count[cls_id] = per_class_count.get(cls_id, 0) + 1\n\n            # Compute approximate mAP\n            approx_map = compute_running_map_approx(per_class_sum, per_class_count)\n\n            # Draw detections and mAP\n            draw_detections(frame, filtered, labels, approx_map)\n\n            # Write frame\n            writer.write(frame)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    elapsed = time.time() - t0\n    approx_map = compute_running_map_approx(per_class_sum, per_class_count)\n    print(\"Processing complete.\")\n    print(f\"Frames processed: {total_frames}\")\n    print(f\"Elapsed time: {elapsed:.2f}s, FPS: { (total_frames / elapsed) if elapsed > 0 else 0.0:.2f}\")\n    print(f\"Approximate mAP (confidence-based proxy): {approx_map:.4f}\")\n    print(f\"Output saved to: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 24
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 48.822,
      "total_cost": 0.04727375,
      "observations": [
        {
          "id": "time-14-45-17-155707_chatcmpl-D6FUzAjtB4z3cxcoz83RbCBZXaYoJ",
          "traceId": "6d015089",
          "type": "GENERATION",
          "name": "6d01_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:45:17.155000+00:00",
          "endTime": "2026-02-06T12:46:05.977000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.04727375,
          "latency": 48822.0,
          "costDetails": {
            "total": 0.04727375
          },
          "totalTokens": 5122,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 4671,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:45:17.130Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:47:04.296Z"
    },
    {
      "id": "10b705aa",
      "timestamp": "2026-02-06T12:42:15.667000+00:00",
      "name": "10b7_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# CONFIGURATION PARAMETERS\n# =========================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# EdgeTPU shared library path on Google Coral Dev Board\nedgetpu_lib_path = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\n# =========================\n# UTILITY FUNCTIONS\n# =========================\ndef load_labels(path):\n    \"\"\"\n    Loads labels from a text file.\n    Supports either:\n      - one label per line (index inferred by line number), or\n      - lines in the format: \"<id> <label...>\"\n    Returns a dict: {id: label}\n    \"\"\"\n    labels = {}\n    if not os.path.exists(path):\n        raise FileNotFoundError(\"Label file not found: {}\".format(path))\n    with open(path, 'r') as f:\n        lines = [l.strip() for l in f.readlines() if l.strip()]\n    # Try to parse \"id label...\" format; fallback to enumerated lines\n    parsed_any_id = False\n    for i, line in enumerate(lines):\n        parts = line.split()\n        if parts and parts[0].isdigit():\n            idx = int(parts[0])\n            label = \" \".join(parts[1:]) if len(parts) > 1 else str(idx)\n            labels[idx] = label\n            parsed_any_id = True\n        else:\n            labels[i] = line  # temporary; if any \"id label\" parsed, this will be overridden later\n    if parsed_any_id:\n        # Ensure all indices present; do nothing extra\n        return labels\n    else:\n        # Overwrite with enumerated if no explicit ids were found\n        return {i: l for i, l in enumerate(lines)}\n\n\ndef make_interpreter(model_path, edgetpu_lib_path):\n    \"\"\"\n    Creates and returns a TFLite Interpreter with EdgeTPU delegate if available.\n    \"\"\"\n    try:\n        delegate = load_delegate(edgetpu_lib_path)\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate])\n    except Exception as e:\n        print(\"Warning: Failed to load EdgeTPU delegate ({}). Falling back to CPU.\".format(e))\n        interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef get_input_details(interpreter):\n    \"\"\"\n    Returns input details: (index, dtype, height, width, quant_scale, quant_zero_point)\n    \"\"\"\n    input_details = interpreter.get_input_details()\n    if not input_details:\n        raise RuntimeError(\"No input details found in the interpreter.\")\n    d = input_details[0]\n    index = d['index']\n    dtype = d['dtype']\n    shape = d['shape']\n    # Expected shape [1, height, width, channels]\n    if len(shape) != 4:\n        raise RuntimeError(\"Unexpected input tensor shape: {}\".format(shape))\n    height, width = int(shape[1]), int(shape[2])\n    scale, zero_point = d.get('quantization', (0.0, 0))\n    return index, dtype, height, width, float(scale), int(zero_point)\n\n\ndef preprocess_frame_bgr(frame_bgr, input_h, input_w, input_dtype, quant_scale, quant_zero):\n    \"\"\"\n    Preprocess OpenCV BGR frame to model input tensor:\n      - convert BGR to RGB\n      - resize to (input_w, input_h)\n      - apply quantization or normalization based on model input type\n      - add batch dimension\n    Returns np.ndarray of shape [1, input_h, input_w, 3] with appropriate dtype.\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (input_w, input_h), interpolation=cv2.INTER_LINEAR)\n    input_data = np.expand_dims(resized, axis=0)\n\n    if input_dtype == np.float32:\n        input_data = input_data.astype(np.float32) / 255.0\n    else:\n        # Quantized path (uint8), apply scale and zero-point if provided\n        if quant_scale and quant_scale > 0:\n            # De-quantization expects: real_value = scale * (quantized_value - zero_point)\n            # For quantization we invert: quantized = real_value / scale + zero_point\n            # real_value currently in [0..255]\n            input_data = input_data.astype(np.float32) / quant_scale + quant_zero\n            input_data = np.clip(np.rint(input_data), 0, 255).astype(np.uint8)\n        else:\n            input_data = input_data.astype(np.uint8)\n    return input_data\n\n\ndef extract_detections(interpreter, score_threshold, img_w, img_h, labels):\n    \"\"\"\n    Extracts detection results (boxes, class_ids, scores) from the interpreter outputs.\n    Returns a list of dicts:\n      [{'bbox': (x_min, y_min, x_max, y_max), 'class_id': int, 'score': float, 'label': str}, ...]\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n    boxes, class_ids, scores, num = None, None, None, None\n\n    # Try to resolve by tensor names first\n    for i, od in enumerate(output_details):\n        name = od.get('name', '').lower()\n        arr = outputs[i]\n        if 'box' in name or 'location' in name:\n            boxes = arr\n        elif 'class' in name:\n            class_ids = arr\n        elif 'score' in name:\n            scores = arr\n        elif 'num' in name:\n            num = arr\n\n    # Fallback: resolve by shape and value ranges if any are still None\n    if boxes is None or class_ids is None or scores is None:\n        for arr in outputs:\n            shape = arr.shape\n            if len(shape) == 3 and shape[-1] == 4 and boxes is None:\n                boxes = arr\n            elif len(shape) == 2 and shape[0] == 1 and scores is None:\n                # Could be scores or class_ids; try value range to distinguish\n                vmin, vmax = np.min(arr), np.max(arr)\n                if 0.0 <= vmin and vmax <= 1.0:\n                    scores = arr\n                else:\n                    class_ids = arr\n            elif len(shape) == 1 and shape[0] == 1 and num is None:\n                num = arr\n\n    if boxes is None or class_ids is None or scores is None:\n        raise RuntimeError(\"Unable to parse detection outputs from the model.\")\n\n    # Squeeze batch dim if present\n    if boxes.ndim == 3 and boxes.shape[0] == 1:\n        boxes = boxes[0]\n    if class_ids.ndim == 2 and class_ids.shape[0] == 1:\n        class_ids = class_ids[0]\n    if scores.ndim == 2 and scores.shape[0] == 1:\n        scores = scores[0]\n    if num is not None:\n        try:\n            n = int(np.squeeze(num))\n        except Exception:\n            n = min(len(scores), len(boxes))\n    else:\n        n = min(len(scores), len(boxes))\n\n    detections = []\n    n = min(n, len(scores), len(boxes))\n    for i in range(n):\n        score = float(scores[i])\n        if score < score_threshold:\n            continue\n        cls = int(class_ids[i]) if isinstance(class_ids[i], (np.integer, int, float)) else int(class_ids[i])\n        # boxes are [ymin, xmin, ymax, xmax] normalized [0,1]\n        y_min, x_min, y_max, x_max = boxes[i]\n        x1 = max(0, min(int(x_min * img_w), img_w - 1))\n        y1 = max(0, min(int(y_min * img_h), img_h - 1))\n        x2 = max(0, min(int(x_max * img_w), img_w - 1))\n        y2 = max(0, min(int(y_max * img_h), img_h - 1))\n        if x2 <= x1 or y2 <= y1:\n            continue\n        label = labels.get(cls, str(cls))\n        detections.append({\n            'bbox': (x1, y1, x2, y2),\n            'class_id': cls,\n            'score': score,\n            'label': label\n        })\n    return detections\n\n\ndef draw_detections_on_frame(frame_bgr, detections, map_value=None):\n    \"\"\"\n    Draws detection rectangles and labels on the frame.\n    Optionally overlays mAP value (map_value in [0..1]) at top-left.\n    \"\"\"\n    # Drawing settings\n    box_color = (0, 255, 0)  # green\n    text_color = (0, 0, 0)   # black text\n    text_bg_color = (255, 255, 255)  # white background\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    h, w = frame_bgr.shape[:2]\n    # scale font roughly by image size\n    font_scale = max(0.4, min(1.2, w / 1280.0))\n    thickness = max(1, int(round(w / 640.0)))\n\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        label = det['label']\n        score = det['score']\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), box_color, thickness)\n        display_text = \"{}: {:.1f}%\".format(label, score * 100.0)\n        (tw, th), baseline = cv2.getTextSize(display_text, font, font_scale, thickness)\n        tx1, ty1 = x1, max(0, y1 - th - baseline - 4)\n        tx2, ty2 = x1 + tw + 6, y1\n        cv2.rectangle(frame_bgr, (tx1, ty1), (tx2, ty2), text_bg_color, -1)\n        cv2.putText(frame_bgr, display_text, (x1 + 3, y1 - baseline - 3), font, font_scale, text_color, thickness, cv2.LINE_AA)\n\n    if map_value is not None:\n        map_text = \"mAP: {:.2f}%\".format(map_value * 100.0)\n        (tw, th), baseline = cv2.getTextSize(map_text, font, font_scale * 1.1, thickness + 1)\n        margin = 8\n        bx1, by1 = margin, margin\n        bx2, by2 = margin + tw + 10, margin + th + baseline + 10\n        cv2.rectangle(frame_bgr, (bx1, by1), (bx2, by2), (0, 0, 0), -1)\n        cv2.putText(frame_bgr, map_text, (bx1 + 5, by2 - baseline - 4), font, font_scale * 1.1, (255, 255, 255), thickness + 1, cv2.LINE_AA)\n\n    return frame_bgr\n\n\ndef compute_proxy_map(scores_by_class):\n    \"\"\"\n    Computes a proxy mAP from detection confidences across classes (no ground-truth available).\n    For each class: AP_class \u2248 mean(confidences for that class above threshold),\n    mAP \u2248 mean(AP_class over classes with detections).\n    Returns value in [0..1].\n    \"\"\"\n    ap_values = []\n    for cls_id, scores in scores_by_class.items():\n        if not scores:\n            continue\n        ap_values.append(float(np.mean(scores)))\n    if not ap_values:\n        return 0.0\n    return float(np.mean(ap_values))\n\n\n# =========================\n# MAIN PIPELINE\n# =========================\ndef main():\n    # 1) Setup: interpreter, labels, input video\n    interpreter = make_interpreter(model_path, edgetpu_lib_path)\n    input_index, input_dtype, in_h, in_w, in_scale, in_zero = get_input_details(interpreter)\n    labels = load_labels(label_path)\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(\"Failed to open input video: {}\".format(input_path))\n\n    # Read source properties\n    src_fps = cap.get(cv2.CAP_PROP_FPS)\n    if not src_fps or src_fps <= 0:\n        src_fps = 30.0\n    src_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    src_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) if cap.get(cv2.CAP_PROP_FRAME_COUNT) > 0 else -1\n\n    print(\"Input video: {} ({}x{}, {:.2f} FPS, {} frames)\".format(input_path, src_w, src_h, src_fps, total_frames if total_frames > 0 else \"unknown\"))\n    print(\"Model input size: {}x{}, dtype: {}, quant_scale: {}, zero_point: {}\".format(in_w, in_h, input_dtype, in_scale, in_zero))\n\n    # 2) Preprocessing + 3) Inference (First pass to collect detections and compute proxy mAP)\n    detections_per_frame = []\n    scores_by_class = {}  # {class_id: [scores]}\n    frame_idx = 0\n    t0 = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        input_tensor = preprocess_frame_bgr(frame, in_h, in_w, input_dtype, in_scale, in_zero)\n        interpreter.set_tensor(input_index, input_tensor)\n        interpreter.invoke()\n\n        detections = extract_detections(interpreter, confidence_threshold, frame.shape[1], frame.shape[0], labels)\n        detections_per_frame.append(detections)\n        # accumulate scores by class\n        for det in detections:\n            cls = det['class_id']\n            score = det['score']\n            scores_by_class.setdefault(cls, []).append(score)\n\n        frame_idx += 1\n        if frame_idx % 50 == 0:\n            print(\"Processed {} frames...\".format(frame_idx))\n\n    cap.release()\n    elapsed = time.time() - t0\n    fps_proc = frame_idx / elapsed if elapsed > 0 else 0.0\n    print(\"First pass completed: {} frames in {:.2f}s ({:.2f} FPS)\".format(frame_idx, elapsed, fps_proc))\n\n    # 4) Compute mAP (proxy due to lack of ground-truth)\n    map_value = compute_proxy_map(scores_by_class)\n    print(\"Computed mAP (proxy): {:.2f}%\".format(map_value * 100.0))\n\n    # 4) Output handling: draw boxes, labels, and mAP; save to output video (Second pass)\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    writer = cv2.VideoWriter(\n        output_path,\n        cv2.VideoWriter_fourcc(*'mp4v'),\n        src_fps,\n        (src_w, src_h)\n    )\n    if not writer.isOpened():\n        raise RuntimeError(\"Failed to open output video writer at: {}\".format(output_path))\n\n    cap2 = cv2.VideoCapture(input_path)\n    if not cap2.isOpened():\n        writer.release()\n        raise RuntimeError(\"Failed to reopen input video for rendering: {}\".format(input_path))\n\n    write_idx = 0\n    t1 = time.time()\n    while True:\n        ret, frame = cap2.read()\n        if not ret:\n            break\n        dets = detections_per_frame[write_idx] if write_idx < len(detections_per_frame) else []\n        annotated = draw_detections_on_frame(frame, dets, map_value=map_value)\n        writer.write(annotated)\n        write_idx += 1\n        if write_idx % 50 == 0:\n            print(\"Rendered {} frames...\".format(write_idx))\n\n    cap2.release()\n    writer.release()\n    elapsed2 = time.time() - t1\n    fps_render = write_idx / elapsed2 if elapsed2 > 0 else 0.0\n    print(\"Rendering completed: {} frames in {:.2f}s ({:.2f} FPS)\".format(write_idx, elapsed2, fps_render))\n    print(\"Output saved to: {}\".format(output_path))\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 23
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 100.003,
      "total_cost": 0.08589375,
      "observations": [
        {
          "id": "time-14-42-16-327726_chatcmpl-D6FS4MpnHT94yPOpivQWoURUI3DBv",
          "traceId": "10b705aa",
          "type": "GENERATION",
          "name": "10b7_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:42:16.327000+00:00",
          "endTime": "2026-02-06T12:43:56.330000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08589375,
          "latency": 100003.0,
          "costDetails": {
            "total": 0.08589375
          },
          "totalTokens": 8984,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 8533,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:42:16.220Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:44:55.949Z"
    },
    {
      "id": "5b8cb190",
      "timestamp": "2026-02-06T12:39:28.647000+00:00",
      "name": "5b8c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# CONFIGURATION PARAMETERS\n# =========================\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"  # fixed closing quote\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\nEDGETPU_SHARED_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n# =========================\n# Utility: Label loading\n# =========================\ndef load_labels(label_path):\n    \"\"\"\n    Loads labels from a file. Supports lines of the form:\n    - \"id label\"\n    - \"id: label\"\n    - \"label\" (implicit incremental id starting from 0)\n    Returns dict: {int_id: str_label}\n    \"\"\"\n    labels = {}\n    next_id = 0\n    try:\n        with open(label_path, \"r\") as f:\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith(\"#\"):\n                    continue\n                if \":\" in line:\n                    left, right = line.split(\":\", 1)\n                    left = left.strip()\n                    right = right.strip()\n                    if left.isdigit():\n                        labels[int(left)] = right\n                    else:\n                        labels[next_id] = line\n                        next_id += 1\n                else:\n                    parts = line.split(maxsplit=1)\n                    if len(parts) == 2 and parts[0].isdigit():\n                        labels[int(parts[0])] = parts[1].strip()\n                    else:\n                        labels[next_id] = line\n                        next_id += 1\n    except Exception:\n        # Fallback if label file cannot be read; use empty dict\n        labels = {}\n    return labels\n\n# =========================\n# Utility: IoU computation\n# =========================\ndef compute_iou(box_a, box_b):\n    \"\"\"\n    box format: [xmin, ymin, xmax, ymax]\n    \"\"\"\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n\n    inter_w = max(0.0, inter_x2 - inter_x1)\n    inter_h = max(0.0, inter_y2 - inter_y1)\n    inter_area = inter_w * inter_h\n\n    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)\n    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)\n\n    union = area_a + area_b - inter_area\n    if union <= 0.0:\n        return 0.0\n    return inter_area / union\n\n# =========================\n# Utility: AP per class (temporal proxy)\n# =========================\ndef average_precision_for_class(gt_boxes, pred_boxes, pred_scores, iou_thresh=0.5):\n    \"\"\"\n    Compute Average Precision (11-point interpolated) for a single class,\n    using previous frame detections as pseudo-ground-truth (temporal proxy).\n    - gt_boxes: list of [xmin, ymin, xmax, ymax]\n    - pred_boxes: list of [xmin, ymin, xmax, ymax]\n    - pred_scores: list of floats\n    Returns AP (float) or None if no gt is available.\n    \"\"\"\n    num_gt = len(gt_boxes)\n    if num_gt == 0:\n        return None\n\n    if len(pred_boxes) == 0:\n        return 0.0\n\n    # Sort predictions by descending score\n    order = np.argsort(-np.array(pred_scores))\n    pred_boxes = [pred_boxes[i] for i in order]\n    pred_scores = [pred_scores[i] for i in order]\n\n    matched_gt = set()\n    tp = np.zeros(len(pred_boxes), dtype=np.float32)\n    fp = np.zeros(len(pred_boxes), dtype=np.float32)\n\n    # Greedy matching\n    for i, pbox in enumerate(pred_boxes):\n        best_iou = 0.0\n        best_j = -1\n        for j, gbox in enumerate(gt_boxes):\n            if j in matched_gt:\n                continue\n            iou = compute_iou(pbox, gbox)\n            if iou >= iou_thresh and iou > best_iou:\n                best_iou = iou\n                best_j = j\n        if best_j >= 0:\n            tp[i] = 1.0\n            matched_gt.add(best_j)\n        else:\n            fp[i] = 1.0\n\n    cum_tp = np.cumsum(tp)\n    cum_fp = np.cumsum(fp)\n    precision = cum_tp / np.maximum(cum_tp + cum_fp, 1e-12)\n    recall = cum_tp / float(num_gt)\n\n    # 11-point interpolated AP\n    ap = 0.0\n    for r in np.linspace(0.0, 1.0, 11):\n        # max precision where recall >= r\n        mask = recall >= r\n        p = np.max(precision[mask]) if np.any(mask) else 0.0\n        ap += p\n    ap /= 11.0\n    return float(ap)\n\ndef temporal_proxy_map(prev_dets, curr_dets, iou_thresh=0.5):\n    \"\"\"\n    Compute temporal proxy mAP between consecutive frames:\n    Uses detections of previous frame as pseudo ground truth for the current frame.\n    - prev_dets, curr_dets: list of dicts with keys: 'bbox' [xmin,ymin,xmax,ymax], 'class_id', 'score'\n    Returns mean AP across classes present in previous frame, or None if undefined.\n    \"\"\"\n    if prev_dets is None or len(prev_dets) == 0:\n        return None\n\n    # Group detections by class\n    classes = set([d['class_id'] for d in prev_dets] + [d['class_id'] for d in curr_dets])\n    ap_values = []\n\n    for c in classes:\n        gt_boxes = [d['bbox'] for d in prev_dets if d['class_id'] == c]\n        pred_boxes = [d['bbox'] for d in curr_dets if d['class_id'] == c]\n        pred_scores = [float(d['score']) for d in curr_dets if d['class_id'] == c]\n        ap_c = average_precision_for_class(gt_boxes, pred_boxes, pred_scores, iou_thresh=iou_thresh)\n        if ap_c is not None:\n            ap_values.append(ap_c)\n\n    if len(ap_values) == 0:\n        return None\n    return float(np.mean(ap_values))\n\n# =========================\n# Interpreter helpers\n# =========================\ndef make_interpreter(model_path, edgetpu_lib):\n    delegates = [load_delegate(edgetpu_lib)]\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=delegates)\n    interpreter.allocate_tensors()\n    return interpreter\n\ndef get_input_size_dtype(interpreter):\n    input_details = interpreter.get_input_details()[0]\n    _, h, w, _ = input_details['shape']\n    dtype = input_details['dtype']\n    return (w, h), dtype\n\ndef set_input(interpreter, frame_bgr, input_size, input_dtype):\n    \"\"\"\n    Preprocess: resize, convert BGR->RGB, set dtype, set input tensor.\n    \"\"\"\n    iw, ih = input_size\n    resized = cv2.resize(frame_bgr, (iw, ih))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    if input_dtype == np.float32:\n        # Normalize to [0,1]\n        tensor = (rgb.astype(np.float32) / 255.0).astype(np.float32)\n    else:\n        tensor = rgb.astype(np.uint8)\n    tensor = np.expand_dims(tensor, axis=0)\n    input_details = interpreter.get_input_details()[0]\n    interpreter.set_tensor(input_details['index'], tensor)\n\ndef run_inference(interpreter):\n    interpreter.invoke()\n    output_details = interpreter.get_output_details()\n\n    # Many EdgeTPU object detection models (SSD) have 4 outputs:\n    # boxes, classes, scores, count\n    # We assume the conventional ordering used by TFLite detection postprocess.\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    count = interpreter.get_tensor(output_details[3]['index'])\n    if np.isscalar(count):\n        num = int(count)\n    else:\n        num = int(count[0]) if count.size > 0 else len(scores)\n\n    # Ensure proper dtypes\n    classes = classes.astype(np.int32)\n    scores = scores.astype(np.float32)\n    boxes = boxes.astype(np.float32)\n\n    # Clip length to 'num' if necessary\n    boxes = boxes[:num]\n    classes = classes[:num]\n    scores = scores[:num]\n\n    return boxes, classes, scores, num\n\ndef scale_and_clip_boxes(box, frame_w, frame_h):\n    \"\"\"\n    Convert normalized [ymin, xmin, ymax, xmax] to pixel [xmin, ymin, xmax, ymax],\n    and clip to frame bounds.\n    \"\"\"\n    ymin, xmin, ymax, xmax = box\n    xmin_px = int(max(0, min(frame_w - 1, xmin * frame_w)))\n    xmax_px = int(max(0, min(frame_w - 1, xmax * frame_w)))\n    ymin_px = int(max(0, min(frame_h - 1, ymin * frame_h)))\n    ymax_px = int(max(0, min(frame_h - 1, ymax * frame_h)))\n    # Ensure proper ordering\n    xmin_px, xmax_px = min(xmin_px, xmax_px), max(xmin_px, xmax_px)\n    ymin_px, ymax_px = min(ymin_px, ymax_px), max(ymin_px, ymax_px)\n    return [xmin_px, ymin_px, xmax_px, ymax_px]\n\ndef color_for_class(class_id):\n    # Deterministic color from class id\n    np.random.seed(class_id + 42)\n    color = tuple(int(x) for x in np.random.randint(0, 255, size=3))\n    return color\n\n# =========================\n# Main pipeline\n# =========================\ndef main():\n    # 1. Setup\n    interpreter = make_interpreter(MODEL_PATH, EDGETPU_SHARED_LIB)\n    labels = load_labels(LABEL_PATH)\n\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(\"Failed to open input video:\", INPUT_PATH)\n        return\n\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 1e-2:\n        fps = 30.0  # fallback\n\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        print(\"Failed to open output video writer:\", OUTPUT_PATH)\n        cap.release()\n        return\n\n    input_size, input_dtype = get_input_size_dtype(interpreter)\n\n    # For mAP (temporal proxy) accumulation\n    prev_dets = None\n    map_sum = 0.0\n    map_count = 0\n\n    frame_index = 0\n    t0 = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # 2. Preprocessing\n        set_input(interpreter, frame, input_size, input_dtype)\n\n        # 3. Inference\n        boxes, classes, scores, num = run_inference(interpreter)\n\n        # 4. Output handling: build detections list, draw, compute temporal proxy mAP, save\n        detections = []\n        # Build list of detections in original frame coordinates\n        for i in range(num):\n            score = float(scores[i])\n            cls_id = int(classes[i])\n            box_norm = boxes[i]  # [ymin, xmin, ymax, xmax] normalized\n            bbox = scale_and_clip_boxes(box_norm, frame_w, frame_h)\n\n            det = {\n                \"bbox\": bbox,\n                \"class_id\": cls_id,\n                \"score\": score\n            }\n            detections.append(det)\n\n        # Compute temporal proxy mAP vs previous frame detections\n        curr_map = temporal_proxy_map(prev_dets, detections, iou_thresh=0.5)\n        if curr_map is not None:\n            map_sum += curr_map\n            map_count += 1\n\n        running_map_text = \"mAP (temporal proxy): N/A\" if map_count == 0 else f\"mAP (temporal proxy): {map_sum / map_count:.3f}\"\n\n        # Draw detections above confidence threshold\n        for det in detections:\n            if det[\"score\"] < CONFIDENCE_THRESHOLD:\n                continue\n            x1, y1, x2, y2 = det[\"bbox\"]\n            cls_id = det[\"class_id\"]\n            score = det[\"score\"]\n            color = color_for_class(cls_id)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n            label = labels.get(cls_id, f\"id:{cls_id}\")\n            caption = f\"{label} {score:.2f}\"\n            # Put background for text for better visibility\n            (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), color, -1)\n            cv2.putText(frame, caption, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Put mAP text on the frame\n        cv2.putText(frame, running_map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (25, 200, 25), 2, cv2.LINE_AA)\n\n        writer.write(frame)\n\n        # Prepare for next iteration\n        prev_dets = detections\n        frame_index += 1\n\n    elapsed = time.time() - t0\n    final_map = (map_sum / map_count) if map_count > 0 else float('nan')\n\n    # Cleanup\n    cap.release()\n    writer.release()\n\n    # Console summary\n    print(\"Processing completed.\")\n    print(f\"Input video: {INPUT_PATH}\")\n    print(f\"Output video: {OUTPUT_PATH}\")\n    print(f\"Frames processed: {frame_index}\")\n    print(f\"Elapsed time: {elapsed:.2f} s, FPS: {frame_index / max(elapsed, 1e-6):.2f}\")\n    if map_count > 0:\n        print(f\"Temporal proxy mAP over video: {final_map:.4f}\")\n    else:\n        print(\"Temporal proxy mAP over video: N/A\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 22
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 87.41,
      "total_cost": 0.08541375,
      "observations": [
        {
          "id": "time-14-39-29-258522_chatcmpl-D6FPNYOqPQiZX9yYpMDOb5QhClvpT",
          "traceId": "5b8cb190",
          "type": "GENERATION",
          "name": "5b8c_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:39:29.258000+00:00",
          "endTime": "2026-02-06T12:40:56.668000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08541375,
          "latency": 87410.0,
          "costDetails": {
            "total": 0.08541375
          },
          "totalTokens": 8936,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 8485,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:39:29.210Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:41:55.462Z"
    },
    {
      "id": "0e34b5a6",
      "timestamp": "2026-02-06T12:36:54.734000+00:00",
      "name": "0e34_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\nedgetpu_lib = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n# =========================\n# Utility Functions\n# =========================\ndef load_labels(path):\n    # Supports \"id label\" or one-label-per-line formats\n    labels = {}\n    if not os.path.exists(path):\n        return labels\n    with open(path, 'r', encoding='utf-8') as f:\n        idx = 0\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n            parts = line.split(maxsplit=1)\n            if parts[0].isdigit():\n                lab_id = int(parts[0])\n                lab_name = parts[1].strip() if len(parts) > 1 else str(lab_id)\n                labels[lab_id] = lab_name\n            else:\n                labels[idx] = line\n                idx += 1\n    return labels\n\ndef make_interpreter(model_path, edgetpu_lib_path):\n    return Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate(edgetpu_lib_path)]\n    )\n\ndef resolve_output_indices(output_details, label_count_hint=None):\n    boxes_idx = None\n    classes_idx = None\n    scores_idx = None\n    count_idx = None\n\n    # Identify by name/shape first\n    for i, d in enumerate(output_details):\n        name = str(d.get('name', '')).lower()\n        shape = d['shape']\n        if (len(shape) == 3 and shape[-1] == 4) or (len(shape) == 2 and shape[-1] == 4):\n            boxes_idx = i\n        elif np.prod(shape) == 1:\n            count_idx = i\n        else:\n            if 'score' in name:\n                scores_idx = i\n            if 'class' in name:\n                classes_idx = i\n\n    # Fallback: choose remaining 1D arrays for classes/scores\n    candidates = [i for i in range(len(output_details)) if i not in [boxes_idx, count_idx] and len(output_details[i]['shape']) == 2]\n    if scores_idx is None and candidates:\n        # Prefer float arrays for scores\n        float_cands = [i for i in candidates if output_details[i]['dtype'] == np.float32]\n        scores_idx = float_cands[0] if float_cands else candidates[0]\n    if classes_idx is None:\n        # Remaining after picking scores\n        rem = [i for i in candidates if i != scores_idx]\n        classes_idx = rem[0] if rem else classes_idx\n\n    # Additional sanity: try to swap if classes look like > label count while scores in [0,1]\n    if classes_idx is not None and scores_idx is not None:\n        # We will check post-inference and swap dynamically if needed\n        pass\n\n    return boxes_idx, classes_idx, scores_idx, count_idx\n\ndef preprocess_frame(frame, input_w, input_h, dtype, quant_params=None):\n    # Resize and convert color space\n    resized = cv2.resize(frame, (input_w, input_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n    if dtype == np.uint8:\n        input_data = np.expand_dims(rgb, axis=0).astype(np.uint8)\n        # If quantization parameters are provided, we typically still pass uint8 0-255\n        # as most TFLite quantized models expect that dynamic range.\n    else:\n        # float32\n        input_data = (np.expand_dims(rgb, axis=0).astype(np.float32) / 255.0)\n\n    return input_data\n\ndef color_for_class(cid):\n    # Deterministic pseudo-color for a class id\n    r = (37 * cid) % 255\n    g = (17 * cid + 99) % 255\n    b = (29 * cid + 199) % 255\n    return int(b), int(g), int(r)\n\ndef compute_running_map(class_scores):\n    # Pseudo-mAP: mean of per-class average confidence over detections >= threshold\n    if not class_scores:\n        return 0.0\n    aps = []\n    for _, scores in class_scores.items():\n        if scores:\n            aps.append(float(np.mean(scores)))\n    if not aps:\n        return 0.0\n    return float(np.mean(aps))\n\n# =========================\n# Main Application\n# =========================\ndef main():\n    # 1. Setup\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(\"Model not found at: \" + model_path)\n    if not os.path.exists(input_path):\n        raise FileNotFoundError(\"Input video not found at: \" + input_path)\n\n    labels = load_labels(label_path)\n    label_count_hint = len(labels) if labels else None\n\n    interpreter = make_interpreter(model_path, edgetpu_lib)\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Input tensor details\n    input_idx = input_details[0]['index']\n    input_shape = input_details[0]['shape']\n    # Expected [1, height, width, 3]\n    in_h = int(input_shape[1])\n    in_w = int(input_shape[2])\n    in_dtype = input_details[0]['dtype']\n    in_quant = input_details[0].get('quantization', (0.0, 0))\n\n    # Resolve outputs\n    boxes_idx, classes_idx, scores_idx, count_idx = resolve_output_indices(output_details, label_count_hint)\n\n    # Open input video\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(\"Failed to open input video: \" + input_path)\n\n    src_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    src_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0:\n        fps = 30.0\n\n    # Prepare output writer\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (src_w, src_h))\n    if not writer.isOpened():\n        # Fallback: try a different codec\n        fourcc_alt = cv2.VideoWriter_fourcc(*'avc1')\n        writer = cv2.VideoWriter(output_path, fourcc_alt, fps, (src_w, src_h))\n        if not writer.isOpened():\n            raise RuntimeError(\"Failed to open output video writer at: \" + output_path)\n\n    # For mAP computation (pseudo)\n    class_scores = {}\n    frame_index = 0\n    prev_time = time.time()\n\n    # 2-4. Processing loop: Preprocess -> Inference -> Output handling\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        frame_index += 1\n\n        # Preprocess\n        input_data = preprocess_frame(frame, in_w, in_h, in_dtype, in_quant)\n\n        # Inference\n        interpreter.set_tensor(input_idx, input_data)\n        interpreter.invoke()\n\n        # Postprocess - read outputs\n        def get_tensor_by_idx(idx):\n            return interpreter.get_tensor(output_details[idx]['index']) if idx is not None else None\n\n        boxes = get_tensor_by_idx(boxes_idx)\n        classes = get_tensor_by_idx(classes_idx)\n        scores = get_tensor_by_idx(scores_idx)\n        count = get_tensor_by_idx(count_idx)\n\n        # Normalize output shapes\n        if boxes is not None and boxes.ndim == 3:\n            boxes = boxes[0]\n        if classes is not None and classes.ndim >= 2:\n            classes = classes[0]\n        if scores is not None and scores.ndim >= 2:\n            scores = scores[0]\n        if count is not None:\n            n = int(np.squeeze(count))\n        else:\n            # Infer N from available tensors\n            n = 0\n            if scores is not None:\n                n = scores.shape[0]\n            elif boxes is not None:\n                n = boxes.shape[0]\n            elif classes is not None:\n                n = classes.shape[0]\n\n        # Sometimes classes/scores indices might be swapped; fix if needed\n        if classes is not None and scores is not None:\n            # If classes look like probabilities and scores look like IDs, swap\n            if np.max(classes) <= 1.01 and np.max(scores) > 1.01:\n                classes, scores = scores, classes\n\n        # Draw detections and collect scores\n        if boxes is None or classes is None or scores is None:\n            # If model outputs do not conform, just write frame\n            writer.write(frame)\n            continue\n\n        for i in range(n):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n\n            # Class ID\n            cid_raw = classes[i]\n            try:\n                cid = int(cid_raw)\n            except Exception:\n                cid = int(np.round(cid_raw))\n\n            # Box coordinates (ymin, xmin, ymax, xmax) normalized [0,1]\n            y_min, x_min, y_max, x_max = boxes[i]\n            left = int(max(0, min(src_w - 1, x_min * src_w)))\n            right = int(max(0, min(src_w - 1, x_max * src_w)))\n            top = int(max(0, min(src_h - 1, y_min * src_h)))\n            bottom = int(max(0, min(src_h - 1, y_max * src_h)))\n\n            # Label text\n            label_str = labels.get(cid, f\"id={cid}\")\n            text = f\"{label_str} {score*100:.1f}%\"\n\n            # Draw rectangle and label\n            color = color_for_class(cid)\n            cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n            # Text background\n            (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            cv2.rectangle(frame, (left, max(0, top - th - 6)), (left + tw + 2, top), color, -1)\n            cv2.putText(frame, text, (left + 1, max(0, top - 4)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n            # Collect scores for pseudo-mAP\n            if cid not in class_scores:\n                class_scores[cid] = []\n            class_scores[cid].append(score)\n\n        # Compute running pseudo-mAP\n        map_value = compute_running_map(class_scores)\n\n        # FPS estimate\n        now = time.time()\n        dt = now - prev_time\n        prev_time = now\n        fps_est = 1.0 / dt if dt > 0 else 0.0\n\n        # Overlay summary text\n        summary = f\"mAP: {map_value:.3f} | FPS: {fps_est:.1f}\"\n        cv2.putText(frame, summary, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 3, cv2.LINE_AA)\n        cv2.putText(frame, summary, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Write to output\n        writer.write(frame)\n\n    # Final summary frame appended optionally (not required)\n    # Cleanup\n    cap.release()\n    writer.release()\n\n    # Print final pseudo-mAP\n    final_map = compute_running_map(class_scores)\n    print(\"Processing completed.\")\n    print(f\"Output saved to: {output_path}\")\n    print(f\"Final mAP (pseudo): {final_map:.4f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 21
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 74.881,
      "total_cost": 0.07198375,
      "observations": [
        {
          "id": "time-14-36-55-364214_chatcmpl-D6FMtWDGTJ2K9zdpUxolOuqAKYCoM",
          "traceId": "0e34b5a6",
          "type": "GENERATION",
          "name": "0e34_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:36:55.364000+00:00",
          "endTime": "2026-02-06T12:38:10.245000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07198375,
          "latency": 74881.0,
          "costDetails": {
            "total": 0.07198375
          },
          "totalTokens": 7593,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 7142,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:36:55.288Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:39:08.557Z"
    },
    {
      "id": "1157b936",
      "timestamp": "2026-02-06T12:34:28.152000+00:00",
      "name": "1157_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\nEDGETPU_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\ndef load_labels(path):\n    labels = {}\n    if not os.path.isfile(path):\n        return labels\n    with open(path, 'r') as f:\n        for idx, line in enumerate(f.readlines()):\n            line = line.strip()\n            if not line:\n                continue\n            # Attempt to parse \"index label\" format; fallback to enumerated labels\n            parts = line.split(maxsplit=1)\n            if len(parts) == 2 and parts[0].isdigit():\n                labels[int(parts[0])] = parts[1].strip()\n            else:\n                labels[idx] = line\n    return labels\n\n\ndef set_input_tensor(interpreter, image_rgb):\n    input_details = interpreter.get_input_details()[0]\n    input_index = input_details['index']\n    input_dtype = input_details['dtype']\n    input_shape = input_details['shape']  # [1, height, width, 3]\n    height, width = input_shape[1], input_shape[2]\n\n    resized = cv2.resize(image_rgb, (width, height))\n    if input_dtype == np.float32:\n        # Normalize to [0,1]\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        # Assume quantized uint8\n        input_data = resized.astype(np.uint8)\n\n    # Add batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    interpreter.set_tensor(input_index, input_data)\n\n\ndef get_output_tensors(interpreter):\n    \"\"\"Retrieve detection outputs in a model-agnostic way, dequantize if required.\"\"\"\n    output_details = interpreter.get_output_details()\n\n    def dequantize(output_detail, data):\n        if np.issubdtype(output_detail['dtype'], np.floating):\n            return data\n        scale, zero_point = output_detail['quantization']\n        if scale == 0:\n            return data.astype(np.float32)\n        return scale * (data.astype(np.float32) - zero_point)\n\n    boxes = None\n    classes = None\n    scores = None\n    count = None\n\n    # Try to identify outputs by shape semantics\n    for od in output_details:\n        data = interpreter.get_tensor(od['index'])\n        dq = dequantize(od, data)\n\n        # Boxes: shape [1, N, 4]\n        if len(dq.shape) == 3 and dq.shape[0] == 1 and dq.shape[2] == 4:\n            boxes = dq[0]\n        # Classes: shape [1, N]\n        elif len(dq.shape) == 2 and dq.shape[0] == 1 and np.issubdtype(dq.dtype, np.floating):\n            # Scores and classes could both be [1, N] float; try to distinguish by value range\n            # Heuristic: classes are near small integers; scores in [0,1]\n            if np.all((dq >= -1) & (dq <= len(dq[0]) + 10)) and np.max(dq) > 1.0:\n                classes = dq[0]\n            else:\n                # Could be scores if within [0,1]\n                if np.max(dq) <= 1.0001:\n                    scores = dq[0]\n                else:\n                    # If ambiguous, will resolve later if one is missing\n                    pass\n        # Count: shape [1] or scalar\n        elif dq.size == 1:\n            count = int(np.squeeze(dq))\n\n    # If ambiguity remains between classes and scores, try to swap if needed\n    if classes is not None and scores is not None:\n        pass\n    elif classes is None or scores is None:\n        # Try a different interpretation based on typical TFLite ordering\n        floats = [interpreter.get_tensor(od['index']).astype(np.float32) for od in output_details]\n        # Look for arrays in [0,1] as scores\n        cand_scores = [f for f in floats if f.ndim == 2 and f.shape[0] == 1 and np.max(f) <= 1.0001]\n        if scores is None and len(cand_scores) > 0:\n            scores = cand_scores[0][0]\n        # Look for arrays with larger integers as classes\n        cand_classes = [f for f in floats if f.ndim == 2 and f.shape[0] == 1 and np.max(f) > 1.0]\n        if classes is None and len(cand_classes) > 0:\n            classes = cand_classes[0][0]\n\n    # Fallbacks\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((len(boxes),), dtype=np.float32)\n    if scores is None:\n        scores = np.zeros((len(boxes),), dtype=np.float32)\n    if count is None:\n        count = len(boxes)\n\n    # Ensure correct sizes\n    n = min(int(count), boxes.shape[0], classes.shape[0], scores.shape[0])\n    return boxes[:n], classes[:n], scores[:n], n\n\n\ndef draw_detections(frame, detections, labels, map_value=None):\n    h, w = frame.shape[:2]\n    for det in detections:\n        ymin, xmin, ymax, xmax = det['bbox']  # normalized\n        score = det['score']\n        cls_id = det['class_id']\n        x1 = max(0, int(xmin * w))\n        y1 = max(0, int(ymin * h))\n        x2 = min(w - 1, int(xmax * w))\n        y2 = min(h - 1, int(ymax * h))\n\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n        label = labels.get(cls_id, f\"id:{cls_id}\")\n        text = f\"{label} {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - 6), (x1 + tw + 2, y1), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    if map_value is not None:\n        map_text = f\"mAP: {map_value:.3f}\"\n        (tw, th), bl = cv2.getTextSize(map_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n        cv2.rectangle(frame, (5, 5), (5 + tw + 10, 5 + th + 10), (0, 0, 0), -1)\n        cv2.putText(frame, map_text, (10, 10 + th), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n\n\ndef main():\n    # Ensure output directory exists\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize TFLite interpreter with EdgeTPU\n    interpreter = Interpreter(\n        model_path=MODEL_PATH,\n        experimental_delegates=[load_delegate(EDGETPU_LIB)]\n    )\n    interpreter.allocate_tensors()\n\n    # Get input size info\n    input_details = interpreter.get_input_details()[0]\n    input_shape = input_details['shape']\n    in_height, in_width = input_shape[1], input_shape[2]\n\n    # Video IO\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    input_fps = cap.get(cv2.CAP_PROP_FPS)\n    if input_fps <= 1e-2:\n        input_fps = 30.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    out = cv2.VideoWriter(OUTPUT_PATH, fourcc, input_fps, (frame_w, frame_h))\n    if not out.isOpened():\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    # Stats for \"mAP\" approximation (proxy due to lack of ground truth)\n    # We'll treat AP per-class as the mean confidence of detections above threshold, then mAP is mean over classes observed.\n    per_class_confidences = {}\n\n    frame_count = 0\n    t0_all = time.time()\n\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # Preprocessing: BGR -> RGB and set tensor\n        frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n        set_input_tensor(interpreter, frame_rgb)\n\n        # Inference\n        t0 = time.time()\n        interpreter.invoke()\n        infer_time_ms = (time.time() - t0) * 1000.0\n\n        # Outputs\n        boxes, classes, scores, count = get_output_tensors(interpreter)\n\n        # Collect detections above threshold\n        detections = []\n        for i in range(count):\n            score = float(scores[i])\n            if score < CONFIDENCE_THRESHOLD:\n                continue\n            cls_id = int(classes[i])\n            ymin, xmin, ymax, xmax = boxes[i]  # normalized [ymin, xmin, ymax, xmax]\n            ymin = float(max(0.0, min(1.0, ymin)))\n            xmin = float(max(0.0, min(1.0, xmin)))\n            ymax = float(max(0.0, min(1.0, ymax)))\n            xmax = float(max(0.0, min(1.0, xmax)))\n            detections.append({\n                'bbox': (ymin, xmin, ymax, xmax),\n                'score': score,\n                'class_id': cls_id\n            })\n            # Update proxy AP stats\n            if cls_id not in per_class_confidences:\n                per_class_confidences[cls_id] = []\n            per_class_confidences[cls_id].append(score)\n\n        # Compute proxy mAP\n        if per_class_confidences:\n            ap_values = [np.mean(per_class_confidences[c]) for c in per_class_confidences]\n            map_value = float(np.mean(ap_values))\n        else:\n            map_value = 0.0\n\n        # Draw and write\n        draw_detections(frame_bgr, detections, labels, map_value=map_value)\n        # Optionally draw FPS/inference time\n        perf_text = f\"Infer: {infer_time_ms:.1f} ms\"\n        cv2.putText(frame_bgr, perf_text, (10, frame_h - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n\n        out.write(frame_bgr)\n\n    total_time = time.time() - t0_all\n    cap.release()\n    out.release()\n\n    # Final report\n    if per_class_confidences:\n        ap_values = {c: float(np.mean(per_class_confidences[c])) for c in per_class_confidences}\n        final_map = float(np.mean(list(ap_values.values())))\n    else:\n        ap_values = {}\n        final_map = 0.0\n\n    print(f\"Processed {frame_count} frames in {total_time:.2f}s.\")\n    print(f\"Approx. mAP (no ground-truth; mean of per-class mean confidences): {final_map:.4f}\")\n    if ap_values:\n        print(\"Per-class AP (approx):\")\n        for c, ap in ap_values.items():\n            name = labels.get(c, f\"id:{c}\")\n            print(f\"  {name} (id {c}): {ap:.4f}\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 20
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 67.929,
      "total_cost": 0.05682375,
      "observations": [
        {
          "id": "time-14-34-28-818446_chatcmpl-D6FKXO6G8VOc3nwKKX2oHwjSOYS2g",
          "traceId": "1157b936",
          "type": "GENERATION",
          "name": "1157_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:34:28.818000+00:00",
          "endTime": "2026-02-06T12:35:36.747000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05682375,
          "latency": 67929.0,
          "costDetails": {
            "total": 0.05682375
          },
          "totalTokens": 6077,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 5626,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:34:28.716Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:36:34.889Z"
    },
    {
      "id": "a680266a",
      "timestamp": "2026-02-06T12:31:28.393000+00:00",
      "name": "a680_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# TFLite/EdgeTPU\ntry:\n    from tflite_runtime.interpreter import Interpreter, load_delegate\nexcept ImportError as e:\n    raise SystemExit(\"tflite_runtime is required on the Coral Dev Board. Install it and retry.\") from e\n\n# ----------------------------\n# CONFIGURATION PARAMETERS\n# ----------------------------\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\nEDGETPU_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\ndef load_labels(path):\n    \"\"\"\n    Loads labels from a text file into a dict of {int_id: label_str}.\n    Supports formats:\n      - One label per line (implicit 0-based indexing)\n      - \"id label\" pairs per line (space or comma separated)\n    \"\"\"\n    labels = {}\n    if not os.path.exists(path):\n        print(f\"Label file not found: {path}\")\n        return labels\n\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        lines = [l.strip() for l in f.readlines() if l.strip()]\n\n    # Detect format\n    indexed = True\n    for line in lines:\n        parts = line.replace(\",\", \" \").split()\n        if len(parts) < 2 or not parts[0].isdigit():\n            indexed = False\n            break\n\n    if indexed:\n        for line in lines:\n            parts = line.replace(\",\", \" \").split()\n            try:\n                idx = int(parts[0])\n                name = \" \".join(parts[1:]).strip()\n                labels[idx] = name\n            except Exception:\n                continue\n    else:\n        for i, line in enumerate(lines):\n            labels[i] = line\n\n    return labels\n\n\ndef get_label(labels, class_id):\n    \"\"\"\n    Safely return a label string for the given class_id.\n    Tries exact id, then id+1 (to handle common 1-based label maps), else str(class_id).\n    \"\"\"\n    if class_id in labels:\n        return labels[class_id]\n    if (class_id + 1) in labels:\n        return labels[class_id + 1]\n    return str(class_id)\n\n\ndef make_interpreter(model_path, edgetpu_lib):\n    \"\"\"\n    Creates a TFLite interpreter with EdgeTPU delegate.\n    \"\"\"\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model not found: {model_path}\")\n\n    try:\n        delegate = load_delegate(edgetpu_lib)\n        interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate])\n    except Exception as e:\n        raise SystemExit(f\"Failed to load EdgeTPU delegate from {edgetpu_lib}: {e}\")\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef prepare_input(frame_bgr, input_details):\n    \"\"\"\n    Preprocesses a frame for the model:\n      - Resize to input size\n      - Convert BGR -> RGB\n      - Convert dtype as needed (uint8 or float32 [0,1])\n      - Add batch dimension\n    Returns:\n      input_tensor (np.ndarray), resized_frame_rgb\n    \"\"\"\n    in_shape = input_details[0][\"shape\"]  # [1, h, w, c]\n    in_h, in_w = int(in_shape[1]), int(in_shape[2])\n    in_dtype = input_details[0][\"dtype\"]\n\n    resized = cv2.resize(frame_bgr, (in_w, in_h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n    if in_dtype == np.float32:\n        input_tensor = np.expand_dims(rgb.astype(np.float32) / 255.0, axis=0)\n    else:\n        input_tensor = np.expand_dims(rgb.astype(in_dtype), axis=0)\n\n    return input_tensor, rgb\n\n\ndef set_input_tensor(interpreter, input_tensor):\n    \"\"\"\n    Copies input_tensor into the interpreter input buffer.\n    \"\"\"\n    input_details = interpreter.get_input_details()\n    index = input_details[0][\"index\"]\n    interpreter.set_tensor(index, input_tensor)\n\n\ndef get_output_tensors(interpreter):\n    \"\"\"\n    Retrieves and normalizes typical TFLite detection model outputs:\n      boxes: [N, 4] in normalized ymin, xmin, ymax, xmax\n      classes: [N] int\n      scores: [N] float\n      count: int\n    Handles common variations in output order/dtypes.\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    tensors = [interpreter.get_tensor(od[\"index\"]) for od in output_details]\n\n    # Flatten any leading batch dim of 1\n    flat = []\n    for t in tensors:\n        tt = t\n        while tt.ndim > 1 and tt.shape[0] == 1:\n            tt = np.squeeze(tt, axis=0)\n        flat.append(tt)\n\n    boxes, classes, scores, count = None, None, None, None\n\n    # Heuristic assignment by shape/dtype\n    for arr in flat:\n        if arr.ndim == 2 and arr.shape[-1] == 4:\n            boxes = arr.astype(np.float32)\n        elif arr.ndim == 1 and arr.dtype.kind in (\"i\", \"u\"):\n            classes = arr.astype(np.int32)\n        elif arr.ndim == 1 and arr.dtype.kind == \"f\":\n            # Could be scores or sometimes boxes if malformed; prefer 1D floats as scores\n            if scores is None:\n                scores = arr.astype(np.float32)\n        elif arr.ndim == 0:\n            count = int(arr)\n\n    # Fallback for common 4-output order [boxes, classes, scores, count]\n    if boxes is None or classes is None or scores is None or count is None:\n        try:\n            od = interpreter.get_output_details()\n            b = interpreter.get_tensor(od[0][\"index\"]).squeeze(axis=0)\n            c = interpreter.get_tensor(od[1][\"index\"]).squeeze(axis=0)\n            s = interpreter.get_tensor(od[2][\"index\"]).squeeze(axis=0)\n            n = interpreter.get_tensor(od[3][\"index\"]).squeeze()\n            if b.ndim == 2 and b.shape[-1] == 4:\n                boxes = b.astype(np.float32)\n                # Sometimes classes/scores are swapped in dtype; fix if needed\n                if c.dtype.kind == \"f\" and s.dtype.kind in (\"i\", \"u\"):\n                    c, s = s, c\n                classes = c.astype(np.int32)\n                scores = s.astype(np.float32)\n                count = int(n)\n        except Exception:\n            pass\n\n    if boxes is None or classes is None or scores is None or count is None:\n        raise RuntimeError(\"Unable to parse model outputs for detection.\")\n\n    # Ensure lengths align with count\n    n = min(count, len(scores), len(classes), len(boxes))\n    return boxes[:n], classes[:n], scores[:n], n\n\n\ndef draw_detections(frame_bgr, detections, labels, running_map_proxy):\n    \"\"\"\n    Draw detection boxes and labels onto the frame.\n    detections: list of (ymin, xmin, ymax, xmax, class_id, score) in absolute pixel coords.\n    \"\"\"\n    h, w = frame_bgr.shape[:2]\n    for (ymin, xmin, ymax, xmax, cid, score) in detections:\n        # Clip to frame\n        xmin = max(0, min(w - 1, int(xmin)))\n        xmax = max(0, min(w - 1, int(xmax)))\n        ymin = max(0, min(h - 1, int(ymin)))\n        ymax = max(0, min(h - 1, int(ymax)))\n\n        color = (0, 255, 0)  # Green boxes\n        cv2.rectangle(frame_bgr, (xmin, ymin), (xmax, ymax), color, 2)\n\n        label = get_label(labels, int(cid))\n        text = f\"{label}: {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame_bgr, (xmin, max(0, ymin - th - 6)), (xmin + tw + 4, ymin), color, -1)\n        cv2.putText(frame_bgr, text, (xmin + 2, ymin - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Overlay \"mAP\" proxy on the top-left\n    map_text = f\"mAP (proxy): {running_map_proxy:.3f}\"\n    (tw, th), bl = cv2.getTextSize(map_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n    cv2.rectangle(frame_bgr, (5, 5), (10 + tw, 10 + th), (255, 255, 255), -1)\n    cv2.putText(frame_bgr, map_text, (8, 8 + th - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2, cv2.LINE_AA)\n\n    return frame_bgr\n\n\ndef main():\n    # 1. Setup: interpreter, labels, video IO\n    print(\"Initializing interpreter with EdgeTPU...\")\n    interpreter = make_interpreter(model_path, EDGETPU_LIB)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    print(\"Interpreter initialized.\")\n\n    labels = load_labels(label_path)\n    if not labels:\n        print(\"Warning: Labels could not be loaded or label file is empty. Class IDs will be displayed.\")\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise SystemExit(f\"Failed to open input video: {input_path}\")\n\n    in_fps = cap.get(cv2.CAP_PROP_FPS)\n    if not in_fps or in_fps <= 0.0 or np.isnan(in_fps):\n        in_fps = 30.0  # fallback\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(output_path, fourcc, in_fps, (width, height))\n    if not writer.isOpened():\n        cap.release()\n        raise SystemExit(f\"Failed to open output video for writing: {output_path}\")\n\n    # Stats for \"mAP\" proxy and performance\n    total_conf_sum = 0.0\n    total_det_count = 0\n    frame_idx = 0\n    t0 = time.time()\n\n    print(\"Processing video...\")\n    while True:\n        ret, frame_bgr = cap.read()\n        if not ret:\n            break\n        frame_idx += 1\n\n        # 2. Preprocessing\n        input_tensor, _ = prepare_input(frame_bgr, input_details)\n\n        # 3. Inference\n        set_input_tensor(interpreter, input_tensor)\n        interpreter.invoke()\n\n        # 4. Output handling\n        boxes, classes, scores, count = get_output_tensors(interpreter)\n\n        detections = []\n        # Convert normalized boxes to absolute pixel coords and filter by confidence\n        for i in range(count):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n            ymin, xmin, ymax, xmax = boxes[i]\n            ymin_abs = ymin * height\n            xmin_abs = xmin * width\n            ymax_abs = ymax * height\n            xmax_abs = xmax * width\n            detections.append((ymin_abs, xmin_abs, ymax_abs, xmax_abs, int(classes[i]), score))\n\n        # Update proxy \"mAP\" as mean confidence across all detections so far\n        if len(detections) > 0:\n            confs = [d[5] for d in detections]\n            total_conf_sum += float(np.sum(confs))\n            total_det_count += len(confs)\n        running_map_proxy = (total_conf_sum / total_det_count) if total_det_count > 0 else 0.0\n\n        # Draw and write frame\n        annotated = draw_detections(frame_bgr.copy(), detections, labels, running_map_proxy)\n        writer.write(annotated)\n\n        # Optional: print simple progress\n        if frame_idx % 50 == 0:\n            elapsed = time.time() - t0\n            fps = frame_idx / max(1e-6, elapsed)\n            print(f\"Processed {frame_idx} frames | Running FPS: {fps:.2f} | Detections so far: {total_det_count} | mAP (proxy): {running_map_proxy:.3f}\")\n\n    # Cleanup\n    cap.release()\n    writer.release()\n\n    total_time = time.time() - t0\n    overall_fps = frame_idx / max(1e-6, total_time)\n    final_map_proxy = (total_conf_sum / total_det_count) if total_det_count > 0 else 0.0\n\n    print(\"--------------------------------------------------\")\n    print(f\"Finished processing {frame_idx} frames\")\n    print(f\"Output saved to: {output_path}\")\n    print(f\"Average processing FPS: {overall_fps:.2f}\")\n    print(f\"Final mAP (proxy): {final_map_proxy:.4f}\")\n    print(\"Note: This 'mAP (proxy)' is computed as the mean of detection confidences across all detections,\")\n    print(\"      since ground-truth annotations are not provided for true mAP computation.\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 19
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 100.477,
      "total_cost": 0.06473375,
      "observations": [
        {
          "id": "time-14-31-28-990257_chatcmpl-D6FHd3pFebOVhtqb55fl9WUy4YNup",
          "traceId": "a680266a",
          "type": "GENERATION",
          "name": "a680_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:31:28.990000+00:00",
          "endTime": "2026-02-06T12:33:09.467000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06473375,
          "latency": 100477.0,
          "costDetails": {
            "total": 0.06473375
          },
          "totalTokens": 6868,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 6417,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:31:28.954Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:34:07.648Z"
    },
    {
      "id": "58bd0a6b",
      "timestamp": "2026-02-06T12:29:10.436000+00:00",
      "name": "58bd_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"  # Read a single video file from the given input_path\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"  # Output the video with drawn rectangles, labels, and mAP\nconfidence_threshold = 0.5\n\n# =========================\n# Utility functions\n# =========================\ndef load_labels(path):\n    labels = {}\n    if not os.path.exists(path):\n        print(f\"Warning: Label file not found at {path}. Detections will use class IDs.\")\n        return labels\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = [l.strip() for l in f.readlines() if l.strip()]\n    for i, line in enumerate(lines):\n        if ':' in line:\n            # Support \"index: label\" format\n            idx_str, name = line.split(':', 1)\n            try:\n                idx = int(idx_str.strip())\n                labels[idx] = name.strip()\n            except ValueError:\n                labels[i] = line.strip()\n        else:\n            labels[i] = line.strip()\n    return labels\n\ndef preprocess_frame(frame_bgr, input_size, input_dtype):\n    # Resize and convert color as most TFLite object detection models expect RGB\n    h, w = input_size\n    resized = cv2.resize(frame_bgr, (w, h))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n    if input_dtype == np.float32:\n        input_data = (rgb.astype(np.float32) / 255.0).astype(np.float32)\n    else:\n        input_data = rgb.astype(input_dtype)\n    # Add batch dimension\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef parse_detections(interpreter, output_details):\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n    for od in output_details:\n        output_data = interpreter.get_tensor(od['index'])\n        # Typical shapes:\n        # boxes: [1, N, 4], classes: [1, N], scores: [1, N], num: [1]\n        if output_data.ndim == 3 and output_data.shape[2] == 4:\n            boxes = output_data[0]\n        elif output_data.ndim == 2:\n            # Could be scores or classes\n            # Heuristic: values <= 1.0 likely scores; otherwise classes\n            arr = output_data[0]\n            if arr.dtype in (np.float32, np.float64):\n                if np.nanmax(arr) <= 1.0:\n                    scores = arr\n                else:\n                    classes = arr.astype(np.int32)\n            else:\n                classes = arr.astype(np.int32)\n        elif output_data.ndim == 1 and output_data.shape[0] == 1:\n            num = int(output_data[0])\n    # Fallbacks in case num is None\n    if num is None and scores is not None:\n        num = scores.shape[0]\n    if boxes is None or classes is None or scores is None or num is None:\n        raise RuntimeError(\"Unexpected TFLite detection output format.\")\n    return boxes, classes, scores, num\n\ndef draw_detections(frame, detections, labels, map_score):\n    # Draw bounding boxes and labels on frame\n    for det in detections:\n        x1, y1, x2, y2, score, class_id = det\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 175, 255), 2)\n        label = labels.get(class_id, str(class_id))\n        caption = f\"{label}: {score:.2f}\"\n        cv2.putText(frame, caption, (x1, max(0, y1 - 7)),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 175, 255), 2, cv2.LINE_AA)\n    # Draw mAP (proxy) on the top-left\n    cv2.putText(frame, f\"mAP: {map_score:.3f}\", (10, 25),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2, cv2.LINE_AA)\n\ndef clip_box_coords(x1, y1, x2, y2, w, h):\n    x1 = max(0, min(w - 1, x1))\n    y1 = max(0, min(h - 1, y1))\n    x2 = max(0, min(w - 1, x2))\n    y2 = max(0, min(h - 1, y2))\n    return x1, y1, x2, y2\n\nclass MAPAggregator:\n    \"\"\"\n    Since ground truth annotations are not available from the provided inputs,\n    we maintain a proxy mAP metric based on the mean of per-class average scores\n    for detections above the confidence threshold. This is not a true mAP, but\n    provides a simple, trackable score over time for visualization purposes.\n    \"\"\"\n    def __init__(self):\n        self.class_conf_sum = {}\n        self.class_count = {}\n\n    def update(self, class_id, score):\n        if class_id not in self.class_conf_sum:\n            self.class_conf_sum[class_id] = 0.0\n            self.class_count[class_id] = 0\n        self.class_conf_sum[class_id] += float(score)\n        self.class_count[class_id] += 1\n\n    def compute(self):\n        valid_classes = [cid for cid, cnt in self.class_count.items() if cnt > 0]\n        if not valid_classes:\n            return 0.0\n        per_class_avgs = [self.class_conf_sum[cid] / self.class_count[cid] for cid in valid_classes]\n        return float(np.mean(per_class_avgs))\n\n# =========================\n# Setup\n# =========================\ndef main():\n    print(\"Initializing TFLite interpreter with EdgeTPU...\")\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n        )\n    except ValueError as e:\n        print(\"Failed to load EdgeTPU delegate. Ensure the EdgeTPU runtime is installed and the path is correct.\")\n        raise e\n\n    interpreter.allocate_tensors()\n\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    # Input tensor details\n    input_shape = input_details[0]['shape']  # [1, height, width, 3]\n    input_h, input_w = int(input_shape[1]), int(input_shape[2])\n    input_dtype = input_details[0]['dtype']\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Open input video\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(f\"Error: Cannot open input video at {input_path}\")\n        return\n\n    in_fps = cap.get(cv2.CAP_PROP_FPS)\n    if not in_fps or in_fps <= 1e-2:\n        in_fps = 30.0  # default fallback\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Prepare output video writer\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, in_fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        print(f\"Error: Cannot open output video for writing at {output_path}\")\n        cap.release()\n        return\n\n    print(\"Processing video...\")\n    print(f\"Input: {input_path}\")\n    print(f\"Output: {output_path}\")\n    print(f\"Model: {model_path}\")\n    print(f\"Labels: {label_path}\")\n    print(f\"Frame size: {frame_w}x{frame_h} @ {in_fps:.2f} FPS\")\n    print(f\"Confidence threshold: {confidence_threshold}\")\n\n    map_aggregator = MAPAggregator()\n    frame_index = 0\n    inference_times = []\n\n    start_time_total = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_index += 1\n\n        # Preprocess\n        input_data = preprocess_frame(frame, (input_h, input_w), input_dtype)\n\n        # Set input tensor\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        # Inference\n        t0 = time.time()\n        interpreter.invoke()\n        t1 = time.time()\n        inference_times.append((t1 - t0) * 1000.0)  # ms\n\n        # Parse outputs\n        boxes, classes, scores, num = parse_detections(interpreter, output_details)\n\n        # Build detections list: (x1, y1, x2, y2, score, class_id)\n        detections = []\n        num = min(num, len(scores), len(classes), len(boxes))\n        for i in range(num):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n            class_id = int(classes[i])\n            # boxes are in [ymin, xmin, ymax, xmax] normalized\n            ymin, xmin, ymax, xmax = boxes[i]\n            x1 = int(xmin * frame_w)\n            y1 = int(ymin * frame_h)\n            x2 = int(xmax * frame_w)\n            y2 = int(ymax * frame_h)\n            x1, y1, x2, y2 = clip_box_coords(x1, y1, x2, y2, frame_w, frame_h)\n            if x2 <= x1 or y2 <= y1:\n                continue\n            detections.append((x1, y1, x2, y2, score, class_id))\n            # Update proxy mAP aggregator\n            map_aggregator.update(class_id, score)\n\n        # Compute proxy mAP\n        map_score = map_aggregator.compute()\n\n        # Draw\n        draw_detections(frame, detections, labels, map_score)\n\n        # Write frame\n        writer.write(frame)\n\n        # Optional progress update\n        if frame_index % 30 == 0:\n            elapsed = time.time() - start_time_total\n            fps_proc = frame_index / max(1e-6, elapsed)\n            print(f\"Processed {frame_index}/{total_frames if total_frames>0 else '?'} frames - ~{fps_proc:.2f} FPS (inference {np.mean(inference_times[-30:]):.1f} ms)\")\n\n    cap.release()\n    writer.release()\n\n    total_elapsed = time.time() - start_time_total\n    avg_inf_ms = float(np.mean(inference_times)) if inference_times else 0.0\n    print(\"Processing complete.\")\n    print(f\"Total time: {total_elapsed:.2f} s\")\n    print(f\"Average inference time: {avg_inf_ms:.2f} ms\")\n    print(f\"Estimated proxy mAP over video: {map_aggregator.compute():.3f}\")\n    print(f\"Saved output video with detections and mAP overlay to: {output_path}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 18
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 57.89,
      "total_cost": 0.05503375,
      "observations": [
        {
          "id": "time-14-29-11-072389_chatcmpl-D6FFPm3Q01kFztMFm9E2HpChvPYfq",
          "traceId": "58bd0a6b",
          "type": "GENERATION",
          "name": "58bd_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:29:11.072000+00:00",
          "endTime": "2026-02-06T12:30:08.962000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05503375,
          "latency": 57890.0,
          "costDetails": {
            "total": 0.05503375
          },
          "totalTokens": 5898,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 5447,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:29:11.011Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:31:07.692Z"
    },
    {
      "id": "10da3a78",
      "timestamp": "2026-02-06T12:25:57.577000+00:00",
      "name": "10da_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# =========================\n# Helper Functions\n# =========================\ndef load_labels(path):\n    labels = {}\n    if not os.path.isfile(path):\n        print(\"Warning: Label file not found at:\", path)\n        return labels\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = [l.strip() for l in f.readlines() if l.strip()]\n    # Try to parse formats:\n    # \"0 person\", \"0: person\", \"person\"\n    # Prefer explicit id if present\n    for idx, line in enumerate(lines):\n        if ':' in line:\n            left, right = line.split(':', 1)\n            left = left.strip()\n            right = right.strip()\n            if left.isdigit():\n                labels[int(left)] = right\n                continue\n        # Space-delimited id and label\n        parts = line.split()\n        if len(parts) > 1 and parts[0].isdigit():\n            lbl_id = int(parts[0])\n            lbl = ' '.join(parts[1:])\n            labels[lbl_id] = lbl\n        else:\n            # Fallback to sequential indexing\n            labels[idx] = line\n    return labels\n\ndef make_interpreter(model_file, edgetpu_lib=\"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"):\n    try:\n        interpreter = Interpreter(\n            model_path=model_file,\n            experimental_delegates=[load_delegate(edgetpu_lib)]\n        )\n        return interpreter\n    except Exception as e:\n        raise RuntimeError(f\"Failed to create TFLite interpreter with EdgeTPU delegate: {e}\")\n\ndef get_output_indices(interpreter):\n    # Attempt to robustly identify output tensors: boxes, classes, scores, count\n    details = interpreter.get_output_details()\n    boxes_idx = classes_idx = scores_idx = count_idx = None\n    for i, d in enumerate(details):\n        name = d.get('name', '')\n        if isinstance(name, bytes):\n            name = name.decode('utf-8', errors='ignore')\n        name_l = name.lower()\n        shape = d.get('shape', [])\n        if 'boxes' in name_l or (len(shape) == 3 and shape[-1] == 4):\n            boxes_idx = i\n        elif 'scores' in name_l:\n            scores_idx = i\n        elif 'classes' in name_l:\n            classes_idx = i\n        elif 'count' in name_l or 'num' in name_l:\n            count_idx = i\n    # Fallback to common SSD order if any missing\n    if any(v is None for v in (boxes_idx, classes_idx, scores_idx, count_idx)):\n        if len(details) >= 4:\n            boxes_idx = 0 if boxes_idx is None else boxes_idx\n            classes_idx = 1 if classes_idx is None else classes_idx\n            scores_idx = 2 if scores_idx is None else scores_idx\n            count_idx = 3 if count_idx is None else count_idx\n        else:\n            raise RuntimeError(\"Unable to determine output tensor indices for the detection model.\")\n    return boxes_idx, classes_idx, scores_idx, count_idx\n\ndef preprocess_frame_bgr_to_input(frame_bgr, input_w, input_h, input_dtype):\n    # Convert BGR to RGB, resize to model input size, and convert dtype as needed\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (input_w, input_h))\n    if input_dtype == np.float32:\n        inp = resized.astype(np.float32) / 255.0\n    else:\n        inp = resized.astype(np.uint8)\n    return np.expand_dims(inp, axis=0)\n\ndef clip_bbox(xmin, ymin, xmax, ymax, w, h):\n    xmin = max(0, min(xmin, w - 1))\n    ymin = max(0, min(ymin, h - 1))\n    xmax = max(0, min(xmax, w - 1))\n    ymax = max(0, min(ymax, h - 1))\n    return int(xmin), int(ymin), int(xmax), int(ymax)\n\ndef draw_detections(frame, detections, labels):\n    for det in detections:\n        xmin, ymin, xmax, ymax = det['bbox']\n        score = det['score']\n        cls = det['class_id']\n        lbl = labels.get(cls, str(cls))\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n        caption = f\"{lbl}: {score:.2f}\"\n        # Background for text for readability\n        (tw, th), baseline = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (xmin, ymin - th - 4), (xmin + tw + 2, ymin), color, -1)\n        cv2.putText(frame, caption, (xmin + 1, ymin - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\ndef iou(boxA, boxB):\n    # Boxes as [xmin, ymin, xmax, ymax]\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0, xB - xA + 1)\n    inter_h = max(0, yB - yA + 1)\n    inter = inter_w * inter_h\n    areaA = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    areaB = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    union = areaA + areaB - inter\n    if union <= 0:\n        return 0.0\n    return inter / union\n\ndef compute_map(detections, ground_truth, iou_threshold=0.5):\n    # detections: list of dicts {'frame': int, 'class_id': int, 'score': float, 'bbox': [xmin,ymin,xmax,ymax]}\n    # ground_truth: dict frame -> list of dicts {'class_id': int, 'bbox': [xmin,ymin,xmax,ymax]}\n    # Returns mAP over classes present in ground truth. If none, returns None.\n    # Organize ground truth by class and frame\n    gt_by_class_frame = {}\n    total_gts_by_class = {}\n    frames_with_gt = set()\n    for fidx, gts in ground_truth.items():\n        for gt in gts:\n            c = gt['class_id']\n            gt_by_class_frame.setdefault(c, {}).setdefault(fidx, []).append({'bbox': gt['bbox'], 'matched': False})\n            total_gts_by_class[c] = total_gts_by_class.get(c, 0) + 1\n            frames_with_gt.add(fidx)\n\n    if not total_gts_by_class:\n        return None\n\n    # Organize detections by class\n    dets_by_class = {}\n    for d in detections:\n        c = d['class_id']\n        dets_by_class.setdefault(c, []).append(d)\n\n    ap_list = []\n    for c, n_gt in total_gts_by_class.items():\n        if n_gt == 0:\n            continue\n        # Gather detections of this class\n        class_dets = dets_by_class.get(c, [])\n        # Sort by confidence desc\n        class_dets = sorted(class_dets, key=lambda x: x['score'], reverse=True)\n\n        tp = np.zeros(len(class_dets), dtype=np.float32)\n        fp = np.zeros(len(class_dets), dtype=np.float32)\n\n        # Copy matches flags fresh for this computation\n        gt_flags = {}\n        for fidx, gts in gt_by_class_frame.get(c, {}).items():\n            gt_flags[fidx] = [False] * len(gts)\n\n        for i, det in enumerate(class_dets):\n            fidx = det['frame']\n            det_box = det['bbox']\n            gts = gt_by_class_frame.get(c, {}).get(fidx, [])\n            best_iou = 0.0\n            best_gt_idx = -1\n            for gi, gt in enumerate(gts):\n                if gt_flags[fidx][gi]:\n                    continue\n                iou_val = iou(det_box, gt['bbox'])\n                if iou_val > best_iou:\n                    best_iou = iou_val\n                    best_gt_idx = gi\n            if best_iou >= iou_threshold and best_gt_idx >= 0:\n                tp[i] = 1.0\n                gt_flags[fidx][best_gt_idx] = True\n            else:\n                fp[i] = 1.0\n\n        tp_cum = np.cumsum(tp)\n        fp_cum = np.cumsum(fp)\n        recalls = tp_cum / float(n_gt)\n        precisions = np.divide(tp_cum, (tp_cum + fp_cum + 1e-12))\n\n        # Compute AP using precision envelope method\n        mrec = np.concatenate(([0.0], recalls, [1.0]))\n        mpre = np.concatenate(([0.0], precisions, [0.0]))\n        for i in range(len(mpre) - 1, 0, -1):\n            mpre[i - 1] = max(mpre[i - 1], mpre[i])\n        # Identify points where recall changes\n        idx = np.where(mrec[1:] != mrec[:-1])[0]\n        ap = np.sum((mrec[idx + 1] - mrec[idx]) * mpre[idx + 1])\n        ap_list.append(ap)\n\n    if not ap_list:\n        return None\n    return float(np.mean(ap_list))\n\ndef parse_ground_truth_sidecar(input_video_path, frame_w, frame_h):\n    # Look for a sidecar file with ground truth boxes.\n    # Supported candidates:\n    #   <input>.gt.txt\n    #   <input>.txt\n    # Each non-empty, non-comment line format:\n    #   frame_index class_id xmin ymin xmax ymax\n    # Coordinates can be absolute pixels or normalized [0,1].\n    candidates = [\n        os.path.splitext(input_video_path)[0] + '.gt.txt',\n        os.path.splitext(input_video_path)[0] + '.txt'\n    ]\n    gt_path = None\n    for p in candidates:\n        if os.path.isfile(p):\n            gt_path = p\n            break\n    if gt_path is None:\n        return {}\n\n    gt_by_frame = {}\n    try:\n        with open(gt_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                s = line.strip()\n                if not s or s.startswith('#'):\n                    continue\n                # allow comma or whitespace separated\n                parts = s.replace(',', ' ').split()\n                if len(parts) < 6:\n                    continue\n                try:\n                    fidx = int(parts[0])\n                    cls = int(parts[1])\n                    x1 = float(parts[2]); y1 = float(parts[3]); x2 = float(parts[4]); y2 = float(parts[5])\n                except Exception:\n                    continue\n                # Determine if normalized\n                is_normalized = (0.0 <= x1 <= 1.0 and 0.0 <= y1 <= 1.0 and 0.0 <= x2 <= 1.0 and 0.0 <= y2 <= 1.0)\n                if is_normalized:\n                    xmin = int(round(x1 * frame_w))\n                    ymin = int(round(y1 * frame_h))\n                    xmax = int(round(x2 * frame_w))\n                    ymax = int(round(y2 * frame_h))\n                else:\n                    xmin = int(round(x1))\n                    ymin = int(round(y1))\n                    xmax = int(round(x2))\n                    ymax = int(round(y2))\n                xmin, ymin, xmax, ymax = clip_bbox(xmin, ymin, xmax, ymax, frame_w, frame_h)\n                gt_by_frame.setdefault(fidx, []).append({\n                    'class_id': cls,\n                    'bbox': [xmin, ymin, xmax, ymax]\n                })\n    except Exception as e:\n        print(\"Warning: Failed to parse ground truth file:\", e)\n        return {}\n    return gt_by_frame\n\n# =========================\n# Main Pipeline\n# =========================\ndef main():\n    # Validate input video\n    if not os.path.isfile(input_path):\n        raise FileNotFoundError(f\"Input video not found: {input_path}\")\n\n    # Prepare output directory\n    out_dir = os.path.dirname(output_path)\n    if out_dir and not os.path.isdir(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize interpreter with EdgeTPU\n    interpreter = make_interpreter(model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_indices = get_output_indices(interpreter)\n    in_h, in_w = input_details[0]['shape'][1], input_details[0]['shape'][2]\n    in_dtype = input_details[0]['dtype']\n    in_index = input_details[0]['index']\n\n    # Setup video capture and writer\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(\"Failed to open input video: \" + input_path)\n\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 0:\n        fps = 30.0\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(\"Failed to open output video for writing: \" + output_path)\n\n    # Attempt to load ground-truth boxes if a sidecar file exists\n    gt_by_frame_full = parse_ground_truth_sidecar(input_path, frame_w, frame_h)\n    has_gt = len(gt_by_frame_full) > 0\n    if not has_gt:\n        print(\"Info: No ground-truth sidecar found. mAP will be shown as N/A.\")\n\n    # Storage for detections across frames for mAP computation\n    all_detections = []\n    frame_index = 0\n\n    start_time = time.time()\n    frames_processed = 0\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Preprocess\n            inp = preprocess_frame_bgr_to_input(frame, in_w, in_h, in_dtype)\n\n            # Inference\n            interpreter.set_tensor(in_index, inp)\n            interpreter.invoke()\n\n            boxes_idx, classes_idx, scores_idx, count_idx = output_indices\n            boxes = interpreter.get_tensor(interpreter.get_output_details()[boxes_idx]['index'])\n            classes = interpreter.get_tensor(interpreter.get_output_details()[classes_idx]['index'])\n            scores = interpreter.get_tensor(interpreter.get_output_details()[scores_idx]['index'])\n            count = interpreter.get_tensor(interpreter.get_output_details()[count_idx]['index'])\n\n            # Squeeze outputs (expected shapes: [1, N, ...])\n            boxes = np.squeeze(boxes)\n            classes = np.squeeze(classes)\n            scores = np.squeeze(scores)\n            if np.ndim(count) > 0:\n                num = int(np.squeeze(count))\n            else:\n                num = boxes.shape[0]\n\n            # Collect detections above threshold\n            detections_this_frame = []\n            for i in range(num):\n                score = float(scores[i])\n                if score < confidence_threshold:\n                    continue\n                # Box is [ymin, xmin, ymax, xmax] normalized\n                ymin, xmin, ymax, xmax = boxes[i]\n                # Convert to absolute pixel coords\n                x1 = int(round(xmin * frame_w))\n                y1 = int(round(ymin * frame_h))\n                x2 = int(round(xmax * frame_w))\n                y2 = int(round(ymax * frame_h))\n                x1, y1, x2, y2 = clip_bbox(x1, y1, x2, y2, frame_w, frame_h)\n                cls_id = int(classes[i])\n                detections_this_frame.append({\n                    'frame': frame_index,\n                    'class_id': cls_id,\n                    'score': score,\n                    'bbox': [x1, y1, x2, y2]\n                })\n\n            # Draw\n            draw_detections(frame, detections_this_frame, labels)\n\n            # Update detection history\n            all_detections.extend(detections_this_frame)\n\n            # Compute running mAP if GT available (using GT up to current frame)\n            map_text = \"mAP: N/A\"\n            if has_gt:\n                # Filter ground-truth up to current frame\n                gt_partial = {fi: gt_by_frame_full[fi] for fi in gt_by_frame_full.keys() if fi <= frame_index}\n                mAP_val = compute_map(all_detections, gt_partial, iou_threshold=0.5)\n                if mAP_val is not None:\n                    map_text = f\"mAP: {mAP_val * 100.0:.2f}%\"\n\n            # Draw mAP on the frame\n            cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (50, 50, 255), 2, cv2.LINE_AA)\n\n            # Write frame\n            writer.write(frame)\n\n            frame_index += 1\n            frames_processed += 1\n\n    finally:\n        cap.release()\n        writer.release()\n\n    elapsed = time.time() - start_time\n    if elapsed > 0 and frames_processed > 0:\n        print(f\"Processed {frames_processed} frames in {elapsed:.2f}s ({frames_processed/elapsed:.2f} FPS).\")\n\n    # Final mAP over entire video if GT exists\n    if has_gt:\n        final_mAP = compute_map(all_detections, gt_by_frame_full, iou_threshold=0.5)\n        if final_mAP is not None:\n            print(f\"Final mAP over video: {final_mAP * 100.0:.2f}%\")\n        else:\n            print(\"Final mAP could not be computed (no ground truth boxes found).\")\n    else:\n        print(\"No ground truth available; mAP was not computed.\")\n\n    print(\"Output saved to:\", output_path)\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 17
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 113.505,
      "total_cost": 0.09883375,
      "observations": [
        {
          "id": "time-14-25-58-201720_chatcmpl-D6FCIYPN5iCX5cV8eFSaTlfJFdFaN",
          "traceId": "10da3a78",
          "type": "GENERATION",
          "name": "10da_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:25:58.201000+00:00",
          "endTime": "2026-02-06T12:27:51.706000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.09883375,
          "latency": 113505.0,
          "costDetails": {
            "total": 0.09883375
          },
          "totalTokens": 10278,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 9827,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:25:58.141Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:28:49.886Z"
    },
    {
      "id": "f52cfec8",
      "timestamp": "2026-02-06T12:20:17.545000+00:00",
      "name": "f52c_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =======================\n# CONFIGURATION PARAMETERS\n# =======================\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONF_THRESHOLD = 0.5  # confidence threshold for drawing boxes\n\n# Optional: limit processed frames to avoid long runs in constrained environments.\n# On Coral Dev Board, this is typically unset.\nMAX_FRAMES = int(os.getenv(\"TFLITE_MAX_FRAMES\", \"0\"))\n\n# =======================\n# HELPER FUNCTIONS\n# =======================\ndef load_labels(path):\n    \"\"\"\n    Loads label map from a text file.\n    Supports formats:\n      - \"0: person\"\n      - \"0 person\"\n      - plain list (index = line number)\n    \"\"\"\n    labels = {}\n    with open(path, 'r') as f:\n        for idx, line in enumerate(f):\n            line = line.strip()\n            if not line:\n                continue\n            key = None\n            name = None\n            if ':' in line:\n                # \"id: name\"\n                parts = line.split(':', 1)\n                if parts[0].strip().isdigit():\n                    key = int(parts[0].strip())\n                    name = parts[1].strip()\n            elif ' ' in line[:4]:\n                # \"id name\" (id then name)\n                head = line.split(None, 1)\n                if head[0].isdigit():\n                    key = int(head[0])\n                    name = head[1].strip()\n            if key is None or name is None:\n                # Fallback: plain list\n                key = idx\n                name = line\n            labels[key] = name\n    return labels\n\ndef make_interpreter(model_path):\n    \"\"\"\n    Creates a TFLite interpreter with EdgeTPU delegate.\n    \"\"\"\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n    interpreter.allocate_tensors()\n    return interpreter\n\ndef get_input_details(interpreter):\n    input_details = interpreter.get_input_details()[0]\n    shape = input_details['shape']\n    # Expect [1, height, width, channels]\n    height, width = int(shape[1]), int(shape[2])\n    dtype = input_details['dtype']\n    quant = input_details.get('quantization', (0.0, 0))\n    return input_details['index'], height, width, dtype, quant\n\ndef set_input(interpreter, input_index, frame_bgr, in_h, in_w, in_dtype, in_quant):\n    \"\"\"\n    Preprocess frame and set input tensor.\n    Handles RGB conversion, resize, dtype and quantization if needed.\n    \"\"\"\n    # Convert BGR (OpenCV) to RGB\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    # Resize to model input size\n    resized = cv2.resize(rgb, (in_w, in_h))\n    if in_dtype == np.float32:\n        # Normalize to [0, 1]\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        # Assume uint8 (quantized)\n        input_data = resized.astype(np.uint8)\n        # If per-tensor quantization is provided and dtype is uint8, we typically pass raw uint8.\n        # (Most EdgeTPU detection models expect uint8 input without manual zero_point/scale here.)\n    # Set input tensor\n    input_data = np.expand_dims(input_data, axis=0)\n    interpreter.set_tensor(input_index, input_data)\n\ndef _dequantize_if_needed(tensor, detail):\n    \"\"\"\n    Dequantize tensor if the output is quantized. Returns float32 array.\n    \"\"\"\n    if detail['dtype'] == np.uint8:\n        scale, zero_point = detail.get('quantization', (0.0, 0))\n        if scale and scale > 0:\n            return scale * (tensor.astype(np.float32) - zero_point)\n        else:\n            return tensor.astype(np.float32)\n    else:\n        return tensor.astype(np.float32)\n\ndef get_detections(interpreter):\n    \"\"\"\n    Retrieves detection outputs: boxes, classes, scores, num.\n    Tries to infer outputs robustly.\n    Returns:\n      boxes: [N, 4] in normalized coordinates (ymin, xmin, ymax, xmax)\n      classes: [N] integers\n      scores: [N] floats in [0,1]\n      count: integer N (number of valid detections to consider)\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    outputs = []\n    for d in output_details:\n        arr = interpreter.get_tensor(d['index'])\n        arr = _dequantize_if_needed(arr, d)\n        outputs.append((d, arr))\n\n    # Attempt robust identification of outputs\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Identify boxes: last dim == 4\n    for d, arr in outputs:\n        if arr.ndim >= 2 and arr.shape[-1] == 4:\n            boxes = arr[0] if arr.ndim == 3 else arr\n            break\n\n    # If boxes not found, fall back to first output\n    if boxes is None and outputs:\n        d, arr = outputs[0]\n        boxes = arr[0] if arr.ndim == 3 else arr\n\n    # Determine N\n    N = boxes.shape[-2] if boxes is not None and boxes.ndim >= 2 else 0\n\n    # Find tensors with N elements (scores and classes)\n    candidate = []\n    for d, arr in outputs:\n        flat_n = arr.size\n        if flat_n == N or (arr.ndim == 2 and arr.shape[-1] == N) or (arr.ndim == 2 and arr.shape[0] == N):\n            candidate.append(arr)\n\n    # If not enough candidates, try to include 1xN shapes\n    if len(candidate) < 2:\n        for d, arr in outputs:\n            if arr.ndim == 2 and (arr.shape[0] == 1 and arr.shape[1] == N):\n                candidate.append(arr)\n\n    # Choose scores as the one within [0,1]\n    chosen_scores = None\n    chosen_classes = None\n    for arr in candidate:\n        a = arr\n        if a.ndim == 2 and a.shape[0] == 1:\n            a = a[0]\n        if a.ndim > 1:\n            continue\n        if a.size == N:\n            if np.all((a >= 0.0) & (a <= 1.0)):\n                chosen_scores = a\n            else:\n                chosen_classes = a\n\n    if chosen_scores is None or chosen_classes is None:\n        # Fallback to common ordering: [boxes, classes, scores, num]\n        if len(outputs) >= 3:\n            boxes = outputs[0][1]\n            classes = outputs[1][1]\n            scores = outputs[2][1]\n            if classes.ndim == 2 and classes.shape[0] == 1:\n                classes = classes[0]\n            if scores.ndim == 2 and scores.shape[0] == 1:\n                scores = scores[0]\n            if boxes.ndim == 3 and boxes.shape[0] == 1:\n                boxes = boxes[0]\n        else:\n            # As last resort, return empty\n            return np.zeros((0, 4), dtype=np.float32), np.zeros((0,), dtype=np.int32), np.zeros((0,), dtype=np.float32), 0\n    else:\n        scores = chosen_scores\n        classes = chosen_classes\n\n    # num_detections if present\n    for d, arr in outputs:\n        if arr.size == 1:\n            num = int(round(float(arr.flatten()[0])))\n            break\n\n    # Normalize shapes\n    if boxes.ndim == 3 and boxes.shape[0] == 1:\n        boxes = boxes[0]\n    if scores.ndim == 2 and scores.shape[0] == 1:\n        scores = scores[0]\n    if classes.ndim == 2 and classes.shape[0] == 1:\n        classes = classes[0]\n\n    classes = classes.astype(np.int32)\n\n    if num is None:\n        num = N\n    num = min(num, len(scores), len(classes), len(boxes))\n\n    return boxes[:num], classes[:num], scores[:num], num\n\ndef draw_detections(frame, boxes, classes, scores, labels, conf_thresh, map_value):\n    \"\"\"\n    Draws detection boxes and mAP text on frame.\n    \"\"\"\n    h, w = frame.shape[:2]\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score < conf_thresh:\n            continue\n        cls_id = int(classes[i])\n        label = labels.get(cls_id, str(cls_id))\n        y_min, x_min, y_max, x_max = boxes[i]\n        # Clip to [0,1]\n        y_min = max(0.0, min(1.0, y_min))\n        x_min = max(0.0, min(1.0, x_min))\n        y_max = max(0.0, min(1.0, y_max))\n        x_max = max(0.0, min(1.0, x_max))\n\n        x1 = int(x_min * w)\n        y1 = int(y_min * h)\n        x2 = int(x_max * w)\n        y2 = int(y_max * h)\n\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n        text = f\"{label}: {score:.2f}\"\n        font = cv2.FONT_HERSHEY_SIMPLEX\n        font_scale = max(0.5, min(w, h) / 1000.0)\n        thickness = 1\n        (tw, th), _ = cv2.getTextSize(text, font, font_scale, thickness)\n        text_bg_tl = (x1, max(0, y1 - th - 4))\n        text_bg_br = (x1 + tw + 4, y1)\n        cv2.rectangle(frame, text_bg_tl, text_bg_br, color, -1)\n        cv2.putText(frame, text, (x1 + 2, y1 - 2), font, font_scale, (0, 0, 0), thickness, cv2.LINE_AA)\n\n    # Draw mAP (proxy) on top-left corner\n    map_text = f\"mAP: {map_value:.3f}\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = max(0.6, min(w, h) / 800.0)\n    thickness = 2\n    cv2.putText(frame, map_text, (10, 30), font, font_scale, (50, 220, 255), thickness, cv2.LINE_AA)\n\ndef compute_proxy_map(confidence_history):\n    \"\"\"\n    Computes a proxy mAP without ground truth, using detection confidences only.\n    For each class, computes AP as the average precision across thresholds T in [0.5..0.95 step 0.05],\n    where precision(T) = fraction of detections with confidence >= T.\n    mAP is the mean over classes that had at least one detection.\n\n    Note: This is NOT a true mAP metric (requires ground-truth). It is a proxy for demonstration.\n    \"\"\"\n    thresholds = np.arange(0.5, 1.0, 0.05)  # 0.50 to 0.95\n    ap_list = []\n    for cls_id, confs in confidence_history.items():\n        if not confs:\n            continue\n        c = np.array(confs, dtype=np.float32)\n        precisions = [(c >= t).mean() for t in thresholds]\n        ap = float(np.mean(precisions)) if len(precisions) > 0 else 0.0\n        ap_list.append(ap)\n    if not ap_list:\n        return 0.0\n    return float(np.mean(ap_list))\n\n# =======================\n# MAIN PIPELINE\n# =======================\ndef main():\n    # Ensure output directory exists\n    out_dir = os.path.dirname(OUTPUT_PATH)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Setup interpreter (EdgeTPU)\n    interpreter = make_interpreter(MODEL_PATH)\n    input_index, in_h, in_w, in_dtype, in_quant = get_input_details(interpreter)\n\n    # Open input video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: cannot open input video: {INPUT_PATH}\")\n        return\n\n    # Prepare output video writer\n    input_fps = cap.get(cv2.CAP_PROP_FPS)\n    if not input_fps or input_fps <= 0:\n        input_fps = 30.0  # fallback\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    if frame_w <= 0 or frame_h <= 0:\n        # Read one frame to get size if properties unavailable\n        ret_probe, probe_frame = cap.read()\n        if not ret_probe:\n            print(\"Error: failed to read from input video to determine frame size.\")\n            cap.release()\n            return\n        frame_h, frame_w = probe_frame.shape[:2]\n        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, input_fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        print(f\"Error: cannot open video writer for: {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # For proxy mAP computation: store confidences by class id\n    confidence_history = {}\n\n    frame_count = 0\n    t_start = time.time()\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Preprocess and set input\n            set_input(interpreter, input_index, frame, in_h, in_w, in_dtype, in_quant)\n\n            # Inference\n            interpreter.invoke()\n\n            # Get detections\n            boxes, classes, scores, count = get_detections(interpreter)\n\n            # Update confidence history for proxy mAP\n            for i in range(count):\n                cls_id = int(classes[i])\n                score = float(scores[i])\n                # collect all predicted confidences (even below display threshold) for proxy mAP\n                if cls_id not in confidence_history:\n                    confidence_history[cls_id] = []\n                confidence_history[cls_id].append(score)\n\n            # Compute proxy mAP so far\n            proxy_map = compute_proxy_map(confidence_history)\n\n            # Draw results\n            draw_detections(frame, boxes, classes, scores, labels, CONF_THRESHOLD, proxy_map)\n\n            # Write frame to output\n            writer.write(frame)\n\n            frame_count += 1\n            if MAX_FRAMES > 0 and frame_count >= MAX_FRAMES:\n                break\n\n    finally:\n        cap.release()\n        writer.release()\n        cv2.destroyAllWindows()\n\n    elapsed = time.time() - t_start\n    final_map = compute_proxy_map(confidence_history)\n    print(f\"Processed {frame_count} frame(s) in {elapsed:.2f}s. Proxy mAP: {final_map:.3f}\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 16
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 260.438,
      "total_cost": 0.166285,
      "observations": [
        {
          "id": "time-14-20-18-164949_chatcmpl-D6F6pJ2ZN4DGxeO21rarASAtED5F0",
          "traceId": "f52cfec8",
          "type": "GENERATION",
          "name": "f52c_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:20:18.164000+00:00",
          "endTime": "2026-02-06T12:21:52.702000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08419375,
          "latency": 94538.0,
          "costDetails": {
            "total": 0.08419375
          },
          "totalTokens": 8814,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 8363,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-22-58-841051_chatcmpl-D6F9P1wnh6d8mGBP4nEoVridhyAKH",
          "traceId": "f52cfec8",
          "type": "GENERATION",
          "name": "f52c_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T12:22:58.841000+00:00",
          "endTime": "2026-02-06T12:24:38.602000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08209125,
          "latency": 99761.0,
          "costDetails": {
            "total": 0.08209125
          },
          "totalTokens": 8651,
          "environment": "default",
          "promptTokens": 505,
          "completionTokens": 8146,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:20:18.111Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:25:36.876Z"
    },
    {
      "id": "cb06a846",
      "timestamp": "2026-02-06T12:16:57.791000+00:00",
      "name": "cb06_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# Application: TFLite object detection with TPU\n# Target: Google Coral Dev Board\n# Note: This script uses a proxy mAP calculation (no ground-truth available). See comments in compute_map_proxy().\n\n# =======================\n# Configuration parameters\n# =======================\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"  # Read a single video file from the given input_path\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"  # Output video with rectangles, labels, and mAP text overlay\nCONFIDENCE_THRESHOLD = 0.5\n\nEDGETPU_SO_PATH = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n# =======================\n# TFLite + EdgeTPU setup\n# =======================\n# Importing tflite_runtime is required by the app logic for invoking the EdgeTPU-accelerated model.\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n\ndef load_labels(path):\n    \"\"\"\n    Load labels from a label map file.\n    Supports lines in either:\n      - \"index label name\"\n      - \"label name\"\n      - \"index: label name\"\n    Returns a dict id(int) -> name(str)\n    \"\"\"\n    labels = {}\n    try:\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            raw = [ln.strip() for ln in f.readlines() if ln.strip() and not ln.strip().startswith(\"#\")]\n        for i, line in enumerate(raw):\n            # Replace ':' with space to support \"0: person\"\n            parts = line.replace(\":\", \" \").split()\n            # Try \"index label...\" format\n            idx = None\n            if parts and all(ch.isdigit() for ch in parts[0]):\n                try:\n                    idx = int(parts[0])\n                    name = \" \".join(parts[1:]).strip() if len(parts) > 1 else str(idx)\n                    labels[idx] = name if name else str(idx)\n                    continue\n                except Exception:\n                    idx = None\n            # Fallback: use line order as index\n            labels[i] = line\n    except Exception:\n        # If label file missing or unreadable, fallback to empty mapping.\n        labels = {}\n    return labels\n\n\ndef make_interpreter(model_path, edgetpu_so_path):\n    \"\"\"\n    Create and allocate a TFLite Interpreter with EdgeTPU delegate.\n    \"\"\"\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate(edgetpu_so_path)]\n        )\n    except Exception:\n        # Fallback: create without delegate if delegate load fails (still runs on CPU but very slow).\n        interpreter = Interpreter(model_path=model_path)\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef resolve_output_indices(interpreter):\n    \"\"\"\n    Resolve indices for detection boxes, classes, scores, and count from output_details.\n    Tries name hints; falls back to standard SSD order for TPU models.\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    name_map = {\"boxes\": None, \"classes\": None, \"scores\": None, \"num\": None}\n\n    # First pass: try by name hints and shape patterns\n    for i, od in enumerate(output_details):\n        name = od.get(\"name\", \"\").lower()\n        shape = od.get(\"shape\", [])\n        # Heuristics\n        if (\"box\" in name or \"bbox\" in name) or (len(shape) == 3 and shape[-1] == 4):\n            name_map[\"boxes\"] = i if name_map[\"boxes\"] is None else name_map[\"boxes\"]\n        elif \"class\" in name:\n            name_map[\"classes\"] = i if name_map[\"classes\"] is None else name_map[\"classes\"]\n        elif \"score\" in name:\n            name_map[\"scores\"] = i if name_map[\"scores\"] is None else name_map[\"scores\"]\n        elif \"num\" in name or \"count\" in name:\n            name_map[\"num\"] = i if name_map[\"num\"] is None else name_map[\"num\"]\n\n    # Fallback to common output order: [boxes, classes, scores, num]\n    if any(v is None for v in name_map.values()):\n        if len(output_details) >= 4:\n            name_map = {\"boxes\": 0, \"classes\": 1, \"scores\": 2, \"num\": 3}\n        else:\n            # Attempt best-effort from shapes\n            # Find boxes = tensor with last dim 4\n            for i, od in enumerate(output_details):\n                if len(od[\"shape\"]) >= 2 and od[\"shape\"][-1] == 4:\n                    name_map[\"boxes\"] = i\n            # Remaining assign arbitrarily if still None\n            idxs_left = [i for i in range(len(output_details)) if i not in name_map.values() or name_map.values() is None]\n            keys_left = [k for k, v in name_map.items() if v is None]\n            for k, i in zip(keys_left, idxs_left):\n                name_map[k] = i\n\n    return name_map[\"boxes\"], name_map[\"classes\"], name_map[\"scores\"], name_map[\"num\"]\n\n\ndef preprocess_frame(frame_bgr, input_w, input_h, input_dtype):\n    \"\"\"\n    Preprocess an OpenCV BGR frame into model input tensor.\n    Most EdgeTPU SSD models are quantized uint8 and expect [0..255] RGB input.\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (input_w, input_h), interpolation=cv2.INTER_LINEAR)\n\n    if input_dtype == np.uint8:\n        input_data = resized.astype(np.uint8)\n    else:\n        # Generic float32 fallback: normalize to [0,1]\n        input_data = resized.astype(np.float32) / 255.0\n\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n\ndef postprocess_detections(boxes, classes, scores, count, frame_w, frame_h):\n    \"\"\"\n    Convert raw model outputs to a list of detection dicts with absolute pixel boxes.\n    boxes are expected normalized [ymin, xmin, ymax, xmax].\n    \"\"\"\n    results = []\n    n = int(count) if isinstance(count, (int, float, np.integer, np.floating)) else int(count[0] if len(count) else 0)\n    n = min(n, len(scores))\n    for i in range(n):\n        score = float(scores[i])\n        cls = int(classes[i])\n        y_min, x_min, y_max, x_max = boxes[i]\n        # Convert to pixel coordinates and clamp\n        y1 = max(0, min(int(y_min * frame_h), frame_h - 1))\n        x1 = max(0, min(int(x_min * frame_w), frame_w - 1))\n        y2 = max(0, min(int(y_max * frame_h), frame_h - 1))\n        x2 = max(0, min(int(x_max * frame_w), frame_w - 1))\n        # Ensure top-left is less than bottom-right\n        x1, x2 = (x1, x2) if x1 <= x2 else (x2, x1)\n        y1, y2 = (y1, y2) if y1 <= y2 else (y2, y1)\n\n        results.append({\n            \"score\": score,\n            \"class_id\": cls,\n            \"bbox\": (x1, y1, x2, y2)  # absolute pixel coords\n        })\n    return results\n\n\ndef detect_objects(interpreter, io_indices, frame_bgr, input_w, input_h, input_idx, input_dtype):\n    \"\"\"\n    Run inference on a single frame and return detections as list of dicts.\n    \"\"\"\n    # Preprocess\n    input_tensor = preprocess_frame(frame_bgr, input_w, input_h, input_dtype)\n    interpreter.set_tensor(input_idx, input_tensor)\n\n    # Inference\n    interpreter.invoke()\n\n    # Outputs\n    boxes_idx, classes_idx, scores_idx, num_idx = io_indices\n    boxes = interpreter.get_tensor(interpreter.get_output_details()[boxes_idx][\"index\"])[0]\n    classes = interpreter.get_tensor(interpreter.get_output_details()[classes_idx][\"index\"])[0]\n    scores = interpreter.get_tensor(interpreter.get_output_details()[scores_idx][\"index\"])[0]\n    num_dets = interpreter.get_tensor(interpreter.get_output_details()[num_idx][\"index\"])[0]\n\n    # Postprocess\n    h, w = frame_bgr.shape[:2]\n    dets = postprocess_detections(boxes, classes, scores, num_dets, w, h)\n    return dets\n\n\ndef class_color(class_id):\n    \"\"\"\n    Deterministic pseudo-color for a class id.\n    \"\"\"\n    r = (class_id * 37) % 256\n    g = (class_id * 17) % 256\n    b = (class_id * 97) % 256\n    return int(b), int(g), int(r)  # OpenCV uses BGR\n\n\ndef compute_map_proxy(preds_by_class, pos_count_by_class):\n    \"\"\"\n    Compute a proxy mAP without ground-truth:\n      - Treat the highest-scoring detection per class per frame as a \"proxy\" True Positive (TP).\n      - All other detections for that class/frame are considered False Positives (FP).\n      - For each class: sort all detections by score desc; AP = average of precision@k at each TP.\n      - mAP = mean(AP over classes with at least one proxy positive).\n    This is a heuristic proxy to produce a stable, reproducible metric for overlay when ground-truth is not available.\n    \"\"\"\n    ap_list = []\n    for cls_id, total_pos in pos_count_by_class.items():\n        if total_pos <= 0:\n            continue\n        preds = preds_by_class.get(cls_id, [])\n        if not preds:\n            ap_list.append(0.0)\n            continue\n        preds_sorted = sorted(preds, key=lambda x: x[0], reverse=True)\n        tp_cum = 0\n        fp_cum = 0\n        ap_sum = 0.0\n        for score, is_tp in preds_sorted:\n            if is_tp:\n                tp_cum += 1\n            else:\n                fp_cum += 1\n            precision = tp_cum / (tp_cum + fp_cum) if (tp_cum + fp_cum) > 0 else 0.0\n            if is_tp:\n                ap_sum += precision\n        ap = ap_sum / float(total_pos) if total_pos > 0 else 0.0\n        ap_list.append(ap)\n    if ap_list:\n        return float(sum(ap_list) / len(ap_list))\n    return 0.0\n\n\ndef main():\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Setup interpreter with EdgeTPU\n    interpreter = make_interpreter(MODEL_PATH, EDGETPU_SO_PATH)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    input_idx = input_details[0][\"index\"]\n    input_h = int(input_details[0][\"shape\"][1])\n    input_w = int(input_details[0][\"shape\"][2])\n    input_dtype = input_details[0][\"dtype\"]\n\n    # Resolve output indices\n    io_indices = resolve_output_indices(interpreter)\n\n    # Open input video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(\"ERROR: Could not open input video:\", INPUT_PATH)\n        return\n\n    in_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    in_fps = cap.get(cv2.CAP_PROP_FPS)\n    if in_fps is None or in_fps <= 0:\n        in_fps = 30.0  # Fallback FPS if metadata missing\n\n    # Prepare output writer\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (in_w, in_h))\n    if not writer.isOpened():\n        print(\"ERROR: Could not open output video for writing:\", OUTPUT_PATH)\n        cap.release()\n        return\n\n    # Proxy mAP accumulators\n    # preds_by_class: class_id -> list of (score, is_tp)\n    preds_by_class = {}\n    # pos_count_by_class: class_id -> count of frames where class had at least one detection (proxy positives)\n    pos_count_by_class = {}\n\n    # Processing loop\n    frame_count = 0\n    t0 = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        # Run detection\n        detections = detect_objects(interpreter, io_indices, frame, input_w, input_h, input_idx, input_dtype)\n\n        # Update proxy AP bookkeeping using ALL detections (no score threshold)\n        # Determine top detection index per class for this frame\n        best_idx_by_class = {}\n        for idx, det in enumerate(detections):\n            cls = det[\"class_id\"]\n            score = det[\"score\"]\n            if cls not in best_idx_by_class:\n                best_idx_by_class[cls] = idx\n            else:\n                prev_best_idx = best_idx_by_class[cls]\n                if score > detections[prev_best_idx][\"score\"]:\n                    best_idx_by_class[cls] = idx\n\n        # Increment proxy positive count per class (one \"GT\" per class per frame if present)\n        for cls in best_idx_by_class.keys():\n            pos_count_by_class[cls] = pos_count_by_class.get(cls, 0) + 1\n\n        # Append predictions per class with TP flag\n        for idx, det in enumerate(detections):\n            cls = det[\"class_id\"]\n            score = float(det[\"score\"])\n            is_tp = (best_idx_by_class.get(cls, -1) == idx)\n            preds_by_class.setdefault(cls, []).append((score, bool(is_tp)))\n\n        # Compute current proxy mAP\n        current_map = compute_map_proxy(preds_by_class, pos_count_by_class)\n\n        # Draw detections above confidence threshold\n        for det in detections:\n            if det[\"score\"] < CONFIDENCE_THRESHOLD:\n                continue\n            x1, y1, x2, y2 = det[\"bbox\"]\n            cls = det[\"class_id\"]\n            score = det[\"score\"]\n            color = class_color(cls)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n            label_name = labels.get(cls, str(cls))\n            caption = \"{} {:.2f}\".format(label_name, score)\n            # Text background\n            (tw, th), base = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n            th2 = th + base + 4\n            y_text = max(0, y1 - th2)\n            cv2.rectangle(frame, (x1, y_text), (x1 + tw + 6, y_text + th2), color, -1)\n            cv2.putText(frame, caption, (x1 + 3, y_text + th + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n        # Overlay mAP (proxy) on frame\n        map_text = \"mAP (proxy): {:.3f}\".format(current_map)\n        cv2.putText(frame, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 220, 10), 2, cv2.LINE_AA)\n\n        # Write to output\n        writer.write(frame)\n\n    t1 = time.time()\n    elapsed = t1 - t0\n    total_fps = (frame_count / elapsed) if elapsed > 0 else 0.0\n    print(\"Processed {} frames in {:.2f}s ({:.2f} FPS).\".format(frame_count, elapsed, total_fps))\n    final_map = compute_map_proxy(preds_by_class, pos_count_by_class)\n    print(\"Final proxy mAP over the video: {:.4f}\".format(final_map))\n    print(\"Output saved to:\", OUTPUT_PATH)\n\n    # Release resources\n    cap.release()\n    writer.release()\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 15
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 120.296,
      "total_cost": 0.09443375,
      "observations": [
        {
          "id": "time-14-16-58-402545_chatcmpl-D6F3aiGQlTSW9l3idWAzjoJ9e2yIG",
          "traceId": "cb06a846",
          "type": "GENERATION",
          "name": "cb06_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:16:58.402000+00:00",
          "endTime": "2026-02-06T12:18:58.698000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.09443375,
          "latency": 120296.0,
          "costDetails": {
            "total": 0.09443375
          },
          "totalTokens": 9838,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 9387,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:16:58.348Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:19:57.616Z"
    },
    {
      "id": "8c72fe3e",
      "timestamp": "2026-02-06T12:14:11.884000+00:00",
      "name": "8c72_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# Configuration parameters\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\nEDGETPU_DELEGATE_PATH = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\ndef load_labels(path):\n    labels = {}\n    if not os.path.isfile(path):\n        print(f\"[WARN] Label file not found at: {path}. Class names will default to IDs.\")\n        return labels\n    with open(path, 'r', encoding='utf-8') as f:\n        for idx, line in enumerate(f):\n            line = line.strip()\n            if not line:\n                continue\n            # Common formats:\n            # 1) \"0 person\"\n            # 2) \"0: person\"\n            # 3) \"person\" (implicit index)\n            if \":\" in line:\n                parts = line.split(\":\", 1)\n            else:\n                parts = line.split(maxsplit=1)\n            if len(parts) == 2 and parts[0].strip().isdigit():\n                labels[int(parts[0].strip())] = parts[1].strip()\n            else:\n                # Fallback: index by line number\n                labels[idx] = line\n    return labels\n\n\ndef make_interpreter(model_path, delegate_path):\n    try:\n        from tflite_runtime.interpreter import Interpreter, load_delegate\n    except ImportError as e:\n        raise SystemExit(f\"[ERROR] Failed to import tflite_runtime. Ensure it is installed on the device. Details: {e}\")\n\n    if not os.path.isfile(model_path):\n        raise SystemExit(f\"[ERROR] TFLite model not found: {model_path}\")\n    if not os.path.isfile(delegate_path):\n        raise SystemExit(f\"[ERROR] EdgeTPU delegate not found: {delegate_path}\")\n\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate(delegate_path)]\n        )\n    except Exception as e:\n        raise SystemExit(f\"[ERROR] Failed to load interpreter with EdgeTPU delegate. Details: {e}\")\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef get_input_details(interpreter):\n    input_details = interpreter.get_input_details()[0]\n    input_index = input_details['index']\n    input_dtype = input_details['dtype']\n    input_shape = input_details['shape']  # [1, height, width, 3]\n    height, width = int(input_shape[1]), int(input_shape[2])\n    quant_params = input_details.get('quantization', (0.0, 0))\n    return input_index, (height, width), input_dtype, quant_params\n\n\ndef preprocess(frame_bgr, input_size, input_dtype, quant_params):\n    # Resize and convert BGR to RGB\n    ih, iw = input_size\n    resized = cv2.resize(frame_bgr, (iw, ih))\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n    # Prepare input tensor\n    if np.issubdtype(input_dtype, np.floating):\n        # Normalize to [0, 1] float32 as a common default for float models\n        input_tensor = rgb.astype(np.float32) / 255.0\n    else:\n        # Quantized input (e.g., uint8/int8) with scale/zero_point\n        scale, zero_point = quant_params if quant_params is not None else (0.0, 0)\n        if scale and scale > 0:\n            input_tensor = rgb.astype(np.float32) / scale + zero_point\n            # Round and clip to valid data type range\n            if input_dtype == np.uint8:\n                input_tensor = np.clip(np.rint(input_tensor), 0, 255).astype(np.uint8)\n            elif input_dtype == np.int8:\n                input_tensor = np.clip(np.rint(input_tensor), -128, 127).astype(np.int8)\n            else:\n                input_tensor = input_tensor.astype(input_dtype)\n        else:\n            # If quantization params are not provided, assume uint8 0-255\n            input_tensor = rgb.astype(input_dtype)\n\n    # Add batch dimension [1, h, w, 3]\n    input_tensor = np.expand_dims(input_tensor, axis=0)\n    return input_tensor\n\n\ndef set_input_tensor(interpreter, input_index, input_tensor):\n    interpreter.set_tensor(input_index, input_tensor)\n\n\ndef _dequantize_if_needed(tensor, details):\n    # Attempt to dequantize tensor if it has quantization parameters and is integer type\n    if not np.issubdtype(tensor.dtype, np.floating):\n        q = details.get('quantization_parameters', details.get('quantization', None))\n        if isinstance(q, dict):\n            scales = q.get('scales', None)\n            zero_points = q.get('zero_points', None)\n            if scales is not None and np.size(scales) > 0:\n                scale = float(scales[0])\n                zero_point = int(zero_points[0]) if (zero_points is not None and np.size(zero_points) > 0) else 0\n                return (tensor.astype(np.float32) - zero_point) * scale\n        elif isinstance(q, (tuple, list)) and len(q) == 2:\n            scale, zero_point = q\n            if scale and scale > 0:\n                return (tensor.astype(np.float32) - zero_point) * scale\n    return tensor\n\n\ndef extract_detections(interpreter, original_hw, conf_threshold):\n    \"\"\"\n    Extracts detection boxes, class IDs, and scores from the interpreter outputs.\n    Returns a list of detections: each is dict with keys: 'bbox' (xmin, ymin, xmax, ymax), 'score', 'class_id'\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    outputs = [interpreter.get_tensor(od['index']) for od in output_details]\n\n    # Try to identify boxes, classes, scores, count from shapes and value ranges\n    boxes = None\n    classes = None\n    scores = None\n    count = None\n\n    # Attempt mapping by common order: boxes, classes, scores, count\n    if len(outputs) >= 3:\n        # Heuristic pass\n        for od, out in zip(output_details, outputs):\n            out_dq = _dequantize_if_needed(out, od)\n            if out_dq.ndim == 3 and out_dq.shape[-1] == 4:\n                boxes = out_dq\n            elif out_dq.ndim == 2:\n                # Could be classes or scores\n                # Scores typically in [0, 1]; classes typically large ints/floats\n                mx = float(np.max(out_dq)) if out_dq.size > 0 else 0.0\n                mn = float(np.min(out_dq)) if out_dq.size > 0 else 0.0\n                # If all values between 0 and 1, more likely scores\n                if mn >= 0.0 and mx <= 1.0 and scores is None:\n                    scores = out_dq\n                else:\n                    classes = out_dq\n            elif out_dq.size == 1 and count is None:\n                count = int(np.rint(out_dq.flatten()[0]))\n\n    # Fallbacks if not determined\n    if boxes is None:\n        for out in outputs:\n            if out.ndim == 3 and out.shape[-1] == 4:\n                boxes = out\n                break\n    if scores is None:\n        # Choose the 2D float tensor with values in [0,1]\n        for out in outputs:\n            if out.ndim == 2 and np.issubdtype(out.dtype, np.floating):\n                if out.size > 0 and float(np.max(out)) <= 1.0 and float(np.min(out)) >= 0.0:\n                    scores = out\n                    break\n    if classes is None:\n        for out in outputs:\n            if out.ndim == 2 and (np.issubdtype(out.dtype, np.integer) or (np.issubdtype(out.dtype, np.floating) and (float(np.max(out)) > 1.0 or float(np.min(out)) < 0.0))):\n                classes = out\n                break\n        # If still None and exactly three outputs, assign whichever 2D float tensor not used as scores\n        if classes is None and len(outputs) == 3:\n            for out in outputs:\n                if out.ndim == 2 and out is not scores:\n                    classes = out\n                    break\n    if count is None and scores is not None:\n        count = scores.shape[1]\n    if count is None and boxes is not None:\n        count = boxes.shape[1]\n\n    if boxes is None or scores is None or classes is None:\n        # Cannot parse outputs reliably\n        return []\n\n    # Ensure proper shapes: squeeze batch dimension [1, N, ...] -> [N, ...]\n    boxes = np.squeeze(boxes, axis=0)\n    scores = np.squeeze(scores, axis=0)\n    classes = np.squeeze(classes, axis=0)\n    if classes.dtype != np.int32 and classes.dtype != np.int64:\n        classes = np.rint(classes).astype(np.int32)\n\n    h, w = original_hw\n    detections = []\n    N = int(count) if count is not None else boxes.shape[0]\n    for i in range(N):\n        score = float(scores[i])\n        if score < conf_threshold:\n            continue\n        cls_id = int(classes[i])\n        # Boxes from TFLite detection postprocess are [ymin, xmin, ymax, xmax] normalized [0..1]\n        y_min, x_min, y_max, x_max = boxes[i].tolist()\n        # Clip to [0,1]\n        x_min = min(max(x_min, 0.0), 1.0)\n        y_min = min(max(y_min, 0.0), 1.0)\n        x_max = min(max(x_max, 0.0), 1.0)\n        y_max = min(max(y_max, 0.0), 1.0)\n\n        xmin_px = int(x_min * w)\n        ymin_px = int(y_min * h)\n        xmax_px = int(x_max * w)\n        ymax_px = int(y_max * h)\n\n        # Sanity check and clip to image bounds\n        xmin_px = max(0, min(xmin_px, w - 1))\n        xmax_px = max(0, min(xmax_px, w - 1))\n        ymin_px = max(0, min(ymin_px, h - 1))\n        ymax_px = max(0, min(ymax_px, h - 1))\n        if xmax_px <= xmin_px or ymax_px <= ymin_px:\n            continue\n\n        detections.append({\n            \"bbox\": (xmin_px, ymin_px, xmax_px, ymax_px),\n            \"score\": score,\n            \"class_id\": cls_id\n        })\n    return detections\n\n\ndef draw_detections(frame_bgr, detections, labels, map_text=\"mAP: N/A\"):\n    # Draw detections on the frame\n    for det in detections:\n        xmin, ymin, xmax, ymax = det[\"bbox\"]\n        score = det[\"score\"]\n        cls_id = det[\"class_id\"]\n\n        # Label lookup with 0-based or 1-based fallback\n        label = labels.get(cls_id, labels.get(cls_id + 1, str(cls_id)))\n        caption = f\"{label}: {score:.2f}\"\n\n        # Choose color based on class id\n        color = ((37 * (cls_id + 1)) % 255, (17 * (cls_id + 1)) % 255, (29 * (cls_id + 1)) % 255)\n\n        cv2.rectangle(frame_bgr, (xmin, ymin), (xmax, ymax), color, 2)\n\n        # Text background\n        (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame_bgr, (xmin, ymin - th - 6), (xmin + tw + 2, ymin), color, thickness=-1)\n        cv2.putText(frame_bgr, caption, (xmin + 1, ymin - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Draw mAP info at top-left\n    cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (30, 220, 30), 2, cv2.LINE_AA)\n    return frame_bgr\n\n\ndef main():\n    print(\"[INFO] Starting 'TFLite object detection with TPU' on Google Coral Dev Board\")\n    print(f\"[INFO] Model: {MODEL_PATH}\")\n    print(f\"[INFO] Labels: {LABEL_PATH}\")\n    print(f\"[INFO] Input video: {INPUT_PATH}\")\n    print(f\"[INFO] Output video: {OUTPUT_PATH}\")\n    print(f\"[INFO] Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n\n    # Setup: Interpreter with EdgeTPU delegate and labels\n    interpreter = make_interpreter(MODEL_PATH, EDGETPU_DELEGATE_PATH)\n    input_index, input_size, input_dtype, quant_params = get_input_details(interpreter)\n    labels = load_labels(LABEL_PATH)\n\n    # Input video\n    if not os.path.isfile(INPUT_PATH):\n        raise SystemExit(f\"[ERROR] Input video not found: {INPUT_PATH}\")\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise SystemExit(f\"[ERROR] Failed to open input video: {INPUT_PATH}\")\n\n    # Output writer setup\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n    in_fps = cap.get(cv2.CAP_PROP_FPS)\n    if in_fps is None or in_fps <= 0 or np.isnan(in_fps):\n        in_fps = 30.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        cap.release()\n        raise SystemExit(f\"[ERROR] Failed to open VideoWriter for: {OUTPUT_PATH}\")\n\n    # Since ground-truth is not provided, mAP cannot be computed; show as N/A.\n    # The pipeline is ready for mAP overlay as text.\n    map_text = \"mAP: N/A (no ground truth)\"\n\n    frame_count = 0\n    avg_inference_ms = None\n    t_start = time.time()\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_count += 1\n\n            # Preprocess\n            t0 = time.time()\n            input_tensor = preprocess(frame, input_size, input_dtype, quant_params)\n            set_input_tensor(interpreter, input_index, input_tensor)\n            # Inference\n            t1 = time.time()\n            interpreter.invoke()\n            t2 = time.time()\n\n            preprocess_ms = (t1 - t0) * 1000.0\n            infer_ms = (t2 - t1) * 1000.0\n\n            if avg_inference_ms is None:\n                avg_inference_ms = infer_ms\n            else:\n                # Exponential moving average for smoother FPS\n                avg_inference_ms = 0.9 * avg_inference_ms + 0.1 * infer_ms\n\n            # Extract detections\n            detections = extract_detections(interpreter, (frame_h, frame_w), CONFIDENCE_THRESHOLD)\n\n            # Draw detections and overlays\n            frame_out = frame.copy()\n            frame_out = draw_detections(frame_out, detections, labels, map_text=map_text)\n\n            # Show FPS (based on inference only)\n            fps_text = f\"TPU Inference: {infer_ms:.1f} ms (avg {avg_inference_ms:.1f} ms) | ~{(1000.0/avg_inference_ms):.1f} FPS\" if avg_inference_ms and avg_inference_ms > 0 else f\"Inference: {infer_ms:.1f} ms\"\n            cv2.putText(frame_out, fps_text, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (50, 190, 255), 2, cv2.LINE_AA)\n\n            writer.write(frame_out)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    elapsed = time.time() - t_start\n    print(f\"[INFO] Processed {frame_count} frames in {elapsed:.2f}s. Output saved to: {OUTPUT_PATH}\")\n    print(\"[INFO] Completed.\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 14
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 85.782,
      "total_cost": 0.07948375,
      "observations": [
        {
          "id": "time-14-14-12-492019_chatcmpl-D6F0u743NUVf5cIKPsqKwNKdsrKon",
          "traceId": "8c72fe3e",
          "type": "GENERATION",
          "name": "8c72_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:14:12.492000+00:00",
          "endTime": "2026-02-06T12:15:38.274000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07948375,
          "latency": 85782.0,
          "costDetails": {
            "total": 0.07948375
          },
          "totalTokens": 8343,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 7892,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:14:12.447Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:16:37.694Z"
    },
    {
      "id": "2c13b817",
      "timestamp": "2026-02-06T12:10:03.941000+00:00",
      "name": "2c13_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# CONFIGURATION PARAMETERS\n# =========================\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONF_THRESHOLD = 0.5\nEDGETPU_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\ndef load_labels(path):\n    \"\"\"\n    Loads labels from a text file.\n    Supports common formats:\n      - \"id label\"\n      - \"id: label\"\n      - \"label\" (index inferred by row order)\n    Returns a dict: {int_id: label_text}\n    \"\"\"\n    labels = {}\n    try:\n        with open(path, \"r\") as f:\n            lines = [l.strip() for l in f.readlines() if l.strip()]\n    except Exception as e:\n        print(\"Failed to read label file:\", e)\n        return labels\n\n    auto_index = 0\n    for line in lines:\n        # Try formats with explicit index first\n        if \":\" in line:\n            # e.g., \"0: person\"\n            parts = line.split(\":\", 1)\n            left = parts[0].strip()\n            right = parts[1].strip()\n            if left.isdigit():\n                labels[int(left)] = right\n                continue\n        parts = line.split()\n        if len(parts) >= 2 and parts[0].isdigit():\n            idx = int(parts[0])\n            name = \" \".join(parts[1:])\n            labels[idx] = name\n            continue\n        # Fallback: use sequential indices\n        labels[auto_index] = line\n        auto_index += 1\n    return labels\n\n\ndef make_interpreter(model_path, edgetpu_lib):\n    \"\"\"\n    Creates TFLite interpreter with EdgeTPU delegate if available.\n    \"\"\"\n    delegates = []\n    if os.path.exists(edgetpu_lib):\n        try:\n            delegates = [load_delegate(edgetpu_lib)]\n        except Exception as e:\n            print(\"Warning: Failed to load EdgeTPU delegate:\", e)\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=delegates)\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef preprocess_frame(frame_bgr, input_details):\n    \"\"\"\n    Preprocesses a single BGR frame to match the model input tensor.\n    Converts BGR to RGB, resizes to model input size, handles dtype/scale.\n    \"\"\"\n    ih, iw = input_details['shape'][1], input_details['shape'][2]\n    dtype = input_details['dtype']\n\n    # Convert BGR -> RGB\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (iw, ih), interpolation=cv2.INTER_LINEAR)\n\n    if dtype == np.float32:\n        inp = resized.astype(np.float32) / 255.0\n    else:\n        inp = resized.astype(dtype)\n\n    inp = np.expand_dims(inp, axis=0)\n    return inp\n\n\ndef map_outputs(interpreter):\n    \"\"\"\n    Robustly maps the four detection outputs: boxes, classes, scores, num_detections.\n    Some models may not provide explicit names or consistent ordering.\n    This function infers the correct tensors by inspecting shapes, dtypes, and value ranges.\n\n    Returns:\n      boxes: np.ndarray with shape [N, 4] in normalized coordinates (ymin, xmin, ymax, xmax)\n      classes: np.ndarray with shape [N] (int)\n      scores: np.ndarray with shape [N] (float)\n      num: int (number of valid detections)\n    \"\"\"\n    details = interpreter.get_output_details()\n    outputs = []\n    for det in details:\n        arr = interpreter.get_tensor(det['index'])\n        name = det.get('name', '')\n        outputs.append((name, arr))\n\n    # Identify candidates\n    boxes = None\n    num = None\n    flat_candidates = []  # arrays with shape [1, N]\n    for name, arr in outputs:\n        a = np.array(arr)\n        if a.ndim == 3 and a.shape[0] == 1 and a.shape[-1] == 4:\n            boxes = a[0]\n        elif a.size == 1:\n            num = int(a.flatten()[0])\n        elif a.ndim == 2 and a.shape[0] == 1 and a.shape[1] >= 1:\n            flat_candidates.append((name, a[0]))\n\n    # Heuristic to separate scores vs classes among flat candidates\n    scores = None\n    classes = None\n\n    def frac_between_0_1(x):\n        if x.size == 0:\n            return 0.0\n        return float(np.mean((x >= 0.0) & (x <= 1.0)))\n\n    def frac_integer_like(x):\n        if x.size == 0:\n            return 0.0\n        return float(np.mean(np.isclose(x, np.round(x))))\n\n    # First try by name keywords if present\n    for name, arr in flat_candidates:\n        lname = name.lower()\n        if \"score\" in lname:\n            scores = arr.astype(np.float32)\n        elif \"class\" in lname:\n            classes = arr.astype(np.int32)\n\n    # If still ambiguous, use value-pattern heuristics\n    if scores is None or classes is None:\n        # Sort by \"score-likeness\": values mostly in [0,1]\n        if len(flat_candidates) >= 1:\n            # Score-like candidate\n            fl_sorted = sorted(flat_candidates, key=lambda t: frac_between_0_1(t[1]), reverse=True)\n            # Class-like candidate: prefer integer-like\n            cl_sorted = sorted(flat_candidates, key=lambda t: (frac_integer_like(t[1]), -frac_between_0_1(t[1])), reverse=True)\n\n            if scores is None:\n                scores = fl_sorted[0][1].astype(np.float32)\n            if classes is None:\n                # pick best integer-like that is not the same array as scores (by identity)\n                for cand_name, cand_arr in cl_sorted:\n                    if not np.may_share_memory(cand_arr, scores):\n                        classes = cand_arr.astype(np.int32)\n                        break\n                if classes is None:\n                    # fallback to second best by score-likeness\n                    if len(fl_sorted) > 1:\n                        classes = fl_sorted[1][1].astype(np.int32)\n                    else:\n                        classes = fl_sorted[0][1].astype(np.int32)\n\n    # Final fallbacks\n    if boxes is None:\n        # Try to find any [N,4] or [1,N,4]\n        for _, a in outputs:\n            if a.ndim == 2 and a.shape[-1] == 4:\n                boxes = a\n                break\n        if boxes is None:\n            raise RuntimeError(\"Could not find detection boxes output tensor.\")\n\n    if num is None:\n        # derive from length of scores or classes or boxes\n        if scores is not None:\n            num = int(scores.shape[0])\n        elif classes is not None:\n            num = int(classes.shape[0])\n        else:\n            num = int(boxes.shape[0])\n\n    # Ensure shapes and dtypes\n    boxes = np.array(boxes).reshape(-1, 4).astype(np.float32)\n    classes = np.array(classes).reshape(-1).astype(np.int32) if classes is not None else np.zeros((boxes.shape[0],), dtype=np.int32)\n    scores = np.array(scores).reshape(-1).astype(np.float32) if scores is not None else np.ones((boxes.shape[0],), dtype=np.float32)\n\n    # Clip to provided num\n    num = max(0, min(num, boxes.shape[0], classes.shape[0], scores.shape[0]))\n    boxes = boxes[:num]\n    classes = classes[:num]\n    scores = scores[:num]\n\n    return boxes, classes, scores, num\n\n\ndef draw_detections(frame_bgr, boxes, classes, scores, labels, threshold, running_map):\n    \"\"\"\n    Draws bounding boxes and labels on the BGR frame for detections above threshold.\n    Also overlays a running \"mAP\" score (proxy: mean confidence of kept detections so far).\n    \"\"\"\n    h, w = frame_bgr.shape[:2]\n    color = (0, 255, 0)\n    thickness = 2\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 0.5\n    font_thickness = 1\n\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score < threshold:\n            continue\n        cls_id = int(classes[i])\n        label = labels.get(cls_id, str(cls_id))\n\n        # boxes are normalized ymin, xmin, ymax, xmax\n        ymin, xmin, ymax, xmax = boxes[i]\n        xmin = max(0, min(w - 1, int(xmin * w)))\n        xmax = max(0, min(w - 1, int(xmax * w)))\n        ymin = max(0, min(h - 1, int(ymin * h)))\n        ymax = max(0, min(h - 1, int(ymax * h)))\n\n        cv2.rectangle(frame_bgr, (xmin, ymin), (xmax, ymax), color, thickness)\n        text = f\"{label}: {score:.2f}\"\n        (tw, th), _ = cv2.getTextSize(text, font, font_scale, font_thickness)\n        text_bg_tl = (xmin, max(0, ymin - th - 6))\n        text_bg_br = (xmin + tw + 6, max(0, ymin))\n        cv2.rectangle(frame_bgr, text_bg_tl, text_bg_br, color, -1)\n        cv2.putText(frame_bgr, text, (xmin + 3, ymin - 4), font, font_scale, (0, 0, 0), font_thickness, cv2.LINE_AA)\n\n    # Overlay running \"mAP\" (proxy: mean of all kept confidences so far)\n    map_text = f\"mAP: {running_map:.3f}\"\n    (mw, mh), _ = cv2.getTextSize(map_text, font, 0.6, 2)\n    pad = 6\n    cv2.rectangle(frame_bgr, (5, 5), (5 + mw + 2 * pad, 5 + mh + 2 * pad), (0, 0, 0), -1)\n    cv2.putText(frame_bgr, map_text, (5 + pad, 5 + mh + pad), font, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n\n\ndef main():\n    # Setup interpreter with EdgeTPU\n    interpreter = make_interpreter(MODEL_PATH, EDGETPU_LIB)\n    input_details = interpreter.get_input_details()[0]\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Video IO\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # fallback\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) or 0)\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) or 0)\n    if width <= 0 or height <= 0:\n        # Try to read one frame to infer size\n        ret_probe, frame_probe = cap.read()\n        if not ret_probe:\n            cap.release()\n            raise RuntimeError(\"Failed to read a frame to infer video size.\")\n        height, width = frame_probe.shape[:2]\n        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # reset to start\n\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    running_score_sum = 0.0\n    running_score_count = 0\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Preprocess\n            inp = preprocess_frame(frame, input_details)\n\n            # Inference\n            interpreter.set_tensor(input_details['index'], inp)\n            interpreter.invoke()\n\n            # Postprocess/mapping\n            boxes, classes, scores, num = map_outputs(interpreter)\n\n            # Filter and accumulate scores for mAP proxy\n            valid_mask = scores >= CONF_THRESHOLD\n            kept_scores = scores[valid_mask]\n            if kept_scores.size > 0:\n                running_score_sum += float(np.sum(kept_scores))\n                running_score_count += int(kept_scores.size)\n            running_map = (running_score_sum / running_score_count) if running_score_count > 0 else 0.0\n\n            # Draw\n            draw_detections(frame, boxes, classes, scores, labels, CONF_THRESHOLD, running_map)\n\n            # Write\n            writer.write(frame)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    # Optionally, print final mAP proxy\n    final_map = (running_score_sum / running_score_count) if running_score_count > 0 else 0.0\n    print(f\"Processing completed. Saved to: {OUTPUT_PATH}\")\n    print(f\"Final mAP (proxy based on mean confidence above threshold {CONF_THRESHOLD}): {final_map:.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 13
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 168.951,
      "total_cost": 0.131085,
      "observations": [
        {
          "id": "time-14-10-04-565771_chatcmpl-D6EwufwFIjRqIoqkLUt55exPFlXvo",
          "traceId": "2c13b817",
          "type": "GENERATION",
          "name": "2c13_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:10:04.565000+00:00",
          "endTime": "2026-02-06T12:11:12.238000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05784375,
          "latency": 67673.0,
          "costDetails": {
            "total": 0.05784375
          },
          "totalTokens": 6179,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 5728,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-11-21-556583_chatcmpl-D6Ey9qqlAE4WhFciqQ6Qh3bzIyhUp",
          "traceId": "2c13b817",
          "type": "GENERATION",
          "name": "2c13_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T12:11:21.556000+00:00",
          "endTime": "2026-02-06T12:12:53.516000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07324125,
          "latency": 91960.0,
          "costDetails": {
            "total": 0.07324125
          },
          "totalTokens": 7836,
          "environment": "default",
          "promptTokens": 585,
          "completionTokens": 7251,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:10:04.494Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:13:51.803Z"
    },
    {
      "id": "8e6d7754",
      "timestamp": "2026-02-06T12:04:20.829000+00:00",
      "name": "8e6d_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration Parameters\n# =========================\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\nEDGETPU_SHARED_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n# =========================\n# Utility Functions\n# =========================\ndef load_labels(path):\n    labels = {}\n    if not os.path.isfile(path):\n        print(f\"Warning: Label file not found at {path}. Using class IDs as labels.\")\n        return labels\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = [l.strip() for l in f.readlines() if l.strip()]\n    # Support two formats:\n    # 1) \"0 person\"\n    # 2) \"person\" (implicit index)\n    for idx, line in enumerate(lines):\n        if line.split(' ', 1)[0].isdigit():\n            label_id, label_name = line.split(' ', 1)\n            labels[int(label_id)] = label_name.strip()\n        else:\n            labels[idx] = line\n    return labels\n\ndef make_interpreter(model_path):\n    delegates = [load_delegate(EDGETPU_SHARED_LIB)]\n    return Interpreter(model_path=model_path, experimental_delegates=delegates)\n\ndef set_input_tensor(interpreter, image):\n    input_details = interpreter.get_input_details()[0]\n    input_index = input_details['index']\n    input_dtype = input_details['dtype']\n    # Ensure correct dtype and scale if needed\n    if input_dtype == np.uint8:\n        tensor = image.astype(np.uint8)\n    else:\n        # float32 model: normalize to [0,1]\n        tensor = image.astype(np.float32) / 255.0\n    interpreter.set_tensor(input_index, np.expand_dims(tensor, axis=0))\n\ndef get_output_tensors(interpreter):\n    # Attempt to parse standard TFLite detection postprocess outputs\n    output_details = interpreter.get_output_details()\n    boxes = None\n    classes = None\n    scores = None\n    num = None\n\n    # Identify tensors by characteristics\n    for od in output_details:\n        data = interpreter.get_tensor(od['index'])\n        shape = data.shape\n        if len(shape) == 2 and shape[-1] == 4:\n            # Some models might output [N,4], but most are [1, N, 4]\n            # Normalize to [1, N, 4]\n            boxes = np.expand_dims(data, axis=0)\n        elif len(shape) == 3 and shape[-1] == 4:\n            boxes = data\n        elif len(shape) == 2:\n            # Could be classes or scores; inspect dtype\n            if data.dtype == np.float32:\n                # Could be scores\n                scores = np.expand_dims(data, axis=0)\n            else:\n                classes = np.expand_dims(data, axis=0)\n        elif len(shape) == 3:\n            # Could be classes or scores with [1, N, 1] or [1, N]\n            if shape[-1] == 1:\n                casted = data.squeeze(-1)\n                if casted.dtype == np.float32:\n                    scores = casted\n                else:\n                    classes = casted\n            else:\n                # [1, N] style\n                if data.dtype == np.float32:\n                    scores = data\n                else:\n                    classes = data\n        elif len(shape) == 1 and shape[0] == 1:\n            num = int(np.squeeze(data).astype(np.int32))\n\n    # Fallback to standard order if still None\n    if boxes is None or scores is None or classes is None:\n        # Try standard indices: 0: boxes, 1: classes, 2: scores, 3: num\n        try:\n            boxes = interpreter.get_tensor(output_details[0]['index'])\n            classes = interpreter.get_tensor(output_details[1]['index'])\n            scores = interpreter.get_tensor(output_details[2]['index'])\n            if len(output_details) > 3:\n                num = int(np.squeeze(interpreter.get_tensor(output_details[3]['index'])).astype(np.int32))\n        except Exception:\n            # As a last resort, return empty results\n            return np.zeros((0, 4), dtype=np.float32), np.zeros((0,), dtype=np.float32), np.zeros((0,), dtype=np.int32), 0\n\n    # Squeeze leading batch dim if present\n    boxes = np.squeeze(boxes, axis=0) if boxes.ndim == 3 else boxes\n    scores = np.squeeze(scores, axis=0) if scores.ndim == 3 else scores\n    classes = np.squeeze(classes, axis=0) if classes.ndim == 3 else classes\n\n    # Ensure shapes: [N,4], [N], [N]\n    if boxes.ndim == 2 and boxes.shape[-1] == 4:\n        N = boxes.shape[0]\n        if scores.ndim == 1 and scores.shape[0] == N and classes.ndim == 1 and classes.shape[0] == N:\n            pass\n        else:\n            # Attempt to squeeze\n            scores = scores.reshape(-1)\n            classes = classes.reshape(-1)\n            N = min(N, scores.shape[0], classes.shape[0])\n            boxes = boxes[:N]\n            scores = scores[:N]\n            classes = classes[:N]\n    else:\n        return np.zeros((0, 4), dtype=np.float32), np.zeros((0,), dtype=np.float32), np.zeros((0,), dtype=np.int32), 0\n\n    if num is None:\n        num = boxes.shape[0]\n    return boxes, scores, classes.astype(np.int32), int(num)\n\ndef letterbox_image(image, target_size):\n    # Not using letterbox for detection postprocess models; simple resize suffices.\n    return cv2.resize(image, target_size, interpolation=cv2.INTER_LINEAR)\n\ndef preprocess_frame(frame, input_details):\n    # Get expected input size\n    _, in_h, in_w, in_c = input_details['shape']\n    # Convert BGR -> RGB\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n    return resized\n\ndef generate_color_for_class(class_id):\n    # Deterministic color for each class id\n    np.random.seed(class_id)\n    color = tuple(int(c) for c in np.random.randint(60, 255, size=3))\n    # Convert to BGR for OpenCV\n    return (color[2], color[1], color[0])\n\ndef draw_label(img, text, x, y, color=(255, 255, 255), scale=0.5, thickness=1):\n    # Draw a filled rectangle with text on top-left at (x, y)\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    (tw, th), bl = cv2.getTextSize(text, font, scale, thickness)\n    # Ensure y is above the bbox if possible\n    y = max(th + 2, y)\n    # Background rectangle\n    cv2.rectangle(img, (x, y - th - 2), (x + tw + 2, y + 2), color, -1)\n    # Put text in black for contrast\n    cv2.putText(img, text, (x + 1, y - 2), font, scale, (0, 0, 0), thickness, cv2.LINE_AA)\n\ndef clip_bbox(x1, y1, x2, y2, width, height):\n    x1 = max(0, min(int(x1), width - 1))\n    y1 = max(0, min(int(y1), height - 1))\n    x2 = max(0, min(int(x2), width - 1))\n    y2 = max(0, min(int(y2), height - 1))\n    return x1, y1, x2, y2\n\ndef main():\n    # Step 1. Setup: load interpreter with EdgeTPU delegate, allocate tensors, load labels, open video IO\n    labels = load_labels(LABEL_PATH)\n\n    interpreter = make_interpreter(MODEL_PATH)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()[0]\n\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: Cannot open input video: {INPUT_PATH}\")\n        return\n\n    # Prepare video writer with same size as input frames\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps <= 1e-3:\n        fps = 30.0  # Fallback if metadata missing\n\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        print(f\"Error: Cannot open output video for writing: {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # Stats and proxy mAP accumulator (mean confidence over kept detections)\n    total_frames = 0\n    total_inference_time = 0.0\n    kept_detections_conf_sum = 0.0\n    kept_detections_count = 0\n\n    # Processing loop\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        total_frames += 1\n\n        # Step 2. Preprocessing\n        input_tensor = preprocess_frame(frame, input_details)\n\n        # Step 3. Inference\n        set_input_tensor(interpreter, input_tensor)\n        t0 = time.time()\n        interpreter.invoke()\n        t1 = time.time()\n        total_inference_time += (t1 - t0)\n\n        # Step 4. Output handling: decode, draw, compute proxy mAP, write\n        boxes, scores, classes, num = get_output_tensors(interpreter)\n\n        # Convert and draw detections\n        for i in range(min(num, boxes.shape[0])):\n            score = float(scores[i])\n            if score < CONFIDENCE_THRESHOLD:\n                continue\n\n            # Typical model returns [ymin, xmin, ymax, xmax] in normalized coords\n            ymin, xmin, ymax, xmax = boxes[i]\n            x1 = int(xmin * frame_w)\n            y1 = int(ymin * frame_h)\n            x2 = int(xmax * frame_w)\n            y2 = int(ymax * frame_h)\n            x1, y1, x2, y2 = clip_bbox(x1, y1, x2, y2, frame_w, frame_h)\n\n            class_id = int(classes[i])\n            class_name = labels.get(class_id, f\"id:{class_id}\")\n            label_text = f\"{class_name} {score:.2f}\"\n\n            color = generate_color_for_class(class_id)\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            # Place label slightly above the top-left corner of the box\n            draw_label(frame, label_text, x1, max(15, y1), color=color, scale=0.55, thickness=1)\n\n            kept_detections_conf_sum += score\n            kept_detections_count += 1\n\n        # Compute and overlay proxy mAP (mean of confidences for kept detections)\n        proxy_map = (kept_detections_conf_sum / kept_detections_count) if kept_detections_count > 0 else 0.0\n        map_text = f\"mAP: {proxy_map:.3f}\"\n        draw_label(frame, map_text, 10, 25, color=(255, 255, 255), scale=0.7, thickness=2)\n\n        writer.write(frame)\n\n    # Release resources\n    cap.release()\n    writer.release()\n\n    # Summary\n    avg_inf_ms = (total_inference_time / max(1, total_frames)) * 1000.0\n    final_proxy_map = (kept_detections_conf_sum / kept_detections_count) if kept_detections_count > 0 else 0.0\n    print(\"Processing complete.\")\n    print(f\"Frames processed: {total_frames}\")\n    print(f\"Average inference time: {avg_inf_ms:.2f} ms\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n    print(f\"Proxy mAP (mean confidence over detections >= {CONFIDENCE_THRESHOLD}): {final_proxy_map:.3f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 12
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 263.399,
      "total_cost": 0.15103,
      "observations": [
        {
          "id": "time-14-04-21-469213_chatcmpl-D6ErNkbaTTT5uj5Vs1dc0Te7Mh169",
          "traceId": "8e6d7754",
          "type": "GENERATION",
          "name": "8e6d_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:04:21.469000+00:00",
          "endTime": "2026-02-06T12:07:09.884000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.08606375,
          "latency": 168415.0,
          "costDetails": {
            "total": 0.08606375
          },
          "totalTokens": 9001,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 8550,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-14-07-19-237101_chatcmpl-D6EuFKslHMmVw59GIYZvndJkeZ48u",
          "traceId": "8e6d7754",
          "type": "GENERATION",
          "name": "8e6d_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T12:07:19.237000+00:00",
          "endTime": "2026-02-06T12:08:44.868000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06496625,
          "latency": 85631.0,
          "costDetails": {
            "total": 0.06496625
          },
          "totalTokens": 7012,
          "environment": "default",
          "promptTokens": 589,
          "completionTokens": 6423,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:04:21.388Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:09:43.219Z"
    },
    {
      "id": "23067f7e",
      "timestamp": "2026-02-06T12:01:34.022000+00:00",
      "name": "2306_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"  # Read a single video file from the given input_path\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"  # Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)\nconfidence_threshold = 0.5\n\n# EdgeTPU shared library path on Google Coral Dev Board (aarch64)\nEDGETPU_SHARED_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\ndef load_labels(path):\n    \"\"\"\n    Load labels from a text file.\n    Supports both:\n      - \"index label\" per line (e.g., \"0 person\")\n      - \"label\" per line (index implied by line order starting at 0)\n    \"\"\"\n    labels = {}\n    try:\n        with open(path, 'r') as f:\n            idx = 0\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                parts = line.split(maxsplit=1)\n                if len(parts) == 2 and parts[0].isdigit():\n                    labels[int(parts[0])] = parts[1].strip()\n                else:\n                    labels[idx] = line\n                    idx += 1\n    except Exception as e:\n        print(f\"Failed to load labels from {path}: {e}\")\n        labels = {}\n    return labels\n\n\ndef make_interpreter_with_edgetpu(model_file, delegate_lib):\n    \"\"\"\n    Create a TFLite interpreter with EdgeTPU delegate.\n    \"\"\"\n    try:\n        interpreter = Interpreter(\n            model_path=model_file,\n            experimental_delegates=[load_delegate(delegate_lib)]\n        )\n        return interpreter\n    except Exception as e:\n        raise RuntimeError(f\"Failed to create EdgeTPU interpreter: {e}\")\n\n\ndef get_input_details(interpreter):\n    input_details = interpreter.get_input_details()[0]\n    height = input_details['shape'][1]\n    width = input_details['shape'][2]\n    dtype = input_details['dtype']\n    return width, height, dtype\n\n\ndef preprocess(frame_bgr, input_w, input_h, input_dtype):\n    \"\"\"\n    Preprocess the input frame:\n      - resize to model input size\n      - convert BGR to RGB\n      - convert dtype as required by the model\n      - add batch dimension\n    \"\"\"\n    resized = cv2.resize(frame_bgr, (input_w, input_h), interpolation=cv2.INTER_LINEAR)\n    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n\n    if input_dtype == np.uint8:\n        input_tensor = rgb.astype(np.uint8)\n    else:\n        # float input expected\n        input_tensor = rgb.astype(np.float32) / 255.0\n\n    input_tensor = np.expand_dims(input_tensor, axis=0)\n    return input_tensor\n\n\ndef extract_detections(interpreter, frame_w, frame_h, conf_thresh):\n    \"\"\"\n    Extract detections from interpreter outputs.\n    Attempts to match typical TFLite detection postprocess tensors:\n      - boxes: [1, N, 4] in normalized ymin, xmin, ymax, xmax\n      - classes: [1, N] or [N]\n      - scores: [1, N] or [N]\n      - count: [1] number of valid detections\n    Returns a list of dicts: { 'bbox': (x1,y1,x2,y2), 'class_id': int, 'score': float }\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    tensors = {}\n    for od in output_details:\n        name = od.get('name', '')\n        data = interpreter.get_tensor(od['index'])\n        tensors[name] = data\n\n    boxes = None\n    classes = None\n    scores = None\n    count = None\n\n    # Try assign by canonical names first\n    for name, data in tensors.items():\n        lname = name.lower()\n        if 'box' in lname:\n            boxes = data\n        elif 'score' in lname:\n            scores = data\n        elif 'class' in lname:\n            classes = data\n        elif 'count' in lname or 'num_detections' in lname:\n            count = data\n\n    # Fallback by shapes if needed\n    if boxes is None or scores is None or classes is None:\n        # Collect all outputs\n        outs = [interpreter.get_tensor(od['index']) for od in output_details]\n        # Identify boxes (has last dim 4)\n        for out in outs:\n            if out.ndim >= 2 and out.shape[-1] == 4:\n                boxes = out\n                break\n        # Identify scores (float, not 4 dims last)\n        cand = []\n        for out in outs:\n            if out is boxes:\n                continue\n            if out.dtype in (np.float32, np.float64) and out.size > 1:\n                cand.append(out)\n        # Among candidates, one likely matches classes (float/int) and scores (float)\n        # Choose the one with more unique fractional parts as scores\n        # But to keep simple: assign the larger-magnitude as scores, the other as classes\n        if len(cand) >= 2:\n            a, b = cand[0], cand[1]\n            if a.max() > b.max():\n                scores, classes = a, b\n            else:\n                scores, classes = b, a\n        elif len(cand) == 1:\n            scores = cand[0]\n\n        # Count (scalar)\n        for out in outs:\n            if out.size == 1 and (out.dtype == np.float32 or out.dtype == np.int32):\n                count = out\n                break\n\n    # Normalize shapes to [N, ...]\n    if boxes is not None and boxes.ndim == 3 and boxes.shape[0] == 1:\n        boxes = boxes[0]\n    if classes is not None and classes.ndim == 2 and classes.shape[0] == 1:\n        classes = classes[0]\n    if scores is not None and scores.ndim == 2 and scores.shape[0] == 1:\n        scores = scores[0]\n\n    n = 0\n    if count is not None:\n        n = int(np.squeeze(count).astype(np.int32))\n    else:\n        if scores is not None:\n            n = scores.shape[0]\n        elif boxes is not None:\n            n = boxes.shape[0]\n\n    detections = []\n    if boxes is None or scores is None or classes is None or n == 0:\n        return detections\n\n    # Determine if boxes are normalized [0,1]\n    is_normalized = True\n    try:\n        if boxes.max() > 2.0:\n            is_normalized = False\n    except Exception:\n        is_normalized = True\n\n    for i in range(n):\n        score = float(scores[i])\n        if score < conf_thresh:\n            continue\n        cls_id = int(classes[i])\n\n        box = boxes[i]\n        if is_normalized:\n            ymin = max(0.0, min(1.0, float(box[0])))\n            xmin = max(0.0, min(1.0, float(box[1])))\n            ymax = max(0.0, min(1.0, float(box[2])))\n            xmax = max(0.0, min(1.0, float(box[3])))\n\n            x1 = int(xmin * frame_w)\n            y1 = int(ymin * frame_h)\n            x2 = int(xmax * frame_w)\n            y2 = int(ymax * frame_h)\n        else:\n            # Assume absolute pixel coordinates\n            x1 = int(max(0, min(frame_w - 1, float(box[1]))))\n            y1 = int(max(0, min(frame_h - 1, float(box[0]))))\n            x2 = int(max(0, min(frame_w - 1, float(box[3]))))\n            y2 = int(max(0, min(frame_h - 1, float(box[2]))))\n\n        # Sanity check and clamp\n        x1, x2 = max(0, min(x1, frame_w - 1)), max(0, min(x2, frame_w - 1))\n        y1, y2 = max(0, min(y1, frame_h - 1)), max(0, min(y2, frame_h - 1))\n        if x2 <= x1 or y2 <= y1:\n            continue\n\n        detections.append({\n            'bbox': (x1, y1, x2, y2),\n            'class_id': cls_id,\n            'score': score\n        })\n\n    return detections\n\n\ndef color_for_class(class_id):\n    # Deterministic pseudo-color based on class id\n    r = int((37 * (class_id + 1)) % 255)\n    g = int((17 * (class_id + 1)) % 255)\n    b = int((29 * (class_id + 1)) % 255)\n    return (b, g, r)\n\n\ndef main():\n    # 1) Setup\n    labels = load_labels(label_path)\n    if not os.path.exists(input_path):\n        raise FileNotFoundError(f\"Input video not found: {input_path}\")\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model not found: {model_path}\")\n\n    print(\"Initializing TFLite interpreter with EdgeTPU...\")\n    interpreter = make_interpreter_with_edgetpu(model_path, EDGETPU_SHARED_LIB)\n    interpreter.allocate_tensors()\n    in_w, in_h, in_dtype = get_input_details(interpreter)\n    print(f\"Model input: {in_w}x{in_h}, dtype={in_dtype}\")\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open video: {input_path}\")\n\n    src_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    src_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 1e-3:\n        fps = 30.0  # Fallback FPS\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (src_w, src_h))\n    if not writer.isOpened():\n        raise RuntimeError(f\"Failed to open video writer: {output_path}\")\n\n    print(f\"Input: {input_path} ({src_w}x{src_h} @ {fps:.2f} fps)\")\n    print(f\"Output: {output_path}\")\n\n    # Statistics for an approximate \"mAP\" (proxy due to lack of ground truth)\n    # We will compute per-class average of detection scores >= threshold,\n    # and then the mean across classes that appeared at least once.\n    per_class_scores = {}  # class_id -> list of scores\n\n    # 2) Process frames\n    frame_count = 0\n    t0 = time.time()\n    last_time = t0\n\n    try:\n        while True:\n            ret, frame_bgr = cap.read()\n            if not ret or frame_bgr is None:\n                break\n\n            frame_count += 1\n\n            # 2) Preprocessing\n            input_tensor = preprocess(frame_bgr, in_w, in_h, in_dtype)\n\n            # 3) Inference\n            input_details = interpreter.get_input_details()\n            interpreter.set_tensor(input_details[0]['index'], input_tensor)\n            interpreter.invoke()\n\n            # 4) Output handling: detections, draw, compute \"mAP (approx.)\"\n            detections = extract_detections(interpreter, src_w, src_h, confidence_threshold)\n\n            for det in detections:\n                x1, y1, x2, y2 = det['bbox']\n                cls_id = det['class_id']\n                score = det['score']\n\n                # Update per-class scores\n                if cls_id not in per_class_scores:\n                    per_class_scores[cls_id] = []\n                per_class_scores[cls_id].append(score)\n\n                # Draw bounding box and label\n                color = color_for_class(cls_id)\n                thickness = max(1, int(round(0.002 * (src_w + src_h))))\n                cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, thickness)\n\n                label_name = labels.get(cls_id, f\"id:{cls_id}\")\n                label_text = f\"{label_name} {score:.2f}\"\n                # Text background\n                (tw, th), bl = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n                y_text = max(th + 4, y1 - 4)\n                cv2.rectangle(frame_bgr, (x1, y_text - th - 4), (x1 + tw + 4, y_text + 2), color, -1)\n                cv2.putText(frame_bgr, label_text, (x1 + 2, y_text - 2),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1, cv2.LINE_AA)\n\n            # Compute approximate mAP (proxy): mean of per-class average scores\n            map_approx = None\n            if len(per_class_scores) > 0:\n                class_means = []\n                for cls_id, scores in per_class_scores.items():\n                    if len(scores) > 0:\n                        class_means.append(float(np.mean(np.array(scores, dtype=np.float32))))\n                if len(class_means) > 0:\n                    map_approx = float(np.mean(np.array(class_means, dtype=np.float32)))\n\n            # Overlay performance info\n            now = time.time()\n            elapsed = now - last_time\n            last_time = now\n            inst_fps = 1.0 / elapsed if elapsed > 1e-6 else 0.0\n\n            info_text = f\"Detections: {len(detections)} | FPS: {inst_fps:.1f}\"\n            cv2.putText(frame_bgr, info_text, (10, 24),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 240, 10), 2, cv2.LINE_AA)\n\n            if map_approx is not None:\n                cv2.putText(frame_bgr, f\"mAP (approx): {map_approx:.3f}\", (10, 50),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 240, 10), 2, cv2.LINE_AA)\n            else:\n                cv2.putText(frame_bgr, \"mAP (approx): N/A\", (10, 50),\n                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (10, 240, 10), 2, cv2.LINE_AA)\n\n            writer.write(frame_bgr)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    total_time = time.time() - t0\n    avg_fps = frame_count / total_time if total_time > 1e-6 else 0.0\n    print(f\"Processed {frame_count} frames in {total_time:.2f}s (avg FPS: {avg_fps:.2f})\")\n    if per_class_scores:\n        class_means = [float(np.mean(np.array(s, dtype=np.float32))) for s in per_class_scores.values() if len(s) > 0]\n        if class_means:\n            map_approx = float(np.mean(np.array(class_means, dtype=np.float32)))\n            print(f\"Final mAP (approx): {map_approx:.4f}\")\n    print(\"Output saved to:\", output_path)\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 11
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 87.792,
      "total_cost": 0.07787375,
      "observations": [
        {
          "id": "time-14-01-34-651276_chatcmpl-D6EohwQ3Vd3yprDbOKnG7g02OL4iA",
          "traceId": "23067f7e",
          "type": "GENERATION",
          "name": "2306_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T12:01:34.651000+00:00",
          "endTime": "2026-02-06T12:03:02.443000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07787375,
          "latency": 87792.0,
          "costDetails": {
            "total": 0.07787375
          },
          "totalTokens": 8182,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 7731,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T12:01:34.588Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:04:00.792Z"
    },
    {
      "id": "6a960a1f",
      "timestamp": "2026-02-06T11:58:59.188000+00:00",
      "name": "6a96_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport cv2\nimport numpy as np\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# --------------------------\n# Configuration Parameters\n# --------------------------\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\nEDGETPU_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n# Optional: ground truth CSV to compute mAP if available.\n# Expected CSV format per line: frame_index,xmin,ymin,xmax,ymax,class_id\n# Coordinates are pixel values in the original frame size.\nGT_PATH = os.getenv(\"GT_PATH\", os.path.splitext(INPUT_PATH)[0] + \"_gt.csv\")\n\n\n# --------------------------\n# Utilities\n# --------------------------\ndef load_labels(label_path):\n    labels = {}\n    if not os.path.exists(label_path):\n        return labels\n    with open(label_path, \"r\") as f:\n        for i, line in enumerate(f.readlines()):\n            line = line.strip()\n            if not line:\n                continue\n            # Try formats:\n            # 1) \"id label\"\n            # 2) \"id: label\"\n            # 3) \"label\" (implied id = line index)\n            if \":\" in line:\n                parts = line.split(\":\", 1)\n                try:\n                    idx = int(parts[0].strip())\n                    labels[idx] = parts[1].strip()\n                except ValueError:\n                    labels[i] = line\n            else:\n                parts = line.split(maxsplit=1)\n                if len(parts) == 2 and parts[0].isdigit():\n                    labels[int(parts[0])] = parts[1].strip()\n                else:\n                    labels[i] = line\n    return labels\n\n\ndef make_interpreter(model_path, edgetpu_lib):\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate(edgetpu_lib)]\n    )\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef preprocess_frame(frame_bgr, input_details):\n    h_in, w_in = input_details['shape'][1], input_details['shape'][2]\n    dtype = input_details['dtype']\n    # Convert BGR to RGB as most TFLite models expect RGB\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (w_in, h_in))\n    if dtype == np.float32:\n        input_tensor = resized.astype(np.float32) / 255.0\n    elif dtype == np.uint8:\n        input_tensor = resized.astype(np.uint8)\n    else:\n        # Fallback: convert to required dtype without scaling\n        input_tensor = resized.astype(dtype)\n    # Add batch dimension\n    input_tensor = np.expand_dims(input_tensor, axis=0)\n    return input_tensor\n\n\ndef extract_detections(interpreter, frame_w, frame_h, confidence_threshold):\n    # Typical EdgeTPU SSD outputs: boxes, classes, scores, num_detections\n    output_details = interpreter.get_output_details()\n\n    # Assume conventional ordering (EdgeTPU models generally follow this):\n    # 0: boxes [1, num, 4], 1: classes [1, num], 2: scores [1, num], 3: num [1]\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0]\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num = int(interpreter.get_tensor(output_details[3]['index'])[0])\n\n    detections = []\n    for i in range(num):\n        score = float(scores[i])\n        if score < confidence_threshold:\n            continue\n        cls_id = int(classes[i])\n        # Boxes are [ymin, xmin, ymax, xmax] in normalized coordinates\n        ymin, xmin, ymax, xmax = boxes[i]\n        # Convert to pixel coordinates in original frame size\n        x1 = int(max(0, xmin * frame_w))\n        y1 = int(max(0, ymin * frame_h))\n        x2 = int(min(frame_w - 1, xmax * frame_w))\n        y2 = int(min(frame_h - 1, ymax * frame_h))\n        # Ensure valid box\n        if x2 <= x1 or y2 <= y1:\n            continue\n        detections.append({\n            \"bbox\": (x1, y1, x2, y2),\n            \"score\": score,\n            \"class_id\": cls_id\n        })\n    return detections\n\n\ndef draw_detections(frame, detections, labels):\n    for det in detections:\n        x1, y1, x2, y2 = det[\"bbox\"]\n        score = det[\"score\"]\n        cls_id = det[\"class_id\"]\n        label = labels.get(cls_id, f\"id:{cls_id}\")\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        text = f\"{label}: {score:.2f}\"\n        t_size, _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        text_bg_br = (x1 + t_size[0] + 4, y1 - t_size[1] - 6)\n        text_bg_tl = (x1, y1 - t_size[1] - 6 if y1 - t_size[1] - 6 > 0 else 0)\n        # Draw filled background for readability\n        cv2.rectangle(frame, text_bg_tl, text_bg_br, color, thickness=-1)\n        cv2.putText(frame, text, (x1 + 2, y1 - 4 if y1 - 4 > 0 else y1 + 14),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), thickness=1, lineType=cv2.LINE_AA)\n\n\ndef load_ground_truth(gt_path):\n    # Returns dict: frame_index -> list of dicts: {\"bbox\":(x1,y1,x2,y2), \"class_id\":int}\n    gts = {}\n    if not os.path.exists(gt_path):\n        return None\n    try:\n        with open(gt_path, \"r\") as f:\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith(\"#\"):\n                    continue\n                parts = line.split(\",\")\n                if len(parts) < 6:\n                    continue\n                fi = int(parts[0])\n                x1, y1, x2, y2 = map(int, parts[1:5])\n                cid = int(parts[5])\n                gts.setdefault(fi, []).append({\"bbox\": (x1, y1, x2, y2), \"class_id\": cid})\n        return gts\n    except Exception:\n        # If parsing fails, return None to skip mAP\n        return None\n\n\ndef iou(boxA, boxB):\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0, xB - xA)\n    inter_h = max(0, yB - yA)\n    inter_area = inter_w * inter_h\n    if inter_area <= 0:\n        return 0.0\n    boxA_area = max(0, (boxA[2] - boxA[0])) * max(0, (boxA[3] - boxA[1]))\n    boxB_area = max(0, (boxB[2] - boxB[0])) * max(0, (boxB[3] - boxB[1]))\n    denom = float(boxA_area + boxB_area - inter_area + 1e-9)\n    return inter_area / denom\n\n\ndef compute_map(predictions, ground_truths, iou_thresh=0.5):\n    \"\"\"\n    predictions: list of dicts with keys: frame_index, class_id, score, bbox\n    ground_truths: dict frame_index -> list of dicts with keys: class_id, bbox\n    Returns mAP across classes that have GTs using 11-point interpolation.\n    \"\"\"\n    if not ground_truths:\n        return None\n\n    # Organize GTs by class and frame\n    gt_by_class_frame = {}\n    total_gt_per_class = {}\n    for fi, anns in ground_truths.items():\n        for ann in anns:\n            cid = ann[\"class_id\"]\n            gt_by_class_frame.setdefault(cid, {}).setdefault(fi, [])\n            gt_by_class_frame[cid][fi].append({\"bbox\": ann[\"bbox\"], \"matched\": False})\n            total_gt_per_class[cid] = total_gt_per_class.get(cid, 0) + 1\n\n    # Organize predictions by class\n    preds_by_class = {}\n    for p in predictions:\n        cid = p[\"class_id\"]\n        preds_by_class.setdefault(cid, []).append(p)\n\n    aps = []\n    for cid, preds in preds_by_class.items():\n        if cid not in total_gt_per_class or total_gt_per_class[cid] == 0:\n            # No GT for this class -> skip from mAP\n            continue\n\n        # Sort predictions by score descending\n        preds_sorted = sorted(preds, key=lambda x: -x[\"score\"])\n        tp = np.zeros(len(preds_sorted), dtype=np.float32)\n        fp = np.zeros(len(preds_sorted), dtype=np.float32)\n\n        # Reset matched flags for this class before evaluation\n        for fi in gt_by_class_frame.get(cid, {}):\n            for entry in gt_by_class_frame[cid][fi]:\n                entry[\"matched\"] = False\n\n        for i, pred in enumerate(preds_sorted):\n            fi = pred[\"frame_index\"]\n            pred_box = pred[\"bbox\"]\n            matched = False\n            if cid in gt_by_class_frame and fi in gt_by_class_frame[cid]:\n                candidates = gt_by_class_frame[cid][fi]\n                best_iou = 0.0\n                best_idx = -1\n                for j, g in enumerate(candidates):\n                    if g[\"matched\"]:\n                        continue\n                    iou_val = iou(pred_box, g[\"bbox\"])\n                    if iou_val > best_iou:\n                        best_iou = iou_val\n                        best_idx = j\n                if best_iou >= iou_thresh and best_idx >= 0:\n                    candidates[best_idx][\"matched\"] = True\n                    matched = True\n            if matched:\n                tp[i] = 1.0\n            else:\n                fp[i] = 1.0\n\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        total_gt = float(total_gt_per_class[cid])\n        recall = cum_tp / (total_gt + 1e-9)\n        precision = cum_tp / (cum_tp + cum_fp + 1e-9)\n\n        # 11-point interpolation AP\n        ap = 0.0\n        for t in np.linspace(0, 1, 11):\n            if np.any(recall >= t):\n                p = np.max(precision[recall >= t])\n            else:\n                p = 0.0\n            ap += p\n        ap /= 11.0\n        aps.append(ap)\n\n    if not aps:\n        return None\n    return float(np.mean(aps))\n\n\n# --------------------------\n# Main Pipeline\n# --------------------------\ndef main():\n    # Setup: interpreter with EdgeTPU\n    interpreter = make_interpreter(MODEL_PATH, EDGETPU_LIB)\n    input_details = interpreter.get_input_details()[0]\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Video IO\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: Could not open input video: {INPUT_PATH}\")\n        return\n\n    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 30.0  # fallback\n\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (orig_w, orig_h))\n    if not writer.isOpened():\n        print(f\"Error: Could not open output video for writing: {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # Load optional ground-truth\n    ground_truths = load_ground_truth(GT_PATH)\n\n    # Storage for predictions to compute mAP over time\n    all_predictions = []\n\n    frame_index = 0\n    t0_total = time.time()\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Preprocessing\n            input_tensor = preprocess_frame(frame, input_details)\n\n            # Inference\n            interpreter.set_tensor(input_details['index'], input_tensor)\n            t0 = time.time()\n            interpreter.invoke()\n            infer_time_ms = (time.time() - t0) * 1000.0\n\n            # Postprocessing: extract detections\n            detections = extract_detections(interpreter, frame_w=orig_w, frame_h=orig_h,\n                                            confidence_threshold=CONFIDENCE_THRESHOLD)\n\n            # Accumulate predictions for mAP\n            for det in detections:\n                all_predictions.append({\n                    \"frame_index\": frame_index,\n                    \"class_id\": det[\"class_id\"],\n                    \"score\": det[\"score\"],\n                    \"bbox\": det[\"bbox\"]\n                })\n\n            # Compute mAP if GT is available\n            map_value = None\n            if ground_truths is not None:\n                # Use all frames up to current for progressive mAP\n                # Filter GTs up to current frame\n                partial_gt = {fi: anns for fi, anns in ground_truths.items() if fi <= frame_index}\n                map_value = compute_map(all_predictions, partial_gt, iou_thresh=0.5)\n\n            # Draw outputs\n            draw_detections(frame, detections, labels)\n\n            # Overlay performance and mAP\n            info_lines = []\n            info_lines.append(f\"Infer: {infer_time_ms:.1f} ms\")\n            if map_value is None:\n                info_lines.append(\"mAP: N/A (no ground truth)\")\n            else:\n                info_lines.append(f\"mAP@0.5: {map_value:.3f}\")\n\n            y0 = 24\n            for line in info_lines:\n                cv2.putText(frame, line, (10, y0), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2, cv2.LINE_AA)\n                y0 += 22\n\n            # Write to output\n            writer.write(frame)\n\n            frame_index += 1\n\n    finally:\n        cap.release()\n        writer.release()\n\n    total_time = time.time() - t0_total\n    print(f\"Processing completed. Frames: {frame_index}, Time: {total_time:.2f}s, FPS (overall): {frame_index / max(total_time, 1e-6):.2f}\")\n    print(f\"Saved output video to: {OUTPUT_PATH}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 10
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 74.941,
      "total_cost": 0.07622375,
      "observations": [
        {
          "id": "time-13-58-59-816039_chatcmpl-D6EmC7zXUzY1st4I0wsUSWlNlW1bR",
          "traceId": "6a960a1f",
          "type": "GENERATION",
          "name": "6a96_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T11:58:59.816000+00:00",
          "endTime": "2026-02-06T12:00:14.757000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07622375,
          "latency": 74941.0,
          "costDetails": {
            "total": 0.07622375
          },
          "totalTokens": 8017,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 7566,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:58:59.752Z",
      "environment": "default",
      "updatedAt": "2026-02-06T12:01:13.480Z"
    },
    {
      "id": "0729e094",
      "timestamp": "2026-02-06T11:56:21.507000+00:00",
      "name": "0729_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n\ndef load_labels(label_path):\n    labels = {}\n    if not os.path.exists(label_path):\n        return labels\n    with open(label_path, 'r') as f:\n        for i, line in enumerate(f.readlines()):\n            line = line.strip()\n            if not line:\n                continue\n            # Try common formats:\n            # 1) \"0 person\"\n            parts = line.split(maxsplit=1)\n            if len(parts) == 2 and parts[0].isdigit():\n                labels[int(parts[0])] = parts[1]\n                continue\n            # 2) \"id:name\" or \"id, name\"\n            sep_idx = None\n            for sep in [':', ',', ';']:\n                if sep in line:\n                    sep_idx = line.find(sep)\n                    break\n            if sep_idx is not None:\n                left = line[:sep_idx].strip()\n                right = line[sep_idx+1:].strip()\n                if left.isdigit():\n                    labels[int(left)] = right\n                    continue\n            # 3) Fallback: line number as id\n            labels[i] = line\n    return labels\n\n\ndef make_interpreter(model_path, edgetpu_lib_path):\n    delegates = []\n    if os.path.exists(edgetpu_lib_path):\n        try:\n            delegates.append(load_delegate(edgetpu_lib_path))\n        except Exception as e:\n            print(\"Warning: Failed to load EdgeTPU delegate:\", e)\n    else:\n        print(\"Warning: EdgeTPU shared library not found at:\", edgetpu_lib_path)\n    return Interpreter(model_path=model_path, experimental_delegates=delegates)\n\n\ndef preprocess_frame(frame_bgr, input_size, input_dtype):\n    ih, iw = input_size\n    # Convert BGR (OpenCV) to RGB (common for TFLite models)\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (iw, ih), interpolation=cv2.INTER_LINEAR)\n    if input_dtype == np.float32:\n        input_data = (resized.astype(np.float32) / 255.0).astype(np.float32)\n    else:\n        # Assume quantized uint8\n        input_data = resized.astype(np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n\ndef get_output_tensors(interpreter):\n    details = interpreter.get_output_details()\n    tensors = [interpreter.get_tensor(d['index']) for d in details]\n    # Heuristic to assign outputs: boxes, classes, scores, count\n    boxes = None\n    classes = None\n    scores = None\n    count = None\n    # Try common SSD order\n    if len(tensors) >= 3:\n        t0, t1, t2 = tensors[0], tensors[1], tensors[2]\n        # Typical: (1,N,4), (1,N), (1,N), (1,)\n        if t0.ndim == 3 and t0.shape[-1] == 4:\n            boxes = t0[0]\n            if t1.ndim >= 2:\n                classes = t1[0].astype(np.int32)\n            if t2.ndim >= 2:\n                scores = t2[0].astype(np.float32)\n            if len(tensors) >= 4 and tensors[3].size >= 1:\n                count = int(tensors[3].flatten()[0])\n    # Fallback scan\n    if boxes is None or classes is None or scores is None:\n        for t in tensors:\n            ts = t.squeeze()\n            if ts.ndim == 2 and ts.shape[-1] == 4 and boxes is None:\n                boxes = ts\n            elif ts.ndim == 1 and np.issubdtype(ts.dtype, np.floating) and scores is None:\n                scores = ts.astype(np.float32)\n            elif ts.ndim == 1 and np.issubdtype(ts.dtype, np.integer) and classes is None:\n                classes = ts.astype(np.int32)\n            elif ts.ndim == 0 and count is None:\n                count = int(ts)\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((boxes.shape[0],), dtype=np.int32)\n    if scores is None:\n        scores = np.zeros((boxes.shape[0],), dtype=np.float32)\n    if count is None:\n        count = min(len(scores), len(boxes))\n    return boxes, classes, scores, count\n\n\ndef detect_objects(interpreter, frame_bgr, threshold):\n    input_details = interpreter.get_input_details()[0]\n    _, ih, iw, _ = input_details['shape']\n    input_dtype = input_details['dtype']\n    input_data = preprocess_frame(frame_bgr, (ih, iw), input_dtype)\n\n    interpreter.set_tensor(input_details['index'], input_data)\n\n    t0 = time.time()\n    interpreter.invoke()\n    inference_ms = (time.time() - t0) * 1000.0\n\n    boxes, classes, scores, count = get_output_tensors(interpreter)\n    H, W = frame_bgr.shape[:2]\n    detections = []\n    for i in range(int(count)):\n        score = float(scores[i])\n        if score < threshold:\n            continue\n        y_min, x_min, y_max, x_max = boxes[i]\n        # Boxes are typically normalized [0,1]\n        x1 = max(0, min(W - 1, int(x_min * W)))\n        y1 = max(0, min(H - 1, int(y_min * H)))\n        x2 = max(0, min(W - 1, int(x_max * W)))\n        y2 = max(0, min(H - 1, int(y_max * H)))\n        # Ensure proper ordering\n        if x2 < x1:\n            x1, x2 = x2, x1\n        if y2 < y1:\n            y1, y2 = y2, y1\n        detections.append({\n            'bbox': (x1, y1, x2, y2),\n            'score': score,\n            'class_id': int(classes[i])\n        })\n    return detections, inference_ms\n\n\ndef draw_detections(frame_bgr, detections, labels, color_map):\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        class_id = det['class_id']\n        score = det['score']\n        color = color_map.get(class_id, (0, 255, 0))\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        label = labels.get(class_id, str(class_id))\n        caption = f\"{label}: {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame_bgr, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), color, -1)\n        cv2.putText(frame_bgr, caption, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n\ndef compute_iou(box_a, box_b):\n    ax1, ay1, ax2, ay2 = box_a\n    bx1, by1, bx2, by2 = box_b\n    inter_x1 = max(ax1, bx1)\n    inter_y1 = max(ay1, by1)\n    inter_x2 = min(ax2, bx2)\n    inter_y2 = min(ay2, by2)\n    inter_w = max(0, inter_x2 - inter_x1 + 1)\n    inter_h = max(0, inter_y2 - inter_y1 + 1)\n    inter_area = inter_w * inter_h\n    area_a = max(0, ax2 - ax1 + 1) * max(0, ay2 - ay1 + 1)\n    area_b = max(0, bx2 - bx1 + 1) * max(0, by2 - by1 + 1)\n    denom = area_a + area_b - inter_area\n    if denom <= 0:\n        return 0.0\n    return inter_area / denom\n\n\ndef compute_map(predictions_by_frame, ground_truth_by_frame, iou_thresh=0.5):\n    # predictions_by_frame: list indexed by frame_idx -> list of {'bbox','class_id','score'}\n    # ground_truth_by_frame: dict frame_idx -> list of {'bbox','class_id'}\n    # If no ground truth available, return None\n    if not ground_truth_by_frame:\n        return None\n\n    # Gather classes from GT\n    classes = set()\n    for gts in ground_truth_by_frame.values():\n        for gt in gts:\n            classes.add(gt['class_id'])\n    if not classes:\n        return None\n\n    ap_list = []\n    for cls in sorted(list(classes)):\n        # Collect predictions for this class\n        preds = []\n        gts_by_frame = {}\n        for fi, preds_list in enumerate(predictions_by_frame):\n            cls_preds = [p for p in preds_list if p['class_id'] == cls]\n            for p in cls_preds:\n                preds.append((fi, p['bbox'], p['score']))\n        # Sort predictions by score descending\n        preds.sort(key=lambda x: x[2], reverse=True)\n\n        # Collect GTs for this class\n        total_gts = 0\n        for fi, gts in ground_truth_by_frame.items():\n            cls_gts = [g for g in gts if g['class_id'] == cls]\n            total_gts += len(cls_gts)\n            if cls_gts:\n                gts_by_frame[fi] = {\n                    'boxes': [g['bbox'] for g in cls_gts],\n                    'matched': [False] * len(cls_gts)\n                }\n        if total_gts == 0:\n            # No GT for this class; skip in mAP calculation\n            continue\n\n        tp = np.zeros(len(preds), dtype=np.float32)\n        fp = np.zeros(len(preds), dtype=np.float32)\n        for i, (fi, pbox, _) in enumerate(preds):\n            if fi not in gts_by_frame:\n                fp[i] = 1.0\n                continue\n            gt_entry = gts_by_frame[fi]\n            ious = [compute_iou(pbox, gbox) for gbox in gt_entry['boxes']]\n            if not ious:\n                fp[i] = 1.0\n                continue\n            max_iou = max(ious)\n            max_idx = int(np.argmax(ious))\n            if max_iou >= iou_thresh and not gt_entry['matched'][max_idx]:\n                tp[i] = 1.0\n                gt_entry['matched'][max_idx] = True\n            else:\n                fp[i] = 1.0\n\n        cum_tp = np.cumsum(tp)\n        cum_fp = np.cumsum(fp)\n        recalls = cum_tp / (total_gts + 1e-8)\n        precisions = cum_tp / (cum_tp + cum_fp + 1e-8)\n\n        # 11-point interpolated AP\n        ap = 0.0\n        for t in np.linspace(0, 1, 11):\n            p = 0.0\n            if len(precisions) > 0:\n                p = np.max(precisions[recalls >= t]) if np.any(recalls >= t) else 0.0\n            ap += p / 11.0\n        ap_list.append(ap)\n\n    if not ap_list:\n        return None\n    return float(np.mean(ap_list))\n\n\ndef ensure_dir_exists(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\n\ndef main():\n    # CONFIGURATION PARAMETERS\n    model_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\n    label_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\n    input_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\n    output_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n    confidence_threshold = 0.5\n\n    # Step 1: Setup\n    labels = load_labels(label_path)\n    interpreter = make_interpreter(model_path, \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\")\n    interpreter.allocate_tensors()\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        print(\"Error: Unable to open input video:\", input_path)\n        return\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    ensure_dir_exists(output_path)\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    if not out.isOpened():\n        print(\"Error: Unable to open output video for writing:\", output_path)\n        cap.release()\n        return\n\n    # Deterministic color map per class id\n    rng = np.random.RandomState(42)\n    color_map = {}\n    for cid in range(0, 256):\n        color_map[cid] = tuple(int(c) for c in rng.randint(0, 255, size=3))\n\n    predictions_by_frame = []\n    # Optional: ground truth annotations not provided in configuration.\n    ground_truth_by_frame = {}  # frame_index -> list of {'bbox':(x1,y1,x2,y2), 'class_id':int}\n\n    frame_index = 0\n    avg_inference_ms = 0.0\n    t_start = time.time()\n\n    # Step 2-4: Process video frames\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        detections, infer_ms = detect_objects(interpreter, frame, confidence_threshold)\n        avg_inference_ms += infer_ms\n        predictions_by_frame.append(detections)\n\n        draw_detections(frame, detections, labels, color_map)\n\n        # Compute mAP if ground truth is available; else N/A\n        current_map = compute_map(predictions_by_frame, ground_truth_by_frame, iou_thresh=0.5)\n        map_text = \"mAP: N/A\" if current_map is None else f\"mAP: {current_map:.3f}\"\n\n        elapsed = time.time() - t_start\n        fps_running = (frame_index + 1) / elapsed if elapsed > 0 else 0.0\n        overlay = f\"Infer: {infer_ms:.1f} ms | FPS: {fps_running:.1f} | {map_text}\"\n        cv2.putText(frame, overlay, (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (10, 255, 10), 1, cv2.LINE_AA)\n\n        out.write(frame)\n        frame_index += 1\n\n    cap.release()\n    out.release()\n\n    if frame_index > 0:\n        avg_inference_ms /= frame_index\n    final_map = compute_map(predictions_by_frame, ground_truth_by_frame, iou_thresh=0.5)\n    print(\"Processing complete.\")\n    print(f\"Frames processed: {frame_index}\")\n    print(f\"Average inference time: {avg_inference_ms:.2f} ms\")\n    if final_map is None:\n        print(\"mAP: N/A (no ground truth provided)\")\n    else:\n        print(f\"mAP@0.5IoU: {final_map:.4f}\")\n    print(\"Saved output video to:\", output_path)\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 9
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 79.106,
      "total_cost": 0.07475375,
      "observations": [
        {
          "id": "time-13-56-22-195029_chatcmpl-D6EjfxQqtmiq5wZ4pn0b4FTEoHLbf",
          "traceId": "0729e094",
          "type": "GENERATION",
          "name": "0729_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T11:56:22.195000+00:00",
          "endTime": "2026-02-06T11:57:41.301000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07475375,
          "latency": 79106.0,
          "costDetails": {
            "total": 0.07475375
          },
          "totalTokens": 7870,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 7419,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:56:22.071Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:58:39.243Z"
    },
    {
      "id": "14b85500",
      "timestamp": "2026-02-06T11:53:29.585000+00:00",
      "name": "14b8_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# ----------------------------- Configuration -----------------------------\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\n\nEDGETPU_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n# ----------------------------- Utilities -----------------------------\ndef load_labels(path):\n    \"\"\"\n    Loads labels from a label map file. Supports:\n    - \"index label\" (space separated)\n    - \"index: label\" or \"index,label\"\n    - plain list (one label per line; index inferred)\n    \"\"\"\n    labels = {}\n    if not os.path.exists(path):\n        print(f\"Warning: Label file not found at {path}. Using empty labels.\")\n        return labels\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for i, line in enumerate(f):\n            line = line.strip()\n            if not line:\n                continue\n            # Try common formats\n            idx = None\n            name = None\n            # Format: \"0 label\" or \"0: label\" or \"0,label\"\n            for sep in [\" \", \":\", \",\", \"\\t\"]:\n                parts = line.split(sep, 1)\n                if len(parts) == 2 and parts[0].strip().isdigit():\n                    idx = int(parts[0].strip())\n                    name = parts[1].strip()\n                    break\n            # Fallback: plain list\n            if idx is None:\n                idx = i\n                name = line\n            labels[idx] = name\n    return labels\n\n\ndef make_interpreter(model_path, edgetpu_lib):\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate(edgetpu_lib)]\n        )\n    except ValueError as e:\n        raise RuntimeError(\n            f\"Failed to load the EdgeTPU delegate from {edgetpu_lib}. \"\n            f\"Ensure the library exists and the model is compiled for EdgeTPU. Error: {e}\"\n        )\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef preprocess(frame_bgr, input_shape, input_dtype):\n    \"\"\"\n    - Convert BGR to RGB\n    - Resize to model input size\n    - Normalize if needed for float models\n    - Add batch dimension\n    \"\"\"\n    h, w = input_shape[1], input_shape[2]\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (w, h), interpolation=cv2.INTER_LINEAR)\n    if input_dtype == np.float32:\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        input_data = resized.astype(input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n\ndef get_output_tensors(interpreter):\n    \"\"\"\n    Retrieve detection outputs from the interpreter in a model-agnostic way.\n    Returns:\n      boxes: (N, 4) in [ymin, xmin, ymax, xmax], normalized [0,1]\n      classes: (N,) int\n      scores: (N,) float32\n      count: int\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    tensors = [interpreter.get_tensor(od[\"index\"]) for od in output_details]\n\n    boxes = None\n    classes = None\n    scores = None\n    count = None\n\n    # Identify by shapes\n    # Typically:\n    # boxes: [1, N, 4] float32\n    # classes: [1, N] float32/int\n    # scores: [1, N] float32\n    # count: [1] float32/int\n    one_d_candidates = []\n    for t in tensors:\n        arr = np.squeeze(t)\n        if arr.ndim == 2 and arr.shape[1] == 4:\n            # Unlikely case, but handle\n            boxes = arr\n        elif arr.ndim == 2 and arr.shape[0] == 4:\n            boxes = arr.T\n        elif arr.ndim == 3 and arr.shape[-1] == 4:\n            boxes = arr[0]\n        elif arr.ndim == 2:\n            # Could be classes or scores\n            one_d_candidates.append(arr[0] if arr.shape[0] == 1 else arr[:, 0])\n        elif arr.ndim == 1 and arr.size > 1:\n            one_d_candidates.append(arr)\n        elif arr.ndim == 0 or (arr.ndim == 1 and arr.size == 1):\n            count = int(round(float(arr)))  # num_detections\n\n    # Distinguish scores vs classes among 1D arrays\n    # Scores are in [0,1], classes are integers (or floats close to ints)\n    s_candidate = None\n    c_candidate = None\n    for cand in one_d_candidates:\n        cmin, cmax = float(np.min(cand)), float(np.max(cand))\n        # Heuristic: scores in [0,1], classes > 1 often\n        if 0.0 <= cmin and cmax <= 1.0:\n            s_candidate = cand\n        else:\n            c_candidate = cand\n\n    if boxes is None:\n        # Some models output boxes as [1, N, 4] float32\n        for t in tensors:\n            arr = np.array(t)\n            if arr.ndim == 3 and arr.shape[-1] == 4:\n                boxes = arr[0]\n                break\n\n    if s_candidate is not None:\n        scores = s_candidate.astype(np.float32)\n    if c_candidate is not None:\n        classes = c_candidate.astype(np.int32)\n\n    # If count not provided, infer from scores or boxes\n    if count is None:\n        if scores is not None:\n            count = int(scores.shape[0])\n        elif boxes is not None:\n            count = int(boxes.shape[0])\n        elif classes is not None:\n            count = int(classes.shape[0])\n        else:\n            count = 0\n\n    # Ensure consistent lengths\n    if boxes is not None and boxes.shape[0] > count:\n        boxes = boxes[:count]\n    if classes is not None and classes.shape[0] > count:\n        classes = classes[:count]\n    if scores is not None and scores.shape[0] > count:\n        scores = scores[:count]\n\n    # Final fallbacks\n    if boxes is None:\n        boxes = np.zeros((0, 4), dtype=np.float32)\n    if classes is None:\n        classes = np.zeros((0,), dtype=np.int32)\n    if scores is None:\n        scores = np.zeros((0,), dtype=np.float32)\n\n    return boxes, classes, scores, count\n\n\ndef draw_detections(frame, boxes, classes, scores, labels, threshold, map_value=None):\n    h, w = frame.shape[:2]\n    for i in range(len(scores)):\n        score = float(scores[i])\n        if score < threshold:\n            continue\n        cls_id = int(classes[i]) if i < len(classes) else -1\n        label = labels.get(cls_id, str(cls_id))\n        ymin, xmin, ymax, xmax = boxes[i]\n        # Convert normalized to pixel coords\n        x1 = max(0, min(w - 1, int(xmin * w)))\n        y1 = max(0, min(h - 1, int(ymin * h)))\n        x2 = max(0, min(w - 1, int(xmax * w)))\n        y2 = max(0, min(h - 1, int(ymax * h)))\n\n        # Color derived from class id for consistency\n        color = tuple(int(c) for c in np.random.RandomState(cls_id).randint(0, 255, size=3))\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n\n        caption = f\"{label}: {score*100:.1f}%\"\n        (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, y1 - th - 6), (x1 + tw + 4, y1), color, -1)\n        cv2.putText(frame, caption, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Draw mAP (proxy) on the frame if provided\n    if map_value is not None:\n        text = f\"mAP: {map_value:.3f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n        cv2.rectangle(frame, (10, 10), (10 + tw + 10, 10 + th + 10), (0, 0, 0), -1)\n        cv2.putText(frame, text, (15, 10 + th + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2, cv2.LINE_AA)\n\n\nclass MAPTracker:\n    \"\"\"\n    A simple proxy metric to estimate mAP-like value without ground truth.\n    For each class, we track:\n      - TP: detections with score >= threshold\n      - FP: detections with score < threshold\n    AP per class is approximated as precision = TP / (TP + FP).\n    mAP is the mean of per-class AP over observed classes.\n    Note: This is NOT a replacement for true mAP which requires ground truth.\n    \"\"\"\n    def __init__(self, threshold=0.5):\n        self.threshold = float(threshold)\n        self.tp = {}   # class_id -> count\n        self.fp = {}   # class_id -> count\n\n    def update(self, classes, scores):\n        for cls_id, sc in zip(classes, scores):\n            cls = int(cls_id)\n            if sc >= self.threshold:\n                self.tp[cls] = self.tp.get(cls, 0) + 1\n            else:\n                self.fp[cls] = self.fp.get(cls, 0) + 1\n\n    def compute_map(self):\n        aps = []\n        all_classes = set(list(self.tp.keys()) + list(self.fp.keys()))\n        for cls in all_classes:\n            tp = self.tp.get(cls, 0)\n            fp = self.fp.get(cls, 0)\n            denom = tp + fp\n            if denom > 0:\n                aps.append(tp / denom)\n        if not aps:\n            return 0.0\n        return float(np.mean(aps))\n\n\ndef main():\n    # Prepare output directory\n    out_dir = os.path.dirname(OUTPUT_PATH)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize interpreter with EdgeTPU\n    interpreter = make_interpreter(MODEL_PATH, EDGETPU_LIB)\n    input_details = interpreter.get_input_details()\n    input_index = input_details[0][\"index\"]\n    input_shape = input_details[0][\"shape\"]  # [1, height, width, channels]\n    input_dtype = input_details[0][\"dtype\"]\n\n    # Open video input\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    # Prepare video writer\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if not fps or fps != fps or fps <= 0:\n        fps = 30.0\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    map_tracker = MAPTracker(threshold=CONFIDENCE_THRESHOLD)\n\n    frame_count = 0\n    inf_times = []\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_count += 1\n\n            # Preprocess\n            input_data = preprocess(frame, input_shape, input_dtype)\n\n            # Set input tensor\n            interpreter.set_tensor(input_index, input_data)\n\n            # Inference\n            t0 = time.time()\n            interpreter.invoke()\n            t1 = time.time()\n            inf_times.append(t1 - t0)\n\n            # Postprocess outputs\n            boxes, classes, scores, count = get_output_tensors(interpreter)\n\n            # Update proxy mAP tracker\n            if len(scores) > 0 and len(classes) == len(scores):\n                map_tracker.update(classes, scores)\n\n            # Compute current mAP (proxy)\n            current_map = map_tracker.compute_map()\n\n            # Draw detections and mAP\n            draw_detections(frame, boxes, classes, scores, labels, CONFIDENCE_THRESHOLD, map_value=current_map)\n\n            # Write frame\n            writer.write(frame)\n\n    finally:\n        cap.release()\n        writer.release()\n\n    # Report\n    avg_inf_ms = (np.mean(inf_times) * 1000.0) if inf_times else 0.0\n    final_map = map_tracker.compute_map()\n    print(\"Processing complete.\")\n    print(f\"Frames processed: {frame_count}\")\n    print(f\"Average inference time: {avg_inf_ms:.2f} ms\")\n    print(f\"Saved output video to: {OUTPUT_PATH}\")\n    print(f\"mAP (proxy without ground truth): {final_map:.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 8
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 91.092,
      "total_cost": 0.06205375,
      "observations": [
        {
          "id": "time-13-53-30-207280_chatcmpl-D6EgsLH5vPA2ilsYC4h6RKxWdDJc8",
          "traceId": "14b85500",
          "type": "GENERATION",
          "name": "14b8_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T11:53:30.207000+00:00",
          "endTime": "2026-02-06T11:55:01.299000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06205375,
          "latency": 91092.0,
          "costDetails": {
            "total": 0.06205375
          },
          "totalTokens": 6600,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 6149,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:53:30.146Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:56:00.874Z"
    },
    {
      "id": "01221cd8",
      "timestamp": "2026-02-06T11:46:19.599000+00:00",
      "name": "0122_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration Parameters\n# =========================\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"  # Read a single video file from the given input_path\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"  # Save processed video with boxes, labels, and mAP\nCONFIDENCE_THRESHOLD = 0.5\n\nEDGETPU_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\n# =========================\n# Utility: Labels\n# =========================\ndef load_labels(path):\n    # Attempts to parse both simple label files and pbtxt-style label maps.\n    labels = {}\n    if not os.path.exists(path):\n        return labels\n\n    with open(path, 'r', encoding='utf-8') as f:\n        lines = [ln.strip() for ln in f.readlines() if ln.strip()]\n\n    # Try pbtxt-style\n    if any(\"item\" in ln for ln in lines) and any(\"id:\" in ln for ln in lines) and any(\"name:\" in ln for ln in lines):\n        current_id = None\n        current_name = None\n        for ln in lines:\n            ln_low = ln.lower()\n            if \"id:\" in ln_low:\n                try:\n                    current_id = int(ln.split(\":\")[1].strip().strip('\"').strip(\"'\"))\n                except Exception:\n                    current_id = None\n            elif \"name:\" in ln_low:\n                # Extract the name value after ':', strip quotes\n                name_part = ln.split(\":\", 1)[1].strip()\n                current_name = name_part.strip('\"').strip(\"'\")\n            elif ln_low.startswith(\"item\"):\n                current_id = None\n                current_name = None\n\n            if current_id is not None and current_name is not None:\n                labels[current_id] = current_name\n                current_id = None\n                current_name = None\n\n        if labels:\n            return labels\n\n    # Fallback: simple formats like \"0 person\" or just \"person\" per line\n    tmp = {}\n    for idx, ln in enumerate(lines):\n        parts = ln.split()\n        if len(parts) >= 2 and parts[0].isdigit():\n            try:\n                tmp[int(parts[0])] = \" \".join(parts[1:])\n            except Exception:\n                tmp[idx] = ln\n        else:\n            tmp[idx] = ln\n    return tmp\n\n\n# =========================\n# TFLite + EdgeTPU\n# =========================\ndef make_interpreter(model_path):\n    interpreter = Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate(EDGETPU_LIB)]\n    )\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef get_input_details(interpreter):\n    details = interpreter.get_input_details()[0]\n    ih, iw = details[\"shape\"][1], details[\"shape\"][2]\n    dtype = details[\"dtype\"]\n    return iw, ih, dtype, details[\"index\"]\n\n\ndef set_input(interpreter, input_index, tensor):\n    interpreter.set_tensor(input_index, tensor)\n\n\n# =========================\n# Preprocessing\n# =========================\ndef preprocess_frame(frame_bgr, input_w, input_h, input_dtype):\n    # Convert BGR to RGB and resize to model input size\n    image_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(image_rgb, (input_w, input_h))\n\n    if input_dtype == np.uint8:\n        input_tensor = np.expand_dims(resized, axis=0).astype(np.uint8)\n    else:\n        # Normalize to [0,1] float32 if model expects float\n        input_tensor = np.expand_dims(resized.astype(np.float32) / 255.0, axis=0)\n    return input_tensor\n\n\n# =========================\n# Inference Output Parsing\n# =========================\ndef _squeeze_safe(arr):\n    try:\n        return np.squeeze(arr)\n    except Exception:\n        return arr\n\n\ndef _safe_get(arr, idx, default_val):\n    if arr is None:\n        return default_val\n    a = arr\n    if np.ndim(a) == 0:\n        try:\n            return a.item()\n        except Exception:\n            return default_val\n    length = a.shape[0]\n    if length == 0:\n        return default_val\n    if idx < length:\n        return a[idx]\n    return a[-1]\n\n\ndef parse_detections(interpreter, threshold, img_w, img_h):\n    # Robustly parse various TFLite detection head formats; avoid index errors\n    output_details = interpreter.get_output_details()\n    boxes = None\n    scores = None\n    classes = None\n    num_dets = None\n\n    # First pass: use names if present\n    for od in output_details:\n        name = od.get(\"name\", \"\").lower()\n        tensor = _squeeze_safe(interpreter.get_tensor(od[\"index\"]))\n        if \"boxes\" in name or \"box\" in name:\n            boxes = tensor\n        elif \"scores\" in name or \"score\" in name:\n            scores = tensor\n        elif \"classes\" in name or \"class\" in name:\n            classes = tensor\n        elif \"num\" in name and \"detection\" in name:\n            # num_detections might be float; cast later\n            num_dets = tensor\n\n    # Second pass: heuristic if any are missing\n    outs = [_squeeze_safe(interpreter.get_tensor(od[\"index\"])) for od in output_details]\n    if boxes is None:\n        cand = [o for o in outs if (o is not None and o.ndim >= 1 and (o.shape[-1] == 4))]\n        if cand:\n            boxes = cand[0]\n    if scores is None:\n        # scores often 1D and length matches boxes count\n        cand = [o for o in outs if (o is not None and o.ndim == 1 and o.dtype != np.object_)]\n        if cand:\n            # Prefer array that matches boxes length if possible\n            if boxes is not None and boxes.ndim >= 2:\n                blen = boxes.shape[0]\n                best = None\n                for c in cand:\n                    if c.shape[0] == blen:\n                        best = c\n                        break\n                scores = best if best is not None else cand[0]\n            else:\n                scores = cand[0]\n    if classes is None:\n        cand = [o for o in outs if (o is not None and o.ndim == 1 and (np.issubdtype(o.dtype, np.integer) or np.issubdtype(o.dtype, np.floating)))]\n        if cand:\n            # Try to match scores length\n            if scores is not None and scores.ndim == 1:\n                slen = scores.shape[0]\n                best = None\n                for c in cand:\n                    if c.shape[0] == slen:\n                        best = c\n                        break\n                classes = best if best is not None else cand[-1]\n            else:\n                classes = cand[-1]\n\n    # Normalize shapes: boxes -> (N,4), scores/classes -> (N,)\n    if boxes is not None:\n        if boxes.ndim == 1 and boxes.size == 4:\n            boxes = boxes.reshape((1, 4))\n        elif boxes.ndim > 2:\n            # If shape like (1, N, 4), squeeze first dim\n            boxes = np.reshape(boxes, (-1, 4))\n    if scores is not None and scores.ndim > 1:\n        scores = scores.reshape(-1)\n    if classes is not None and classes.ndim > 1:\n        classes = classes.reshape(-1)\n\n    # num_detections handling\n    if num_dets is not None:\n        try:\n            num_val = int(np.round(float(_squeeze_safe(num_dets))))\n        except Exception:\n            num_val = None\n    else:\n        num_val = None\n\n    # Decide number of detections robustly\n    candidates = []\n    if boxes is not None and boxes.ndim == 2:\n        candidates.append(boxes.shape[0])\n    if scores is not None and scores.ndim == 1:\n        candidates.append(scores.shape[0])\n    if classes is not None and classes.ndim == 1:\n        candidates.append(classes.shape[0])\n    if num_val is not None:\n        candidates.append(num_val)\n    n = min(candidates) if candidates else 0\n    if n <= 0:\n        return []\n\n    detections = []\n    # Determine if boxes are normalized (0..1) or absolute\n    def to_pixel_box(b):\n        y_min, x_min, y_max, x_max = float(b[0]), float(b[1]), float(b[2]), float(b[3])\n        if max(abs(y_min), abs(x_min), abs(y_max), abs(x_max)) <= 1.5:\n            # normalized\n            x1 = int(max(0, min(img_w - 1, x_min * img_w)))\n            y1 = int(max(0, min(img_h - 1, y_min * img_h)))\n            x2 = int(max(0, min(img_w - 1, x_max * img_w)))\n            y2 = int(max(0, min(img_h - 1, y_max * img_h)))\n        else:\n            x1 = int(max(0, min(img_w - 1, x_min)))\n            y1 = int(max(0, min(img_h - 1, y_min)))\n            x2 = int(max(0, min(img_w - 1, x_max)))\n            y2 = int(max(0, min(img_h - 1, y_max)))\n        # Ensure proper ordering\n        x1, x2 = sorted((x1, x2))\n        y1, y2 = sorted((y1, y2))\n        return x1, y1, x2, y2\n\n    for i in range(n):\n        score = float(_safe_get(scores, i, 0.0)) if scores is not None else 0.0\n        if score < threshold:\n            continue\n        # Use class id if available; handle short arrays safely\n        cls_val = _safe_get(classes, i, -1) if classes is not None else -1\n        try:\n            cls_id = int(round(float(cls_val)))\n        except Exception:\n            cls_id = -1\n\n        if boxes is None:\n            continue\n        if i < boxes.shape[0]:\n            box = boxes[i]\n        else:\n            box = boxes[-1]\n        x1, y1, x2, y2 = to_pixel_box(box)\n        if x2 <= x1 or y2 <= y1:\n            continue\n\n        detections.append({\n            \"bbox\": (x1, y1, x2, y2),\n            \"score\": score,\n            \"class_id\": cls_id\n        })\n\n    return detections\n\n\n# =========================\n# Drawing and Metrics\n# =========================\ndef draw_detections(frame_bgr, detections, labels, map_value=None):\n    for det in detections:\n        x1, y1, x2, y2 = det[\"bbox\"]\n        score = det[\"score\"]\n        cid = det[\"class_id\"]\n        label_text = labels.get(cid, f\"id:{cid}\") if labels else f\"id:{cid}\"\n        color = (0, 255, 0)\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        text = f\"{label_text} {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        ty = max(0, y1 - 8)\n        cv2.rectangle(frame_bgr, (x1, ty - th - 4), (x1 + tw + 2, ty + 2), (0, 0, 0), -1)\n        cv2.putText(frame_bgr, text, (x1 + 1, ty), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    if map_value is not None:\n        map_text = f\"mAP: {map_value:.3f}\"\n        cv2.putText(frame_bgr, map_text, (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (60, 220, 220), 2, cv2.LINE_AA)\n\n\ndef update_map_stats(stats, detections):\n    # Stats: dict[class_id] -> list of detection confidences\n    for det in detections:\n        cid = det[\"class_id\"]\n        sc = float(det[\"score\"])\n        if cid not in stats:\n            stats[cid] = []\n        stats[cid].append(sc)\n\n\ndef compute_naive_map(stats):\n    # NOTE: Without ground truth, true mAP cannot be computed.\n    # Here we use a naive proxy: AP for a class = mean(confidences of its detections).\n    # mAP = mean of per-class AP for classes that had any detections.\n    if not stats:\n        return 0.0\n    aps = []\n    for cid, scores in stats.items():\n        if len(scores) == 0:\n            continue\n        aps.append(float(np.mean(scores)))\n    if not aps:\n        return 0.0\n    return float(np.mean(aps))\n\n\n# =========================\n# Main Pipeline\n# =========================\ndef main():\n    # Prepare output directory\n    out_dir = os.path.dirname(OUTPUT_PATH)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Set up TFLite EdgeTPU interpreter\n    interpreter = make_interpreter(MODEL_PATH)\n    in_w, in_h, in_dtype, in_index = get_input_details(interpreter)\n\n    # Open video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"Error: cannot open input video: {INPUT_PATH}\")\n        return\n\n    # Prepare writer\n    in_fps = cap.get(cv2.CAP_PROP_FPS)\n    if not in_fps or in_fps <= 0 or np.isnan(in_fps):\n        in_fps = 30.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    if frame_w <= 0 or frame_h <= 0:\n        # Fallback if metadata not available\n        ret, test_frame = cap.read()\n        if not ret:\n            print(\"Error: unable to read frames from input video.\")\n            cap.release()\n            return\n        frame_h, frame_w = test_frame.shape[:2]\n        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        print(f\"Error: cannot open video writer for: {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # Metrics\n    map_stats = {}\n    last_log = time.time()\n    frame_count = 0\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            frame_count += 1\n            input_tensor = preprocess_frame(frame, in_w, in_h, in_dtype)\n            set_input(interpreter, in_index, input_tensor)\n\n            # Inference\n            interpreter.invoke()\n\n            # Parse detections robustly (avoid index errors as seen previously)\n            detections = parse_detections(interpreter, CONFIDENCE_THRESHOLD, frame_w, frame_h)\n\n            # Update metrics (naive proxy since ground truth is unavailable)\n            update_map_stats(map_stats, detections)\n            curr_map = compute_naive_map(map_stats)\n\n            # Draw results and mAP on frame\n            draw_detections(frame, detections, labels, map_value=curr_map)\n\n            # Write to output\n            writer.write(frame)\n\n            # Optional simple logging every few seconds\n            if time.time() - last_log > 5.0:\n                print(f\"Processed {frame_count} frames; current naive mAP={curr_map:.3f}\")\n                last_log = time.time()\n    finally:\n        cap.release()\n        writer.release()\n\n    print(f\"Done. Output saved to: {OUTPUT_PATH}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 7
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 350.327,
      "total_cost": 0.1951675,
      "observations": [
        {
          "id": "time-13-46-20-192077_chatcmpl-D6EZw46RakggPfCn44nYVMbDCP7I8",
          "traceId": "01221cd8",
          "type": "GENERATION",
          "name": "0122_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T11:46:20.192000+00:00",
          "endTime": "2026-02-06T11:48:07.020000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06729375,
          "latency": 106828.0,
          "costDetails": {
            "total": 0.06729375
          },
          "totalTokens": 7124,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 6673,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-48-11-958181_chatcmpl-D6Ebk7ofmn5hHutMGavqsuS61uf0O",
          "traceId": "01221cd8",
          "type": "GENERATION",
          "name": "0122_tpusg_gen_attempt#2",
          "startTime": "2026-02-06T11:48:11.958000+00:00",
          "endTime": "2026-02-06T11:50:03.257000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05602,
          "latency": 111299.0,
          "costDetails": {
            "total": 0.05602
          },
          "totalTokens": 6064,
          "environment": "default",
          "promptTokens": 528,
          "completionTokens": 5536,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        },
        {
          "id": "time-13-50-12-367537_chatcmpl-D6EdgIgornSKeS7LUkyw72pKIwDny",
          "traceId": "01221cd8",
          "type": "GENERATION",
          "name": "0122_tpusg_gen_attempt#3",
          "startTime": "2026-02-06T11:50:12.367000+00:00",
          "endTime": "2026-02-06T11:52:10.519000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07185375,
          "latency": 118152.0,
          "costDetails": {
            "total": 0.07185375
          },
          "totalTokens": 7741,
          "environment": "default",
          "promptTokens": 635,
          "completionTokens": 7106,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:46:20.377Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:53:08.774Z"
    },
    {
      "id": "29759cf7",
      "timestamp": "2026-02-06T11:43:15.803000+00:00",
      "name": "2975_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# =========================\n# Configuration Parameters\n# =========================\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# EdgeTPU shared library path for Coral Dev Board\nedgetpu_shared_lib = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\n# =========================\n# Utility Functions\n# =========================\ndef load_labels(path):\n    \"\"\"\n    Load labels from a label map file.\n    Supports formats:\n      - \"0 person\"\n      - \"person\" (index auto-assigned)\n      - Ignores empty lines and lines starting with '#'\n    Returns: dict[int, str]\n    \"\"\"\n    labels = {}\n    try:\n        with open(path, 'r') as f:\n            idx_auto = 0\n            for line in f:\n                line = line.strip()\n                if not line or line.startswith('#'):\n                    continue\n                parts = line.split(maxsplit=1)\n                if len(parts) == 2 and parts[0].isdigit():\n                    idx = int(parts[0])\n                    name = parts[1].strip()\n                else:\n                    idx = idx_auto\n                    name = line\n                    idx_auto += 1\n                labels[idx] = name\n    except Exception as e:\n        print(f\"Warning: Failed to load labels from {path}: {e}\")\n    return labels\n\n\ndef make_interpreter(model_path, delegate_path):\n    \"\"\"\n    Create a TFLite interpreter with EdgeTPU delegate.\n    \"\"\"\n    try:\n        interpreter = Interpreter(\n            model_path=model_path,\n            experimental_delegates=[load_delegate(delegate_path)]\n        )\n    except ValueError as e:\n        raise RuntimeError(f\"Failed to load the EdgeTPU delegate from {delegate_path}: {e}\")\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef preprocess_frame(frame_bgr, input_h, input_w, input_dtype, input_quant):\n    \"\"\"\n    Preprocess frame:\n      - Convert BGR to RGB\n      - Resize to model input size\n      - Quantize/normalize according to input dtype\n    Returns: np.ndarray with shape [1, H, W, 3] and proper dtype\n    \"\"\"\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (input_w, input_h), interpolation=cv2.INTER_LINEAR)\n\n    if input_dtype == np.uint8:\n        input_data = resized.astype(np.uint8)\n    elif input_dtype == np.float32:\n        input_data = (resized.astype(np.float32) / 255.0).astype(np.float32)\n    else:\n        # Handle other quantized types (e.g., int8) via provided quantization params if available\n        scale, zero_point = input_quant if input_quant is not None else (0.0, 0)\n        if scale and scale > 0:\n            # Assume real-value input in [0,1]; normalize then quantize\n            norm = resized.astype(np.float32) / 255.0\n            quantized = norm / scale + zero_point\n            qmin, qmax = (np.iinfo(input_dtype).min, np.iinfo(input_dtype).max)\n            input_data = np.clip(np.round(quantized), qmin, qmax).astype(input_dtype)\n        else:\n            # Fallback: direct cast\n            input_data = resized.astype(input_dtype)\n\n    # Add batch dimension\n    return np.expand_dims(input_data, axis=0)\n\n\ndef parse_detections(interpreter, output_details, frame_w, frame_h, score_threshold):\n    \"\"\"\n    Parse detections from interpreter outputs. Assumes standard TFLite detection output:\n      - boxes: [1, N, 4] (ymin, xmin, ymax, xmax) normalized\n      - classes: [1, N]\n      - scores: [1, N]\n      - num_detections: [1]\n    Returns: list of dict with keys: bbox (x1,y1,x2,y2), class_id, score\n    \"\"\"\n    # Standard order in most TFLite detection models\n    boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n    classes = interpreter.get_tensor(output_details[1]['index'])[0].astype(np.int32)\n    scores = interpreter.get_tensor(output_details[2]['index'])[0]\n    num = int(interpreter.get_tensor(output_details[3]['index'])[0]) if len(output_details) > 3 else len(scores)\n\n    detections = []\n    for i in range(num):\n        score = float(scores[i])\n        if score < score_threshold:\n            continue\n        y_min, x_min, y_max, x_max = boxes[i]\n        # Convert to absolute coordinates\n        x1 = int(max(0, min(frame_w - 1, x_min * frame_w)))\n        y1 = int(max(0, min(frame_h - 1, y_min * frame_h)))\n        x2 = int(max(0, min(frame_w - 1, x_max * frame_w)))\n        y2 = int(max(0, min(frame_h - 1, y_max * frame_h)))\n        detections.append({\n            \"bbox\": (x1, y1, x2, y2),\n            \"class_id\": int(classes[i]),\n            \"score\": score\n        })\n    return detections\n\n\ndef draw_detections(frame_bgr, detections, labels, map_value=None):\n    \"\"\"\n    Draw bounding boxes and labels on the frame.\n    Optionally overlay running mAP value.\n    \"\"\"\n    for det in detections:\n        x1, y1, x2, y2 = det[\"bbox\"]\n        cid = det[\"class_id\"]\n        score = det[\"score\"]\n        label = labels.get(cid, str(cid))\n        # Deterministic color per class\n        color = (int(37 * (cid + 1) % 255), int(17 * (cid + 1) % 255), int(29 * (cid + 1) % 255))\n\n        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), color, 2)\n        caption = f\"{label}: {score:.2f}\"\n        (tw, th), bl = cv2.getTextSize(caption, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame_bgr, (x1, max(0, y1 - th - 8)), (x1 + tw + 6, y1), color, -1)\n        cv2.putText(frame_bgr, caption, (x1 + 3, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)\n\n    # Overlay running mAP value if provided\n    if map_value is not None:\n        text = f\"mAP: {map_value:.3f}\"\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n        cv2.rectangle(frame_bgr, (8, 8), (8 + tw + 12, 8 + th + 14), (0, 0, 0), -1)\n        cv2.putText(frame_bgr, text, (14, 8 + th + 3), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n\n    return frame_bgr\n\n\ndef compute_running_map(confidences_by_class):\n    \"\"\"\n    Compute a running mAP proxy over seen detections.\n    Since no ground truth is provided, we approximate AP per class as\n    the mean confidence of detections for that class, and mAP as the mean\n    of these per-class means.\n    Returns: float in [0,1] (approximation).\n    \"\"\"\n    per_class_avgs = []\n    for _, scores in confidences_by_class.items():\n        if scores:\n            per_class_avgs.append(float(np.mean(scores)))\n    if not per_class_avgs:\n        return 0.0\n    return float(np.mean(per_class_avgs))\n\n\n# =========================\n# Main Processing Pipeline\n# =========================\ndef main():\n    # Ensure output directory exists\n    out_dir = os.path.dirname(output_path)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(label_path)\n\n    # Initialize TFLite interpreter with EdgeTPU\n    interpreter = make_interpreter(model_path, edgetpu_shared_lib)\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()\n    in_h, in_w = int(input_details['shape'][1]), int(input_details['shape'][2])\n    in_dtype = input_details['dtype']\n    in_quant = input_details.get('quantization', (0.0, 0)) if 'quantization' in input_details else (0.0, 0)\n\n    # Video IO setup\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0  # Fallback FPS\n\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {output_path}\")\n\n    # For running mAP approximation\n    confidences_by_class = {}\n    frame_idx = 0\n    t0 = time.time()\n    last_log = t0\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break  # End of video\n\n            # Preprocess\n            input_tensor = preprocess_frame(frame, in_h, in_w, in_dtype, in_quant)\n            interpreter.set_tensor(input_details['index'], input_tensor)\n\n            # Inference\n            inf_start = time.time()\n            interpreter.invoke()\n            inf_end = time.time()\n\n            # Parse detections\n            detections = parse_detections(interpreter, output_details, frame_w, frame_h, confidence_threshold)\n\n            # Update confidences for mAP approximation\n            for det in detections:\n                cls_id = det[\"class_id\"]\n                score = det[\"score\"]\n                if cls_id not in confidences_by_class:\n                    confidences_by_class[cls_id] = []\n                confidences_by_class[cls_id].append(score)\n\n            # Compute running mAP\n            running_map = compute_running_map(confidences_by_class)\n\n            # Draw and write frame\n            annotated = draw_detections(frame.copy(), detections, labels, map_value=running_map)\n            writer.write(annotated)\n\n            frame_idx += 1\n\n            # Optional progress logging every few seconds\n            now = time.time()\n            if now - last_log > 5.0:\n                rt_fps = frame_idx / (now - t0 + 1e-9)\n                print(f\"[Progress] Frames processed: {frame_idx}, Avg FPS: {rt_fps:.2f}, Running mAP: {running_map:.3f}\")\n                last_log = now\n\n    finally:\n        cap.release()\n        writer.release()\n\n    final_map = compute_running_map(confidences_by_class)\n    print(f\"Processing complete.\")\n    print(f\"Output saved to: {output_path}\")\n    print(f\"Approximate mAP over video: {final_map:.4f}\")\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 6
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 104.553,
      "total_cost": 0.07515375,
      "observations": [
        {
          "id": "time-13-43-16-443038_chatcmpl-D6EWyQmpAradpaloex6bxb6cNDwIg",
          "traceId": "29759cf7",
          "type": "GENERATION",
          "name": "2975_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T11:43:16.443000+00:00",
          "endTime": "2026-02-06T11:45:00.996000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.07515375,
          "latency": 104553.0,
          "costDetails": {
            "total": 0.07515375
          },
          "totalTokens": 7910,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 7459,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:43:16.362Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:45:59.039Z"
    },
    {
      "id": "01a7a20c",
      "timestamp": "2026-02-06T11:39:39.690000+00:00",
      "name": "01a7_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\n# TFLite runtime for EdgeTPU\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# ==========================\n# Configuration Parameters\n# ==========================\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\n\nINPUT_DESCRIPTION = \"Read a single video file from the given input_path\"\nOUTPUT_DESCRIPTION = \"Output the video file with rectangles drew on the detected objects, along with texts of labels and calculated mAP(mean average precision)\"\n\nEDGETPU_SHARED_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n# ==========================\n# Utilities\n# ==========================\ndef load_labels(label_path):\n    labels = {}\n    try:\n        with open(label_path, \"r\") as f:\n            for idx, line in enumerate(f):\n                line = line.strip()\n                if not line:\n                    continue\n                labels[idx] = line\n    except Exception:\n        # If label file cannot be read, fallback to empty labels\n        labels = {}\n    return labels\n\ndef make_interpreter(model_path, edgetpu_lib):\n    delegate = load_delegate(edgetpu_lib)\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate])\n    interpreter.allocate_tensors()\n    return interpreter\n\ndef get_input_size(interpreter):\n    input_details = interpreter.get_input_details()[0]\n    _, height, width, channels = input_details['shape']\n    return width, height, channels, input_details['dtype']\n\ndef set_input_tensor(interpreter, data):\n    input_index = interpreter.get_input_details()[0]['index']\n    interpreter.set_tensor(input_index, data)\n\ndef preprocess_frame(frame, in_w, in_h, in_dtype):\n    # Convert BGR to RGB and resize to model input size\n    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (in_w, in_h), interpolation=cv2.INTER_LINEAR)\n    if in_dtype == np.uint8:\n        input_data = np.expand_dims(resized, axis=0).astype(np.uint8)\n    else:\n        # Fallback for float models (scale to [0,1])\n        input_data = np.expand_dims(resized.astype(np.float32) / 255.0, axis=0)\n    return input_data\n\ndef _dequantize(arr, detail):\n    # Dequantize if needed\n    if 'quantization' in detail:\n        scale, zero_point = detail['quantization']\n        if scale and scale > 0:\n            return scale * (arr.astype(np.float32) - zero_point)\n    return arr\n\ndef extract_detections(interpreter, frame_w, frame_h):\n    \"\"\"\n    Returns list of detections: [{'box':(x1,y1,x2,y2), 'score':float, 'class_id':int}]\n    \"\"\"\n    output_details = interpreter.get_output_details()\n    outputs = [interpreter.get_tensor(d['index']) for d in output_details]\n\n    # Try to identify outputs by name\n    boxes = classes = scores = num = None\n    for d, arr in zip(output_details, outputs):\n        name = d.get('name', '')\n        if isinstance(name, bytes):\n            name = name.decode('ascii', errors='ignore')\n        arr = _dequantize(arr, d)\n        if 'box' in name or 'boxes' in name:\n            boxes = arr[0]\n        elif 'score' in name or 'scores' in name:\n            scores = arr[0]\n        elif 'class' in name or 'classes' in name:\n            classes = arr[0]\n        elif 'num' in name:\n            try:\n                num = int(np.squeeze(arr))\n            except Exception:\n                num = None\n\n    # Fallback heuristic if names were not helpful\n    if boxes is None or classes is None or scores is None:\n        # Identify boxes by shape (1, N, 4)\n        for d, arr in zip(output_details, outputs):\n            arr = _dequantize(arr, d)\n            if arr.ndim == 3 and arr.shape[0] == 1 and arr.shape[2] == 4:\n                boxes = arr[0]\n        # Identify scores and classes among (1, N) tensors\n        one_n_tensors = []\n        for d, arr in zip(output_details, outputs):\n            arr = _dequantize(arr, d)\n            if arr.ndim == 2 and arr.shape[0] == 1:\n                one_n_tensors.append(arr[0])\n        if len(one_n_tensors) >= 2:\n            # Choose the array with values mostly in [0,1] as scores\n            scores_candidate, classes_candidate = None, None\n            if np.mean((one_n_tensors[0] >= 0) & (one_n_tensors[0] <= 1)) > 0.8:\n                scores_candidate = one_n_tensors[0]\n                classes_candidate = one_n_tensors[1]\n            elif np.mean((one_n_tensors[1] >= 0) & (one_n_tensors[1] <= 1)) > 0.8:\n                scores_candidate = one_n_tensors[1]\n                classes_candidate = one_n_tensors[0]\n            else:\n                # Default order\n                scores_candidate = one_n_tensors[0]\n                classes_candidate = one_n_tensors[1]\n            scores = scores_candidate\n            classes = classes_candidate\n\n    if boxes is None or classes is None or scores is None:\n        return []\n\n    if num is None:\n        num = min(len(scores), len(classes), len(boxes))\n\n    detections = []\n    for i in range(int(num)):\n        score = float(scores[i])\n        cls = int(classes[i])\n        y_min, x_min, y_max, x_max = boxes[i]\n        # Convert normalized coords to absolute pixel coords\n        x1 = int(max(0, min(1, x_min)) * frame_w)\n        y1 = int(max(0, min(1, y_min)) * frame_h)\n        x2 = int(max(0, min(1, x_max)) * frame_w)\n        y2 = int(max(0, min(1, y_max)) * frame_h)\n        # Ensure proper ordering\n        x1, x2 = sorted((x1, x2))\n        y1, y2 = sorted((y1, y2))\n        # Skip invalid boxes\n        if x2 <= x1 or y2 <= y1:\n            continue\n        detections.append({'box': (x1, y1, x2, y2), 'score': score, 'class_id': cls})\n    return detections\n\ndef iou(boxA, boxB):\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    inter_w = max(0, xB - xA)\n    inter_h = max(0, yB - yA)\n    inter_area = inter_w * inter_h\n    if inter_area <= 0:\n        return 0.0\n    boxA_area = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n    boxB_area = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n    denom = float(boxA_area + boxB_area - inter_area)\n    if denom <= 0:\n        return 0.0\n    return inter_area / denom\n\ndef nms_per_class(dets, iou_thresh=0.5):\n    # Group by class\n    dets_by_cls = {}\n    for d in dets:\n        dets_by_cls.setdefault(d['class_id'], []).append(d)\n\n    kept = []\n    for cls, items in dets_by_cls.items():\n        # Sort by score descending\n        items_sorted = sorted(items, key=lambda x: x['score'], reverse=True)\n        selected = []\n        while items_sorted:\n            current = items_sorted.pop(0)\n            selected.append(current)\n            remaining = []\n            for other in items_sorted:\n                if iou(current['box'], other['box']) <= iou_thresh:\n                    remaining.append(other)\n            items_sorted = remaining\n        kept.extend(selected)\n    return kept\n\n# ==========================\n# Proxy mAP computation (temporal consistency-based)\n# ==========================\nclass TemporalMAP:\n    def __init__(self, thresholds=None, iou_threshold=0.5):\n        # thresholds for confidence\n        if thresholds is None:\n            self.thresholds = [round(0.5 + 0.05 * i, 2) for i in range(10)]  # 0.50..0.95\n        else:\n            self.thresholds = thresholds\n        self.iou_threshold = iou_threshold\n        # metrics[t][class_id] = {'TP': int, 'FP': int, 'FN': int}\n        self.metrics = {t: {} for t in self.thresholds}\n        self.prev_dets = []  # list of {'box','score','class_id'}\n\n    def update(self, curr_dets):\n        # Use NMS-normalized detections as input to metrics\n        prev = self.prev_dets\n        self.prev_dets = curr_dets\n\n        if not prev:\n            return  # no updates on first frame\n\n        for t in self.thresholds:\n            # Organize by class for both prev (as proxy GT) and current predictions\n            prev_by_c = {}\n            curr_by_c = {}\n            for d in prev:\n                if d['score'] >= t:\n                    prev_by_c.setdefault(d['class_id'], []).append(d['box'])\n            for d in curr_dets:\n                if d['score'] >= t:\n                    curr_by_c.setdefault(d['class_id'], []).append((d['box'], d['score']))\n\n            all_classes = set(prev_by_c.keys()) | set(curr_by_c.keys())\n            for cls in all_classes:\n                gt_boxes = prev_by_c.get(cls, [])\n                preds = curr_by_c.get(cls, [])\n                preds = sorted(preds, key=lambda x: x[1], reverse=True)  # sort by score\n                used_gt = set()\n                TP = 0\n                FP = 0\n                for pred_box, _ in preds:\n                    match_idx = -1\n                    best_iou = 0.0\n                    for gi, gt_box in enumerate(gt_boxes):\n                        if gi in used_gt:\n                            continue\n                        i = iou(pred_box, gt_box)\n                        if i > best_iou:\n                            best_iou = i\n                            match_idx = gi\n                    if best_iou >= self.iou_threshold and match_idx >= 0:\n                        TP += 1\n                        used_gt.add(match_idx)\n                    else:\n                        FP += 1\n                FN = len(gt_boxes) - len(used_gt)\n\n                stats = self.metrics[t].setdefault(cls, {'TP': 0, 'FP': 0, 'FN': 0})\n                stats['TP'] += TP\n                stats['FP'] += FP\n                stats['FN'] += FN\n\n    def compute_map(self):\n        # Compute AP per class as mean precision across thresholds; mAP is mean over classes\n        class_aps = []\n        for cls in self._all_classes_seen():\n            precisions = []\n            for t in self.thresholds:\n                stats = self.metrics[t].get(cls, {'TP': 0, 'FP': 0, 'FN': 0})\n                tp = stats['TP']\n                fp = stats['FP']\n                fn = stats['FN']\n                denom = tp + fp\n                if denom > 0:\n                    precision = tp / float(denom)\n                    precisions.append(precision)\n            if precisions:\n                ap = float(np.mean(precisions))\n                class_aps.append(ap)\n        if class_aps:\n            return float(np.mean(class_aps))\n        return 0.0\n\n    def _all_classes_seen(self):\n        classes = set()\n        for t in self.thresholds:\n            classes.update(self.metrics[t].keys())\n        return classes\n\n# ==========================\n# Drawing\n# ==========================\ndef draw_detections(frame, dets, labels, map_value):\n    for d in dets:\n        if d['score'] < CONFIDENCE_THRESHOLD:\n            continue\n        x1, y1, x2, y2 = d['box']\n        cls = d['class_id']\n        score = d['score']\n        color = (0, 200, 0)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        label_text = labels.get(cls, f\"ID:{cls}\")\n        text = f\"{label_text} {score:.2f}\"\n        (tw, th), baseline = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (x1, max(0, y1 - th - baseline - 2)), (x1 + tw + 2, y1), color, -1)\n        cv2.putText(frame, text, (x1 + 1, y1 - 3), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n\n    # Draw mAP\n    map_text = f\"mAP: {map_value:.3f}\"\n    (tw, th), baseline = cv2.getTextSize(map_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)\n    cv2.rectangle(frame, (5, 5), (5 + tw + 10, 5 + th + baseline + 10), (50, 50, 50), -1)\n    cv2.putText(frame, map_text, (10, 10 + th), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n    return frame\n\n# ==========================\n# Main\n# ==========================\ndef main():\n    print(\"Application: TFLite object detection with TPU\")\n    print(\"Deployment Target: Google Coral Dev Board\")\n    print(f\"Input: {INPUT_PATH}\")\n    print(f\" - {INPUT_DESCRIPTION}\")\n    print(f\"Output: {OUTPUT_PATH}\")\n    print(f\" - {OUTPUT_DESCRIPTION}\")\n    print(f\"Model: {MODEL_PATH}\")\n    print(f\"Labels: {LABEL_PATH}\")\n    print(f\"Confidence threshold: {CONFIDENCE_THRESHOLD}\")\n\n    # Prepare output directory\n    out_dir = os.path.dirname(OUTPUT_PATH)\n    if out_dir and not os.path.exists(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize interpreter with EdgeTPU\n    interpreter = make_interpreter(MODEL_PATH, EDGETPU_SHARED_LIB)\n    in_w, in_h, in_ch, in_dtype = get_input_size(interpreter)\n\n    # Open video\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        print(f\"ERROR: Cannot open input video: {INPUT_PATH}\")\n        return\n\n    # Get video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    out_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    out_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Output writer\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (out_w, out_h))\n    if not writer.isOpened():\n        print(f\"ERROR: Cannot open output video for writing: {OUTPUT_PATH}\")\n        cap.release()\n        return\n\n    # Metrics\n    temporal_map = TemporalMAP()\n    frame_count = 0\n    t0 = time.time()\n    last_map = 0.0\n\n    try:\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame_count += 1\n\n            # Preprocess\n            input_data = preprocess_frame(frame, in_w, in_h, in_dtype)\n            set_input_tensor(interpreter, input_data)\n\n            # Inference\n            start_inf = time.time()\n            interpreter.invoke()\n            inf_time = (time.time() - start_inf) * 1000.0  # ms\n\n            # Postprocess: extract detections\n            detections = extract_detections(interpreter, frame_w=out_w, frame_h=out_h)\n\n            # NMS per class\n            detections_nms = nms_per_class(detections, iou_thresh=0.5)\n\n            # Update temporal mAP (proxy using previous frame as reference)\n            temporal_map.update(detections_nms)\n            last_map = temporal_map.compute_map()\n\n            # Draw and write frame\n            annotated = draw_detections(frame.copy(), detections_nms, labels, last_map)\n            # Optionally draw inference time\n            inf_text = f\"Inference: {inf_time:.1f} ms\"\n            cv2.putText(annotated, inf_text, (10, out_h - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (30, 255, 30), 1, cv2.LINE_AA)\n\n            writer.write(annotated)\n\n            # Optional: print progress every 30 frames\n            if frame_count % 30 == 0:\n                elapsed = time.time() - t0\n                fps_run = frame_count / max(1e-6, elapsed)\n                print(f\"[{frame_count} frames] {fps_run:.2f} FPS, current mAP: {last_map:.3f}\")\n\n    finally:\n        cap.release()\n        writer.release()\n\n    total_time = time.time() - t0\n    overall_fps = frame_count / max(1e-6, total_time)\n    final_map = temporal_map.compute_map()\n    print(f\"Processing complete. Frames: {frame_count}, Time: {total_time:.2f}s, Avg FPS: {overall_fps:.2f}\")\n    print(f\"Final mAP: {final_map:.3f}\")\n    print(f\"Saved annotated video to: {OUTPUT_PATH}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 5
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 136.601,
      "total_cost": 0.09309375,
      "observations": [
        {
          "id": "time-13-39-40-345631_chatcmpl-D6ETUWByEHN0wAOJzFuwdoIMCoFfp",
          "traceId": "01a7a20c",
          "type": "GENERATION",
          "name": "01a7_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T11:39:40.345000+00:00",
          "endTime": "2026-02-06T11:41:56.946000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.09309375,
          "latency": 136601.0,
          "costDetails": {
            "total": 0.09309375
          },
          "totalTokens": 9704,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 9253,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:39:40.256Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:42:55.471Z"
    },
    {
      "id": "5ec23437",
      "timestamp": "2026-02-06T11:37:02.807000+00:00",
      "name": "5ec2_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\n\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\nEDGETPU_SHARED_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n\ndef load_labels(path):\n    labels = {}\n    if not os.path.exists(path):\n        return labels\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for i, line in enumerate(f.readlines()):\n            line = line.strip()\n            if not line:\n                continue\n            # Supported formats:\n            #  - \"0: person\"\n            #  - \"0 person\"\n            #  - \"person\" (index inferred by line number)\n            #  - \"item { id: 1 name: 'person' }\" (best effort)\n            if \"item\" in line and \"id:\" in line and \"name:\" in line:\n                # Best-effort parsing of TF object_detection labelmap.pbtxt line (single-line)\n                try:\n                    parts = line.replace(\"{\", \" \").replace(\"}\", \" \").replace('\"', \"'\").split()\n                    id_idx = parts.index(\"id:\") + 1\n                    name_idx = parts.index(\"name:\") + 1\n                    cls_id = int(parts[id_idx])\n                    name = parts[name_idx].strip(\"'\").strip('\"')\n                    labels[cls_id] = name\n                except Exception:\n                    continue\n            else:\n                if \":\" in line:\n                    left, right = line.split(\":\", 1)\n                    try:\n                        cls_id = int(left.strip())\n                        labels[cls_id] = right.strip()\n                        continue\n                    except ValueError:\n                        pass\n                parts = line.split()\n                if len(parts) >= 2 and parts[0].isdigit():\n                    cls_id = int(parts[0])\n                    labels[cls_id] = \" \".join(parts[1:]).strip()\n                else:\n                    # Fallback: index by line number if no explicit id found\n                    labels[i] = line\n    return labels\n\n\ndef make_interpreter(model_path):\n    delegates = [load_delegate(EDGETPU_SHARED_LIB)]\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=delegates)\n    interpreter.allocate_tensors()\n    return interpreter\n\n\ndef get_output_tensors(interpreter):\n    # Returns a dict with keys: 'boxes', 'classes', 'scores', 'num'\n    details = interpreter.get_output_details()\n    out = {\"boxes\": None, \"classes\": None, \"scores\": None, \"num\": None}\n    for d in details:\n        shape = d[\"shape\"]\n        name = d.get(\"name\", \"\").lower()\n        if len(shape) == 3 and shape[-1] == 4:\n            out[\"boxes\"] = d\n        elif len(shape) == 2:\n            # Could be classes or scores\n            # Heuristics via name or dtype\n            if \"class\" in name:\n                out[\"classes\"] = d\n            elif \"score\" in name:\n                out[\"scores\"] = d\n            else:\n                # If not informative, decide later based on dtype\n                if out[\"scores\"] is None and np.dtype(d[\"dtype\"]) in (np.float32, np.float64):\n                    out[\"scores\"] = d\n                elif out[\"classes\"] is None:\n                    out[\"classes\"] = d\n        elif len(shape) == 1 and shape[0] == 1:\n            out[\"num\"] = d\n    # Final sanity: all must be present\n    if any(v is None for v in out.values()):\n        # Attempt fallback by ordering: typical order is [boxes, classes, scores, num]\n        if len(details) >= 4:\n            out[\"boxes\"] = details[0]\n            out[\"classes\"] = details[1]\n            out[\"scores\"] = details[2]\n            out[\"num\"] = details[3]\n    return out\n\n\ndef preprocess_frame(frame_bgr, input_size, input_dtype):\n    # Convert BGR to RGB and resize to model input size\n    h_in, w_in = input_size\n    img_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(img_rgb, (w_in, h_in))\n    if input_dtype == np.float32:\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        input_data = resized.astype(np.uint8)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\n\ndef draw_detections(frame, dets, labels):\n    # dets: list of tuples (ymin, xmin, ymax, xmax, class_id, score)\n    for (ymin, xmin, ymax, xmax, class_id, score) in dets:\n        # Draw rectangle\n        color = (0, 255, 0)\n        cv2.rectangle(frame, (xmin, ymin), (xmax, ymax), color, 2)\n\n        # Prepare label text\n        label = labels.get(class_id, str(class_id))\n        text = f\"{label}: {score:.2f}\"\n\n        # Text background\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n        cv2.rectangle(frame, (xmin, max(0, ymin - th - 6)), (xmin + tw + 4, ymin), color, -1)\n        cv2.putText(frame, text, (xmin + 2, max(0, ymin - 4)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n    return frame\n\n\ndef compute_map_placeholder():\n    # Without ground-truth annotations, true mAP cannot be computed.\n    # This function exists to conform to the output requirement and returns None.\n    return None\n\n\ndef main():\n    # Setup: Interpreter with EdgeTPU\n    if not os.path.exists(MODEL_PATH):\n        raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n    interpreter = make_interpreter(MODEL_PATH)\n\n    # Input/output tensor details\n    input_details = interpreter.get_input_details()\n    output_map = get_output_tensors(interpreter)\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Open video input\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    in_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    in_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0\n\n    # Prepare video writer for output\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (in_width, in_height))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    # Determine model input shape\n    # Expecting [1, height, width, 3]\n    in_shape = input_details[0][\"shape\"]\n    model_h, model_w = int(in_shape[1]), int(in_shape[2])\n    input_dtype = input_details[0][\"dtype\"]\n\n    frame_index = 0\n    t0 = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Preprocessing\n        input_tensor = preprocess_frame(frame, (model_h, model_w), input_dtype)\n\n        # Inference\n        interpreter.set_tensor(input_details[0][\"index\"], input_tensor)\n        interpreter.invoke()\n\n        # Collect outputs\n        boxes = interpreter.get_tensor(output_map[\"boxes\"][\"index\"])\n        classes = interpreter.get_tensor(output_map[\"classes\"][\"index\"])\n        scores = interpreter.get_tensor(output_map[\"scores\"][\"index\"])\n        num = interpreter.get_tensor(output_map[\"num\"][\"index\"])\n\n        # Squeeze to 1D lists\n        boxes = np.squeeze(boxes)\n        classes = np.squeeze(classes)\n        scores = np.squeeze(scores)\n        num_det = int(np.squeeze(num))\n\n        # Postprocessing: scale boxes to original image size and filter by confidence threshold\n        detections = []\n        for i in range(num_det):\n            score = float(scores[i])\n            if score < CONFIDENCE_THRESHOLD:\n                continue\n            cls_id = int(classes[i])\n            ymin, xmin, ymax, xmax = boxes[i]\n\n            # Clamp to [0,1]\n            ymin = max(0.0, min(1.0, float(ymin)))\n            xmin = max(0.0, min(1.0, float(xmin)))\n            ymax = max(0.0, min(1.0, float(ymax)))\n            xmax = max(0.0, min(1.0, float(xmax)))\n\n            # Scale to original frame size\n            x1 = int(xmin * in_width)\n            y1 = int(ymin * in_height)\n            x2 = int(xmax * in_width)\n            y2 = int(ymax * in_height)\n\n            # Ensure valid coordinates\n            x1 = max(0, min(in_width - 1, x1))\n            x2 = max(0, min(in_width - 1, x2))\n            y1 = max(0, min(in_height - 1, y1))\n            y2 = max(0, min(in_height - 1, y2))\n\n            detections.append((y1, x1, y2, x2, cls_id, score))\n\n        # Draw detections\n        output_frame = draw_detections(frame.copy(), detections, labels)\n\n        # Calculate mAP (placeholder, no GT available)\n        current_map = compute_map_placeholder()\n        map_text = \"mAP: N/A (no ground truth)\"\n        if current_map is not None:\n            map_text = f\"mAP: {current_map:.3f}\"\n\n        # Overlay mAP and FPS info\n        elapsed = time.time() - t0\n        fps_runtime = (frame_index + 1) / elapsed if elapsed > 0 else 0.0\n        info_text = f\"{map_text} | FPS: {fps_runtime:.1f}\"\n        (tw, th), _ = cv2.getTextSize(info_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        cv2.rectangle(output_frame, (5, 5), (5 + tw + 8, 5 + th + 12), (50, 50, 50), -1)\n        cv2.putText(output_frame, info_text, (9, 5 + th + 2), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n\n        # Write output frame\n        writer.write(output_frame)\n\n        frame_index += 1\n\n    cap.release()\n    writer.release()\n\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 4
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 77.639,
      "total_cost": 0.05444375,
      "observations": [
        {
          "id": "time-13-37-03-416807_chatcmpl-D6EQx95y4xpvDeKPMQf3h3YNigUwv",
          "traceId": "5ec23437",
          "type": "GENERATION",
          "name": "5ec2_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T11:37:03.416000+00:00",
          "endTime": "2026-02-06T11:38:21.055000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05444375,
          "latency": 77639.0,
          "costDetails": {
            "total": 0.05444375
          },
          "totalTokens": 5839,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 5388,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:37:03.369Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:39:19.588Z"
    },
    {
      "id": "d5c062d7",
      "timestamp": "2026-02-06T11:34:00.083000+00:00",
      "name": "d5c0_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\ndef load_labels(label_path):\n    labels = {}\n    with open(label_path, 'r') as f:\n        for idx, line in enumerate(f.readlines()):\n            line = line.strip()\n            if not line:\n                continue\n            # Support \"id label\" or just \"label\" formats\n            parts = line.split(maxsplit=1)\n            if len(parts) == 2 and parts[0].isdigit():\n                labels[int(parts[0])] = parts[1]\n            else:\n                labels[idx] = line\n    return labels\n\ndef get_interpreter(model_path, edgetpu_lib):\n    # Initialize TFLite interpreter with EdgeTPU delegate\n    return Interpreter(\n        model_path=model_path,\n        experimental_delegates=[load_delegate(edgetpu_lib)]\n    )\n\ndef preprocess_frame(frame, input_size, input_dtype):\n    h_in, w_in = input_size\n    # BGR to RGB, resize to model's input size\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(frame_rgb, (w_in, h_in))\n    if input_dtype == np.float32:\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        input_data = resized.astype(input_dtype)\n    input_data = np.expand_dims(input_data, axis=0)\n    return input_data\n\ndef parse_outputs(interpreter, output_details):\n    boxes = None\n    classes = None\n    scores = None\n    count = None\n    for od in output_details:\n        data = interpreter.get_tensor(od['index'])\n        # Typical TF Lite detection output tensors:\n        # boxes: [1, num, 4]; classes: [1, num]; scores: [1, num]; count: [1]\n        if data.ndim == 3 and data.shape[-1] == 4:\n            boxes = data[0]\n        elif data.ndim == 2 and data.shape[0] == 1:\n            vec = data[0]\n            # Heuristic to distinguish classes vs scores\n            if np.max(vec) <= 1.0:\n                scores = vec\n            else:\n                classes = vec\n        elif data.size == 1:\n            try:\n                count = int(np.squeeze(data))\n            except Exception:\n                count = None\n    # Fallbacks if count is None\n    if scores is not None and count is None:\n        count = scores.shape[0]\n    return boxes, classes, scores, count\n\ndef class_color(class_id):\n    # Deterministic pseudo-random color per class id\n    rng = np.random.default_rng(class_id)\n    color = rng.integers(0, 256, size=3, dtype=np.uint8)\n    return int(color[0]), int(color[1]), int(color[2])\n\ndef draw_label_with_background(image, text, org, color, font_scale=0.5, thickness=1):\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    (tw, th), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n    x, y = org\n    # Draw filled rectangle for text background\n    cv2.rectangle(image, (x, y - th - baseline), (x + tw, y + baseline), (0, 0, 0), -1)\n    # Put text\n    cv2.putText(image, text, (x, y), font, font_scale, color, thickness, cv2.LINE_AA)\n\ndef main():\n    # CONFIGURATION PARAMETERS\n    model_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\n    label_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\n    input_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\n    output_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\n    confidence_threshold = 0.5\n    edgetpu_lib = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\n    # Sanity checks\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f\"Model not found at: {model_path}\")\n    if not os.path.exists(label_path):\n        raise FileNotFoundError(f\"Label map not found at: {label_path}\")\n    if not os.path.exists(input_path):\n        raise FileNotFoundError(f\"Input video not found at: {input_path}\")\n\n    # Setup: load labels\n    labels = load_labels(label_path)\n\n    # Setup: load interpreter with EdgeTPU delegate\n    interpreter = get_interpreter(model_path, edgetpu_lib)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n\n    input_h = int(input_details[0]['shape'][1])\n    input_w = int(input_details[0]['shape'][2])\n    input_dtype = input_details[0]['dtype']\n\n    # Read input video\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {input_path}\")\n\n    orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps is None or fps <= 0 or np.isnan(fps):\n        fps = 30.0\n\n    # Prepare VideoWriter for output\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (orig_w, orig_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {output_path}\")\n\n    # Processing loop\n    frame_idx = 0\n    all_drawn_scores = []  # We'll compute a proxy mAP as the running mean confidence of drawn detections\n\n    start_time_overall = time.time()\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_idx += 1\n\n        # Preprocess\n        input_data = preprocess_frame(frame, (input_h, input_w), input_dtype)\n\n        # Inference\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n        inf_start = time.time()\n        interpreter.invoke()\n        inf_end = time.time()\n\n        # Parse outputs\n        boxes, classes, scores, count = parse_outputs(interpreter, output_details)\n        if boxes is None or classes is None or scores is None or count is None:\n            # If output format not recognized, skip drawing\n            writer.write(frame)\n            continue\n\n        # Draw detections above threshold\n        drawn = 0\n        for i in range(count):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n\n            ymin, xmin, ymax, xmax = boxes[i]\n            # Convert normalized [0,1] coords to pixel coords in original frame\n            left = max(0, min(orig_w - 1, int(xmin * orig_w)))\n            top = max(0, min(orig_h - 1, int(ymin * orig_h)))\n            right = max(0, min(orig_w - 1, int(xmax * orig_w)))\n            bottom = max(0, min(orig_h - 1, int(ymax * orig_h)))\n\n            # Draw bounding box\n            cid = int(classes[i])\n            color = class_color(cid)\n            cv2.rectangle(frame, (left, top), (right, bottom), color, 2)\n\n            # Prepare label text\n            label_text = labels.get(cid, str(cid))\n            text = f\"{label_text}: {score:.2f}\"\n            # Draw label background with text (top-left of box)\n            text_x = left\n            text_y = max(10, top - 5)\n            draw_label_with_background(frame, text, (text_x, text_y), (255, 255, 255), font_scale=0.5, thickness=1)\n\n            all_drawn_scores.append(score)\n            drawn += 1\n\n        # Compute a running \"mAP\" proxy as mean confidence of drawn detections\n        map_proxy = float(np.mean(all_drawn_scores)) if all_drawn_scores else 0.0\n\n        # Overlay inference time and mAP on frame\n        info_color = (255, 255, 255)\n        info_bg = (0, 0, 0)\n        header_text = f\"Detections: {drawn} | Inference: {(inf_end - inf_start)*1000:.1f} ms | mAP: {map_proxy*100:.2f}%\"\n        # Draw header background\n        (tw, th), baseline = cv2.getTextSize(header_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        cv2.rectangle(frame, (5, 5), (5 + tw + 10, 5 + th + baseline + 10), info_bg, -1)\n        cv2.putText(frame, header_text, (10, 10 + th), cv2.FONT_HERSHEY_SIMPLEX, 0.6, info_color, 2, cv2.LINE_AA)\n\n        # Write frame to output\n        writer.write(frame)\n\n    # Cleanup\n    cap.release()\n    writer.release()\n    elapsed = time.time() - start_time_overall\n\n    # Final report (printed to stdout)\n    total_frames = max(0, frame_idx)\n    print(\"Processing complete.\")\n    print(f\"Input video: {input_path}\")\n    print(f\"Output video: {output_path}\")\n    print(f\"Total frames: {total_frames}\")\n    print(f\"Total time: {elapsed:.2f} s\")\n    if all_drawn_scores:\n        print(f\"mAP (proxy, mean confidence of drawn detections): {np.mean(all_drawn_scores)*100:.2f}%\")\n    else:\n        print(\"mAP (proxy): N/A (no detections above threshold)\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 3
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 102.909,
      "total_cost": 0.06681375,
      "observations": [
        {
          "id": "time-13-34-00-891727_chatcmpl-D6EO1PvcPK9aKhXjHLK8wEbkFolwE",
          "traceId": "d5c062d7",
          "type": "GENERATION",
          "name": "d5c0_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T11:34:00.891000+00:00",
          "endTime": "2026-02-06T11:35:43.800000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06681375,
          "latency": 102909.0,
          "costDetails": {
            "total": 0.06681375
          },
          "totalTokens": 7076,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 6625,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:34:00.657Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:36:43.020Z"
    },
    {
      "id": "85a8a61f",
      "timestamp": "2026-02-06T11:31:21.154000+00:00",
      "name": "85a8_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nMODEL_PATH = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nLABEL_PATH = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\nINPUT_PATH = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\nOUTPUT_PATH = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nCONFIDENCE_THRESHOLD = 0.5\nEDGETPU_SHARED_LIB = \"/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0\"\n\ndef load_labels(label_path):\n    labels = {}\n    if not os.path.isfile(label_path):\n        print(f\"Warning: Label file not found at {label_path}. Proceeding without labels.\")\n        return labels\n    with open(label_path, 'r') as f:\n        for i, line in enumerate(f.readlines()):\n            line = line.strip()\n            if not line:\n                continue\n            # Try common formats: \"0 label\", \"0: label\", \"label\"\n            idx = None\n            name = None\n            parts = line.split()\n            # Try \"0 label\" format\n            if len(parts) >= 2 and parts[0].isdigit():\n                idx = int(parts[0])\n                name = \" \".join(parts[1:])\n            else:\n                # Try \"0: label\" format\n                if \":\" in line:\n                    left, right = line.split(\":\", 1)\n                    left = left.strip()\n                    right = right.strip()\n                    if left.isdigit():\n                        idx = int(left)\n                        name = right\n                # Fallback: plain list, use line number as index\n                if idx is None:\n                    idx = i\n                    name = line\n            labels[idx] = name\n    return labels\n\ndef make_interpreter(model_path, edgetpu_lib):\n    delegate = load_delegate(edgetpu_lib)\n    interpreter = Interpreter(model_path=model_path, experimental_delegates=[delegate])\n    interpreter.allocate_tensors()\n    return interpreter\n\ndef get_input_details(interpreter):\n    input_details = interpreter.get_input_details()[0]\n    height, width = input_details['shape'][1], input_details['shape'][2]\n    dtype = input_details['dtype']\n    return input_details['index'], height, width, dtype\n\ndef get_output_indices(interpreter):\n    # Detect the common detection postprocess outputs\n    # We will map them by shape:\n    # boxes: [1, N, 4]\n    # classes: [1, N]\n    # scores: [1, N]\n    # count: [1]\n    outs = interpreter.get_output_details()\n    idx_boxes = idx_classes = idx_scores = idx_count = None\n    for od in outs:\n        shp = od['shape']\n        if len(shp) == 3 and shp[-1] == 4:\n            idx_boxes = od['index']\n        elif len(shp) == 2:\n            # Could be classes or scores\n            if 'quantization_parameters' in od and od['dtype'] == np.float32:\n                # Can't differentiate reliably; use name hints if present\n                name = od.get('name', '').lower()\n                if 'score' in name:\n                    idx_scores = od['index']\n                elif 'class' in name:\n                    idx_classes = od['index']\n                else:\n                    # Assign later if still None\n                    if idx_scores is None:\n                        idx_scores = od['index']\n                    else:\n                        idx_classes = od['index']\n            else:\n                # Fallback\n                name = od.get('name', '').lower()\n                if 'score' in name:\n                    idx_scores = od['index']\n                elif 'class' in name:\n                    idx_classes = od['index']\n                else:\n                    if idx_scores is None:\n                        idx_scores = od['index']\n                    else:\n                        idx_classes = od['index']\n        elif len(shp) == 1 and shp[0] == 1:\n            idx_count = od['index']\n    return idx_boxes, idx_classes, idx_scores, idx_count\n\ndef preprocess_frame(frame, input_h, input_w, input_dtype):\n    # Resize to model input size\n    resized = cv2.resize(frame, (input_w, input_h))\n    if input_dtype == np.float32:\n        # Normalize to [0,1]\n        input_data = resized.astype(np.float32) / 255.0\n    else:\n        # Assume uint8\n        input_data = resized.astype(np.uint8)\n    # Add batch dimension\n    return np.expand_dims(input_data, axis=0)\n\ndef draw_detections(frame, detections, labels, color=(0, 255, 0)):\n    # detections: list of dict with keys 'bbox', 'score', 'class_id'\n    h, w = frame.shape[:2]\n    for det in detections:\n        y_min, x_min, y_max, x_max = det['bbox']\n        # Coordinates are normalized [0,1]; map to frame size\n        x1 = max(0, min(w - 1, int(x_min * w)))\n        y1 = max(0, min(h - 1, int(y_min * h)))\n        x2 = max(0, min(w - 1, int(x_max * w)))\n        y2 = max(0, min(h - 1, int(y_max * h)))\n\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        class_id = int(det['class_id'])\n        score = det['score']\n        label_text = labels.get(class_id, str(class_id))\n        caption = f\"{label_text}: {score:.2f}\"\n        # Put label above box if possible\n        y_text = y1 - 10 if y1 - 10 > 10 else y1 + 20\n        cv2.putText(frame, caption, (x1, y_text), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2, cv2.LINE_AA)\n\ndef main():\n    # Validate input file\n    if not os.path.isfile(INPUT_PATH):\n        raise FileNotFoundError(f\"Input video not found: {INPUT_PATH}\")\n\n    # Load labels\n    labels = load_labels(LABEL_PATH)\n\n    # Initialize interpreter with EdgeTPU delegate\n    interpreter = make_interpreter(MODEL_PATH, EDGETPU_SHARED_LIB)\n    input_index, in_h, in_w, in_dtype = get_input_details(interpreter)\n    out_idx_boxes, out_idx_classes, out_idx_scores, out_idx_count = get_output_indices(interpreter)\n\n    if None in (out_idx_boxes, out_idx_classes, out_idx_scores, out_idx_count):\n        raise RuntimeError(\"Failed to identify all required output tensors from the model.\")\n\n    # Setup video IO\n    cap = cv2.VideoCapture(INPUT_PATH)\n    if not cap.isOpened():\n        raise RuntimeError(f\"Failed to open input video: {INPUT_PATH}\")\n\n    in_fps = cap.get(cv2.CAP_PROP_FPS)\n    if not in_fps or np.isnan(in_fps) or in_fps <= 0:\n        in_fps = 30.0\n    frame_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(OUTPUT_PATH, fourcc, in_fps, (frame_w, frame_h))\n    if not writer.isOpened():\n        cap.release()\n        raise RuntimeError(f\"Failed to open output video for writing: {OUTPUT_PATH}\")\n\n    # Metrics accumulators\n    total_scores_sum = 0.0\n    total_detections_count = 0\n    frame_count = 0\n    fps_smooth = None\n    t_prev = time.time()\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        frame_count += 1\n\n        input_data = preprocess_frame(frame, in_h, in_w, in_dtype)\n        # Set input tensor\n        interpreter.set_tensor(input_index, input_data)\n\n        # Inference\n        t0 = time.time()\n        interpreter.invoke()\n        t1 = time.time()\n        dt = t1 - t0\n        # Smooth FPS estimate\n        inst_fps = 1.0 / dt if dt > 0 else 0.0\n        if fps_smooth is None:\n            fps_smooth = inst_fps\n        else:\n            fps_smooth = 0.9 * fps_smooth + 0.1 * inst_fps\n\n        # Extract outputs\n        boxes = interpreter.get_tensor(out_idx_boxes)[0]  # [N,4] in y_min, x_min, y_max, x_max\n        classes = interpreter.get_tensor(out_idx_classes)[0]  # [N]\n        scores = interpreter.get_tensor(out_idx_scores)[0]  # [N]\n        count = int(interpreter.get_tensor(out_idx_count)[0])\n\n        # Build detections list with thresholding\n        detections = []\n        for i in range(count):\n            score = float(scores[i])\n            if score < CONFIDENCE_THRESHOLD:\n                continue\n            cls = int(classes[i])\n            bbox = boxes[i]  # normalized\n            # Ensure bbox in [0,1]\n            y_min, x_min, y_max, x_max = bbox\n            y_min = max(0.0, min(1.0, y_min))\n            x_min = max(0.0, min(1.0, x_min))\n            y_max = max(0.0, min(1.0, y_max))\n            x_max = max(0.0, min(1.0, x_max))\n            detections.append({'bbox': (y_min, x_min, y_max, x_max), 'score': score, 'class_id': cls})\n\n        # Update proxy mAP metric (mean of scores over all detections so far)\n        if detections:\n            scores_this_frame = [d['score'] for d in detections]\n            total_scores_sum += float(np.sum(scores_this_frame))\n            total_detections_count += len(scores_this_frame)\n        proxy_map = (total_scores_sum / total_detections_count) if total_detections_count > 0 else 0.0\n\n        # Draw detections and overlay metrics\n        draw_detections(frame, detections, labels, color=(0, 255, 0))\n        # Overlays: FPS and mAP\n        cv2.putText(frame, f\"FPS: {fps_smooth:.1f}\", (10, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2, cv2.LINE_AA)\n        cv2.putText(frame, f\"mAP: {proxy_map:.3f}\", (10, 55), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2, cv2.LINE_AA)\n\n        # Write to output\n        writer.write(frame)\n\n        # Progress logging every 50 frames\n        if frame_count % 50 == 0:\n            now = time.time()\n            elapsed = now - t_prev\n            t_prev = now\n            print(f\"Processed {frame_count} frames | approx {50.0/elapsed:.2f} FPS | current detections: {len(detections)}\")\n\n    # Cleanup\n    cap.release()\n    writer.release()\n\n    print(\"Processing complete.\")\n    print(f\"Total frames: {frame_count}\")\n    print(f\"Output saved to: {OUTPUT_PATH}\")\n    print(f\"Final mAP (proxy): { (total_scores_sum / total_detections_count) if total_detections_count > 0 else 0.0 :.4f}\")\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 2
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 78.796,
      "total_cost": 0.05459375,
      "observations": [
        {
          "id": "time-13-31-21-786454_chatcmpl-D6ELSkY5d5cYPqHf8Yd7O13DNF8NZ",
          "traceId": "85a8a61f",
          "type": "GENERATION",
          "name": "85a8_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T11:31:21.786000+00:00",
          "endTime": "2026-02-06T11:32:40.582000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.05459375,
          "latency": 78796.0,
          "costDetails": {
            "total": 0.05459375
          },
          "totalTokens": 5854,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 5403,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:31:21.716Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:33:39.959Z"
    },
    {
      "id": "31b5468d",
      "timestamp": "2026-02-06T11:28:00.254000+00:00",
      "name": "31b5_tpu_sketch_generator",
      "input": null,
      "output": {
        "status": "success",
        "sketch": "import os\nimport time\nimport numpy as np\nimport cv2\nfrom tflite_runtime.interpreter import Interpreter, load_delegate\n\n# Configuration parameters\nmodel_path = \"/home/mendel/tinyml_autopilot/models/edgetpu_detect.tflite\"\nlabel_path = \"/home/mendel/tinyml_autopilot/models/labelmap.txt\"\ninput_path = \"/home/mendel/tinyml_autopilot/data/sheeps.mp4\"\noutput_path = \"/home/mendel/tinyml_autopilot/results/sheeps_detections.mp4\"\nconfidence_threshold = 0.5\n\n# --------------------------\n# Utilities\n# --------------------------\ndef load_labels(path):\n    labels = {}\n    if not os.path.exists(path):\n        return labels\n    with open(path, 'r', encoding='utf-8') as f:\n        for i, line in enumerate(f):\n            line = line.strip()\n            if not line:\n                continue\n            # Try \"index label\" format, else fallback to line order\n            parts = line.split(maxsplit=1)\n            if len(parts) == 2 and parts[0].isdigit():\n                labels[int(parts[0])] = parts[1].strip()\n            else:\n                labels[i] = line\n    return labels\n\ndef make_interpreter(model_file):\n    return Interpreter(\n        model_path=model_file,\n        experimental_delegates=[load_delegate('/usr/lib/aarch64-linux-gnu/libedgetpu.so.1.0')]\n    )\n\ndef preprocess_frame(frame_bgr, input_size, input_dtype):\n    h, w = input_size\n    # Convert BGR to RGB and resize\n    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    resized = cv2.resize(rgb, (w, h))\n    input_data = np.expand_dims(resized, axis=0)\n    if input_dtype == np.float32:\n        input_data = input_data.astype(np.float32) / 255.0\n    else:\n        input_data = input_data.astype(np.uint8)\n    return input_data\n\ndef run_inference(interpreter, input_data, input_index):\n    interpreter.set_tensor(input_index, input_data)\n    interpreter.invoke()\n    output_details = interpreter.get_output_details()\n    # Typical EdgeTPU detection model outputs:\n    # 0: boxes [1, num, 4], 1: classes [1, num], 2: scores [1, num], 3: num [1]\n    boxes = interpreter.get_tensor(output_details[0]['index'])\n    classes = interpreter.get_tensor(output_details[1]['index'])\n    scores = interpreter.get_tensor(output_details[2]['index'])\n    num = interpreter.get_tensor(output_details[3]['index'])\n    boxes = np.squeeze(boxes)\n    classes = np.squeeze(classes).astype(np.int32)\n    scores = np.squeeze(scores).astype(np.float32)\n    num = int(np.squeeze(num))\n    return boxes, classes, scores, num\n\ndef class_color(class_id):\n    # Deterministic color per class id\n    rng = np.random.RandomState(class_id)\n    color = rng.randint(0, 255, size=3).tolist()\n    return (int(color[0]), int(color[1]), int(color[2]))\n\ndef draw_detections(frame, detections, labels, map_value=None):\n    for det in detections:\n        x1, y1, x2, y2 = det['bbox']\n        class_id = det['class_id']\n        score = det['score']\n        color = class_color(class_id)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        label_text = labels.get(class_id, str(class_id))\n        text = \"{}: {:.2f}\".format(label_text, score)\n        (tw, th), bl = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n        cv2.rectangle(frame, (x1, max(0, y1 - th - 6)), (x1 + tw + 4, y1), color, -1)\n        cv2.putText(frame, text, (x1 + 2, y1 - 4), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2, cv2.LINE_AA)\n    if map_value is not None:\n        map_text = \"mAP: {:.2f}%\".format(map_value * 100.0)\n        cv2.putText(frame, map_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (50, 220, 50), 2, cv2.LINE_AA)\n\ndef compute_ap_11pt(precisions, recalls):\n    # VOC 2007 11-point interpolation\n    ap = 0.0\n    for t in [i / 10.0 for i in range(11)]:  # 0.0, 0.1, ..., 1.0\n        p = 0.0\n        for pr, rc in zip(precisions, recalls):\n            if rc >= t and pr > p:\n                p = pr\n        ap += p\n    return ap / 11.0\n\ndef compute_map_pseudo(dets_by_class):\n    # Pseudo mAP without ground truth:\n    # Assume at most 1 true object per frame per class.\n    # For each class, within each frame: highest-score detection -> TP, others -> FP.\n    aps = []\n    for class_id, frame_scores in dets_by_class.items():\n        # frame_scores: dict frame_id -> list of scores\n        total_positives = 0\n        scored_tuples = []  # (score, is_tp)\n        for _, scores in frame_scores.items():\n            if not scores:\n                continue\n            scores_sorted = sorted(scores, reverse=True)\n            # one TP per frame (the top), rest FP\n            scored_tuples.append((scores_sorted[0], 1))\n            for s in scores_sorted[1:]:\n                scored_tuples.append((s, 0))\n            total_positives += 1\n        if total_positives == 0 or len(scored_tuples) == 0:\n            continue\n        # Sort all detections by score descending\n        scored_tuples.sort(key=lambda x: x[0], reverse=True)\n        precisions = []\n        recalls = []\n        tp_cum = 0\n        fp_cum = 0\n        for s, is_tp in scored_tuples:\n            if is_tp == 1:\n                tp_cum += 1\n            else:\n                fp_cum += 1\n            prec = tp_cum / float(tp_cum + fp_cum)\n            rec = tp_cum / float(total_positives)\n            precisions.append(prec)\n            recalls.append(rec)\n        ap = compute_ap_11pt(precisions, recalls)\n        aps.append(ap)\n    if len(aps) == 0:\n        return 0.0\n    return float(np.mean(aps))\n\ndef ensure_dir_for_file(path):\n    d = os.path.dirname(path)\n    if d and not os.path.exists(d):\n        os.makedirs(d, exist_ok=True)\n\n# --------------------------\n# Main application\n# --------------------------\ndef main():\n    # Setup: load interpreter and labels, open input\n    labels = load_labels(label_path)\n    interpreter = make_interpreter(model_path)\n    interpreter.allocate_tensors()\n    input_details = interpreter.get_input_details()[0]\n    in_h = int(input_details['shape'][1])\n    in_w = int(input_details['shape'][2])\n    in_dtype = input_details['dtype']\n    in_index = input_details['index']\n\n    cap = cv2.VideoCapture(input_path)\n    if not cap.isOpened():\n        raise RuntimeError(\"Cannot open input video: {}\".format(input_path))\n\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    if fps <= 0 or np.isnan(fps):\n        fps = 30.0\n    src_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    src_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # First pass: run inference, collect detections per frame and per class for mAP\n    frame_detections = []  # list index by frame_id: list of detection dicts\n    dets_by_class = {}     # class_id -> {frame_id: [scores]}\n    frame_id = 0\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        input_data = preprocess_frame(frame, (in_h, in_w), in_dtype)\n        boxes, classes, scores, num = run_inference(interpreter, input_data, in_index)\n\n        detections_this_frame = []\n        for i in range(num):\n            score = float(scores[i])\n            if score < confidence_threshold:\n                continue\n            class_id = int(classes[i])\n            # Boxes are normalized [ymin, xmin, ymax, xmax]\n            ymin, xmin, ymax, xmax = boxes[i]\n            x1 = max(0, min(src_w - 1, int(xmin * src_w)))\n            y1 = max(0, min(src_h - 1, int(ymin * src_h)))\n            x2 = max(0, min(src_w - 1, int(xmax * src_w)))\n            y2 = max(0, min(src_h - 1, int(ymax * src_h)))\n            if x2 <= x1 or y2 <= y1:\n                continue\n            detections_this_frame.append({\n                'bbox': (x1, y1, x2, y2),\n                'class_id': class_id,\n                'score': score\n            })\n            # For mAP (pseudo) accumulation\n            if class_id not in dets_by_class:\n                dets_by_class[class_id] = {}\n            if frame_id not in dets_by_class[class_id]:\n                dets_by_class[class_id][frame_id] = []\n            dets_by_class[class_id][frame_id].append(score)\n\n        frame_detections.append(detections_this_frame)\n        frame_id += 1\n\n    cap.release()\n\n    # Compute mAP (pseudo, due to lack of ground truth)\n    map_value = compute_map_pseudo(dets_by_class)\n\n    # Second pass: write output video with rectangles, labels, and mAP overlay\n    ensure_dir_for_file(output_path)\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    writer = cv2.VideoWriter(output_path, fourcc, fps, (src_w, src_h))\n    cap2 = cv2.VideoCapture(input_path)\n    if not cap2.isOpened():\n        writer.release()\n        raise RuntimeError(\"Cannot reopen input video for writing: {}\".format(input_path))\n\n    frame_id = 0\n    while True:\n        ret, frame = cap2.read()\n        if not ret:\n            break\n        dets = frame_detections[frame_id] if frame_id < len(frame_detections) else []\n        draw_detections(frame, dets, labels, map_value=map_value)\n        writer.write(frame)\n        frame_id += 1\n\n    cap2.release()\n    writer.release()\n\nif __name__ == \"__main__\":\n    main()"
      },
      "session_id": "gpt-5-2025-08-07_6b01_tpusg_batch",
      "metadata": {
        "num_run": 1
      },
      "tags": [
        "benchmark",
        "gpt-5-2025-08-07",
        "tpu_sketch_generator"
      ],
      "latency": 118.673,
      "total_cost": 0.06563375,
      "observations": [
        {
          "id": "time-13-28-00-501416_chatcmpl-D6EIDHjDFruN1EfuGy95SUr24kJGd",
          "traceId": "31b5468d",
          "type": "GENERATION",
          "name": "31b5_tpusg_gen_attempt#1",
          "startTime": "2026-02-06T11:28:00.501000+00:00",
          "endTime": "2026-02-06T11:29:59.174000+00:00",
          "model": "gpt-5-2025-08-07",
          "modelParameters": {
            "extra_body": "{}"
          },
          "level": "DEFAULT",
          "calculatedTotalCost": 0.06563375,
          "latency": 118673.0,
          "costDetails": {
            "total": 0.06563375
          },
          "totalTokens": 6958,
          "environment": "default",
          "promptTokens": 451,
          "completionTokens": 6507,
          "statusMessage": null,
          "calculatedInputCost": null,
          "calculatedOutputCost": null
        }
      ],
      "createdAt": "2026-02-06T11:28:00.785Z",
      "environment": "default",
      "updatedAt": "2026-02-06T11:31:00.779Z"
    }
  ],
  "meta": {
    "total_items": 30
  }
}