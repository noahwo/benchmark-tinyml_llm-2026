{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ababb9",
   "metadata": {},
   "source": [
    "# CodeBERT Similarity Calculations\n",
    "A consolidated notebook to run PSG, SG, and TPU-SG similarity scoring and persist results.\n",
    "\n",
    "**Sections**\n",
    "1. Configure Paths and Environment\n",
    "2. Import Dependencies and Utilities\n",
    "3. Load Reference File and Discover Candidates\n",
    "4. Run Similarity Scoring Loop (All Three Scripts)\n",
    "5. Append and Merge CSV Outputs\n",
    "6. Quick Summary and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62f0beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configure Paths and Environment\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if \"__file__\" in globals():\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "else:\n",
    "    # Notebook execution: assume cwd is the repo root\n",
    "    SCRIPT_DIR = Path.cwd() / \"codebertscore-similarity/analyser-results\"\n",
    "    if not SCRIPT_DIR.exists():\n",
    "        # Fallback to cwd if running from inside analyser-results already\n",
    "        SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "# Derive project root based on known layout\n",
    "if SCRIPT_DIR.name == \"analyser-results\" and SCRIPT_DIR.parent.name == \"codebertscore-similarity\":\n",
    "    PROJECT_ROOT = SCRIPT_DIR.parent.parent\n",
    "else:\n",
    "    PROJECT_ROOT = SCRIPT_DIR\n",
    "\n",
    "CODE_BERT_PACKAGE = PROJECT_ROOT / \"codebertscore-similarity\"\n",
    "sys.path.insert(0, str(CODE_BERT_PACKAGE))\n",
    "\n",
    "# Per-script configuration matching the original files\n",
    "CONFIGS = {\n",
    "    \"psg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/TFLite_detection_video.py\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/psg/*.py\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_psg.csv\",\n",
    "        \"lang\": \"python\",\n",
    "    },\n",
    "    \"sg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/object_color_classify.ino\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/sg/*.ino\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_sg.csv\",\n",
    "        \"lang\": \"c_sharp\",\n",
    "    },\n",
    "    \"tpusg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/TFLite_detection_video_TPU.py\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/tpusg/*.py\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_tpusg.csv\",\n",
    "        \"lang\": \"python\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Include both 2025 and 2026 runs under data_analysis\n",
    "CANDIDATE_ROOT = PROJECT_ROOT / \"data_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a73db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Dependencies and Utilities\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import code_bert_score\n",
    "\n",
    "\n",
    "def load_file_content(file_path: Path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_model_name(candidate_path: Path):\n",
    "    # .../<run>/exported_valid_code/<variant>/file.ext\n",
    "    try:\n",
    "        return candidate_path.parents[2].name\n",
    "    except IndexError:\n",
    "        return candidate_path.stem\n",
    "\n",
    "\n",
    "def ensure_header(csv_path: Path, columns):\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not csv_path.exists():\n",
    "        pd.DataFrame(columns=columns).to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "def get_processed_ids(csv_path: Path):\n",
    "    if not csv_path.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[\"Candidate_ID\"])\n",
    "        if \"Candidate_ID\" in df.columns:\n",
    "            return set(df[\"Candidate_ID\"].astype(str).dropna())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing CSV {csv_path}: {e}\")\n",
    "    return set()\n",
    "\n",
    "\n",
    "def get_processed_paths(csv_path: Path):\n",
    "    if not csv_path.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[\"Candidate_Path\"])\n",
    "        if \"Candidate_Path\" in df.columns:\n",
    "            return set(df[\"Candidate_Path\"].astype(str).dropna())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing CSV {csv_path}: {e}\")\n",
    "    return set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bfa9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[psg] reference loaded; candidates: 217\n",
      "[sg] reference loaded; candidates: 54\n",
      "[tpusg] reference loaded; candidates: 228\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 3. Load Reference File and Discover Candidates\n",
    "references = {}\n",
    "candidate_sets = {}\n",
    "\n",
    "for key, cfg in CONFIGS.items():\n",
    "    ref_content = load_file_content(cfg[\"reference\"])\n",
    "    if not ref_content:\n",
    "        print(f\"Failed to load reference for {key}: {cfg['reference']}\")\n",
    "        continue\n",
    "    references[key] = ref_content\n",
    "    # search across all runs under data_analysis (e.g., 2025, 2026, etc.)\n",
    "    candidate_files = sorted(CANDIDATE_ROOT.glob(f\"*/{cfg['candidate_glob']}\"))\n",
    "    candidate_sets[key] = candidate_files\n",
    "    print(f\"[{key}] reference loaded; candidates: {len(candidate_files)}\")\n",
    "\n",
    "# Device selection\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58dd0ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[psg] wrote 36 rows to /home/han/Projects/benchmark-tinyml_llm-2026/codebertscore-similarity/analyser-results/similarity_results_psg.csv\n",
      "[sg] no new rows written\n",
      "[tpusg] wrote 60 rows to /home/han/Projects/benchmark-tinyml_llm-2026/codebertscore-similarity/analyser-results/similarity_results_tpusg.csv\n"
     ]
    }
   ],
   "source": [
    "# 4. Run Similarity Scoring Loop (All Three Scripts)\n",
    "results = {}\n",
    "common_columns = [\n",
    "    \"Candidate_ID\",\n",
    "    \"Candidate_Path\",\n",
    "    \"Model\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    # \"Reference_File\",\n",
    "    # \"Language\",\n",
    "    \"Processor\",\n",
    "    # \"Timestamp\",\n",
    "]\n",
    "\n",
    "for key, cfg in CONFIGS.items():\n",
    "    ensure_header(cfg[\"output_csv\"], common_columns)\n",
    "    processed_ids = get_processed_ids(cfg[\"output_csv\"])\n",
    "    processed_paths = get_processed_paths(cfg[\"output_csv\"])\n",
    "    candidate_files = candidate_sets.get(key, [])\n",
    "    ref_content = references.get(key)\n",
    "    if not ref_content:\n",
    "        continue\n",
    "\n",
    "    loop_records = []\n",
    "    for candidate_path in candidate_files:\n",
    "        candidate_id = candidate_path.relative_to(PROJECT_ROOT).as_posix()\n",
    "        candidate_posix = candidate_path.as_posix()\n",
    "        if candidate_id in processed_ids or candidate_posix in processed_paths:\n",
    "            continue\n",
    "\n",
    "        candidate_content = load_file_content(candidate_path)\n",
    "        if not candidate_content:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            P, R, F1, F3 = code_bert_score.score(\n",
    "                cands=[candidate_content],\n",
    "                refs=[ref_content],\n",
    "                lang=cfg[\"lang\"],\n",
    "                device=DEVICE,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            p_val = P[0].item()\n",
    "            r_val = R[0].item()\n",
    "            f1_val = F1[0].item()\n",
    "            f3_val = F3[0].item()\n",
    "\n",
    "            model_name = extract_model_name(candidate_path)\n",
    "            timestamp = datetime.now().isoformat()\n",
    "\n",
    "            record = {\n",
    "                \"Candidate_ID\": candidate_id,\n",
    "                # \"Candidate_Path\": candidate_posix,\n",
    "                \"Model\": model_name.split(\"_\")[1],  # e.g., \"02.03_gpt5-mpu\" -> \"gpt5-mpu\"\n",
    "                \"Precision\": p_val,\n",
    "                \"Recall\": r_val,\n",
    "                \"F1\": f1_val,\n",
    "                \"F3\": f3_val,\n",
    "                # \"Reference_File\": cfg[\"reference\"],\n",
    "                # \"Language\": cfg[\"lang\"],\n",
    "                \"Processor\": key,\n",
    "                # \"Timestamp\": timestamp,\n",
    "            }\n",
    "            loop_records.append(record)\n",
    "        except Exception as e:\n",
    "            print(f\"[{key}] Error processing {candidate_path}: {e}\")\n",
    "\n",
    "    if loop_records:\n",
    "        df_loop = pd.DataFrame(loop_records)\n",
    "        df_loop.to_csv(cfg[\"output_csv\"], mode=\"a\", header=False, index=False)\n",
    "        results[key] = df_loop\n",
    "        print(f\"[{key}] wrote {len(df_loop)} rows to {cfg['output_csv']}\")\n",
    "    else:\n",
    "        print(f\"[{key}] no new rows written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95b7ccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined rows: 500\n",
      "Run counts (candidates -> csv rows):\n",
      "psg: 217 candidates; 218 rows in csv\n",
      "sg: 54 candidates; 54 rows in csv\n",
      "tpusg: 228 candidates; 228 rows in csv\n",
      "Totals discovered: 499 candidates across runs; 500 rows across csvs\n"
     ]
    }
   ],
   "source": [
    "# 5. Append and Merge CSV Outputs\n",
    "merged = []\n",
    "run_candidate_counts = {k: len(v) for k, v in candidate_sets.items()}\n",
    "output_row_counts = {}\n",
    "for key, cfg in CONFIGS.items():\n",
    "    if cfg[\"output_csv\"].exists():\n",
    "        try:\n",
    "            df = pd.read_csv(cfg[\"output_csv\"])\n",
    "            df[\"run\"] = key\n",
    "            merged.append(df)\n",
    "            output_row_counts[key] = len(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[{key}] Error reading {cfg['output_csv']}: {e}\")\n",
    "\n",
    "if merged:\n",
    "    combined = pd.concat(merged, ignore_index=True)\n",
    "    print(f\"Combined rows: {len(combined)}\")\n",
    "else:\n",
    "    combined = pd.DataFrame()\n",
    "    print(\"No CSVs to combine yet.\")\n",
    "\n",
    "print(\"Run counts (candidates -> csv rows):\")\n",
    "for key in CONFIGS:\n",
    "    cand = run_candidate_counts.get(key, 0)\n",
    "    rows = output_row_counts.get(key, 0)\n",
    "    print(f\"{key}: {cand} candidates; {rows} rows in csv\")\n",
    "total_candidates = sum(run_candidate_counts.values())\n",
    "total_rows = sum(output_row_counts.values())\n",
    "print(f\"Totals discovered: {total_candidates} candidates across runs; {total_rows} rows across csvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "424b667a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_ID</th>\n",
       "      <th>Candidate_Path</th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F3</th>\n",
       "      <th>Processor</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_analysis/2026/02.03_gpt5-mpu/exported_val...</td>\n",
       "      <td>/home/han/Projects/benchmark-tinyml_llm-2026/d...</td>\n",
       "      <td>02.03_gpt5-mpu</td>\n",
       "      <td>0.792295</td>\n",
       "      <td>0.867710</td>\n",
       "      <td>0.828290</td>\n",
       "      <td>0.859529</td>\n",
       "      <td>psg</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_analysis/2026/02.03_gpt5-mpu/exported_val...</td>\n",
       "      <td>/home/han/Projects/benchmark-tinyml_llm-2026/d...</td>\n",
       "      <td>02.03_gpt5-mpu</td>\n",
       "      <td>0.784481</td>\n",
       "      <td>0.873618</td>\n",
       "      <td>0.826654</td>\n",
       "      <td>0.863803</td>\n",
       "      <td>psg</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_analysis/2026/02.03_gpt5-mpu/exported_val...</td>\n",
       "      <td>/home/han/Projects/benchmark-tinyml_llm-2026/d...</td>\n",
       "      <td>02.03_gpt5-mpu</td>\n",
       "      <td>0.784981</td>\n",
       "      <td>0.876138</td>\n",
       "      <td>0.828058</td>\n",
       "      <td>0.866081</td>\n",
       "      <td>psg</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_analysis/2026/02.03_gpt5-mpu/exported_val...</td>\n",
       "      <td>/home/han/Projects/benchmark-tinyml_llm-2026/d...</td>\n",
       "      <td>02.03_gpt5-mpu</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.864467</td>\n",
       "      <td>0.820617</td>\n",
       "      <td>0.855326</td>\n",
       "      <td>psg</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_analysis/2026/02.03_gpt5-mpu/exported_val...</td>\n",
       "      <td>/home/han/Projects/benchmark-tinyml_llm-2026/d...</td>\n",
       "      <td>02.03_gpt5-mpu</td>\n",
       "      <td>0.777338</td>\n",
       "      <td>0.871617</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.861172</td>\n",
       "      <td>psg</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Candidate_ID  \\\n",
       "0  data_analysis/2026/02.03_gpt5-mpu/exported_val...   \n",
       "1  data_analysis/2026/02.03_gpt5-mpu/exported_val...   \n",
       "2  data_analysis/2026/02.03_gpt5-mpu/exported_val...   \n",
       "3  data_analysis/2026/02.03_gpt5-mpu/exported_val...   \n",
       "4  data_analysis/2026/02.03_gpt5-mpu/exported_val...   \n",
       "\n",
       "                                      Candidate_Path           Model  \\\n",
       "0  /home/han/Projects/benchmark-tinyml_llm-2026/d...  02.03_gpt5-mpu   \n",
       "1  /home/han/Projects/benchmark-tinyml_llm-2026/d...  02.03_gpt5-mpu   \n",
       "2  /home/han/Projects/benchmark-tinyml_llm-2026/d...  02.03_gpt5-mpu   \n",
       "3  /home/han/Projects/benchmark-tinyml_llm-2026/d...  02.03_gpt5-mpu   \n",
       "4  /home/han/Projects/benchmark-tinyml_llm-2026/d...  02.03_gpt5-mpu   \n",
       "\n",
       "   Precision    Recall        F1        F3 Processor  run  \n",
       "0   0.792295  0.867710  0.828290  0.859529       psg  psg  \n",
       "1   0.784481  0.873618  0.826654  0.863803       psg  psg  \n",
       "2   0.784981  0.876138  0.828058  0.866081       psg  psg  \n",
       "3   0.781000  0.864467  0.820617  0.855326       psg  psg  \n",
       "4   0.777338  0.871617  0.821782  0.861172       psg  psg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_ID</th>\n",
       "      <th>Candidate_Path</th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F3</th>\n",
       "      <th>Processor</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...</td>\n",
       "      <td>/home/han/Projects/benchmark-tinyml_llm-2026/d...</td>\n",
       "      <td>08.03_abla-og-qw32-tpusg</td>\n",
       "      <td>0.902264</td>\n",
       "      <td>0.842597</td>\n",
       "      <td>0.871410</td>\n",
       "      <td>0.848207</td>\n",
       "      <td>tpusg</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...</td>\n",
       "      <td>/home/han/Projects/benchmark-tinyml_llm-2026/d...</td>\n",
       "      <td>08.03_abla-og-qw32-tpusg</td>\n",
       "      <td>0.894800</td>\n",
       "      <td>0.828178</td>\n",
       "      <td>0.860201</td>\n",
       "      <td>0.834390</td>\n",
       "      <td>tpusg</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...</td>\n",
       "      <td>/home/han/Projects/benchmark-tinyml_llm-2026/d...</td>\n",
       "      <td>08.03_abla-og-qw32-tpusg</td>\n",
       "      <td>0.896713</td>\n",
       "      <td>0.831326</td>\n",
       "      <td>0.862782</td>\n",
       "      <td>0.837433</td>\n",
       "      <td>tpusg</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...</td>\n",
       "      <td>/home/han/Projects/benchmark-tinyml_llm-2026/d...</td>\n",
       "      <td>08.03_abla-og-qw32-tpusg</td>\n",
       "      <td>0.892663</td>\n",
       "      <td>0.829386</td>\n",
       "      <td>0.859862</td>\n",
       "      <td>0.835307</td>\n",
       "      <td>tpusg</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...</td>\n",
       "      <td>/home/han/Projects/benchmark-tinyml_llm-2026/d...</td>\n",
       "      <td>08.03_abla-og-qw32-tpusg</td>\n",
       "      <td>0.896405</td>\n",
       "      <td>0.835720</td>\n",
       "      <td>0.864999</td>\n",
       "      <td>0.841416</td>\n",
       "      <td>tpusg</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Candidate_ID  \\\n",
       "495  data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...   \n",
       "496  data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...   \n",
       "497  data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...   \n",
       "498  data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...   \n",
       "499  data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...   \n",
       "\n",
       "                                        Candidate_Path  \\\n",
       "495  /home/han/Projects/benchmark-tinyml_llm-2026/d...   \n",
       "496  /home/han/Projects/benchmark-tinyml_llm-2026/d...   \n",
       "497  /home/han/Projects/benchmark-tinyml_llm-2026/d...   \n",
       "498  /home/han/Projects/benchmark-tinyml_llm-2026/d...   \n",
       "499  /home/han/Projects/benchmark-tinyml_llm-2026/d...   \n",
       "\n",
       "                        Model  Precision    Recall        F1        F3  \\\n",
       "495  08.03_abla-og-qw32-tpusg   0.902264  0.842597  0.871410  0.848207   \n",
       "496  08.03_abla-og-qw32-tpusg   0.894800  0.828178  0.860201  0.834390   \n",
       "497  08.03_abla-og-qw32-tpusg   0.896713  0.831326  0.862782  0.837433   \n",
       "498  08.03_abla-og-qw32-tpusg   0.892663  0.829386  0.859862  0.835307   \n",
       "499  08.03_abla-og-qw32-tpusg   0.896405  0.835720  0.864999  0.841416   \n",
       "\n",
       "    Processor    run  \n",
       "495     tpusg  tpusg  \n",
       "496     tpusg  tpusg  \n",
       "497     tpusg  tpusg  \n",
       "498     tpusg  tpusg  \n",
       "499     tpusg  tpusg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts by run:\n",
      "run\n",
      "psg      218\n",
      "sg        54\n",
      "tpusg    228\n",
      "Name: Candidate_ID, dtype: int64\n",
      "F1 stats:\n",
      "count    499.000000\n",
      "mean       0.851895\n",
      "std        0.022798\n",
      "min        0.615672\n",
      "25%        0.838571\n",
      "50%        0.852422\n",
      "75%        0.868371\n",
      "max        0.889104\n",
      "Name: F1, dtype: float64\n",
      "Coverage (unique Candidate_Path):\n",
      "psg: 217 rows vs 217 candidates\n",
      "sg: 54 rows vs 54 candidates\n",
      "tpusg: 228 rows vs 228 candidates\n",
      "Total candidates: 499; total rows: 500\n"
     ]
    }
   ],
   "source": [
    "# 6. Quick Summary and Verification\n",
    "if not combined.empty:\n",
    "    display(combined.head())\n",
    "    display(combined.tail())\n",
    "    print(\"Counts by run:\")\n",
    "    print(combined.groupby(\"run\")[\"Candidate_ID\"].count())\n",
    "    print(\"F1 stats:\")\n",
    "    print(combined[\"F1\"].describe())\n",
    "    coverage = combined.groupby(\"run\")[\"Candidate_Path\"].nunique()\n",
    "    discovered = {k: len(v) for k, v in candidate_sets.items()}\n",
    "    print(\"Coverage (unique Candidate_Path):\")\n",
    "    for key in CONFIGS:\n",
    "        seen = coverage.get(key, 0) if hasattr(coverage, \"get\") else 0\n",
    "        print(f\"{key}: {seen} rows vs {discovered.get(key, 0)} candidates\")\n",
    "    print(f\"Total candidates: {sum(discovered.values())}; total rows: {len(combined)}\")\n",
    "else:\n",
    "    print(\"No data to summarize yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psg: Model normalized (unique before=9, after=10)\n",
      "sg: Model normalized (unique before=3, after=3)\n",
      "tpusg: Model normalized (unique before=9, after=9)\n"
     ]
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb3a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
