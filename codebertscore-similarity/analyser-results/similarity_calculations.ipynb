{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ababb9",
   "metadata": {},
   "source": [
    "# CodeBERT Similarity Calculations\n",
    "A consolidated notebook to run PSG, SG, and TPU-SG similarity scoring and persist results.\n",
    "\n",
    "**Sections**\n",
    "1. Configure Paths and Environment\n",
    "2. Import Dependencies and Utilities\n",
    "3. Load Reference File and Discover Candidates\n",
    "4. Run Similarity Scoring Loop (All Three Scripts)\n",
    "5. Append and Merge CSV Outputs\n",
    "6. Quick Summary and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0beb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m SCRIPT_DIR \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;18;43m__file__\u001b[39;49m)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m      6\u001b[0m PROJECT_ROOT \u001b[38;5;241m=\u001b[39m SCRIPT_DIR\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mparent\n\u001b[1;32m      7\u001b[0m CODE_BERT_PACKAGE \u001b[38;5;241m=\u001b[39m PROJECT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcodebertscore-similarity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Configure Paths and Environment\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if \"__file__\" in globals():\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "else:\n",
    "    # Notebook execution: assume cwd is the repo root\n",
    "    SCRIPT_DIR = Path.cwd() / \"codebertscore-similarity/analyser-results\"\n",
    "    if not SCRIPT_DIR.exists():\n",
    "        # Fallback to cwd if running from inside analyser-results already\n",
    "        SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "# Derive project root based on known layout\n",
    "if SCRIPT_DIR.name == \"analyser-results\" and SCRIPT_DIR.parent.name == \"codebertscore-similarity\":\n",
    "    PROJECT_ROOT = SCRIPT_DIR.parent.parent\n",
    "else:\n",
    "    PROJECT_ROOT = SCRIPT_DIR\n",
    "\n",
    "CODE_BERT_PACKAGE = PROJECT_ROOT / \"codebertscore-similarity\"\n",
    "sys.path.insert(0, str(CODE_BERT_PACKAGE))\n",
    "\n",
    "# Per-script configuration matching the original files\n",
    "CONFIGS = {\n",
    "    \"psg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/TFLite_detection_video.py\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/psg/*.py\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_psg.csv\",\n",
    "        \"lang\": \"python\",\n",
    "    },\n",
    "    \"sg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/object_color_classify.ino\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/sg/*.ino\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_sg.csv\",\n",
    "        \"lang\": \"c_sharp\",\n",
    "    },\n",
    "    \"tpusg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/TFLite_detection_video_TPU.py\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/tpusg/*.py\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_tpusg.csv\",\n",
    "        \"lang\": \"python\",\n",
    "    },\n",
    "}\n",
    "\n",
    "CANDIDATE_ROOT = PROJECT_ROOT / \"data_analysis/2026\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Dependencies and Utilities\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import code_bert_score\n",
    "\n",
    "\n",
    "def load_file_content(file_path: Path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_model_name(candidate_path: Path):\n",
    "    # .../<run>/exported_valid_code/<variant>/file.ext\n",
    "    try:\n",
    "        return candidate_path.parents[2].name\n",
    "    except IndexError:\n",
    "        return candidate_path.stem\n",
    "\n",
    "\n",
    "def ensure_header(csv_path: Path, columns):\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not csv_path.exists():\n",
    "        pd.DataFrame(columns=columns).to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "def get_processed_ids(csv_path: Path):\n",
    "    if not csv_path.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if \"Candidate_ID\" in df.columns:\n",
    "            return set(df[\"Candidate_ID\"].astype(str))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing CSV {csv_path}: {e}\")\n",
    "    return set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Reference File and Discover Candidates\n",
    "references = {}\n",
    "candidate_sets = {}\n",
    "\n",
    "for key, cfg in CONFIGS.items():\n",
    "    ref_content = load_file_content(cfg[\"reference\"])\n",
    "    if not ref_content:\n",
    "        print(f\"Failed to load reference for {key}: {cfg['reference']}\")\n",
    "        continue\n",
    "    references[key] = ref_content\n",
    "    candidate_files = sorted(CANDIDATE_ROOT.glob(cfg[\"candidate_glob\"]))\n",
    "    candidate_sets[key] = candidate_files\n",
    "    print(f\"[{key}] reference loaded; candidates: {len(candidate_files)}\")\n",
    "\n",
    "# Device selection\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Run Similarity Scoring Loop (All Three Scripts)\n",
    "results = {}\n",
    "common_columns = [\n",
    "    \"Candidate_ID\",\n",
    "    \"Candidate_Path\",\n",
    "    \"Model\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    # \"Reference_File\",\n",
    "    # \"Language\",\n",
    "    \"Processor\",\n",
    "    # \"Timestamp\",\n",
    "]\n",
    "\n",
    "for key, cfg in CONFIGS.items():\n",
    "    ensure_header(cfg[\"output_csv\"], common_columns)\n",
    "    processed_ids = get_processed_ids(cfg[\"output_csv\"])\n",
    "    candidate_files = candidate_sets.get(key, [])\n",
    "    ref_content = references.get(key)\n",
    "    if not ref_content:\n",
    "        continue\n",
    "\n",
    "    loop_records = []\n",
    "    for candidate_path in candidate_files:\n",
    "        candidate_id = candidate_path.relative_to(PROJECT_ROOT).as_posix()\n",
    "        if candidate_id in processed_ids:\n",
    "            continue\n",
    "\n",
    "        candidate_content = load_file_content(candidate_path)\n",
    "        if not candidate_content:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            P, R, F1, F3 = code_bert_score.score(\n",
    "                cands=[candidate_content],\n",
    "                refs=[ref_content],\n",
    "                lang=cfg[\"lang\"],\n",
    "                device=DEVICE,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            p_val = P[0].item()\n",
    "            r_val = R[0].item()\n",
    "            f1_val = F1[0].item()\n",
    "            f3_val = F3[0].item()\n",
    "\n",
    "            model_name = extract_model_name(candidate_path)\n",
    "            timestamp = datetime.now().isoformat()\n",
    "\n",
    "            record = {\n",
    "                \"Candidate_ID\": candidate_id,\n",
    "                \"Candidate_Path\": candidate_path.as_posix(),\n",
    "                \"Model\": model_name,\n",
    "                \"Precision\": p_val,\n",
    "                \"Recall\": r_val,\n",
    "                \"F1\": f1_val,\n",
    "                \"F3\": f3_val,\n",
    "                # \"Reference_File\": cfg[\"reference\"],\n",
    "                # \"Language\": cfg[\"lang\"],\n",
    "                \"Processor\": key,\n",
    "                # \"Timestamp\": timestamp,\n",
    "            }\n",
    "            loop_records.append(record)\n",
    "        except Exception as e:\n",
    "            print(f\"[{key}] Error processing {candidate_path}: {e}\")\n",
    "\n",
    "    if loop_records:\n",
    "        df_loop = pd.DataFrame(loop_records)\n",
    "        df_loop.to_csv(cfg[\"output_csv\"], mode=\"a\", header=False, index=False)\n",
    "        results[key] = df_loop\n",
    "        print(f\"[{key}] wrote {len(df_loop)} rows to {cfg['output_csv']}\")\n",
    "    else:\n",
    "        print(f\"[{key}] no new rows written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Append and Merge CSV Outputs\n",
    "merged = []\n",
    "for key, cfg in CONFIGS.items():\n",
    "    if cfg[\"output_csv\"].exists():\n",
    "        try:\n",
    "            df = pd.read_csv(cfg[\"output_csv\"])\n",
    "            df[\"run\"] = key\n",
    "            merged.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[{key}] Error reading {cfg['output_csv']}: {e}\")\n",
    "\n",
    "if merged:\n",
    "    combined = pd.concat(merged, ignore_index=True)\n",
    "    print(f\"Combined rows: {len(combined)}\")\n",
    "else:\n",
    "    combined = pd.DataFrame()\n",
    "    print(\"No CSVs to combine yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Quick Summary and Verification\n",
    "if not combined.empty:\n",
    "    display(combined.head())\n",
    "    display(combined.tail())\n",
    "    print(\"Counts by run:\")\n",
    "    print(combined.groupby(\"run\")[\"Candidate_ID\"].count())\n",
    "    print(\"F1 stats:\")\n",
    "    print(combined[\"F1\"].describe())\n",
    "else:\n",
    "    print(\"No data to summarize yet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
