{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ababb9",
   "metadata": {},
   "source": [
    "# CodeBERT Similarity Calculations\n",
    "A consolidated notebook to run PSG, SG, and TPU-SG similarity scoring and persist results.\n",
    "\n",
    "**Sections**\n",
    "1. Configure Paths and Environment\n",
    "2. Import Dependencies and Utilities\n",
    "3. Load Reference File and Discover Candidates\n",
    "4. Run Similarity Scoring Loop (All Three Scripts)\n",
    "5. Append and Merge CSV Outputs\n",
    "6. Quick Summary and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62f0beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configure Paths and Environment\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if \"__file__\" in globals():\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "else:\n",
    "    # Notebook execution: assume cwd is the repo root\n",
    "    SCRIPT_DIR = Path.cwd() / \"codebertscore-similarity/analyser-results\"\n",
    "    if not SCRIPT_DIR.exists():\n",
    "        # Fallback to cwd if running from inside analyser-results already\n",
    "        SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "# Derive project root based on known layout\n",
    "if SCRIPT_DIR.name == \"analyser-results\" and SCRIPT_DIR.parent.name == \"codebertscore-similarity\":\n",
    "    PROJECT_ROOT = SCRIPT_DIR.parent.parent\n",
    "else:\n",
    "    PROJECT_ROOT = SCRIPT_DIR\n",
    "\n",
    "CODE_BERT_PACKAGE = PROJECT_ROOT / \"codebertscore-similarity\"\n",
    "sys.path.insert(0, str(CODE_BERT_PACKAGE))\n",
    "\n",
    "# Per-script configuration matching the original files\n",
    "CONFIGS = {\n",
    "    \"psg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/TFLite_detection_video.py\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/psg/*.py\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_psg.csv\",\n",
    "        \"lang\": \"python\",\n",
    "    },\n",
    "    \"sg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/object_color_classify.ino\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/sg/*.ino\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_sg.csv\",\n",
    "        \"lang\": \"c_sharp\",\n",
    "    },\n",
    "    \"tpusg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/TFLite_detection_video_TPU.py\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/tpusg/*.py\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_tpusg.csv\",\n",
    "        \"lang\": \"python\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Include both 2025 and 2026 runs under data_analysis\n",
    "CANDIDATE_ROOT = PROJECT_ROOT / \"data_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a73db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Dependencies and Utilities\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import code_bert_score\n",
    "\n",
    "\n",
    "def load_file_content(file_path: Path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_model_name(candidate_path: Path):\n",
    "    # .../<run>/exported_valid_code/<variant>/file.ext\n",
    "    try:\n",
    "        return candidate_path.parents[2].name\n",
    "    except IndexError:\n",
    "        return candidate_path.stem\n",
    "\n",
    "\n",
    "def ensure_header(csv_path: Path, columns):\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not csv_path.exists():\n",
    "        pd.DataFrame(columns=columns).to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "def get_processed_ids(csv_path: Path):\n",
    "    if not csv_path.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[\"Candidate_ID\"])\n",
    "        if \"Candidate_ID\" in df.columns:\n",
    "            return set(df[\"Candidate_ID\"].astype(str).dropna())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing CSV {csv_path}: {e}\")\n",
    "    return set()\n",
    "\n",
    "\n",
    "def get_processed_paths(csv_path: Path):\n",
    "    if not csv_path.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[\"Candidate_Path\"])\n",
    "        if \"Candidate_Path\" in df.columns:\n",
    "            return set(df[\"Candidate_Path\"].astype(str).dropna())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing CSV {csv_path}: {e}\")\n",
    "    return set()\n",
    "\n",
    "\n",
    "def parse_model_fields(raw_model):\n",
    "    \"\"\"Extract base model (phi4|qw32|gpt5) and prompt_level (abla-l1|abla-l2|original) from raw model text.\"\"\"\n",
    "    text = str(raw_model)\n",
    "    if \"_\" in text:\n",
    "        _, text = text.split(\"_\", 1)\n",
    "    prompt_level = \"abla-l1\" if \"abla-l1\" in text else \"abla-l2\" if \"abla-l2\" in text else \"original\"\n",
    "    base_model = next((m for m in (\"phi4\", \"qw32\", \"gpt5\") if m in text), text.split(\"-\")[0])\n",
    "    return base_model, prompt_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bfa9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[psg] reference loaded; candidates: 217\n",
      "[sg] reference loaded; candidates: 54\n",
      "[tpusg] reference loaded; candidates: 228\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 3. Load Reference File and Discover Candidates\n",
    "references = {}\n",
    "candidate_sets = {}\n",
    "\n",
    "for key, cfg in CONFIGS.items():\n",
    "    ref_content = load_file_content(cfg[\"reference\"])\n",
    "    if not ref_content:\n",
    "        print(f\"Failed to load reference for {key}: {cfg['reference']}\")\n",
    "        continue\n",
    "    references[key] = ref_content\n",
    "    # search across all runs under data_analysis (e.g., 2025, 2026, etc.)\n",
    "    candidate_files = sorted(CANDIDATE_ROOT.glob(f\"*/{cfg['candidate_glob']}\"))\n",
    "    candidate_sets[key] = candidate_files\n",
    "    print(f\"[{key}] reference loaded; candidates: {len(candidate_files)}\")\n",
    "\n",
    "# Device selection\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58dd0ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[psg] no new rows written\n",
      "[sg] no new rows written\n",
      "[tpusg] no new rows written\n"
     ]
    }
   ],
   "source": [
    "# 4. Run Similarity Scoring Loop (All Three Scripts)\n",
    "results = {}\n",
    "common_columns = [\n",
    "    \"Candidate_ID\",\n",
    "    \"Model\",\n",
    "    \"prompt_level\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    # \"Reference_File\",\n",
    "    # \"Language\",\n",
    "    \"Processor\",\n",
    "    # \"Timestamp\",\n",
    "]\n",
    "\n",
    "for key, cfg in CONFIGS.items():\n",
    "    ensure_header(cfg[\"output_csv\"], common_columns)\n",
    "    processed_ids = get_processed_ids(cfg[\"output_csv\"])\n",
    "    candidate_files = candidate_sets.get(key, [])\n",
    "    ref_content = references.get(key)\n",
    "    if not ref_content:\n",
    "        continue\n",
    "\n",
    "    loop_records = []\n",
    "    for candidate_path in candidate_files:\n",
    "        candidate_id = candidate_path.relative_to(PROJECT_ROOT).as_posix()\n",
    "        if candidate_id in processed_ids:\n",
    "            continue\n",
    "\n",
    "        candidate_content = load_file_content(candidate_path)\n",
    "        if not candidate_content:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            P, R, F1, F3 = code_bert_score.score(\n",
    "                cands=[candidate_content],\n",
    "                refs=[ref_content],\n",
    "                lang=cfg[\"lang\"],\n",
    "                device=DEVICE,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            p_val = P[0].item()\n",
    "            r_val = R[0].item()\n",
    "            f1_val = F1[0].item()\n",
    "            f3_val = F3[0].item()\n",
    "\n",
    "            model_name = extract_model_name(candidate_path)\n",
    "            base_model, prompt_level = parse_model_fields(model_name)\n",
    "            timestamp = datetime.now().isoformat()\n",
    "\n",
    "            record = {\n",
    "                \"Candidate_ID\": candidate_id,\n",
    "                \"Model\": base_model,\n",
    "                \"prompt_level\": prompt_level,\n",
    "                \"Precision\": p_val,\n",
    "                \"Recall\": r_val,\n",
    "                \"F1\": f1_val,\n",
    "                \"F3\": f3_val,\n",
    "                # \"Reference_File\": cfg[\"reference\"],\n",
    "                # \"Language\": cfg[\"lang\"],\n",
    "                \"Processor\": key,\n",
    "                # \"Timestamp\": timestamp,\n",
    "            }\n",
    "            loop_records.append(record)\n",
    "        except Exception as e:\n",
    "            print(f\"[{key}] Error processing {candidate_path}: {e}\")\n",
    "\n",
    "    if loop_records:\n",
    "        df_loop = pd.DataFrame(loop_records)\n",
    "        df_loop.to_csv(cfg[\"output_csv\"], mode=\"a\", header=False, index=False)\n",
    "        results[key] = df_loop\n",
    "        print(f\"[{key}] wrote {len(df_loop)} rows to {cfg['output_csv']}\")\n",
    "    else:\n",
    "        print(f\"[{key}] no new rows written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95b7ccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined rows: 499\n",
      "Run counts (candidates -> csv rows):\n",
      "psg: 217 candidates; 217 rows in csv\n",
      "sg: 54 candidates; 54 rows in csv\n",
      "tpusg: 228 candidates; 228 rows in csv\n",
      "Totals discovered: 499 candidates across runs; 499 rows across csvs\n"
     ]
    }
   ],
   "source": [
    "# 5. Append and Merge CSV Outputs\n",
    "merged = []\n",
    "run_candidate_counts = {k: len(v) for k, v in candidate_sets.items()}\n",
    "output_row_counts = {}\n",
    "for key, cfg in CONFIGS.items():\n",
    "    if cfg[\"output_csv\"].exists():\n",
    "        try:\n",
    "            df = pd.read_csv(cfg[\"output_csv\"])\n",
    "            df[\"run\"] = key\n",
    "            merged.append(df)\n",
    "            output_row_counts[key] = len(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[{key}] Error reading {cfg['output_csv']}: {e}\")\n",
    "\n",
    "if merged:\n",
    "    combined = pd.concat(merged, ignore_index=True)\n",
    "    print(f\"Combined rows: {len(combined)}\")\n",
    "else:\n",
    "    combined = pd.DataFrame()\n",
    "    print(\"No CSVs to combine yet.\")\n",
    "\n",
    "print(\"Run counts (candidates -> csv rows):\")\n",
    "for key in CONFIGS:\n",
    "    cand = run_candidate_counts.get(key, 0)\n",
    "    rows = output_row_counts.get(key, 0)\n",
    "    print(f\"{key}: {cand} candidates; {rows} rows in csv\")\n",
    "total_candidates = sum(run_candidate_counts.values())\n",
    "total_rows = sum(output_row_counts.values())\n",
    "print(f\"Totals discovered: {total_candidates} candidates across runs; {total_rows} rows across csvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "424b667a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F3</th>\n",
       "      <th>Processor</th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_analysis/2026/02.03_gpt5-mpu/exported_val...</td>\n",
       "      <td>gpt5</td>\n",
       "      <td>0.792295</td>\n",
       "      <td>0.867710</td>\n",
       "      <td>0.828290</td>\n",
       "      <td>0.859529</td>\n",
       "      <td>psg</td>\n",
       "      <td>original</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_analysis/2026/02.03_gpt5-mpu/exported_val...</td>\n",
       "      <td>gpt5</td>\n",
       "      <td>0.784481</td>\n",
       "      <td>0.873618</td>\n",
       "      <td>0.826654</td>\n",
       "      <td>0.863803</td>\n",
       "      <td>psg</td>\n",
       "      <td>original</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_analysis/2026/02.03_gpt5-mpu/exported_val...</td>\n",
       "      <td>gpt5</td>\n",
       "      <td>0.784981</td>\n",
       "      <td>0.876138</td>\n",
       "      <td>0.828058</td>\n",
       "      <td>0.866081</td>\n",
       "      <td>psg</td>\n",
       "      <td>original</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_analysis/2026/02.03_gpt5-mpu/exported_val...</td>\n",
       "      <td>gpt5</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.864467</td>\n",
       "      <td>0.820617</td>\n",
       "      <td>0.855326</td>\n",
       "      <td>psg</td>\n",
       "      <td>original</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_analysis/2026/02.03_gpt5-mpu/exported_val...</td>\n",
       "      <td>gpt5</td>\n",
       "      <td>0.777338</td>\n",
       "      <td>0.871617</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.861172</td>\n",
       "      <td>psg</td>\n",
       "      <td>original</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Candidate_ID Model  Precision  \\\n",
       "0  data_analysis/2026/02.03_gpt5-mpu/exported_val...  gpt5   0.792295   \n",
       "1  data_analysis/2026/02.03_gpt5-mpu/exported_val...  gpt5   0.784481   \n",
       "2  data_analysis/2026/02.03_gpt5-mpu/exported_val...  gpt5   0.784981   \n",
       "3  data_analysis/2026/02.03_gpt5-mpu/exported_val...  gpt5   0.781000   \n",
       "4  data_analysis/2026/02.03_gpt5-mpu/exported_val...  gpt5   0.777338   \n",
       "\n",
       "     Recall        F1        F3 Processor prompt_type  run  \n",
       "0  0.867710  0.828290  0.859529       psg    original  psg  \n",
       "1  0.873618  0.826654  0.863803       psg    original  psg  \n",
       "2  0.876138  0.828058  0.866081       psg    original  psg  \n",
       "3  0.864467  0.820617  0.855326       psg    original  psg  \n",
       "4  0.871617  0.821782  0.861172       psg    original  psg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F3</th>\n",
       "      <th>Processor</th>\n",
       "      <th>prompt_type</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...</td>\n",
       "      <td>qw32</td>\n",
       "      <td>0.902264</td>\n",
       "      <td>0.842597</td>\n",
       "      <td>0.871410</td>\n",
       "      <td>0.848207</td>\n",
       "      <td>tpusg</td>\n",
       "      <td>original</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...</td>\n",
       "      <td>qw32</td>\n",
       "      <td>0.894800</td>\n",
       "      <td>0.828178</td>\n",
       "      <td>0.860201</td>\n",
       "      <td>0.834390</td>\n",
       "      <td>tpusg</td>\n",
       "      <td>original</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...</td>\n",
       "      <td>qw32</td>\n",
       "      <td>0.896713</td>\n",
       "      <td>0.831326</td>\n",
       "      <td>0.862782</td>\n",
       "      <td>0.837433</td>\n",
       "      <td>tpusg</td>\n",
       "      <td>original</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...</td>\n",
       "      <td>qw32</td>\n",
       "      <td>0.892663</td>\n",
       "      <td>0.829386</td>\n",
       "      <td>0.859862</td>\n",
       "      <td>0.835307</td>\n",
       "      <td>tpusg</td>\n",
       "      <td>original</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...</td>\n",
       "      <td>qw32</td>\n",
       "      <td>0.896405</td>\n",
       "      <td>0.835720</td>\n",
       "      <td>0.864999</td>\n",
       "      <td>0.841416</td>\n",
       "      <td>tpusg</td>\n",
       "      <td>original</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Candidate_ID Model  Precision  \\\n",
       "494  data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...  qw32   0.902264   \n",
       "495  data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...  qw32   0.894800   \n",
       "496  data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...  qw32   0.896713   \n",
       "497  data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...  qw32   0.892663   \n",
       "498  data_analysis/2025/08.03_abla-og-qw32-tpusg/ex...  qw32   0.896405   \n",
       "\n",
       "       Recall        F1        F3 Processor prompt_type    run  \n",
       "494  0.842597  0.871410  0.848207     tpusg    original  tpusg  \n",
       "495  0.828178  0.860201  0.834390     tpusg    original  tpusg  \n",
       "496  0.831326  0.862782  0.837433     tpusg    original  tpusg  \n",
       "497  0.829386  0.859862  0.835307     tpusg    original  tpusg  \n",
       "498  0.835720  0.864999  0.841416     tpusg    original  tpusg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts by run:\n",
      "run\n",
      "psg      217\n",
      "sg        54\n",
      "tpusg    228\n",
      "Name: Candidate_ID, dtype: int64\n",
      "F1 stats:\n",
      "count    499.000000\n",
      "mean       0.851895\n",
      "std        0.022798\n",
      "min        0.615672\n",
      "25%        0.838571\n",
      "50%        0.852422\n",
      "75%        0.868371\n",
      "max        0.889104\n",
      "Name: F1, dtype: float64\n",
      "Coverage (rows vs discovered candidates):\n",
      "psg: 217 rows vs 217 candidates\n",
      "sg: 54 rows vs 54 candidates\n",
      "tpusg: 228 rows vs 228 candidates\n",
      "Total candidates: 499; total rows: 499\n"
     ]
    }
   ],
   "source": [
    "# 6. Quick Summary and Verification\n",
    "if not combined.empty:\n",
    "    display(combined.head())\n",
    "    display(combined.tail())\n",
    "    print(\"Counts by run:\")\n",
    "    print(combined.groupby(\"run\")[\"Candidate_ID\"].count())\n",
    "    print(\"F1 stats:\")\n",
    "    print(combined[\"F1\"].describe())\n",
    "    discovered = {k: len(v) for k, v in candidate_sets.items()}\n",
    "    print(\"Coverage (rows vs discovered candidates):\")\n",
    "    for key in CONFIGS:\n",
    "        rows = len(combined[combined[\"run\"] == key])\n",
    "        print(f\"{key}: {rows} rows vs {discovered.get(key, 0)} candidates\")\n",
    "    print(f\"Total candidates: {sum(discovered.values())}; total rows: {len(combined)}\")\n",
    "else:\n",
    "    print(\"No data to summarize yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6665065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psg: cleaned 217 rows -> /home/han/Projects/benchmark-tinyml_llm-2026/codebertscore-similarity/analyser-results/similarity_results_psg.csv\n",
      "sg: cleaned 54 rows -> /home/han/Projects/benchmark-tinyml_llm-2026/codebertscore-similarity/analyser-results/similarity_results_sg.csv\n",
      "tpusg: cleaned 228 rows -> /home/han/Projects/benchmark-tinyml_llm-2026/codebertscore-similarity/analyser-results/similarity_results_tpusg.csv\n",
      "Backfill complete.\n"
     ]
    }
   ],
   "source": [
    "# 7. Backfill prompt_level and normalize Model for existing CSV outputs\n",
    "def backfill_outputs():\n",
    "    for c in [\"psg\", \"sg\", \"tpusg\"]:\n",
    "        csv_path = CONFIGS[c][\"output_csv\"]\n",
    "        if not csv_path.exists():\n",
    "            print(f\"{c}: {csv_path} missing, skipping\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if df.empty:\n",
    "            print(f\"{c}: {csv_path} empty, skipping\")\n",
    "            continue\n",
    "        # parse Model into base_model and prompt_level\n",
    "        parsed = df[\"Model\"].apply(parse_model_fields).apply(pd.Series)\n",
    "        df[\"Model\"] = parsed[0]\n",
    "        df[\"prompt_level\"] = parsed[1]\n",
    "        # drop Candidate_Path if present\n",
    "        if \"Candidate_Path\" in df.columns:\n",
    "            df = df.drop(columns=[\"Candidate_Path\"])\n",
    "        # ensure column order\n",
    "        ordered_cols = [\"Candidate_ID\", \"Model\", \"Precision\", \"Recall\", \"F1\", \"F3\", \"Processor\",\"prompt_level\", ]\n",
    "        df = df[[col for col in ordered_cols if col in df.columns]]\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"{c}: cleaned {len(df)} rows -> {csv_path}\")\n",
    "    print(\"Backfill complete.\")\n",
    "\n",
    "backfill_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb3a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
