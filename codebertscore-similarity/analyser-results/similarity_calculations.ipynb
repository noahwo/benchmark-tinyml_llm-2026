{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ababb9",
   "metadata": {},
   "source": [
    "# CodeBERT Similarity Calculations\n",
    "A consolidated notebook to run PSG, SG, and TPU-SG similarity scoring and persist results.\n",
    "\n",
    "**Sections**\n",
    "1. Configure Paths and Environment\n",
    "2. Import Dependencies and Utilities\n",
    "3. Load Reference File and Discover Candidates\n",
    "4. Run Similarity Scoring Loop (All Three Scripts)\n",
    "5. Append and Merge CSV Outputs\n",
    "6. Quick Summary and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configure Paths and Environment\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if \"__file__\" in globals():\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "else:\n",
    "    # Notebook execution: assume cwd is the repo root\n",
    "    SCRIPT_DIR = Path.cwd() / \"codebertscore-similarity/analyser-results\"\n",
    "    if not SCRIPT_DIR.exists():\n",
    "        # Fallback to cwd if running from inside analyser-results already\n",
    "        SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "# Derive project root based on known layout\n",
    "if SCRIPT_DIR.name == \"analyser-results\" and SCRIPT_DIR.parent.name == \"codebertscore-similarity\":\n",
    "    PROJECT_ROOT = SCRIPT_DIR.parent.parent\n",
    "else:\n",
    "    PROJECT_ROOT = SCRIPT_DIR\n",
    "\n",
    "CODE_BERT_PACKAGE = PROJECT_ROOT / \"codebertscore-similarity\"\n",
    "sys.path.insert(0, str(CODE_BERT_PACKAGE))\n",
    "\n",
    "# Per-script configuration matching the original files\n",
    "CONFIGS = {\n",
    "    \"psg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/TFLite_detection_video.py\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/psg/*.py\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_psg.csv\",\n",
    "        \"lang\": \"python\",\n",
    "    },\n",
    "    \"sg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/object_color_classify.ino\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/sg/*.ino\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_sg.csv\",\n",
    "        \"lang\": \"c_sharp\",\n",
    "    },\n",
    "    \"tpusg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/TFLite_detection_video_TPU.py\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/tpusg/*.py\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_tpusg.csv\",\n",
    "        \"lang\": \"python\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Include both 2025 and 2026 runs under langfuse_export\n",
    "CANDIDATE_ROOT = PROJECT_ROOT / \"langfuse_export\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a73db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Dependencies and Utilities\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import code_bert_score\n",
    "\n",
    "# Verify PyTorch version (must be 2.6+ for security fix CVE-2025-32434)\n",
    "torch_version = torch.__version__.split('+')[0]  # Remove +cu124 suffix\n",
    "torch_major, torch_minor = map(int, torch_version.split('.')[:2])\n",
    "if torch_major < 2 or (torch_major == 2 and torch_minor < 6):\n",
    "    print(\"â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\")\n",
    "    print(\"â•‘  âŒ ERROR: PyTorch version too old!                             â•‘\")\n",
    "    print(\"â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\")\n",
    "    print(f\"\\nCurrent version: {torch.__version__}\")\n",
    "    print(f\"Required version: â‰¥ 2.6.0\")\n",
    "    print()\n",
    "    print(\"PyTorch was upgraded in the terminal, but this kernel is still using the old version.\")\n",
    "    print()\n",
    "    print(\"ðŸ”§ FIX: Restart the kernel to load PyTorch 2.6.0\")\n",
    "    print(\"   1. Click 'Kernel' â†’ 'Restart Kernel' (or press Ctrl+Shift+P)\")\n",
    "    print(\"   2. Re-run all cells from the beginning\")\n",
    "    print()\n",
    "    raise RuntimeError(f\"PyTorch {torch.__version__} is too old. Need 2.6.0+ for CVE-2025-32434 fix. Restart kernel!\")\n",
    "\n",
    "print(f\"âœ“ PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Enable CUDA optimizations for better performance\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.set_grad_enabled(False)  # Disable gradients for inference-only\n",
    "    \n",
    "    # Get GPU info\n",
    "    gpu_name = torch.cuda.get_device_name()\n",
    "    compute_cap = torch.cuda.get_device_capability()\n",
    "    total_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    \n",
    "    print(f\"âœ“ GPU: {gpu_name}\")\n",
    "    print(f\"âœ“ Compute Capability: {compute_cap[0]}.{compute_cap[1]}\")\n",
    "    print(f\"âœ“ Memory: {total_memory_gb:.1f} GB\")\n",
    "    \n",
    "    # Enable TF32 on Ampere+ GPUs (RTX 30xx, A100, etc.) - Pascal/GTX 1060 doesn't support this\n",
    "    if compute_cap[0] >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "        print(\"âœ“ Enabled TF32 precision (Ampere+ GPU)\")\n",
    "    else:\n",
    "        print(f\"â„¹ TF32 not available (requires Ampere+ GPU, you have compute {compute_cap[0]}.{compute_cap[1]})\")\n",
    "    \n",
    "    print(\"âœ“ CUDA optimizations enabled\")\n",
    "\n",
    "\n",
    "def load_file_content(file_path: Path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_model_name(candidate_path: Path):\n",
    "    # .../<run>/exported_valid_code/<variant>/file.ext\n",
    "    try:\n",
    "        return candidate_path.parents[2].name\n",
    "    except IndexError:\n",
    "        return candidate_path.stem\n",
    "\n",
    "\n",
    "def ensure_header(csv_path: Path, columns):\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not csv_path.exists():\n",
    "        pd.DataFrame(columns=columns).to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "def get_processed_ids(csv_path: Path):\n",
    "    if not csv_path.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[\"Candidate_ID\"])\n",
    "        if \"Candidate_ID\" in df.columns:\n",
    "            return set(df[\"Candidate_ID\"].astype(str).dropna())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing CSV {csv_path}: {e}\")\n",
    "    return set()\n",
    "\n",
    "\n",
    "def get_processed_paths(csv_path: Path):\n",
    "    if not csv_path.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[\"Candidate_Path\"])\n",
    "        if \"Candidate_Path\" in df.columns:\n",
    "            return set(df[\"Candidate_Path\"].astype(str).dropna())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing CSV {csv_path}: {e}\")\n",
    "    return set()\n",
    "\n",
    "\n",
    "def parse_model_fields(raw_model):\n",
    "    \"\"\"Extract base model (phi4|qw32|gpt5) and prompt_level (abla-l1|abla-l2|original) from raw model text.\"\"\"\n",
    "    text = str(raw_model)\n",
    "    if \"_\" in text:\n",
    "        _, text = text.split(\"_\", 1)\n",
    "        \n",
    "    levels = [\"abla-l1\", \"abla-l2\", \"abla-1p\", \"abla-2p\"]\n",
    "    prompt_level = next((lvl for lvl in levels if lvl in text), \"original\")\n",
    "    base_model = next((m for m in (\"phi4\", \"qw32\", \"gpt5\") if m in text), text.split(\"-\")[0])\n",
    "    \n",
    "    # Normalize model names\n",
    "    model_mapping = {\n",
    "        \"gpt5\": \"gpt-5\",\n",
    "        \"qw32\": \"qwen32\",\n",
    "    }\n",
    "    base_model = model_mapping.get(base_model, base_model)\n",
    "    \n",
    "    return base_model, prompt_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfa9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Reference File and Discover Candidates\n",
    "references = {}\n",
    "candidate_sets = {}\n",
    "\n",
    "for key, cfg in CONFIGS.items():\n",
    "    ref_content = load_file_content(cfg[\"reference\"])\n",
    "    if not ref_content:\n",
    "        print(f\"Failed to load reference for {key}: {cfg['reference']}\")\n",
    "        continue\n",
    "    references[key] = ref_content\n",
    "    # search across all runs under langfuse_export (e.g., 2025, 2026, etc.)\n",
    "    candidate_files = sorted(CANDIDATE_ROOT.glob(f\"*/{cfg['candidate_glob']}\"))\n",
    "    candidate_sets[key] = candidate_files\n",
    "    print(f\"[{key}] reference loaded; candidates: {len(candidate_files)}\")\n",
    "\n",
    "# Device selection\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Batch size configuration based on GPU memory\n",
    "if DEVICE == \"cuda\":\n",
    "    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    compute_cap = torch.cuda.get_device_capability()\n",
    "    \n",
    "    # Auto-adjust batch size based on available VRAM\n",
    "    if total_mem_gb <= 6:\n",
    "        BATCH_SIZE = 20  # GTX 1060 6GB, GTX 1650, etc.\n",
    "        print(\"âš ï¸  6GB GPU detected - using batch_size=20 to prevent OOM\")\n",
    "    elif total_mem_gb <= 8:\n",
    "        BATCH_SIZE = 24  # 8GB cards\n",
    "    elif total_mem_gb <= 10:\n",
    "        BATCH_SIZE = 32  # RTX 3060 Ti, RTX 3080 (10GB)\n",
    "    elif total_mem_gb <= 12:\n",
    "        BATCH_SIZE = 48  # RTX 3060 (12GB)\n",
    "    else:\n",
    "        BATCH_SIZE = 64  # 16GB+ cards\n",
    "    \n",
    "    print(f\"Using device: {DEVICE} ({total_mem_gb:.1f}GB)\")\n",
    "    print(f\"Batch size: {BATCH_SIZE}\")\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "    print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dd0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Run Similarity Scoring Loop (All Three Scripts)\n",
    "results = {}\n",
    "common_columns = [\n",
    "    \"Candidate_ID\",\n",
    "    \"Model\",\n",
    "    \"Prompt_level\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    # \"Reference_File\",\n",
    "    # \"Language\",\n",
    "    \"Processor\",\n",
    "    # \"Timestamp\",\n",
    "]\n",
    "\n",
    "for key, cfg in CONFIGS.items():\n",
    "    ensure_header(cfg[\"output_csv\"], common_columns)\n",
    "    processed_ids = get_processed_ids(cfg[\"output_csv\"])\n",
    "    candidate_files = candidate_sets.get(key, [])\n",
    "    ref_content = references.get(key)\n",
    "    if not ref_content:\n",
    "        continue\n",
    "\n",
    "    loop_records = []\n",
    "    for candidate_path in candidate_files:\n",
    "        candidate_id = candidate_path.relative_to(PROJECT_ROOT).as_posix()\n",
    "        if candidate_id in processed_ids:\n",
    "            continue\n",
    "\n",
    "        candidate_content = load_file_content(candidate_path)\n",
    "        if not candidate_content:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            P, R, F1, F3 = code_bert_score.score(\n",
    "                cands=[candidate_content],\n",
    "                refs=[ref_content],\n",
    "                lang=cfg[\"lang\"],\n",
    "                device=DEVICE,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            p_val = P[0].item()\n",
    "            r_val = R[0].item()\n",
    "            f1_val = F1[0].item()\n",
    "            f3_val = F3[0].item()\n",
    "\n",
    "            model_name = extract_model_name(candidate_path)\n",
    "            base_model, prompt_level = parse_model_fields(model_name)\n",
    "            timestamp = datetime.now().isoformat()\n",
    "\n",
    "            record = {\n",
    "                \"Candidate_ID\": candidate_id,\n",
    "                \"Model\": base_model,\n",
    "                \"Prompt_level\": prompt_level,\n",
    "                \"Precision\": p_val,\n",
    "                \"Recall\": r_val,\n",
    "                \"F1\": f1_val,\n",
    "                \"F3\": f3_val,\n",
    "                # \"Reference_File\": cfg[\"reference\"],\n",
    "                # \"Language\": cfg[\"lang\"],\n",
    "                \"Processor\": key,\n",
    "                # \"Timestamp\": timestamp,\n",
    "            }\n",
    "            loop_records.append(record)\n",
    "        except Exception as e:\n",
    "            print(f\"[{key}] Error processing {candidate_path}: {e}\")\n",
    "\n",
    "    if loop_records:\n",
    "        df_loop = pd.DataFrame(loop_records)\n",
    "        df_loop.to_csv(cfg[\"output_csv\"], mode=\"a\", header=False, index=False)\n",
    "        results[key] = df_loop\n",
    "        print(f\"[{key}] wrote {len(df_loop)} rows to {cfg['output_csv']}\")\n",
    "    else:\n",
    "        print(f\"[{key}] no new rows written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b7ccd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Append and Merge CSV Outputs\n",
    "merged = []\n",
    "run_candidate_counts = {k: len(v) for k, v in candidate_sets.items()}\n",
    "output_row_counts = {}\n",
    "for key, cfg in CONFIGS.items():\n",
    "    if cfg[\"output_csv\"].exists():\n",
    "        try:\n",
    "            df = pd.read_csv(cfg[\"output_csv\"])\n",
    "            df[\"run\"] = key\n",
    "            merged.append(df)\n",
    "            output_row_counts[key] = len(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[{key}] Error reading {cfg['output_csv']}: {e}\")\n",
    "\n",
    "# 1. Filter out empty DataFrames to avoid the warning\n",
    "valid_merged = [df for df in merged if not df.empty]\n",
    "\n",
    "# 2. Check if there are any valid DataFrames left to concatenate\n",
    "if valid_merged:\n",
    "    combined = pd.concat(valid_merged, ignore_index=True)\n",
    "    print(f\"Combined rows: {len(combined)}\")\n",
    "else:\n",
    "    combined = pd.DataFrame()\n",
    "    print(\"No CSVs to combine yet.\")\n",
    "\n",
    "print(\"Run counts (candidates -> csv rows):\")\n",
    "for key in CONFIGS:\n",
    "    cand = run_candidate_counts.get(key, 0)\n",
    "    rows = output_row_counts.get(key, 0)\n",
    "    print(f\"{key}: {cand} candidates; {rows} rows in csv\")\n",
    "total_candidates = sum(run_candidate_counts.values())\n",
    "total_rows = sum(output_row_counts.values())\n",
    "print(f\"Totals discovered: {total_candidates} candidates across runs; {total_rows} rows across csvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424b667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Quick Summary and Verification\n",
    "if not combined.empty:\n",
    "    display(combined.head())\n",
    "    display(combined.tail())\n",
    "    print(\"Counts by run:\")\n",
    "    print(combined.groupby(\"run\")[\"Candidate_ID\"].count())\n",
    "    print(\"F1 stats:\")\n",
    "    print(combined[\"F1\"].describe())\n",
    "    discovered = {k: len(v) for k, v in candidate_sets.items()}\n",
    "    print(\"Coverage (rows vs discovered candidates):\")\n",
    "    for key in CONFIGS:\n",
    "        rows = len(combined[combined[\"run\"] == key])\n",
    "        print(f\"{key}: {rows} rows vs {discovered.get(key, 0)} candidates\")\n",
    "    print(f\"Total candidates: {sum(discovered.values())}; total rows: {len(combined)}\")\n",
    "else:\n",
    "    print(\"No data to summarize yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6665065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Backfill prompt_level and normalize Model for existing CSV outputs\n",
    "\n",
    "def parse_from_candidate_id(candidate_id: str):\n",
    "    \"\"\"Derive base model and prompt level from the Candidate_ID path token.\"\"\"\n",
    "    text = str(candidate_id)\n",
    "    run_segment = next(\n",
    "        (seg for seg in Path(text).parts if \"abla\" in seg or \"original\" in seg or \"og\" in seg),\n",
    "        text,\n",
    "    )\n",
    "    return parse_model_fields(run_segment)\n",
    "\n",
    "def backfill_outputs():\n",
    "    for c in [\"psg\", \"sg\", \"tpusg\"]:\n",
    "        csv_path = CONFIGS[c][\"output_csv\"]\n",
    "        if not csv_path.exists():\n",
    "            print(f\"{c}: {csv_path} missing, skipping\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if df.empty:\n",
    "            print(f\"{c}: {csv_path} empty, skipping\")\n",
    "            continue\n",
    "        if \"Candidate_ID\" not in df.columns:\n",
    "            print(f\"{c}: Candidate_ID missing in {csv_path}, skipping backfill\")\n",
    "            continue\n",
    "        base_prompt = df[\"Candidate_ID\"].apply(parse_from_candidate_id).apply(pd.Series)\n",
    "        df[\"Model\"] = base_prompt[0]\n",
    "        df[\"Prompt_level\"] = base_prompt[1]\n",
    "        if \"Candidate_Path\" in df.columns:\n",
    "            df = df.drop(columns=[\"Candidate_Path\"])\n",
    "        ordered_cols = [\n",
    "            \"Candidate_ID\",\n",
    "            \"Model\",\n",
    "            \"Precision\",\n",
    "            \"Recall\",\n",
    "            \"F1\",\n",
    "            \"F3\",\n",
    "            \"Processor\",\n",
    "            \"Prompt_level\",\n",
    "        ]\n",
    "        df = df[[col for col in ordered_cols if col in df.columns]]\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"{c}: cleaned {len(df)} rows -> {csv_path}\")\n",
    "    print(\"Backfill complete.\")\n",
    "\n",
    "backfill_outputs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb3a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
