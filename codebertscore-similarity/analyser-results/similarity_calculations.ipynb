{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ababb9",
   "metadata": {},
   "source": [
    "# CodeBERT Similarity Calculations\n",
    "A consolidated notebook to run PSG, SG, and TPU-SG similarity scoring and persist results.\n",
    "\n",
    "**Sections**\n",
    "1. Configure Paths and Environment\n",
    "2. Import Dependencies and Utilities\n",
    "3. Load Reference File and Discover Candidates\n",
    "4. Run Similarity Scoring Loop (All Three Scripts)\n",
    "5. Append and Merge CSV Outputs\n",
    "6. Quick Summary and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f0beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Configure Paths and Environment\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "if \"__file__\" in globals():\n",
    "    SCRIPT_DIR = Path(__file__).resolve().parent\n",
    "else:\n",
    "    # Notebook execution: assume cwd is the repo root\n",
    "    SCRIPT_DIR = Path.cwd() / \"codebertscore-similarity/analyser-results\"\n",
    "    if not SCRIPT_DIR.exists():\n",
    "        # Fallback to cwd if running from inside analyser-results already\n",
    "        SCRIPT_DIR = Path.cwd()\n",
    "\n",
    "# Derive project root based on known layout\n",
    "if SCRIPT_DIR.name == \"analyser-results\" and SCRIPT_DIR.parent.name == \"codebertscore-similarity\":\n",
    "    PROJECT_ROOT = SCRIPT_DIR.parent.parent\n",
    "else:\n",
    "    PROJECT_ROOT = SCRIPT_DIR\n",
    "\n",
    "CODE_BERT_PACKAGE = PROJECT_ROOT / \"codebertscore-similarity\"\n",
    "sys.path.insert(0, str(CODE_BERT_PACKAGE))\n",
    "\n",
    "# Per-script configuration matching the original files\n",
    "CONFIGS = {\n",
    "    \"psg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/TFLite_detection_video.py\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/psg/*.py\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_psg.csv\",\n",
    "        \"lang\": \"python\",\n",
    "    },\n",
    "    \"sg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/object_color_classify.ino\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/sg/*.ino\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_sg.csv\",\n",
    "        \"lang\": \"c_sharp\",\n",
    "    },\n",
    "    \"tpusg\": {\n",
    "        \"reference\": PROJECT_ROOT / \"codebertscore-similarity/references/TFLite_detection_video_TPU.py\",\n",
    "        \"candidate_glob\": \"*/exported_valid_code/tpusg/*.py\",\n",
    "        \"output_csv\": PROJECT_ROOT / \"codebertscore-similarity/analyser-results/similarity_results_tpusg.csv\",\n",
    "        \"lang\": \"python\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Include both 2025 and 2026 runs under langfuse_export\n",
    "CANDIDATE_ROOT = PROJECT_ROOT / \"langfuse_export\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a73db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Import Dependencies and Utilities\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import torch\n",
    "import code_bert_score\n",
    "\n",
    "\n",
    "def load_file_content(file_path: Path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_model_name(candidate_path: Path):\n",
    "    # .../<run>/exported_valid_code/<variant>/file.ext\n",
    "    try:\n",
    "        return candidate_path.parents[2].name\n",
    "    except IndexError:\n",
    "        return candidate_path.stem\n",
    "\n",
    "\n",
    "def ensure_header(csv_path: Path, columns):\n",
    "    csv_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not csv_path.exists():\n",
    "        pd.DataFrame(columns=columns).to_csv(csv_path, index=False)\n",
    "\n",
    "\n",
    "def get_processed_ids(csv_path: Path):\n",
    "    if not csv_path.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[\"Candidate_ID\"])\n",
    "        if \"Candidate_ID\" in df.columns:\n",
    "            return set(df[\"Candidate_ID\"].astype(str).dropna())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing CSV {csv_path}: {e}\")\n",
    "    return set()\n",
    "\n",
    "\n",
    "def get_processed_paths(csv_path: Path):\n",
    "    if not csv_path.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, usecols=[\"Candidate_Path\"])\n",
    "        if \"Candidate_Path\" in df.columns:\n",
    "            return set(df[\"Candidate_Path\"].astype(str).dropna())\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading existing CSV {csv_path}: {e}\")\n",
    "    return set()\n",
    "\n",
    "\n",
    "def parse_model_fields(raw_model):\n",
    "    \"\"\"Extract base model (phi4|qw32|gpt5) and prompt_level (abla-l1|abla-l2|original) from raw model text.\"\"\"\n",
    "    text = str(raw_model)\n",
    "    if \"_\" in text:\n",
    "        _, text = text.split(\"_\", 1)\n",
    "        \n",
    "    levels = [\"abla-l1\", \"abla-l2\", \"abla-1p\", \"abla-2p\"]\n",
    "    prompt_level = next((lvl for lvl in levels if lvl in text), \"original\")\n",
    "    base_model = next((m for m in (\"phi4\", \"qw32\", \"gpt5\") if m in text), text.split(\"-\")[0])\n",
    "    \n",
    "    # Normalize model names\n",
    "    model_mapping = {\n",
    "        \"gpt5\": \"gpt-5\",\n",
    "        \"qw32\": \"qwen32\",\n",
    "    }\n",
    "    base_model = model_mapping.get(base_model, base_model)\n",
    "    \n",
    "    return base_model, prompt_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bfa9797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[psg] reference loaded; candidates: 276\n",
      "[sg] reference loaded; candidates: 54\n",
      "[tpusg] reference loaded; candidates: 306\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 3. Load Reference File and Discover Candidates\n",
    "references = {}\n",
    "candidate_sets = {}\n",
    "\n",
    "for key, cfg in CONFIGS.items():\n",
    "    ref_content = load_file_content(cfg[\"reference\"])\n",
    "    if not ref_content:\n",
    "        print(f\"Failed to load reference for {key}: {cfg['reference']}\")\n",
    "        continue\n",
    "    references[key] = ref_content\n",
    "    # search across all runs under langfuse_export (e.g., 2025, 2026, etc.)\n",
    "    candidate_files = sorted(CANDIDATE_ROOT.glob(f\"*/{cfg['candidate_glob']}\"))\n",
    "    candidate_sets[key] = candidate_files\n",
    "    print(f\"[{key}] reference loaded; candidates: {len(candidate_files)}\")\n",
    "\n",
    "# Device selection\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58dd0ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[psg] no new rows written\n",
      "[sg] no new rows written\n",
      "[tpusg] no new rows written\n"
     ]
    }
   ],
   "source": [
    "# 4. Run Similarity Scoring Loop (All Three Scripts)\n",
    "results = {}\n",
    "common_columns = [\n",
    "    \"Candidate_ID\",\n",
    "    \"Model\",\n",
    "    \"prompt_level\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"F1\",\n",
    "    \"F3\",\n",
    "    # \"Reference_File\",\n",
    "    # \"Language\",\n",
    "    \"Processor\",\n",
    "    # \"Timestamp\",\n",
    "]\n",
    "\n",
    "for key, cfg in CONFIGS.items():\n",
    "    ensure_header(cfg[\"output_csv\"], common_columns)\n",
    "    processed_ids = get_processed_ids(cfg[\"output_csv\"])\n",
    "    candidate_files = candidate_sets.get(key, [])\n",
    "    ref_content = references.get(key)\n",
    "    if not ref_content:\n",
    "        continue\n",
    "\n",
    "    loop_records = []\n",
    "    for candidate_path in candidate_files:\n",
    "        candidate_id = candidate_path.relative_to(PROJECT_ROOT).as_posix()\n",
    "        if candidate_id in processed_ids:\n",
    "            continue\n",
    "\n",
    "        candidate_content = load_file_content(candidate_path)\n",
    "        if not candidate_content:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            P, R, F1, F3 = code_bert_score.score(\n",
    "                cands=[candidate_content],\n",
    "                refs=[ref_content],\n",
    "                lang=cfg[\"lang\"],\n",
    "                device=DEVICE,\n",
    "                verbose=False,\n",
    "            )\n",
    "\n",
    "            p_val = P[0].item()\n",
    "            r_val = R[0].item()\n",
    "            f1_val = F1[0].item()\n",
    "            f3_val = F3[0].item()\n",
    "\n",
    "            model_name = extract_model_name(candidate_path)\n",
    "            base_model, prompt_level = parse_model_fields(model_name)\n",
    "            timestamp = datetime.now().isoformat()\n",
    "\n",
    "            record = {\n",
    "                \"Candidate_ID\": candidate_id,\n",
    "                \"Model\": base_model,\n",
    "                \"prompt_level\": prompt_level,\n",
    "                \"Precision\": p_val,\n",
    "                \"Recall\": r_val,\n",
    "                \"F1\": f1_val,\n",
    "                \"F3\": f3_val,\n",
    "                # \"Reference_File\": cfg[\"reference\"],\n",
    "                # \"Language\": cfg[\"lang\"],\n",
    "                \"Processor\": key,\n",
    "                # \"Timestamp\": timestamp,\n",
    "            }\n",
    "            loop_records.append(record)\n",
    "        except Exception as e:\n",
    "            print(f\"[{key}] Error processing {candidate_path}: {e}\")\n",
    "\n",
    "    if loop_records:\n",
    "        df_loop = pd.DataFrame(loop_records)\n",
    "        df_loop.to_csv(cfg[\"output_csv\"], mode=\"a\", header=False, index=False)\n",
    "        results[key] = df_loop\n",
    "        print(f\"[{key}] wrote {len(df_loop)} rows to {cfg['output_csv']}\")\n",
    "    else:\n",
    "        print(f\"[{key}] no new rows written\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95b7ccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined rows: 636\n",
      "Run counts (candidates -> csv rows):\n",
      "psg: 276 candidates; 276 rows in csv\n",
      "sg: 54 candidates; 54 rows in csv\n",
      "tpusg: 306 candidates; 306 rows in csv\n",
      "Totals discovered: 636 candidates across runs; 636 rows across csvs\n"
     ]
    }
   ],
   "source": [
    "# 5. Append and Merge CSV Outputs\n",
    "merged = []\n",
    "run_candidate_counts = {k: len(v) for k, v in candidate_sets.items()}\n",
    "output_row_counts = {}\n",
    "for key, cfg in CONFIGS.items():\n",
    "    if cfg[\"output_csv\"].exists():\n",
    "        try:\n",
    "            df = pd.read_csv(cfg[\"output_csv\"])\n",
    "            df[\"run\"] = key\n",
    "            merged.append(df)\n",
    "            output_row_counts[key] = len(df)\n",
    "        except Exception as e:\n",
    "            print(f\"[{key}] Error reading {cfg['output_csv']}: {e}\")\n",
    "\n",
    "if merged:\n",
    "    combined = pd.concat(merged, ignore_index=True)\n",
    "    print(f\"Combined rows: {len(combined)}\")\n",
    "else:\n",
    "    combined = pd.DataFrame()\n",
    "    print(\"No CSVs to combine yet.\")\n",
    "\n",
    "print(\"Run counts (candidates -> csv rows):\")\n",
    "for key in CONFIGS:\n",
    "    cand = run_candidate_counts.get(key, 0)\n",
    "    rows = output_row_counts.get(key, 0)\n",
    "    print(f\"{key}: {cand} candidates; {rows} rows in csv\")\n",
    "total_candidates = sum(run_candidate_counts.values())\n",
    "total_rows = sum(output_row_counts.values())\n",
    "print(f\"Totals discovered: {total_candidates} candidates across runs; {total_rows} rows across csvs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "424b667a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F3</th>\n",
       "      <th>Processor</th>\n",
       "      <th>prompt_level</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>langfuse_export/2026/02.03_gpt5-mpu/exported_v...</td>\n",
       "      <td>gpt5</td>\n",
       "      <td>0.7922946810722351</td>\n",
       "      <td>0.867710</td>\n",
       "      <td>0.828290</td>\n",
       "      <td>0.859529</td>\n",
       "      <td>psg</td>\n",
       "      <td>original</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>langfuse_export/2026/02.03_gpt5-mpu/exported_v...</td>\n",
       "      <td>gpt5</td>\n",
       "      <td>0.7844812870025635</td>\n",
       "      <td>0.873618</td>\n",
       "      <td>0.826654</td>\n",
       "      <td>0.863803</td>\n",
       "      <td>psg</td>\n",
       "      <td>original</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>langfuse_export/2026/02.03_gpt5-mpu/exported_v...</td>\n",
       "      <td>gpt5</td>\n",
       "      <td>0.7849806547164917</td>\n",
       "      <td>0.876138</td>\n",
       "      <td>0.828058</td>\n",
       "      <td>0.866081</td>\n",
       "      <td>psg</td>\n",
       "      <td>original</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>langfuse_export/2026/02.03_gpt5-mpu/exported_v...</td>\n",
       "      <td>gpt5</td>\n",
       "      <td>0.7809998989105225</td>\n",
       "      <td>0.864467</td>\n",
       "      <td>0.820617</td>\n",
       "      <td>0.855326</td>\n",
       "      <td>psg</td>\n",
       "      <td>original</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>langfuse_export/2026/02.03_gpt5-mpu/exported_v...</td>\n",
       "      <td>gpt5</td>\n",
       "      <td>0.7773381471633911</td>\n",
       "      <td>0.871617</td>\n",
       "      <td>0.821782</td>\n",
       "      <td>0.861172</td>\n",
       "      <td>psg</td>\n",
       "      <td>original</td>\n",
       "      <td>psg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Candidate_ID Model  \\\n",
       "0  langfuse_export/2026/02.03_gpt5-mpu/exported_v...  gpt5   \n",
       "1  langfuse_export/2026/02.03_gpt5-mpu/exported_v...  gpt5   \n",
       "2  langfuse_export/2026/02.03_gpt5-mpu/exported_v...  gpt5   \n",
       "3  langfuse_export/2026/02.03_gpt5-mpu/exported_v...  gpt5   \n",
       "4  langfuse_export/2026/02.03_gpt5-mpu/exported_v...  gpt5   \n",
       "\n",
       "            Precision    Recall        F1        F3 Processor prompt_level  \\\n",
       "0  0.7922946810722351  0.867710  0.828290  0.859529       psg     original   \n",
       "1  0.7844812870025635  0.873618  0.826654  0.863803       psg     original   \n",
       "2  0.7849806547164917  0.876138  0.828058  0.866081       psg     original   \n",
       "3  0.7809998989105225  0.864467  0.820617  0.855326       psg     original   \n",
       "4  0.7773381471633911  0.871617  0.821782  0.861172       psg     original   \n",
       "\n",
       "   run  \n",
       "0  psg  \n",
       "1  psg  \n",
       "2  psg  \n",
       "3  psg  \n",
       "4  psg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Candidate_ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>F3</th>\n",
       "      <th>Processor</th>\n",
       "      <th>prompt_level</th>\n",
       "      <th>run</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>langfuse_export/2026/02.23_abla-2p-phi4/export...</td>\n",
       "      <td>phi4</td>\n",
       "      <td>abla-2p</td>\n",
       "      <td>0.912027</td>\n",
       "      <td>0.848934</td>\n",
       "      <td>0.879350</td>\n",
       "      <td>0.8548478484153748</td>\n",
       "      <td>abla-2p</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>langfuse_export/2026/02.23_abla-2p-phi4/export...</td>\n",
       "      <td>phi4</td>\n",
       "      <td>abla-2p</td>\n",
       "      <td>0.902719</td>\n",
       "      <td>0.850820</td>\n",
       "      <td>0.876002</td>\n",
       "      <td>0.8557400703430176</td>\n",
       "      <td>abla-2p</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>langfuse_export/2026/02.23_abla-2p-phi4/export...</td>\n",
       "      <td>phi4</td>\n",
       "      <td>abla-2p</td>\n",
       "      <td>0.902719</td>\n",
       "      <td>0.850820</td>\n",
       "      <td>0.876002</td>\n",
       "      <td>0.8557400703430176</td>\n",
       "      <td>abla-2p</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>634</th>\n",
       "      <td>langfuse_export/2026/02.23_abla-2p-phi4/export...</td>\n",
       "      <td>phi4</td>\n",
       "      <td>abla-2p</td>\n",
       "      <td>0.911781</td>\n",
       "      <td>0.846815</td>\n",
       "      <td>0.878098</td>\n",
       "      <td>0.8528918623924255</td>\n",
       "      <td>abla-2p</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>langfuse_export/2026/02.23_abla-2p-phi4/export...</td>\n",
       "      <td>phi4</td>\n",
       "      <td>abla-2p</td>\n",
       "      <td>0.911781</td>\n",
       "      <td>0.846815</td>\n",
       "      <td>0.878098</td>\n",
       "      <td>0.8528918623924255</td>\n",
       "      <td>abla-2p</td>\n",
       "      <td>tpusg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Candidate_ID Model Precision  \\\n",
       "631  langfuse_export/2026/02.23_abla-2p-phi4/export...  phi4   abla-2p   \n",
       "632  langfuse_export/2026/02.23_abla-2p-phi4/export...  phi4   abla-2p   \n",
       "633  langfuse_export/2026/02.23_abla-2p-phi4/export...  phi4   abla-2p   \n",
       "634  langfuse_export/2026/02.23_abla-2p-phi4/export...  phi4   abla-2p   \n",
       "635  langfuse_export/2026/02.23_abla-2p-phi4/export...  phi4   abla-2p   \n",
       "\n",
       "       Recall        F1        F3           Processor prompt_level    run  \n",
       "631  0.912027  0.848934  0.879350  0.8548478484153748      abla-2p  tpusg  \n",
       "632  0.902719  0.850820  0.876002  0.8557400703430176      abla-2p  tpusg  \n",
       "633  0.902719  0.850820  0.876002  0.8557400703430176      abla-2p  tpusg  \n",
       "634  0.911781  0.846815  0.878098  0.8528918623924255      abla-2p  tpusg  \n",
       "635  0.911781  0.846815  0.878098  0.8528918623924255      abla-2p  tpusg  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts by run:\n",
      "run\n",
      "psg      276\n",
      "sg        54\n",
      "tpusg    306\n",
      "Name: Candidate_ID, dtype: int64\n",
      "F1 stats:\n",
      "count    636.000000\n",
      "mean       0.850260\n",
      "std        0.020654\n",
      "min        0.615672\n",
      "25%        0.837927\n",
      "50%        0.848072\n",
      "75%        0.864797\n",
      "max        0.889104\n",
      "Name: F1, dtype: float64\n",
      "Coverage (rows vs discovered candidates):\n",
      "psg: 276 rows vs 276 candidates\n",
      "sg: 54 rows vs 54 candidates\n",
      "tpusg: 306 rows vs 306 candidates\n",
      "Total candidates: 636; total rows: 636\n"
     ]
    }
   ],
   "source": [
    "# 6. Quick Summary and Verification\n",
    "if not combined.empty:\n",
    "    display(combined.head())\n",
    "    display(combined.tail())\n",
    "    print(\"Counts by run:\")\n",
    "    print(combined.groupby(\"run\")[\"Candidate_ID\"].count())\n",
    "    print(\"F1 stats:\")\n",
    "    print(combined[\"F1\"].describe())\n",
    "    discovered = {k: len(v) for k, v in candidate_sets.items()}\n",
    "    print(\"Coverage (rows vs discovered candidates):\")\n",
    "    for key in CONFIGS:\n",
    "        rows = len(combined[combined[\"run\"] == key])\n",
    "        print(f\"{key}: {rows} rows vs {discovered.get(key, 0)} candidates\")\n",
    "    print(f\"Total candidates: {sum(discovered.values())}; total rows: {len(combined)}\")\n",
    "else:\n",
    "    print(\"No data to summarize yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6665065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "psg: cleaned 276 rows -> /home/han/Projects/benchmark-tinyml_llm-2026/codebertscore-similarity/analyser-results/similarity_results_psg.csv\n",
      "sg: cleaned 54 rows -> /home/han/Projects/benchmark-tinyml_llm-2026/codebertscore-similarity/analyser-results/similarity_results_sg.csv\n",
      "tpusg: cleaned 306 rows -> /home/han/Projects/benchmark-tinyml_llm-2026/codebertscore-similarity/analyser-results/similarity_results_tpusg.csv\n",
      "Backfill complete.\n"
     ]
    }
   ],
   "source": [
    "# 7. Backfill prompt_level and normalize Model for existing CSV outputs\n",
    "\n",
    "def parse_from_candidate_id(candidate_id: str):\n",
    "    \"\"\"Derive base model and prompt level from the Candidate_ID path token.\"\"\"\n",
    "    text = str(candidate_id)\n",
    "    run_segment = next(\n",
    "        (seg for seg in Path(text).parts if \"abla\" in seg or \"original\" in seg or \"og\" in seg),\n",
    "        text,\n",
    "    )\n",
    "    return parse_model_fields(run_segment)\n",
    "\n",
    "def backfill_outputs():\n",
    "    for c in [\"psg\", \"sg\", \"tpusg\"]:\n",
    "        csv_path = CONFIGS[c][\"output_csv\"]\n",
    "        if not csv_path.exists():\n",
    "            print(f\"{c}: {csv_path} missing, skipping\")\n",
    "            continue\n",
    "        df = pd.read_csv(csv_path)\n",
    "        if df.empty:\n",
    "            print(f\"{c}: {csv_path} empty, skipping\")\n",
    "            continue\n",
    "        if \"Candidate_ID\" not in df.columns:\n",
    "            print(f\"{c}: Candidate_ID missing in {csv_path}, skipping backfill\")\n",
    "            continue\n",
    "        base_prompt = df[\"Candidate_ID\"].apply(parse_from_candidate_id).apply(pd.Series)\n",
    "        df[\"Model\"] = base_prompt[0]\n",
    "        df[\"prompt_level\"] = base_prompt[1]\n",
    "        if \"Candidate_Path\" in df.columns:\n",
    "            df = df.drop(columns=[\"Candidate_Path\"])\n",
    "        ordered_cols = [\n",
    "            \"Candidate_ID\",\n",
    "            \"Model\",\n",
    "            \"Precision\",\n",
    "            \"Recall\",\n",
    "            \"F1\",\n",
    "            \"F3\",\n",
    "            \"Processor\",\n",
    "            \"prompt_level\",\n",
    "        ]\n",
    "        df = df[[col for col in ordered_cols if col in df.columns]]\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"{c}: cleaned {len(df)} rows -> {csv_path}\")\n",
    "    print(\"Backfill complete.\")\n",
    "\n",
    "backfill_outputs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdb3a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
